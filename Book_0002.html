<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Tables Viewer v2.1</title>
<link rel="stylesheet" href="assets/style.css">
</head>
<body>

<nav id="tocSidebar">
<h2 style="display:flex; justify-content:space-between; align-items:center;">
  Table of Contents
  <button id="tocToggle" onclick="toggleTOC()">Hide <span id="tocArrow">▼</span></button>
</h2>
<ul id="toc">
<li><a href="#Table1">Table 1</a></li>
<li><a href="#Table2">Table 2</a></li>
<li><a href="#Table3">Table 3</a></li>
<li><a href="#Table4">Table 4</a></li>
<li><a href="#Table5">Table 5</a></li>
<li><a href="#Table6">Table 6</a></li>
<li><a href="#Table7">Table 7</a></li>
<li><a href="#Table8">Table 8</a></li>
<li><a href="#Table9">Table 9</a></li>
</ul><button id='modeBtn' onclick='toggleMode()'>Dark mode</button></nav><div id='mainContent'>
<div id="stickyMainHeader">
  <h1 style="margin:0;">Tables Viewer v2.1</h1>
  <div style="margin:10px 0; display:flex; align-items:center;">
    <input type="text" id="searchBox" onkeyup="searchTable()" placeholder="Search for keywords...">
    <button onclick="document.getElementById('searchBox').value=''; searchTable();">Reset</button>
  </div>
  <div style="margin-bottom:10px;">
    <button onclick="copyAllTablesPlain()">Copy All Tables (Plain Text)</button>
    <button onclick="copyAllTablesMarkdown()">Copy All Tables (Markdown)</button>
    <button onclick="resetAllTables()">Reset All Tables</button>
  </div>
</div>

<div class="table-wrapper">
  <h3 id="Table1" style="margin-top:30px; margin-bottom:10px;">Table 1</h3>
  <div class="table-container">
    <div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;">
      <div class="copy-buttons">
        <button onclick="copyTablePlain(this)">Copy Plain Table</button>
        <button onclick="copyTableMarkdown(this)">Copy Markdown Table</button>
      </div>
    </div>
    <table>
      <thead><tr>

        <th style="width:28.57%;" onclick="sortTableByColumn(0,0)" role="button" aria-label="Sort by Core Concept / Summary">
          <div class="th-with-sort">
            <div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Core Concept / Summary</div>
            <button class="sort-btn sort-state-0" onclick="event.stopPropagation(); headerSortButtonClicked(0,0, this)" title="Toggle sort" aria-label="Toggle sort">
              <span class="sort-icon" aria-hidden="true">
                <svg viewBox="0 0 24 24" width="12" height="12" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                  <path d="M7 14l5-5 5 5"></path>
                  <path d="M7 10l5 5 5-5"></path>
                </svg>
              </span>
            </button>
          </div>
        </th>
        <th style="width:71.43%;" onclick="sortTableByColumn(0,1)" role="button" aria-label="Sort by Supporting Details / Examples">
          <div class="th-with-sort">
            <div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Supporting Details / Examples</div>
            <button class="sort-btn sort-state-0" onclick="event.stopPropagation(); headerSortButtonClicked(0,1, this)" title="Toggle sort" aria-label="Toggle sort">
              <span class="sort-icon" aria-hidden="true">
                <svg viewBox="0 0 24 24" width="12" height="12" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                  <path d="M7 14l5-5 5 5"></path>
                  <path d="M7 10l5 5 5-5"></path>
                </svg>
              </span>
            </button>
          </div>
        </th></tr></thead><tbody><tr><td data-label="Core Concept / Summary"><b>Introduction to Prompting<b>         </td><td data-label="Supporting Details / Examples">Prompting is the process of instructing AI models to produce specific outputs. Effective prompts provide clear guidance, reduce ambiguity, and improve the quality of generated content. Poor prompts often result in irrelevant, vague, or inconsistent outputs. Prompt engineering is a critical skill, applicable across text, code, and image generation.                                                                                         </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Principle 1: Give Direction<b>       </td><td data-label="Supporting Details / Examples">AI works best when it has explicit instructions. A vague prompt such as "Write about AI" produces broad or generic responses. Direction includes specifying audience, purpose, tone, and desired role. Example: "Write a 300-word blog post explaining AI ethics to beginners, using simple examples and a friendly tone." Including roles (“You are a software engineer”) aligns the AI’s perspective with the task.                                 </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Principle 2: Specify Format<b>       </td><td data-label="Supporting Details / Examples">Defining the desired output format reduces ambiguity and post-processing. Formats include bullet points, lists, tables, essays, or structured JSON. Example: Instead of "Summarize this article," specify "Summarize this article in 5 bullet points highlighting key findings." Structured outputs ensure predictability and easier downstream usage.                                                                                                </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Principle 3: Provide Examples<b>     </td><td data-label="Supporting Details / Examples">Few-shot or one-shot examples anchor the AI to the desired style, tone, or content. Example: When generating email templates, providing 2–3 completed emails as examples improves relevance and consistency. Well-chosen examples reduce hallucinations and enhance output quality. Examples can also illustrate structure, length, and style expectations.                                                                                           </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Principle 4: Evaluate Quality<b>     </td><td data-label="Supporting Details / Examples">Iterative evaluation is essential. Outputs should be checked for correctness, relevance, style, and completeness. Example: If a marketing copy misses a call-to-action, revise the prompt to emphasize it. Automated evaluation can include keyword checks, scoring metrics, or secondary models to rate output quality. Continuous evaluation informs prompt refinement.                                                                             </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Principle 5: Divide Labor<b>         </td><td data-label="Supporting Details / Examples">Large or complex tasks should be split into smaller subtasks. Instead of "Write a full business report," divide into research, outline, drafting sections, and final compilation. Multi-step prompting improves quality control, error handling, and alignment with goals. Example (image generation): first generate scene composition, then character details, and finally color styling.                                                           </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Naive vs Engineered Prompts<b>       </td><td data-label="Supporting Details / Examples">Naive prompts are short, under-specified, and often ambiguous. Engineered prompts leverage all Five Principles: direction, format, examples, evaluation, and divided labor. Example comparison: "Write a story about AI" (naive) vs. "Write a 500-word sci-fi story about an AI that gains consciousness, structured in three acts, with dialogue examples and moral reflection" (engineered). Engineered prompts yield richer, more precise outputs. </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Token Management and Cost<b>         </td><td data-label="Supporting Details / Examples">Longer prompts and examples consume more tokens, impacting cost. Strategic balance is key. Multi-step prompts may cost more upfront but reduce overall token usage by minimizing iterations. Example: detailed multi-part prompts for research summaries reduce total tokens needed compared to repeated vague prompts.                                                                                                                               </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Practical Applications<b>            </td><td data-label="Supporting Details / Examples">The Five Principles apply across text, code, and image generation. Text examples: summarization, blog writing, research extraction. Code examples: generating Python scripts, SQL queries, or debugging instructions. Image examples: clear scene description, style specification, reference images, iterative refinement. These principles consistently improve output quality.                                                                     </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Midjourney Example<b>                </td><td data-label="Supporting Details / Examples">Effective image prompting follows the same principles. Naive prompt: "Draw a castle." Engineered prompt: "Draw a medieval castle at sunset, high-resolution fantasy style, using reference images A and B, first sketch composition, then color refinement." Community prompt libraries demonstrate higher-quality results with structured prompts.                                                                                                   </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Iterative Prompt Refinement<b>       </td><td data-label="Supporting Details / Examples">First outputs rarely meet expectations. Refinement involves analyzing AI responses, identifying missing elements or ambiguity, and updating prompts accordingly. Iterative cycles, with evaluation checkpoints and adjusted examples, progressively align outputs with goals.                                                                                                                                                                         </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Checklist for Effective Prompting<b> </td><td data-label="Supporting Details / Examples">- ✅ Give clear direction (purpose, audience, role)  <br> <br>- ✅ Specify format (list, table, JSON, essay)  <br> <br>- ✅ Provide examples (few-shot or single-shot)  <br> <br>- ✅ Evaluate quality (correctness, relevance, style)  <br> <br>- ✅ Divide labor (multi-step, modular tasks)  <br> <br>- ✅ Balance token usage (long enough for clarity, concise enough for efficiency)  <br> <br>- ✅ Iterate and refine prompts regularly                                       </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Common Pitfalls<b>                   </td><td data-label="Supporting Details / Examples">- Overly vague prompts → unpredictable outputs.  <br> <br>- Overly long prompts → confusion or wasted tokens.  <br> <br>- Lack of examples → inconsistent tone/style.  <br> <br>- Ignoring evaluation → repeated errors.  <br> <br>- Single-step complex tasks → incomplete or inaccurate outputs.                                                                                                                                                                    </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Key Takeaways / Summary<b>           </td><td data-label="Supporting Details / Examples">The Five Principles—Direction, Format, Examples, Evaluation, Divide Labor—maximize AI output quality. Applying them to text, code, and images yields predictable, high-quality results. Iteration and refinement are integral. Small changes in wording, structure, or examples can drastically alter outcomes. Mastery of prompt engineering increases efficiency, reduces costs, and expands creative possibilities.                                </td></tr>
</tbody></table></div></div>

<div class="table-wrapper">
  <h3 id="Table2" style="margin-top:30px; margin-bottom:10px;">Table 2</h3>
  <div class="table-container">
    <div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;">
      <div class="copy-buttons">
        <button onclick="copyTablePlain(this)">Copy Plain Table</button>
        <button onclick="copyTableMarkdown(this)">Copy Markdown Table</button>
      </div>
    </div>
    <table>
      <thead><tr>

        <th style="width:28.57%;" onclick="sortTableByColumn(1,0)" role="button" aria-label="Sort by Core Concept / Summary">
          <div class="th-with-sort">
            <div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Core Concept / Summary</div>
            <button class="sort-btn sort-state-0" onclick="event.stopPropagation(); headerSortButtonClicked(1,0, this)" title="Toggle sort" aria-label="Toggle sort">
              <span class="sort-icon" aria-hidden="true">
                <svg viewBox="0 0 24 24" width="12" height="12" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                  <path d="M7 14l5-5 5 5"></path>
                  <path d="M7 10l5 5 5-5"></path>
                </svg>
              </span>
            </button>
          </div>
        </th>
        <th style="width:71.43%;" onclick="sortTableByColumn(1,1)" role="button" aria-label="Sort by Supporting Details / Examples">
          <div class="th-with-sort">
            <div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Supporting Details / Examples</div>
            <button class="sort-btn sort-state-0" onclick="event.stopPropagation(); headerSortButtonClicked(1,1, this)" title="Toggle sort" aria-label="Toggle sort">
              <span class="sort-icon" aria-hidden="true">
                <svg viewBox="0 0 24 24" width="12" height="12" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                  <path d="M7 14l5-5 5 5"></path>
                  <path d="M7 10l5 5 5-5"></path>
                </svg>
              </span>
            </button>
          </div>
        </th></tr></thead><tbody><tr><td data-label="Core Concept / Summary"><b>Introduction to Large Language Models (LLMs)<b> </td><td data-label="Supporting Details / Examples">Large Language Models are AI models trained on massive text corpora to understand and generate human-like text. They learn patterns, syntax, semantics, and context, enabling them to generate coherent sentences, paragraphs, and documents. LLMs underpin applications in chatbots, content generation, translation, summarization, and code assistance.                                                                                                         </td></tr>
<tr><td data-label="Core Concept / Summary"><b>How LLMs Work<b>                                </td><td data-label="Supporting Details / Examples">LLMs use neural networks—often transformer architectures—that process sequences of tokens. Each token is encoded into a vector representation. The model predicts the next token based on context and prior tokens. Attention mechanisms allow the model to weigh the importance of different words in context. Training involves adjusting billions of parameters through exposure to large datasets, optimizing the likelihood of correct next-token prediction. </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Tokenization and Embeddings<b>                  </td><td data-label="Supporting Details / Examples">Text is broken into tokens (words, subwords, or characters). Tokenizers like BPE (Byte-Pair Encoding) or WordPiece split text efficiently. Each token is embedded into a high-dimensional vector space, capturing semantic relationships. Example: "king" and "queen" embeddings are closer than "king" and "car," reflecting semantic similarity. Embeddings serve as inputs to transformer layers.                                                               </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Transformer Architecture<b>                     </td><td data-label="Supporting Details / Examples">Transformers are composed of encoder and decoder layers (or just decoder for generative LLMs). Each layer includes multi-head self-attention and feedforward neural networks. Self-attention computes relationships between all tokens in a sequence. Positional encodings preserve token order. Transformers allow parallel processing, making LLMs scalable to billions of parameters.                                                                           </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Pretraining and Fine-tuning<b>                  </td><td data-label="Supporting Details / Examples">LLMs are pretrained on massive text corpora to learn general language patterns. Pretraining is usually unsupervised or self-supervised (predict masked tokens or next token). Fine-tuning adapts the model to specific tasks using smaller, labeled datasets. Example: GPT models are pretrained generatively and can be fine-tuned for summarization, question answering, or code generation.                                                                     </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Zero-shot, One-shot, Few-shot Learning<b>       </td><td data-label="Supporting Details / Examples">LLMs can perform tasks without explicit retraining using prompt-based instruction:  <br> <br>- <b>Zero-shot:<b> task described in prompt, no examples.  <br> <br>- <b>One-shot:<b> one example provided.  <br> <br>- <b>Few-shot:<b> several examples provided.  <br> Example: For sentiment analysis, providing a few labeled examples in the prompt allows the model to classify unseen sentences correctly.                                                                       </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Capabilities of LLMs<b>                         </td><td data-label="Supporting Details / Examples">LLMs excel at natural language understanding and generation: text summarization, translation, Q\&A, dialogue, storytelling, and code writing. They can generate contextually relevant and coherent outputs over long sequences. Some LLMs can reason, follow instructions, or apply logical patterns in text. Limitations include hallucinations, sensitivity to prompt phrasing, and occasional factual errors.                                                   </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Limitations and Challenges<b>                   </td><td data-label="Supporting Details / Examples">- <b>Hallucinations:<b> generating plausible but incorrect information.  <br> <br>- <b>Context Window:<b> limited number of tokens the model can attend to.  <br> <br>- <b>Bias:<b> models reflect biases in training data.  <br> <br>- <b>Compute Costs:<b> large models require significant memory and processing power.  <br> Example: GPT-3 can produce convincing but false answers to factual questions if the prompt is ambiguous.                                              </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Applications in Text Generation<b>              </td><td data-label="Supporting Details / Examples">- <b>Chatbots & virtual assistants:<b> human-like conversation.  <br> <br>- <b>Content creation:<b> blogs, stories, emails, social media posts.  <br> <br>- <b>Summarization:<b> condensing articles or documents.  <br> <br>- <b>Translation:<b> multilingual text conversion.  <br> <br>- <b>Code generation:<b> generating Python, SQL, or other programming languages.  <br> Example: An LLM can take a technical article and produce a plain-language summary for general readers.      </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Prompt Engineering for LLMs<b>                  </td><td data-label="Supporting Details / Examples">Effective prompting is crucial for LLM performance. Prompts can specify task, format, style, or examples. Multi-step prompts can improve complex output. Example: “Summarize this article in 5 bullet points, highlight key statistics, and include a conclusion in one sentence.” Proper prompts reduce hallucinations and improve relevance.                                                                                                                     </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Evaluation of Generated Text<b>                 </td><td data-label="Supporting Details / Examples">Outputs must be evaluated for:  <br> <br>- <b>Accuracy:<b> factual correctness.  <br> <br>- <b>Coherence:<b> logical flow.  <br> <br>- <b>Relevance:<b> aligned with task or prompt.  <br> <br>- <b>Style/Format:<b> matches requested tone and format.  <br> Example: Summarized text should retain all main points, statistics, and logical structure from the source document.                                                                                                          </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Future Trends<b>                                </td><td data-label="Supporting Details / Examples">LLMs continue evolving:  <br> <br>- <b>Larger models:<b> more parameters, higher accuracy.  <br> <br>- <b>Multimodal capabilities:<b> integrating text, image, and audio.  <br> <br>- <b>Better alignment:<b> reducing hallucinations, improving safety and bias mitigation.  <br> <br>- <b>Efficient architectures:<b> reducing compute costs while maintaining performance.  <br> Example: Future LLMs could generate technical reports with diagrams and tables directly from raw data. </td></tr>
</tbody></table></div></div>

<div class="table-wrapper">
  <h3 id="Table3" style="margin-top:30px; margin-bottom:10px;">Table 3</h3>
  <div class="table-container">
    <div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;">
      <div class="copy-buttons">
        <button onclick="copyTablePlain(this)">Copy Plain Table</button>
        <button onclick="copyTableMarkdown(this)">Copy Markdown Table</button>
      </div>
    </div>
    <table>
      <thead><tr>

        <th style="width:28.57%;" onclick="sortTableByColumn(2,0)" role="button" aria-label="Sort by Core Concept / Summary">
          <div class="th-with-sort">
            <div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Core Concept / Summary</div>
            <button class="sort-btn sort-state-0" onclick="event.stopPropagation(); headerSortButtonClicked(2,0, this)" title="Toggle sort" aria-label="Toggle sort">
              <span class="sort-icon" aria-hidden="true">
                <svg viewBox="0 0 24 24" width="12" height="12" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                  <path d="M7 14l5-5 5 5"></path>
                  <path d="M7 10l5 5 5-5"></path>
                </svg>
              </span>
            </button>
          </div>
        </th>
        <th style="width:71.43%;" onclick="sortTableByColumn(2,1)" role="button" aria-label="Sort by Supporting Details / Examples">
          <div class="th-with-sort">
            <div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Supporting Details / Examples</div>
            <button class="sort-btn sort-state-0" onclick="event.stopPropagation(); headerSortButtonClicked(2,1, this)" title="Toggle sort" aria-label="Toggle sort">
              <span class="sort-icon" aria-hidden="true">
                <svg viewBox="0 0 24 24" width="12" height="12" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                  <path d="M7 14l5-5 5 5"></path>
                  <path d="M7 10l5 5 5-5"></path>
                </svg>
              </span>
            </button>
          </div>
        </th></tr></thead><tbody><tr><td data-label="Core Concept / Summary"><b>Introduction to Standard Practices<b>     </td><td data-label="Supporting Details / Examples">Text generation with ChatGPT benefits from structured methods that ensure consistent, high-quality outputs. Standard practices help reduce hallucinations, improve relevance, and make outputs easier to evaluate. Proper practices combine prompt engineering, iterative evaluation, and multi-step workflows.                                                                                                                                   </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Effective Prompting<b>                    </td><td data-label="Supporting Details / Examples">Prompts should be explicit, specifying:  <br> <br>- Purpose or task  <br> <br>- Output format (essay, list, table, JSON)  <br> <br>- Tone and style  <br> <br>- Role or persona for the AI  <br> <b>Example:<b> “You are a historian. Summarize the causes of World War I in 5 bullet points with clear dates and key figures.” Clear prompting ensures outputs match user intent.                                                                                  </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Providing Examples (Few-shot Learning)<b> </td><td data-label="Supporting Details / Examples">Supplying examples helps the model understand expectations:  <br> <br>- <b>Zero-shot:<b> task described, no examples.  <br> <br>- <b>One-shot:<b> one example provided.  <br> <br>- <b>Few-shot:<b> multiple examples provided.  <br> <b>Example:<b> Provide two email drafts as examples when asking ChatGPT to write a new email to ensure tone and style consistency.                                                                                              </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Defining Output Format<b>                 </td><td data-label="Supporting Details / Examples">Always specify the expected format to reduce ambiguity and rework:  <br> <br>- Lists, tables, bullet points, essays, JSON, or code blocks.  <br> <b>Example:<b> “Generate a table comparing features of Python, Java, and C++ with columns: Language, Typing, Paradigm, and Use Cases.” Proper format guidance saves time in post-processing.                                                                                                           </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Iterative Refinement<b>                   </td><td data-label="Supporting Details / Examples">Complex outputs often require multiple iterations.  <br> <b>Steps:<b>  <br> <br>1. Generate initial output.  <br> <br>2. Review for accuracy, completeness, and style.  <br> <br>3. Revise the prompt or provide additional instructions.  <br> <br>4. Repeat until desired quality is achieved.  <br> <b>Example:<b> Summarizing a long report may require prompting ChatGPT to break it into sections before combining them.                                        </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Step-by-Step Decomposition<b>             </td><td data-label="Supporting Details / Examples">Break complex tasks into manageable sub-tasks:  <br> <br>- Research → Outline → Draft → Review → Finalize.  <br> <br>- For code generation: plan → write functions → integrate → test.  <br> <b>Example:<b> Generating a business plan: first produce market analysis, then financial plan, finally combined report. Dividing labor reduces errors and improves output consistency.                                                                         </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Evaluation of Output<b>                   </td><td data-label="Supporting Details / Examples">Evaluate for:  <br> <br>- Accuracy and factual correctness  <br> <br>- Coherence and logical flow  <br> <br>- Style and tone  <br> <br>- Completeness relative to task instructions  <br> <b>Example:<b> Compare ChatGPT-generated summaries with source text to check for missing key points or misinterpretation.                                                                                                                                                 </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Handling Hallucinations<b>                </td><td data-label="Supporting Details / Examples">ChatGPT may produce plausible but incorrect information. Strategies to reduce hallucinations:  <br> <br>- Specify verifiable facts in prompt.  <br> <br>- Ask for citations or references.  <br> <br>- Use step-by-step reasoning prompts.  <br> <b>Example:<b> “List three confirmed achievements of Ada Lovelace with historical references.”                                                                                                                 </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Token and Context Management<b>           </td><td data-label="Supporting Details / Examples">Large tasks may exceed context window. Practices:  <br> <br>- Chunk text into smaller segments.  <br> <br>- Maintain sequence order for context.  <br> <br>- Summarize intermediate outputs to retain essential context.  <br> <b>Example:<b> Splitting a 10,000-word article into 2,000-word segments for stepwise summarization.                                                                                                                              </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Best Practices Checklist<b>               </td><td data-label="Supporting Details / Examples">- ✅ Give explicit direction and task definition  <br> <br>- ✅ Provide examples when appropriate (few-shot)  <br> <br>- ✅ Specify output format clearly  <br> <br>- ✅ Break complex tasks into subtasks  <br> <br>- ✅ Iteratively refine outputs  <br> <br>- ✅ Evaluate for accuracy, coherence, and completeness  <br> <br>- ✅ Manage token limits and context window  <br> <br>- ✅ Minimize hallucinations using references and stepwise reasoning                           </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Applications of Standard Practices<b>     </td><td data-label="Supporting Details / Examples">- Content creation: articles, blogs, marketing copy.  <br> <br>- Summarization: reports, research papers, meeting notes.  <br> <br>- Code generation: scripts, queries, debugging assistance.  <br> <br>- Educational tools: exercises, explanations, tutorials.  <br> <b>Example:<b> A prompt guiding ChatGPT to create a lesson plan can specify objectives, duration, activities, and learning outcomes for clear, actionable outputs.                       </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Common Pitfalls to Avoid<b>               </td><td data-label="Supporting Details / Examples">- Vague or under-specified prompts → unpredictable outputs.  <br> <br>- Ignoring output format → additional editing needed.  <br> <br>- Skipping iterative refinement → errors propagate.  <br> <br>- Attempting complex tasks in a single step → reduced accuracy.  <br> <br>- Not providing examples when style or tone matters → inconsistency.                                                                                                                </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Key Takeaways / Summary<b>                </td><td data-label="Supporting Details / Examples">Applying standard practices ensures high-quality, reliable, and contextually accurate text generation with ChatGPT. Explicit prompts, examples, output format, stepwise decomposition, iterative refinement, and careful evaluation form the foundation. Following these practices reduces errors, improves relevance, and enables the model to produce outputs that are ready for practical use across text, code, and educational applications. </td></tr>
</tbody></table></div></div>

<div class="table-wrapper">
  <h3 id="Table4" style="margin-top:30px; margin-bottom:10px;">Table 4</h3>
  <div class="table-container">
    <div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;">
      <div class="copy-buttons">
        <button onclick="copyTablePlain(this)">Copy Plain Table</button>
        <button onclick="copyTableMarkdown(this)">Copy Markdown Table</button>
      </div>
    </div>
    <table>
      <thead><tr>

        <th style="width:28.57%;" onclick="sortTableByColumn(3,0)" role="button" aria-label="Sort by Core Concept / Summary">
          <div class="th-with-sort">
            <div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Core Concept / Summary</div>
            <button class="sort-btn sort-state-0" onclick="event.stopPropagation(); headerSortButtonClicked(3,0, this)" title="Toggle sort" aria-label="Toggle sort">
              <span class="sort-icon" aria-hidden="true">
                <svg viewBox="0 0 24 24" width="12" height="12" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                  <path d="M7 14l5-5 5 5"></path>
                  <path d="M7 10l5 5 5-5"></path>
                </svg>
              </span>
            </button>
          </div>
        </th>
        <th style="width:71.43%;" onclick="sortTableByColumn(3,1)" role="button" aria-label="Sort by Supporting Details / Examples">
          <div class="th-with-sort">
            <div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Supporting Details / Examples</div>
            <button class="sort-btn sort-state-0" onclick="event.stopPropagation(); headerSortButtonClicked(3,1, this)" title="Toggle sort" aria-label="Toggle sort">
              <span class="sort-icon" aria-hidden="true">
                <svg viewBox="0 0 24 24" width="12" height="12" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                  <path d="M7 14l5-5 5 5"></path>
                  <path d="M7 10l5 5 5-5"></path>
                </svg>
              </span>
            </button>
          </div>
        </th></tr></thead><tbody><tr><td data-label="Core Concept / Summary"><b>Introduction to LangChain<b>                     </td><td data-label="Supporting Details / Examples">LangChain is a framework for developing applications powered by LLMs. It enables structured prompt management, chaining multiple LLM calls, and integrating external data sources. LangChain simplifies building complex workflows such as question answering, summarization, and multi-step reasoning.                                                                                                                                                        </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Chains<b>                                        </td><td data-label="Supporting Details / Examples">Chains link multiple components or steps together to perform complex tasks. Types include:  <br> <br>- <b>SequentialChain:<b> executes multiple LLM calls in order.  <br> <br>- <b>SimpleSequentialChain:<b> basic stepwise chain.  <br> <br>- <b>LLMChain:<b> combines an LLM with a prompt template for structured execution.  <br> <b>Example:<b> Summarizing a document, then translating the summary into another language using a sequential chain of LLM calls.             </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Prompt Templates<b>                              </td><td data-label="Supporting Details / Examples">Templates define structured prompts with placeholders for dynamic input. Key for consistency and reusability.  <br> <b>Example:<b> “Summarize the following text for a {audience} in {language}.” Replacing `{audience}` and `{language}` allows flexible generation without rewriting prompts.                                                                                                                                                                  </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Memory<b>                                        </td><td data-label="Supporting Details / Examples">Memory allows context persistence across multiple interactions or steps. Types include:  <br> <br>- <b>ConversationBufferMemory:<b> retains entire chat history.  <br> <br>- <b>ConversationSummaryMemory:<b> stores summarized history for efficiency.  <br> <br>- <b>Custom Memory:<b> user-defined storage for specific context.  <br> <b>Example:<b> A chatbot that remembers previous user queries and adjusts responses accordingly.                                         </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Agents<b>                                        </td><td data-label="Supporting Details / Examples">Agents combine LLMs with tools and reasoning capabilities to perform decision-making tasks.  <br> Types:  <br> <br>- <b>Zero-Shot Agent:<b> chooses actions without prior examples.  <br> <br>- <b>ReAct Agent:<b> reasons step-by-step with tool use.  <br> <br>- <b>Multi-Tool Agent:<b> uses multiple external tools (APIs, calculators, databases).  <br> <b>Example:<b> An agent receives a question, decides to fetch data from a database, then summarizes it using an LLM. </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Tool Integration<b>                              </td><td data-label="Supporting Details / Examples">LangChain allows LLMs to interact with external tools:  <br> <br>- APIs, databases, calculators, search engines.  <br> <br>- Enhances reasoning, factual accuracy, and utility.  <br> <b>Example:<b> Combining an LLM with a stock market API to provide real-time financial advice.                                                                                                                                                                                     </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Retrieval-Augmented Generation (RAG)<b>          </td><td data-label="Supporting Details / Examples">RAG enhances LLM responses by incorporating external knowledge sources:  <br> <br>- Retrieves relevant documents from vector databases.  <br> <br>- Combines retrieved data with LLM prompts for contextually accurate generation.  <br> <b>Example:<b> ChatGPT answers technical questions by first retrieving sections from a product manual.                                                                                                                          </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Callbacks and Logging<b>                         </td><td data-label="Supporting Details / Examples">Callbacks monitor and log chain execution, LLM responses, and intermediate steps. Useful for debugging and performance analysis.  <br> <b>Example:<b> Logging all intermediate summaries generated during multi-step summarization to ensure accuracy and completeness.                                                                                                                                                                                          </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Best Practices for LangChain Workflows<b>        </td><td data-label="Supporting Details / Examples">- ✅ Use prompt templates for consistency.  <br> <br>- ✅ Persist context with appropriate memory type.  <br> <br>- ✅ Chain steps logically; break complex tasks into manageable components.  <br> <br>- ✅ Integrate tools when needed for accurate and enriched outputs.  <br> <br>- ✅ Implement retrieval mechanisms for factual grounding.  <br> <br>- ✅ Log outputs and intermediate results for review and debugging.                                                           </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Error Handling and Robustness<b>                 </td><td data-label="Supporting Details / Examples">- Validate inputs before LLM calls.  <br> <br>- Handle API or tool failures gracefully.  <br> <br>- Include fallback responses when LLM output is inadequate.  <br> <b>Example:<b> If a tool fails to return data, generate a default summary indicating missing information.                                                                                                                                                                                            </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Applications of Advanced LangChain Techniques<b> </td><td data-label="Supporting Details / Examples">- Intelligent chatbots with memory and tool integration.  <br> <br>- Automated research assistants fetching and summarizing documents.  <br> <br>- Multi-step content generation pipelines (e.g., summarize, translate, format).  <br> <br>- Decision-making agents for business, finance, or analytics.  <br> <b>Example:<b> An educational assistant generating stepwise explanations with citations and examples for students.                                            </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Key Takeaways / Summary<b>                       </td><td data-label="Supporting Details / Examples">LangChain extends LLM capabilities through structured chaining, memory, agents, tool integration, and retrieval mechanisms. Applying these advanced techniques allows developers to build robust, multi-step, context-aware applications that combine reasoning, external knowledge, and dynamic workflows. Iterative design, logging, and testing are essential for high-quality outputs.                                                                     </td></tr>
</tbody></table></div></div>

<div class="table-wrapper">
  <h3 id="Table5" style="margin-top:30px; margin-bottom:10px;">Table 5</h3>
  <div class="table-container">
    <div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;">
      <div class="copy-buttons">
        <button onclick="copyTablePlain(this)">Copy Plain Table</button>
        <button onclick="copyTableMarkdown(this)">Copy Markdown Table</button>
      </div>
    </div>
    <table>
      <thead><tr>

        <th style="width:28.57%;" onclick="sortTableByColumn(4,0)" role="button" aria-label="Sort by Core Concept / Summary">
          <div class="th-with-sort">
            <div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Core Concept / Summary</div>
            <button class="sort-btn sort-state-0" onclick="event.stopPropagation(); headerSortButtonClicked(4,0, this)" title="Toggle sort" aria-label="Toggle sort">
              <span class="sort-icon" aria-hidden="true">
                <svg viewBox="0 0 24 24" width="12" height="12" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                  <path d="M7 14l5-5 5 5"></path>
                  <path d="M7 10l5 5 5-5"></path>
                </svg>
              </span>
            </button>
          </div>
        </th>
        <th style="width:71.43%;" onclick="sortTableByColumn(4,1)" role="button" aria-label="Sort by Supporting Details / Examples">
          <div class="th-with-sort">
            <div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Supporting Details / Examples</div>
            <button class="sort-btn sort-state-0" onclick="event.stopPropagation(); headerSortButtonClicked(4,1, this)" title="Toggle sort" aria-label="Toggle sort">
              <span class="sort-icon" aria-hidden="true">
                <svg viewBox="0 0 24 24" width="12" height="12" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                  <path d="M7 14l5-5 5 5"></path>
                  <path d="M7 10l5 5 5-5"></path>
                </svg>
              </span>
            </button>
          </div>
        </th></tr></thead><tbody><tr><td data-label="Core Concept / Summary"><b>Introduction to Vector Databases<b>    </td><td data-label="Supporting Details / Examples">Vector databases store <b>high-dimensional embeddings<b>, allowing similarity search across unstructured data such as text, images, audio, and more. Unlike relational databases, they are optimized for <b>semantic similarity<b>, not exact matches. They are essential for <b>retrieval-augmented generation (RAG)<b> in LLM workflows, enabling LLMs to fetch relevant external knowledge and produce context-aware outputs. Leading solutions include <b>FAISS<b> (open-source, local/GPU) and <b>Pinecone<b> (managed, cloud-based). Using vector databases improves search accuracy, reduces hallucinations, and allows dynamic context retrieval for large-scale data.                                                                                        </td></tr>
<tr><td data-label="Core Concept / Summary"><b>FAISS Overview<b>                      </td><td data-label="Supporting Details / Examples">FAISS (Facebook AI Similarity Search) is an <b>open-source library optimized for fast similarity search<b> of dense vectors. Key features:  <br> <br>- <b>GPU acceleration<b> supports large-scale datasets.  <br> <br>- <b>Multiple indexing structures<b> (Flat, IVF, HNSW, PQ) for balancing speed, accuracy, and memory usage.  <br> <br>- <b>Python and C++ APIs<b> for flexible integration.  <br> <b>Example:<b> Indexing 1 million sentence embeddings for semantic search:  <br> `import faiss`  <br> `index = faiss.IndexFlatL2(d)`  <br> `index.add(embedding<i>matrix)`  <br> FAISS allows efficient retrieval with nearest-neighbor queries, supporting both exact and approximate searches. Its performance scales with GPU support and index optimization.           </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Pinecone Overview<b>                   </td><td data-label="Supporting Details / Examples">Pinecone is a <b>managed vector database platform<b> designed for cloud scalability. Features:  <br> <br>- <b>Automatic scaling<b> for millions of vectors.  <br> <br>- <b>High availability and multi-region replication.<b>  <br> <br>- <b>Built-in similarity search and metadata filtering<b> for contextual queries.  <br> <b>Example:<b> Storing product embeddings in Pinecone to enable semantic search across a global catalog:  <br> `import pinecone`  <br> `pinecone.init(api<i>key="YOUR<i>KEY")`  <br> `index = pinecone.Index("products")`  <br> `index.upsert(vectors=[(id, embedding, metadata)])`  <br> Pinecone abstracts hardware and scaling, allowing developers to focus on <b>LLM integration and retrieval pipelines<b>.                                       </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Embedding Generation<b>                </td><td data-label="Supporting Details / Examples">Embeddings convert raw data into <b>numerical vectors<b> that capture semantic meaning. Key steps:  <br> <br>1. <b>Select embedding model<b> (e.g., OpenAI text-embedding-3-small or text-embedding-3-large).  <br> <br>2. <b>Preprocess data<b> (clean text, remove stopwords if needed).  <br> <br>3. <b>Batch processing<b> for efficiency.  <br> <br>4. <b>Normalize vectors<b> if using cosine similarity.  <br> <b>Example:<b> Generating embeddings for 10,000 customer reviews:  <br> <br>- Batch size = 500 reviews per API call.  <br> <br>- Store resulting embeddings in FAISS or Pinecone.  <br> Embeddings form the foundation for <b>semantic search, clustering, recommendation systems<b>, and RAG pipelines.                                                                    </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Indexing Vectors in FAISS<b>           </td><td data-label="Supporting Details / Examples">To use FAISS effectively:  <br> <br>1. <b>Choose index type<b> based on data size and performance (e.g., `IndexFlatL2` for small datasets, `IndexIVFPQ` for millions of vectors).  <br> <br>2. <b>Train index<b> if required (IVF or PQ indices require training on representative vectors).  <br> <br>3. <b>Add vectors<b> to the index using `index.add(embedding<i>matrix)`.  <br> <br>4. <b>Save and load index<b> for future use (`faiss.write<i>index`, `faiss.read<i>index`).  <br> <b>Example:<b> For 1M embeddings:  <br> `d = 768`  <br> `index = faiss.IndexIVFPQ(faiss.IndexFlatL2(d), d, 100, 8, <br>8)`  <br> `index.train(training<i>matrix)`  <br> `index.add(embedding<i>matrix)`  <br> Proper index selection balances <b>query speed, memory footprint, and retrieval accuracy<b>. </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Querying Vectors<b>                    </td><td data-label="Supporting Details / Examples">Queries require embedding the input first:  <br> <br>- Generate query embedding using the same model as the dataset.  <br> <br>- Perform <b>nearest-neighbor search<b> to retrieve top-k similar items.  <br> <br>- Optionally <b>filter by metadata<b> for context-aware results.  <br> <b>Example:<b> Search for the most similar articles to a user query:  <br> `D, I = index.search(query<i>vector, k=5)`  <br> Returns <b>distances and indices<b> of top matches. Effective querying ensures accurate, relevant, and fast semantic retrieval.                                                                                                                                                                                                                              </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Pinecone Index Setup<b>                </td><td data-label="Supporting Details / Examples">Setting up Pinecone:  <br> <br>1. Create index in dashboard or via API.  <br> <br>2. Configure <b>dimension<b>, <b>metric<b> (cosine, Euclidean, dot-product), and <b>replication factor<b>.  <br> <br>3. Insert vectors with optional <b>metadata<b> for filtering.  <br> <b>Example:<b>  <br> `index.upsert(vectors=[("id1", vector1, {"category":"books"}), ...])`  <br> Pinecone automatically handles <b>scaling, storage, and replication<b>, enabling seamless integration with LangChain for retrieval-augmented generation.                                                                                                                                                                                                                                                </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Integration with LangChain<b>          </td><td data-label="Supporting Details / Examples">LangChain uses vector databases to implement <b>RAG pipelines<b>:  <br> <br>- <b>Step 1:<b> Convert user query to embedding.  <br> <br>- <b>Step 2:<b> Search FAISS or Pinecone for top-k similar documents.  <br> <br>- <b>Step 3:<b> Provide retrieved documents as context to the LLM.  <br> <b>Example:<b> Question-answering agent workflow:  <br> `query<i>embedding = model.embed(query)`  <br> `results = pinecone<i>index.query(query<i>embedding, top<i>k=5)`  <br> `response = llm.generate(prompt<i>with<i>context(results))`  <br> This approach improves factual accuracy and ensures the model references <b>real data<b> rather than hallucinating.                                                                                                                             </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Best Practices for Vector Databases<b> </td><td data-label="Supporting Details / Examples">- ✅ Normalize vectors for cosine similarity.  <br> <br>- ✅ Choose index type according to scale, speed, and accuracy needs.  <br> <br>- ✅ Include metadata for filtering and advanced queries.  <br> <br>- ✅ Batch insertions to optimize performance.  <br> <br>- ✅ Monitor index health and periodically re-index to accommodate data growth.  <br> <br>- ✅ Log queries and responses for evaluation and troubleshooting.                                                                                                                                                                                                                                                                                                                                                     </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Scaling and Performance<b>             </td><td data-label="Supporting Details / Examples">FAISS and Pinecone handle scaling differently:  <br> <br>- <b>FAISS:<b> GPU acceleration, sharding, and optimized indexing for very large datasets.  <br> <br>- <b>Pinecone:<b> Auto-scaling, managed replication, multi-region deployment.  <br> Consider <b>latency vs throughput trade-offs<b>, and select index types and configurations that match your workflow.  <br> <b>Example:<b> Sharding FAISS across 4 GPUs to manage 100M embeddings, ensuring sub-second query time.                                                                                                                                                                                                                                                                                         </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Applications of Vector Databases<b>    </td><td data-label="Supporting Details / Examples">- <b>Semantic search:<b> e.g., finding similar documents or FAQs.  <br> <br>- <b>RAG pipelines:<b> supplying relevant context to LLMs.  <br> <br>- <b>Recommendation systems:<b> content-based or hybrid recommendations.  <br> <br>- <b>Clustering and deduplication:<b> grouping similar items.  <br> <br>- <b>Image/video similarity search:<b> embeddings for multimedia data.  <br> <b>Example:<b> A support chatbot retrieves top-5 relevant answers from a large FAQ database based on semantic similarity rather than exact keyword matching.                                                                                                                                                                                                                                   </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Key Takeaways / Summary<b>             </td><td data-label="Supporting Details / Examples">Vector databases are <b>critical for modern LLM workflows<b>, enabling efficient similarity search and context-aware retrieval. FAISS provides <b>high-performance local indexing<b>, while Pinecone offers <b>scalable cloud-based solutions<b>. Effective embedding generation, indexing, querying, and integration with LangChain ensures <b>robust, accurate, and scalable text generation pipelines<b>. Following best practices guarantees high performance, maintainability, and quality outputs.                                                                                                                                                                                                                                                            </td></tr>
</tbody></table></div></div>

<div class="table-wrapper">
  <h3 id="Table6" style="margin-top:30px; margin-bottom:10px;">Table 6</h3>
  <div class="table-container">
    <div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;">
      <div class="copy-buttons">
        <button onclick="copyTablePlain(this)">Copy Plain Table</button>
        <button onclick="copyTableMarkdown(this)">Copy Markdown Table</button>
      </div>
    </div>
    <table>
      <thead><tr>

        <th style="width:28.57%;" onclick="sortTableByColumn(5,0)" role="button" aria-label="Sort by Core Concept / Summary">
          <div class="th-with-sort">
            <div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Core Concept / Summary</div>
            <button class="sort-btn sort-state-0" onclick="event.stopPropagation(); headerSortButtonClicked(5,0, this)" title="Toggle sort" aria-label="Toggle sort">
              <span class="sort-icon" aria-hidden="true">
                <svg viewBox="0 0 24 24" width="12" height="12" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                  <path d="M7 14l5-5 5 5"></path>
                  <path d="M7 10l5 5 5-5"></path>
                </svg>
              </span>
            </button>
          </div>
        </th>
        <th style="width:71.43%;" onclick="sortTableByColumn(5,1)" role="button" aria-label="Sort by Supporting Details / Examples">
          <div class="th-with-sort">
            <div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Supporting Details / Examples</div>
            <button class="sort-btn sort-state-0" onclick="event.stopPropagation(); headerSortButtonClicked(5,1, this)" title="Toggle sort" aria-label="Toggle sort">
              <span class="sort-icon" aria-hidden="true">
                <svg viewBox="0 0 24 24" width="12" height="12" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                  <path d="M7 14l5-5 5 5"></path>
                  <path d="M7 10l5 5 5-5"></path>
                </svg>
              </span>
            </button>
          </div>
        </th></tr></thead><tbody><tr><td data-label="Core Concept / Summary"><b>Introduction to Autonomous Agents<b> </td><td data-label="Supporting Details / Examples">Autonomous agents are <b>self-directed programs<b> that perceive their environment, reason about actions, and execute tasks with minimal human intervention. In the context of LLMs, they leverage <b>memory and tools<b> to perform complex workflows, maintain context over multiple steps, and achieve goals dynamically. Agents extend the capabilities of LLMs beyond single-turn interactions, allowing them to <b>plan, execute, and revise actions<b> autonomously.                                                                                                                                                                                                                                                                                                                                                    </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Agent Components<b>                  </td><td data-label="Supporting Details / Examples">Key components include:  <br> <br>1. <b>Observation<b> – the agent perceives the environment or receives input data.  <br> <br>2. <b>Memory<b> – stores conversation history, state, and summary context to enable multi-turn reasoning.  <br> <br>3. <b>Decision-Making / Reasoning<b> – the agent chooses an action based on observations and goals, often leveraging LLM reasoning.  <br> <br>4. <b>Actions / Tools<b> – APIs, functions, or external programs the agent can invoke to affect the environment.  <br> <br>5. <b>Reward / Feedback<b> – optional reinforcement signals guide agent performance.  <br> <b>Example:<b> A research assistant agent reads documents (observation), tracks previous queries (memory), chooses to search a database or summarize content (actions/tools), and logs success metrics (reward).                  </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Memory in Agents<b>                  </td><td data-label="Supporting Details / Examples">Memory allows agents to <b>retain and retrieve information<b> over multiple steps:  <br> <br>- <b>Short-term / Buffer memory:<b> keeps recent interactions.  <br> <br>- <b>Summary / Condensed memory:<b> stores key points from past interactions.  <br> <br>- <b>Custom / External memory:<b> connects to databases or vector stores for long-term retrieval.  <br> <b>Example:<b> When a user asks a multi-part question, the agent recalls earlier steps from buffer memory to provide a coherent final answer. Memory is critical for <b>multi-turn reasoning and avoiding repetition<b>.                                                                                                                                                                                                                                                   </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Tools in Agents<b>                   </td><td data-label="Supporting Details / Examples">Tools allow agents to <b>interact with external systems<b> and extend capabilities beyond LLM text generation:  <br> <br>- <b>APIs<b> – e.g., search engines, weather data, or product databases.  <br> <br>- <b>Functions / Scripts<b> – pre-defined Python functions or scripts the agent can call.  <br> <br>- <b>Vector Stores / Embeddings<b> – retrieve semantically relevant context.  <br> <b>Example:<b> Agent decides to call `get<i>weather(location)` when a user asks about local weather conditions. Tools are integrated via structured action templates.                                                                                                                                                                                                                                                                         </td></tr>
<tr><td data-label="Core Concept / Summary"><b>ReAct Framework for Agents<b>        </td><td data-label="Supporting Details / Examples">ReAct (Reason + Act) framework combines <b>LLM reasoning with actionable steps<b>:  <br> <br>- <b>Observation:<b> agent perceives input or environment.  <br> <br>- <b>Thought:<b> agent generates reasoning about next step.  <br> <br>- <b>Action:<b> agent decides which tool or function to invoke.  <br> <br>- <b>Action\<i>Input:<b> specific parameters for the chosen action.  <br> <br>- <b>Loop:<b> observation → thought → action → observation, repeated until task completion.  <br> <b>Example Prompt Pattern:<b>  <br> `Observation: User asked about stock prices`  <br> `Thought: I need to fetch the latest stock data`  <br> `Action: get<i>stock<i>price`  <br> `Action<i>Input: "AAPL"`  <br> `Observation: Returned stock price is $150`  <br> `Thought: I now can answer the user`  <br> `Final Answer: The current price of AAPL is $150.` </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Agent Loops / Execution Flow<b>      </td><td data-label="Supporting Details / Examples">The <b>agent loop<b> follows a cyclical process:  <br> <br>1. Receive input / observe environment.  <br> <br>2. Generate reasoning (thought).  <br> <br>3. Select action or tool.  <br> <br>4. Execute action with parameters.  <br> <br>5. Record results in memory.  <br> <br>6. Repeat until goal achieved or termination condition met.  <br> <b>Example Pseudocode:<b>  <br> `python  <br> while not done:  <br> &nbsp;&nbsp;observation = get<i>input()  <br> &nbsp;&nbsp;thought = llm<i>reason(observation, memory)  <br> &nbsp;&nbsp;action, params = parse<i>action(thought)  <br> &nbsp;&nbsp;result = execute(action, params)  <br> &nbsp;&nbsp;memory.update(result)  <br> done = check<i>goal(memory)  <br> `                                                                                                                                  </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Memory-Augmented Action<b>           </td><td data-label="Supporting Details / Examples">Agents leverage memory for <b>decision-making and context retrieval<b>:  <br> <br>- Retrieve relevant past interactions from buffer or vector memory.  <br> <br>- Condense long histories into summaries to reduce token usage.  <br> <br>- Use retrieved memory to influence next reasoning step.  <br> <b>Example:<b> For a multi-step research query, agent recalls prior retrieved documents to refine search and avoid redundant API calls.                                                                                                                                                                                                                                                                                                                                                                                         </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Tool Integration and API Calls<b>    </td><td data-label="Supporting Details / Examples">Structured tool calls ensure deterministic behavior:  <br> <br>- Tools are registered with the agent with clear input/output specifications.  <br> <br>- Agents generate actions referencing tool names and arguments.  <br> <br>- Outputs are captured in memory for next reasoning step.  <br> <b>Example Tool Definition:<b>  <br> `python  <br> def get<i>weather(location):  <br> &nbsp;&nbsp;# call weather API and return forecast  <br> `  <br> Agent generates:  <br> `Action: get<i>weather`  <br> `Action<i>Input: "Jakarta"`  <br> Memory logs the result for follow-up queries.                                                                                                                                                                                                                                                 </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Planning and Multi-Step Tasks<b>     </td><td data-label="Supporting Details / Examples">Advanced agents can <b>plan sequences of actions<b> using memory and reasoning:  <br> <br>- Break down complex goals into sub-goals.  <br> <br>- Predict dependencies between actions.  <br> <br>- Adjust plan dynamically based on observation results.  <br> <b>Example:<b> Agent tasked with booking travel:  <br> <br>1. Search flights.  <br> <br>2. Search hotels.  <br> <br>3. Reserve transportation.  <br> Each step uses results from the previous action and stores them in memory for planning the next step.                                                                                                                                                                                                                                                                                                                            </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Safety, Logging, and Feedback<b>     </td><td data-label="Supporting Details / Examples">Agents must include <b>monitoring and safety mechanisms<b>:  <br> <br>- Log all observations, thoughts, actions, and results.  <br> <br>- Use feedback or rewards to refine reasoning and action selection.  <br> <br>- Handle errors gracefully (tool failures, API errors).  <br> <b>Example:<b> If a tool fails, agent logs failure, chooses alternate action, and updates memory with outcome.                                                                                                                                                                                                                                                                                                                                                                                                                                       </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Applications of Autonomous Agents<b> </td><td data-label="Supporting Details / Examples">- Customer support chatbots capable of multi-turn problem solving.  <br> <br>- Research assistants retrieving, summarizing, and cross-referencing documents.  <br> <br>- Workflow automation, e.g., scheduling, monitoring, or data extraction.  <br> <br>- Multi-modal agents combining text, images, and API-based reasoning.  <br> <b>Example:<b> A medical research agent queries databases, extracts relevant studies, summarizes key points, and outputs an integrated report for the user.                                                                                                                                                                                                                                                                                                                                      </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Key Takeaways / Summary<b>           </td><td data-label="Supporting Details / Examples">Autonomous agents with memory and tools <b>extend LLM capabilities<b> by enabling multi-step reasoning, action execution, and dynamic task management. Memory allows agents to <b>maintain context<b>, while tools provide <b>external interaction and functional augmentation<b>. The ReAct framework structures reasoning and actions in a loop, ensuring clarity and traceability. Proper design, logging, and error handling make agents reliable, scalable, and safe for complex workflows. Integrating agents with LangChain, vector databases, and custom APIs enables <b>powerful, context-aware, autonomous text generation and task execution<b>.                                                                                                                                                                      </td></tr>
</tbody></table></div></div>

<div class="table-wrapper">
  <h3 id="Table7" style="margin-top:30px; margin-bottom:10px;">Table 7</h3>
  <div class="table-container">
    <div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;">
      <div class="copy-buttons">
        <button onclick="copyTablePlain(this)">Copy Plain Table</button>
        <button onclick="copyTableMarkdown(this)">Copy Markdown Table</button>
      </div>
    </div>
    <table>
      <thead><tr>

        <th style="width:28.57%;" onclick="sortTableByColumn(6,0)" role="button" aria-label="Sort by Core Concept / Summary">
          <div class="th-with-sort">
            <div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Core Concept / Summary</div>
            <button class="sort-btn sort-state-0" onclick="event.stopPropagation(); headerSortButtonClicked(6,0, this)" title="Toggle sort" aria-label="Toggle sort">
              <span class="sort-icon" aria-hidden="true">
                <svg viewBox="0 0 24 24" width="12" height="12" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                  <path d="M7 14l5-5 5 5"></path>
                  <path d="M7 10l5 5 5-5"></path>
                </svg>
              </span>
            </button>
          </div>
        </th>
        <th style="width:71.43%;" onclick="sortTableByColumn(6,1)" role="button" aria-label="Sort by Supporting Details / Examples">
          <div class="th-with-sort">
            <div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Supporting Details / Examples</div>
            <button class="sort-btn sort-state-0" onclick="event.stopPropagation(); headerSortButtonClicked(6,1, this)" title="Toggle sort" aria-label="Toggle sort">
              <span class="sort-icon" aria-hidden="true">
                <svg viewBox="0 0 24 24" width="12" height="12" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                  <path d="M7 14l5-5 5 5"></path>
                  <path d="M7 10l5 5 5-5"></path>
                </svg>
              </span>
            </button>
          </div>
        </th></tr></thead><tbody><tr><td data-label="Core Concept / Summary"><b>Introduction to MidJourney<b>          </td><td data-label="Supporting Details / Examples">MidJourney is an <b>AI-powered image generation platform<b> that converts text prompts into visual art. It leverages <b>diffusion-based models<b> and integrates user-friendly prompt interfaces to allow users to generate images with high fidelity and stylistic flexibility. MidJourney is widely used by artists, designers, and content creators to explore creative concepts efficiently.                                                                                                         </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Prompt Structure and Best Practices<b> </td><td data-label="Supporting Details / Examples">Effective prompts include clear descriptions, stylistic cues, and modifiers:  <br> <br>- <b>Content description:<b> What should be in the image.  <br> <br>- <b>Style keywords:<b> e.g., “oil painting,” “cinematic,” “hyper-realistic.”  <br> <br>- <b>Lighting, mood, or environment cues:<b> e.g., “sunset,” “foggy,” “dramatic lighting.”  <br> <b>Example Prompt:<b> “A futuristic city skyline at sunset, cyberpunk style, cinematic lighting, ultra-detailed.”                                                    </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Prompt Weighting and Parameters<b>     </td><td data-label="Supporting Details / Examples">MidJourney allows users to influence image generation using <b>weighting<b> and <b>parameters<b>:  <br> <br>- <b>--ar<b> specifies aspect ratio (e.g., `--ar 16:9`).  <br> <br>- <b>--q<b> defines quality vs. speed trade-off (e.g., `--q 2` for higher quality).  <br> <br>- <b>--v<b> chooses the MidJourney model version.  <br> <br>- <b>Prompt weighting:<b> words or phrases can be emphasized using `::` notation (e.g., `"castle::2, fog::1"`).                                                                         </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Stylistic Controls<b>                  </td><td data-label="Supporting Details / Examples">MidJourney supports <b>style injection<b> to produce specific artistic effects:  <br> <br>- Users can invoke known styles (e.g., “digital art,” “watercolor,” “3D render”).  <br> <br>- Fine-tuning via <b>adjectives<b> or <b>artist references<b> enhances realism or creativity.  <br> <b>Example:<b> `"portrait of a lion, in the style of Rembrandt, dramatic chiaroscuro, highly detailed"` produces a baroque-inspired image.                                                                                 </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Aspect Ratio and Composition<b>        </td><td data-label="Supporting Details / Examples">Aspect ratio influences composition and framing:  <br> <br>- <b>--ar 1:1<b> for square images, suitable for icons or thumbnails.  <br> <br>- <b>--ar 16:9<b> for cinematic, landscape-oriented scenes.  <br> <br>- MidJourney interprets prompts relative to aspect ratio, affecting object placement and spatial balance.  <br> <b>Example:<b> Using `--ar 3:2` emphasizes horizontal layouts for panoramic landscapes.                                                                                               </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Image Variations and Upscaling<b>      </td><td data-label="Supporting Details / Examples">MidJourney enables <b>iteration on outputs<b>:  <br> <br>- <b>Variations (V buttons):<b> generate similar images with slight differences.  <br> <br>- <b>Upscaling (U buttons):<b> increase resolution while preserving detail.  <br> <br>- <b>Light/Heavy upscaling:<b> options to retain stylization or enhance realism.  <br> <b>Example Workflow:<b> User generates 4 thumbnails → selects one → upscales → creates refined variations for final selection.                                                            </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Seed Control and Reproducibility<b>    </td><td data-label="Supporting Details / Examples">The <b>seed parameter<b> allows reproducibility of results:  <br> <br>- Random seed by default, controlled using `--seed` flag.  <br> <br>- Identical prompts and seeds yield consistent outputs across generations.  <br> <b>Example:<b> `--seed 12345` ensures that the same prompt generates a reproducible image in multiple runs.                                                                                                                                                                           </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Blend and Remix Features<b>            </td><td data-label="Supporting Details / Examples">MidJourney supports <b>blending multiple images<b> as input:  <br> <br>- Combine references or sketches with text prompts.  <br> <br>- Remix allows modifications of prior generations.  <br> <b>Example:<b> Upload a rough sketch → prompt “enhance with cyberpunk city style” → generate a refined, stylized image.                                                                                                                                                                                            </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Iterative Refinement Practices<b>      </td><td data-label="Supporting Details / Examples">To achieve high-quality outputs, iterative refinement is essential:  <br> <br>- Start with broad descriptive prompts.  <br> <br>- Generate multiple variations.  <br> <br>- Upscale and re-prompt with adjusted stylistic cues.  <br> <br>- Combine or blend selected images for final composition.  <br> <b>Example Workflow:<b> Initial concept → style adjustment → lighting tweak → final render selection.                                                                                                        </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Avoiding Common Pitfalls<b>            </td><td data-label="Supporting Details / Examples">Common issues in MidJourney include:  <br> <br>- Overly complex prompts causing incoherence.  <br> <br>- Conflicting style cues leading to mixed aesthetics.  <br> <br>- Extreme aspect ratios or unusual parameters causing distorted images.  <br> <b>Best Practices:<b> Keep prompt concise but descriptive, test variations iteratively, and adjust parameters stepwise.                                                                                                                                       </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Advanced Prompt Techniques<b>          </td><td data-label="Supporting Details / Examples">Advanced users can:  <br> <br>- Use <b>multi-prompt syntax<b> to weight elements differently.  <br> <br>- Incorporate <b>artist and medium references<b> for specific aesthetic effects.  <br> <br>- Combine <b>image + text prompts<b> for enhanced control.  <br> <b>Example:<b> `"dragon::2, castle::1, foggy::0.5 --ar 16:9 --v 5"` produces a dragon-centric image with a subtle foggy atmosphere.                                                                                                                  </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Community Models and Versions<b>       </td><td data-label="Supporting Details / Examples">MidJourney evolves through <b>version updates<b> and <b>community-shared models<b>:  <br> <br>- Different versions may emphasize style, realism, or artistic creativity.  <br> <br>- Users should check release notes for <b>model strengths and limitations<b>.  <br> <b>Example:<b> Version 5 favors realism and high detail; earlier versions may produce more stylized or abstract outputs.                                                                                                                      </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Applications of MidJourney<b>          </td><td data-label="Supporting Details / Examples">- Concept art and creative illustration.  <br> <br>- Marketing and visual content generation.  <br> <br>- Storyboarding and game asset prototyping.  <br> <br>- Educational visuals and imaginative exploration.  <br> <b>Example:<b> Artists use MidJourney to produce rapid mood boards or visualize ideas without extensive manual rendering.                                                                                                                                                                   </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Key Takeaways / Summary<b>             </td><td data-label="Supporting Details / Examples">MidJourney streamlines <b>text-to-image generation<b> through structured prompts, parameter control, and iterative refinement. Best practices include: clear prompt structure, weighting, stylistic guidance, aspect ratio consideration, seed control, and variation/upscaling management. Combining these tools allows users to <b>efficiently generate high-quality, reproducible, and stylistically coherent images<b>, making MidJourney a versatile platform for artists, designers, and creators. </td></tr>
</tbody></table></div></div>

<div class="table-wrapper">
  <h3 id="Table8" style="margin-top:30px; margin-bottom:10px;">Table 8</h3>
  <div class="table-container">
    <div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;">
      <div class="copy-buttons">
        <button onclick="copyTablePlain(this)">Copy Plain Table</button>
        <button onclick="copyTableMarkdown(this)">Copy Markdown Table</button>
      </div>
    </div>
    <table>
      <thead><tr>

        <th style="width:28.57%;" onclick="sortTableByColumn(7,0)" role="button" aria-label="Sort by Core Concept / Summary">
          <div class="th-with-sort">
            <div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Core Concept / Summary</div>
            <button class="sort-btn sort-state-0" onclick="event.stopPropagation(); headerSortButtonClicked(7,0, this)" title="Toggle sort" aria-label="Toggle sort">
              <span class="sort-icon" aria-hidden="true">
                <svg viewBox="0 0 24 24" width="12" height="12" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                  <path d="M7 14l5-5 5 5"></path>
                  <path d="M7 10l5 5 5-5"></path>
                </svg>
              </span>
            </button>
          </div>
        </th>
        <th style="width:71.43%;" onclick="sortTableByColumn(7,1)" role="button" aria-label="Sort by Supporting Details / Examples">
          <div class="th-with-sort">
            <div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Supporting Details / Examples</div>
            <button class="sort-btn sort-state-0" onclick="event.stopPropagation(); headerSortButtonClicked(7,1, this)" title="Toggle sort" aria-label="Toggle sort">
              <span class="sort-icon" aria-hidden="true">
                <svg viewBox="0 0 24 24" width="12" height="12" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                  <path d="M7 14l5-5 5 5"></path>
                  <path d="M7 10l5 5 5-5"></path>
                </svg>
              </span>
            </button>
          </div>
        </th></tr></thead><tbody><tr><td data-label="Core Concept / Summary"><b>Introduction to Stable Diffusion<b>        </td><td data-label="Supporting Details / Examples">Stable Diffusion is an open-source <b>text-to-image generation model<b> based on <b>latent diffusion<b>. It produces high-quality images from text prompts and allows fine-grained control over styles, composition, and content. It is widely used for research, art, and prototyping because it can be <b>run locally<b>, ensuring privacy and faster iteration.                                                                                                                                                                                                                                                                </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Prompt Engineering for Stable Diffusion<b> </td><td data-label="Supporting Details / Examples">Effective prompts include clear descriptions, style references, and contextual cues:  <br> <br>- <b>Content description:<b> objects, characters, or scene elements.  <br> <br>- <b>Style cues:<b> e.g., “digital painting,” “watercolor,” “photorealistic.”  <br> <br>- <b>Lighting, mood, and environment:<b> e.g., “sunset,” “cinematic lighting,” “foggy forest.”  <br> <b>Example Prompt:<b> `"A futuristic cityscape at night, cyberpunk aesthetic, neon lights, ultra-detailed, photorealistic"`                                                                                                                                          </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Control of Image Generation Parameters<b>  </td><td data-label="Supporting Details / Examples">Stable Diffusion allows adjustment of generation parameters:  <br> <br>- <b>Steps:<b> number of diffusion iterations; more steps improve detail.  <br> <br>- <b>CFG (classifier-free guidance) scale:<b> balances prompt adherence versus creativity.  <br> <br>- <b>Seed:<b> reproducibility across runs.  <br> <b>Example:<b> `steps=50, CFG=7.5, seed=12345` ensures a high-fidelity, reproducible image.                                                                                                                                                                                                                                    </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Latent Space Manipulation<b>               </td><td data-label="Supporting Details / Examples">Users can manipulate the latent space to refine outputs:  <br> <br>- <b>Interpolation:<b> blend two latent vectors to create hybrid images.  <br> <br>- <b>Noise injection:<b> control randomness to produce variations.  <br> <br>- <b>Vector arithmetic:<b> e.g., `latent<i>dragon <br>- latent<i>cat + latent<i>wolf` to produce novel composites.                                                                                                                                                                                                                                                                                                   </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Image-to-Image Generation (img2img)<b>     </td><td data-label="Supporting Details / Examples">Img2img allows modifying an existing image guided by a prompt:  <br> <br>- Upload a base image → define prompt → adjust strength (noise control).  <br> <br>- <b>Strength parameter:<b> 0 → minor edits; 1 → full regeneration.  <br> <b>Example:<b> Transform a rough sketch into a fully rendered digital painting using `"sketch of a forest cabin, photorealistic, morning light"` with strength=0.7.                                                                                                                                                                                                                               </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Inpainting Techniques<b>                   </td><td data-label="Supporting Details / Examples">Inpainting modifies specific regions of an image while keeping the rest intact:  <br> <br>- Mask areas to edit → provide a prompt describing desired changes.  <br> <br>- Useful for repairing images, adding objects, or changing backgrounds.  <br> <b>Example Workflow:<b> Mask sky region → prompt `"sunset with dramatic clouds"` → regenerate sky without affecting foreground.                                                                                                                                                                                                                                                 </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Outpainting for Extended Scenes<b>         </td><td data-label="Supporting Details / Examples">Outpainting extends images beyond original borders:  <br> <br>- Expands canvas → generates consistent content based on context.  <br> <br>- Useful for panoramic scenes, immersive environments, or expanding compositions.  <br> <b>Example:<b> Original painting of a castle → expand left and right → prompt `"surrounding landscape, misty mountains"` → panoramic view.                                                                                                                                                                                                                                                          </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Fine-Tuning and Custom Models<b>           </td><td data-label="Supporting Details / Examples">Users can train or fine-tune Stable Diffusion on specific datasets:  <br> <br>- <b>LoRA (Low-Rank Adaptation):<b> injects new styles or characters without retraining the full model.  <br> <br>- <b>DreamBooth:<b> personalizes the model for specific subjects.  <br> <br>- Custom checkpoints allow control over style, characters, or domain-specific content.  <br> <b>Example:<b> Fine-tune on a dataset of fantasy creatures → generate unique, stylistically consistent monsters.                                                                                                                                                     </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Prompt Weighting and Advanced Syntax<b>    </td><td data-label="Supporting Details / Examples">Stable Diffusion supports weighted prompts for nuanced control:  <br> <br>- Parentheses: increase weight `(castle)` or double `((castle))`.  <br> <br>- Square brackets: reduce weight `[fog]`.  <br> <br>- Multi-prompt concatenation allows complex scene compositions.  <br> <b>Example:<b> `"((dragon)) flying over castle, [fog], cinematic lighting, photorealistic"` → emphasizes dragon and castle, downplays fog.                                                                                                                                                                                                                </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Iterative Refinement Practices<b>          </td><td data-label="Supporting Details / Examples">To achieve optimal images, iterative refinement is key:  <br> <br>- Start with a broad prompt.  <br> <br>- Generate multiple outputs → select promising images.  <br> <br>- Apply img2img, inpainting, or outpainting → tweak prompt or parameters.  <br> <b>Workflow Example:<b> Base prompt → variations → inpaint sky → upscale → final render selection.                                                                                                                                                                                                                                                                              </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Upscaling and Super-Resolution<b>          </td><td data-label="Supporting Details / Examples">Stable Diffusion integrates upscaling for final image quality:  <br> <br>- <b>ESRGAN or Real-ESRGAN:<b> preserve detail while increasing resolution.  <br> <br>- Combines with inpainting to refine edges and textures.  <br> <b>Example:<b> Generate 512×512 image → upscale to 2048×2048 → apply inpainting for sharpness and style consistency.                                                                                                                                                                                                                                                                                      </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Safety and Filtering Controls<b>           </td><td data-label="Supporting Details / Examples">Some versions include <b>NSFW or harmful content filters<b>:  <br> <br>- Ensure responsible image generation.  <br> <br>- Users can adjust or disable filters locally with caution.  <br> <br>- Useful for research or niche artistic purposes where safe content is already curated.                                                                                                                                                                                                                                                                                                                                                     </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Applications of Stable Diffusion<b>        </td><td data-label="Supporting Details / Examples">- Concept art, illustration, and visual storytelling.  <br> <br>- Game asset generation and prototyping.  <br> <br>- Scientific visualization or educational imagery.  <br> <br>- Personalized image creation for marketing or social media.  <br> <b>Example:<b> Artist generates a fantasy world map → refines characters, landscapes, and lighting → produces assets for a game environment.                                                                                                                                                                                                                                           </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Key Takeaways / Summary<b>                 </td><td data-label="Supporting Details / Examples">Stable Diffusion is a <b>flexible and powerful text-to-image generation tool<b>. Best practices include:  <br> <br>- Structured prompt engineering with style and content clarity.  <br> <br>- Control of generation parameters (steps, CFG, seed).  <br> <br>- Latent space manipulation, img2img, inpainting, and outpainting.  <br> <br>- Fine-tuning models with LoRA or DreamBooth for specific styles.  <br> <br>- Iterative refinement, upscaling, and responsible filtering.  <br> These techniques enable high-quality, reproducible, and creatively versatile image generation suitable for artists, researchers, and content creators. </td></tr>
</tbody></table></div></div>

<div class="table-wrapper">
  <h3 id="Table9" style="margin-top:30px; margin-bottom:10px;">Table 9</h3>
  <div class="table-container">
    <div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;">
      <div class="copy-buttons">
        <button onclick="copyTablePlain(this)">Copy Plain Table</button>
        <button onclick="copyTableMarkdown(this)">Copy Markdown Table</button>
      </div>
    </div>
    <table>
      <thead><tr>

        <th style="width:28.57%;" onclick="sortTableByColumn(8,0)" role="button" aria-label="Sort by Core Concept / Summary">
          <div class="th-with-sort">
            <div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Core Concept / Summary</div>
            <button class="sort-btn sort-state-0" onclick="event.stopPropagation(); headerSortButtonClicked(8,0, this)" title="Toggle sort" aria-label="Toggle sort">
              <span class="sort-icon" aria-hidden="true">
                <svg viewBox="0 0 24 24" width="12" height="12" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                  <path d="M7 14l5-5 5 5"></path>
                  <path d="M7 10l5 5 5-5"></path>
                </svg>
              </span>
            </button>
          </div>
        </th>
        <th style="width:71.43%;" onclick="sortTableByColumn(8,1)" role="button" aria-label="Sort by Supporting Details / Examples">
          <div class="th-with-sort">
            <div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Supporting Details / Examples</div>
            <button class="sort-btn sort-state-0" onclick="event.stopPropagation(); headerSortButtonClicked(8,1, this)" title="Toggle sort" aria-label="Toggle sort">
              <span class="sort-icon" aria-hidden="true">
                <svg viewBox="0 0 24 24" width="12" height="12" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                  <path d="M7 14l5-5 5 5"></path>
                  <path d="M7 10l5 5 5-5"></path>
                </svg>
              </span>
            </button>
          </div>
        </th></tr></thead><tbody><tr><td data-label="Core Concept / Summary"><b>Introduction to AI-Powered Applications<b> </td><td data-label="Supporting Details / Examples">AI-powered applications integrate machine learning, natural language processing, computer vision, or other AI capabilities into software solutions. They can automate tasks, provide intelligent insights, or enhance user experiences. Examples include chatbots, recommendation engines, image recognition tools, and autonomous agents.                                                                                                                                                                                                                                                                                                                                                      </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Architecture Overview<b>                   </td><td data-label="Supporting Details / Examples">Building AI applications requires a clear architecture:  <br> <br>- <b>Frontend:<b> interface for user interaction (web, mobile, desktop).  <br> <br>- <b>Backend:<b> manages data processing, model inference, APIs.  <br> <br>- <b>Model Layer:<b> hosts AI models, either locally or via cloud services.  <br> <br>- <b>Data Layer:<b> handles storage, retrieval, and preprocessing of training/inference data.  <br> <b>Example:<b> A chatbot application may have a React frontend, FastAPI backend, a GPT model hosted on a server, and a database for conversation history.                                                                                                                                       </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Choosing AI Models<b>                      </td><td data-label="Supporting Details / Examples">Selecting the right AI model depends on the application:  <br> <br>- <b>Text Generation:<b> GPT, LLaMA, or OpenAI API.  <br> <br>- <b>Image Generation:<b> Stable Diffusion, Midjourney API.  <br> <br>- <b>Speech Recognition:<b> Whisper, DeepSpeech.  <br> <br>- <b>Recommendation Engines:<b> Collaborative filtering, embeddings.  <br> Consider trade-offs in <b>accuracy, latency, cost, and scalability<b>.                                                                                                                                                                                                                                                                                                       </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Data Preparation and Preprocessing<b>      </td><td data-label="Supporting Details / Examples">Proper data handling is crucial:  <br> <br>- <b>Cleaning:<b> remove duplicates, fix formatting, handle missing values.  <br> <br>- <b>Normalization / Tokenization:<b> for text or numeric inputs.  <br> <br>- <b>Augmentation:<b> generate additional training examples for robustness.  <br> <br>- <b>Splitting:<b> training, validation, and test sets to avoid overfitting.  <br> <b>Example:<b> For a text classifier, tokenize sentences, remove stop words, and create embeddings before feeding into a model.                                                                                                                                                                                                     </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Integration of AI Models<b>                </td><td data-label="Supporting Details / Examples">AI models can be integrated via APIs, SDKs, or direct embedding:  <br> <br>- <b>Local Inference:<b> run models on your own servers.  <br> <br>- <b>Cloud APIs:<b> OpenAI, Hugging Face, Stability.ai for managed services.  <br> <br>- <b>SDKs and Libraries:<b> LangChain, PyTorch, TensorFlow, or ONNX Runtime.  <br> <b>Example:<b> Using FastAPI, expose GPT text generation as an endpoint `/generate` for frontend consumption.                                                                                                                                                                                                                                                                               </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Prompt and Workflow Management<b>          </td><td data-label="Supporting Details / Examples">Effective prompts and workflows are key for LLM-based applications:  <br> <br>- Use structured prompts to elicit consistent responses.  <br> <br>- Implement conversation state, memory, or context windows.  <br> <br>- Automate pipelines with LangChain or similar orchestration tools.  <br> <b>Example:<b> In a customer support bot, maintain conversation history and guide GPT to answer only within domain-specific knowledge.                                                                                                                                                                                                                                                                       </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Vector Databases and Semantic Search<b>    </td><td data-label="Supporting Details / Examples">Many AI apps rely on embeddings for semantic search or retrieval:  <br> <br>- Generate vector representations of text or images.  <br> <br>- Store in FAISS, Pinecone, or Milvus.  <br> <br>- Retrieve nearest neighbors for recommendations, Q\&A, or image search.  <br> <b>Example:<b> User query → embed → search vector DB → return relevant documents → feed to LLM for answer generation.                                                                                                                                                                                                                                                                                                              </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Autonomous Agents and Tool Integration<b>  </td><td data-label="Supporting Details / Examples">Advanced AI applications include agents with memory and tools:  <br> <br>- Agents can call APIs, perform tasks, and maintain state.  <br> <br>- Integration with calculators, search engines, or external databases enhances capabilities.  <br> <b>Example:<b> An AI travel planner agent can retrieve flight data, book hotels, and summarize recommendations for the user.                                                                                                                                                                                                                                                                                                                             </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Testing and Evaluation<b>                  </td><td data-label="Supporting Details / Examples">Rigorous testing ensures reliability and accuracy:  <br> <br>- <b>Unit Testing:<b> verify components function correctly.  <br> <br>- <b>Integration Testing:<b> ensure smooth communication between modules.  <br> <br>- <b>Model Evaluation:<b> accuracy, BLEU, ROUGE, FID for image/text tasks.  <br> <b>Example:<b> Test chatbot with a set of sample queries and evaluate response relevance and correctness.                                                                                                                                                                                                                                                                                                   </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Deployment Strategies<b>                   </td><td data-label="Supporting Details / Examples">Deployment depends on scale and requirements:  <br> <br>- <b>Containerization:<b> Docker or Kubernetes for scalable deployment.  <br> <br>- <b>Serverless:<b> AWS Lambda, Azure Functions for lightweight services.  <br> <br>- <b>Cloud Hosting:<b> AWS, GCP, or Azure with GPU support for inference.  <br> <b>Example:<b> Deploy an AI image generation service on GCP with GPU instance → expose REST API → integrate with frontend.                                                                                                                                                                                                                                                                            </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Security, Privacy, and Compliance<b>       </td><td data-label="Supporting Details / Examples">Consider legal and ethical implications:  <br> <br>- Data privacy: handle PII carefully.  <br> <br>- Model outputs: filter unsafe or biased content.  <br> <br>- Compliance: GDPR, HIPAA, or other regional regulations.  <br> <b>Example:<b> Mask sensitive data before sending to third-party LLM APIs; log user consent.                                                                                                                                                                                                                                                                                                                                                                                   </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Monitoring and Maintenance<b>              </td><td data-label="Supporting Details / Examples">Continuous monitoring ensures stability:  <br> <br>- Track usage, errors, response times.  <br> <br>- Monitor model drift or degraded performance over time.  <br> <br>- Retrain or update models as necessary.  <br> <b>Example:<b> Use Prometheus + Grafana to monitor API response times and model latency; trigger retraining when performance drops below thresholds.                                                                                                                                                                                                                                                                                                                                    </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Scalability and Optimization<b>            </td><td data-label="Supporting Details / Examples">Optimize for cost and performance:  <br> <br>- Batch requests for GPU efficiency.  <br> <br>- Use mixed precision or quantization for faster inference.  <br> <br>- Horizontal scaling with microservices.  <br> <b>Example:<b> Convert LLM weights to 8-bit quantization → reduce memory usage and inference time without major accuracy loss.                                                                                                                                                                                                                                                                                                                                                               </td></tr>
<tr><td data-label="Core Concept / Summary"><b>User Experience and Feedback Loops<b>      </td><td data-label="Supporting Details / Examples">A well-designed AI app requires iterative feedback:  <br> <br>- Collect user interactions → fine-tune prompts or models.  <br> <br>- Implement fallback responses for errors or unexpected outputs.  <br> <br>- Continuously improve UX based on analytics.  <br> <b>Example:<b> Chatbot logs misunderstood queries → retrain model with additional examples → improve response quality.                                                                                                                                                                                                                                                                                                                      </td></tr>
<tr><td data-label="Core Concept / Summary"><b>Key Takeaways / Summary<b>                 </td><td data-label="Supporting Details / Examples">Building AI-powered applications combines <b>software engineering, ML modeling, and user-centric design<b>. Best practices include:  <br> <br>- Clear architecture with frontend, backend, model, and data layers.  <br> <br>- Proper data preprocessing, model selection, and integration.  <br> <br>- Use vector databases, autonomous agents, and workflows for intelligent applications.  <br> <br>- Testing, deployment, monitoring, and optimization ensure robustness.  <br> <br>- Ethical, privacy, and compliance considerations guide responsible AI deployment.  <br> Following these principles allows creation of scalable, maintainable, and intelligent AI applications suitable for real-world usage. </td></tr>
</tbody></table></div></div>

<script src="assets/script.js"></script>
</body>
</html>
