<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='utf-8'><meta name='viewport' content='width=device-width,initial-scale=1'>
<title>Tables Viewer v2.1</title>
<style>:root{--main-header-height:56px;} .table-wrapper{opacity:0;min-height:24px;transition:opacity .12s linear}</style>
<link rel="stylesheet" href="assets/style.css?v=1759925496">
<link rel="stylesheet" href="assets/overrides.css?v=1768028808">
</head><body>
<div id="tables-viewer" role="region" aria-label="Tables viewer">
<div id="stickyMainHeader">
<div id="tv-header">
<div><h1>Tables Viewer v2.1</h1></div>
<div style="display:flex;gap:8px;align-items:center;">
<input id="searchBox" class="tv-search" type="search" placeholder="Search" aria-label="Search tables" />
<button id="modeBtn" type="button" onclick="toggleMode()" aria-label="Toggle theme">Theme</button>
<button id="toggleAllBtn" type="button" aria-label="Toggle all" onclick="toggleAllTables()">Collapse All Tables</button>
<button id="copyAllPlainBtn" class="copy-btn" type="button" onclick="copyAllTablesPlain()" aria-label="Copy all tables as plain text">Copy All Tables (Plain Text)</button>
<button id="copyAllTablesBtn" class="copy-all-btn" type="button" onclick="copyAllTablesMarkdown()" aria-label="Copy All Tables (Markdown)">Copy All Tables (Markdown)</button>
<button id="copyAllMdBtn" style="display:none" aria-hidden="true"></button>
<button id="resetAllBtn" type="button" onclick="resetAllTables()" aria-label="Reset All Tables">Reset All Tables</button>
</div></div>
<script>
(function(){
  function ensureDelegation(){
    try {
      var vis = document.getElementById('copyAllTablesBtn');
      var alias = document.getElementById('copyAllMdBtn');
      if(!vis || !alias) return;
      alias.addEventListener = function(type, listener, options){
        vis.addEventListener(type, listener, options);
      };
      alias.removeEventListener = function(type, listener, options){
        vis.removeEventListener(type, listener, options);
      };
      Object.defineProperty(alias, 'onclick', {
        set: function(fn){ vis.onclick = fn; },
        get: function(){ return vis.onclick; },
        configurable: true
      });
      alias.focus = function(){ vis.focus(); };
      alias.blur = function(){ vis.blur(); };
    } catch(e) {
      try{ console && console.warn && console.warn('alias delegation failed', e); }catch(_){} 
    }
  }
  if(document.readyState === 'loading'){
    document.addEventListener('DOMContentLoaded', ensureDelegation);
  } else {
    ensureDelegation();
  }
})();
</script>
<noscript><div style='color:#b91c1c'>JavaScript is disabled. Tables will be shown statically. For large tables enable JS for virtualization.</div></noscript>
<div id="tocBar" role="navigation" aria-label="Table of contents"><ul><li class="toc-item"><a class="toc-link" href="#Table1">Table 1</a></li>
<li class="toc-item"><a class="toc-link" href="#Table2">Table 2</a></li>
<li class="toc-item"><a class="toc-link" href="#Table3">Table 3</a></li>
<li class="toc-item"><a class="toc-link" href="#Table4">Table 4</a></li>
<li class="toc-item"><a class="toc-link" href="#Table5">Table 5</a></li>
<li class="toc-item"><a class="toc-link" href="#Table6">Table 6</a></li>
<li class="toc-item"><a class="toc-link" href="#Table7">Table 7</a></li>
<li class="toc-item"><a class="toc-link" href="#Table8">Table 8</a></li>
<li class="toc-item"><a class="toc-link" href="#Table9">Table 9</a></li>
<li class="toc-item"><a class="toc-link" href="#Table10">Table 10</a></li>
<li class="toc-item"><a class="toc-link" href="#Table11">Table 11</a></li>
<li class="toc-item"><a class="toc-link" href="#Table12">Table 12</a></li>
<li class="toc-item"><a class="toc-link" href="#Table13">Table 13</a></li>
<li class="toc-item"><a class="toc-link" href="#Table14">Table 14</a></li>
<li class="toc-item"><a class="toc-link" href="#Table15">Table 15</a></li>
<li class="toc-item"><a class="toc-link" href="#Table16">Table 16</a></li>
<li class="toc-item"><a class="toc-link" href="#Table17">Table 17</a></li></ul></div></div>
<div class="table-caption" id="Table1" data-table="Docu_0167_01" style="margin-top:2mm;margin-left:3mm;"><strong>Table 1</strong></div>
<div class="table-wrapper" data-table-id="table-1"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by **e-Bupot — Import Guidance**"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>e-Bupot — Import Guidance</strong></div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="e-Bupot — Import Guidance"> <strong>What this document is for</strong><br>This is a very detailed, hand-holding guide for preparing, checking, and submitting withholding (PPh) data to e-Bupot (DJP) or PJAP providers. It assumes you prefer step-by-step instructions and concrete examples (Excel, CSV, Python, Power Query). Follow the lists, copy examples, and test on one small file before doing a full run. </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>Top-level workflow (one-line)</strong><br>Export payroll → clean & normalize → map to e-Bupot template → pre-validate locally → upload to PJAP/DJP → fix flagged rows → submit → reconcile receipts → archive. </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>Why this matters (simple)</strong><br>Platforms reject files when fields are wrong. Fixing problems early saves time and avoids penalties. Always keep originals, validation logs, and receipts. </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>Terminology (plain)</strong><br>- <strong>NPWP</strong>: taxpayer ID (15 digits canonical).<br>- <strong>DPP</strong>: tax base (amount used to calculate tax).<br>- <strong>PPh / tax_withheld</strong>: tax amount withheld.<br>- <strong>BPE</strong>: electronic receipt after DJP accepts submission.<br>- <strong>PJAP</strong>: third-party tax filing providers (OnlinePajak, KlikPajak, etc.). </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>When to import vs manual entry (rule-of-thumb)</strong><br>- Import when you have many rows (≥20) or repeating monthly files.<br>- Enter manually for a few corrections (<20) or one-off cases. </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>What file types to use</strong><br>- <strong>DJP</strong>: DJP expects specific <strong>CSV</strong> or <strong>XML</strong> formats; use the DJP template/XSD.<br>- <strong>PJAP</strong>: vendors accept <strong>XLSX</strong> templates and sometimes CSV; vendor templates are easiest.<br>- <strong>XML</strong> only if your system supports generating valid XSD/XML for DJP. </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>Canonical columns you must have (example)</strong><br>period, npwp, name, address, tax_article, dpp, tax_rate, tax_withheld, withholding_date, ntpn, invoice_ref, employee_id, remarks. Keep original source columns as <code>raw_*</code> for audit. </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>Sample CSV (copy this and test)</strong><br>``<code>\nperiod,npwp,name,address,tax_article,dpp,tax_rate,tax_withheld,withholding_date,ntpn,invoice_ref,employee_id,remarks\n2025-01,123456789012345,JOHN DOE,Jl. Merdeka 1,23,1000000.00,2.5,25000.00,2025-01-25,,INV-2025-0001,EMP001,monthly payroll\n</code>`` </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>Very first safety checks (do these before edit)</strong><br>1. Make a copy of the original export (never edit original).<br>2. Compute file checksum (sha256) and save it next to the file. Example command: <code>sha256sum payroll.csv</code>.<br>3. Work on a copy named <code>payroll_normalize_v1.csv</code>. </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>Step A — Normalizing NPWP (how to make it canonical)</strong><br>Goal: digits-only, 15 digits. If NPWP contains dots/dashes remove them.<br><strong>Excel quick:</strong> if NPWP in A2 → <code>=TRIM(SUBSTITUTE(SUBSTITUTE(SUBSTITUTE(A2,&quot;.&quot;,&quot;&quot;),&quot;-&quot;,&quot;&quot;),&quot; &quot;,&quot;&quot;))</code> then verify length. <br><strong>Power Query (M) quick:</strong> <code>Text.Select([NPWP],{&quot;0&quot;..&quot;9&quot;})</code> returns digits only.<br><strong>Python snippet (exact):</strong> <code>import re; npwp = re.sub(r&#x27;\D&#x27;,&#x27;&#x27;,s); if len(npwp)!=15: flag()</code><br><strong>If length <15 or >15:</strong> mark row for manual review (do NOT invent digits). </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>Step B — Normalize numbers & currency</strong><br>- Remove thousands separators (commas or dots depending on locale).<br>- Use dot <code>.</code> as decimal separator for export.<br>- Use exact 2 decimal places for money fields. <br><strong>Excel formula (strip thousands commas):</strong> <code>=VALUE(SUBSTITUTE(A2,&quot;,&quot;,&quot;&quot;))</code> then format as number with 2 decimals.<br><strong>Python (recommended for accuracy):</strong> use Decimal:<br>``<code>py\nfrom decimal import Decimal, ROUND_HALF_UP\ndef to_decimal(s):\n    s = str(s).replace(&#x27;,&#x27;,&#x27;&#x27;).strip()\n    d = Decimal(s)\n    return d.quantize(Decimal(&#x27;0.01&#x27;), rounding=ROUND_HALF_UP)\n</code>`` </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>Step C — Dates</strong><br>- Use <code>YYYY-MM-DD</code> for date fields.<br>- Period is <code>YYYY-MM</code> (month).<br>- If payroll export lacks day, prefer <code>period</code>+<code>-25</code> (typical pay date) but <strong>flag inferred dates</strong> so reviewer checks them. </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>Step D — Field mapping (how to map your export to e-Bupot)</strong><br>1. Create a mapping sheet (two columns): <code>source_header</code> → <code>target_header</code> (canonical). Keep this file under version control. Example row: <code>GrossPay -&gt; dpp</code>.<br>2. Use matching heuristics: NPWP aliases (<code>npwp_no</code>, <code>tax_id</code>), name (<code>employee_name</code>, <code>nama_karyawan</code>).<br>3. For ambiguous headers, use sample rows to decide mapping; document decision. </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>Step E — Local pre-validation checklist (run before upload)</strong><br>- Required columns present? (npwp, dpp, tax_withheld, tax_article, period).<br>- NPWP normalized and length=15.<br>- Amount fields numeric and ≥0.<br>- tax_withheld ≈ round(dpp * tax_rate/100, 2). Tolerance: 1-2 IDR for rounding differences; if larger, investigate.<br>- No duplicate <code>invoice_ref</code> or <code>employee_id</code> if uniqueness required.<br>- Totals check: <code>sum(dpp)</code> and <code>sum(tax_withheld)</code> match payroll summary. </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>How to calculate tax_withheld reliably</strong><br>Use Decimal with rounding half-up (common business rule). Example in Python:<br>``<code>py\nfrom decimal import Decimal, ROUND_HALF_UP\ntax_withheld = (Decimal(dpp) * Decimal(tax_rate) / Decimal(&#x27;100&#x27;)).quantize(Decimal(&#x27;0.01&#x27;), ROUND_HALF_UP)\n</code>``<br>Do not rely on floating-point binary rounding. </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>CSV export rules (DJP preferred)</strong><br>- Save as UTF-8. <br>- Use comma <code>,</code> as delimiter. <br>- Quote fields with commas or newlines. <br>- No BOM required but allowed; test with target platform.<br>- Header row must match DJP order or your mapping profile must produce the exact DJP field order. </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>XML export rules (only if required)</strong><br>- Generate XML that exactly follows DJP XSD (element names and order).<br>- Validate XML against XSD before uploading. Use <code>xmllint --schema schema.xsd file.xml</code> or code libraries that validate. </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>Uploading: step-by-step (PJAP — typical)</strong><br>1. Login to PJAP portal with authorized account.<br>2. Choose import → select template (or upload CSV).<br>3. Upload file; system runs validation and shows errors/warnings with row numbers.<br>4. Fix flagged rows locally or use the in-portal correction UI (if available).<br>5. Re-upload corrected rows. <br>6. Authorize submission; PJAP forwards to DJP (if integrated).<br>7. Save the provider receipt and any BPE returned. </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>Uploading: step-by-step (DJP portal — typical)</strong><br>1. Login DJP Online / e-Bupot section.<br>2. Select period and import file → upload CSV or XML.<br>3. DJP runs validation; it returns a validation report (row errors, codes).<br>4. Fix errors locally; if DJP accepts partial rows, re-upload only failing rows (if the portal supports incremental import).<br>5. Once accepted, generate and download bukti potong (return docs) and SPT Masa submission receipts. </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>Common platform validation messages & fixes (concrete)</strong><br>- <strong>"NPWP invalid"</strong> → ensure digits-only 15; check leading zeros removed by Excel (Excel may convert to number and strip leading zeros) — format cell as Text or keep as string in CSV.<br>- <strong>"Missing required field: tax_article"</strong> → fill with correct article code (21,23 etc.).<br>- <strong>"Amount not numeric"</strong> → remove currency symbols and thousands separators; ensure dot decimal.<br>- <strong>"Sum mismatch"</strong> → compare sums; check whether rounding or missing rows caused difference. </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>If you see platform error codes (what to include when contacting support)</strong><br>Provide: job_id/file_checksum, sample failing row, exact error string, validation log, and screenshots. That helps support find the problem quickly. </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>Partial failure strategy (how to re-submit only failing rows)</strong><br>1. From validation report, extract failing row numbers.<br>2. Create a sub-file with only failing rows (keep exact header and metadata).<br>3. Correct and re-upload that sub-file. Label filenames clearly (e.g., <code>payroll_fails_fix_2025-01.csv</code>). </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>Reconciliation after successful upload</strong><br>1. Download BPE or platform receipt.<br>2. Match platform-received IDs to your <code>row_id</code> list (use invoice_ref or file row mapping).<br>3. Mark rows as submitted in payroll system and store receipt IDs (NTPN, bukti_potong id).<br>4. Run totals reconciliation: <code>sum(dpp_uploaded)</code> vs <code>sum(dpp_source)</code>. Investigate and record any differences. </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>Record-keeping & audit (what to save)</strong><br>- Original export file + checksum.<br>- Normalized/converted file you uploaded.<br>- Validation logs (platform + your local validator).<br>- BPE / submission receipts and time stamps.<br>- Mapping profile and rule version used (store <code>config/validation_rules.yaml</code> snapshot).<br>- Reconciliation report. Retain according to statutory retention rules. </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>Security & access control (must-haves)</strong><br>- Upload/download only over HTTPS and authenticate with strong credentials.<br>- Role separation: preparer vs reviewer vs approver. Require two-person approval for final submission if policy requires.<br>- Store files encrypted at rest; limit access to authorized users.<br>- Rotate API keys and store secrets in a secrets manager, not in spreadsheets. </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>Idempotency & job safety (enterprise best-practice)</strong><br>- Use <code>idempotency_key</code> per file (e.g., sha256 of contents + submitter id) so retries won’t create duplicates.<br>- Keep job metadata: job_id, checksum, rules_version, username, timestamps, status. </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>Automation helpers — Power Query steps (quick)</strong><br>Power Query (Excel) sample to normalize NPWP and numbers: <br>1. <code>Transform Column</code> → <code>Replace Values</code> remove <code>.</code>, <code>-</code>, <code> </code> (space).<br>2. <code>Add Column</code> → <code>Text.Select([NPWP], {&quot;0&quot;..&quot;9&quot;})</code> to keep digits only.<br>3. For amounts: remove currency symbol then <code>Number.FromText(Text.Replace([Amount],&quot;.&quot;,&quot;&quot;))</code> depends on locale.<br>Keep query steps in repo for repeatable use. </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>Automation helpers — Python (practical snippet)</strong><br>``<code>py\nimport csv, re\nfrom decimal import Decimal, ROUND_HALF_UP\n\ndef norm_npwp(s):\n    return re.sub(r&#x27;\\D&#x27;,&#x27;&#x27;,str(s))\n\ndef norm_amount(s):\n    s = str(s).replace(&#x27;,&#x27;,&#x27;&#x27;)\n    return Decimal(s).quantize(Decimal(&#x27;0.01&#x27;), rounding=ROUND_HALF_UP)\n\nwith open(&#x27;payroll.csv&#x27;, newline=&#x27;&#x27;, encoding=&#x27;utf-8&#x27;) as f:\n    r = csv.DictReader(f)\n    out = []\n    for i,row in enumerate(r,1):\n        npwp = norm_npwp(row[&#x27;npwp&#x27;])\n        dpp = norm_amount(row[&#x27;dpp&#x27;])\n        rate = Decimal(row[&#x27;tax_rate&#x27;])\n        pph = (dpp * rate / Decimal(&#x27;100&#x27;)).quantize(Decimal(&#x27;0.01&#x27;), ROUND_HALF_UP)\n        out.append({**row,&#x27;npwp&#x27;:npwp,&#x27;dpp&#x27;:str(dpp),&#x27;tax_withheld&#x27;:str(pph),&#x27;row_id&#x27;:f\&quot;{i:06d}\&quot;})\n</code>`` </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>Power Query / Excel pitfalls to watch</strong><br>- Excel may auto-convert long numeric NPWP to scientific notation — format column as Text before import.<br>- CSV opened directly in Excel might mis-handle UTF-8; use Data → From Text/CSV and set encoding to UTF-8.<br>- Check hidden columns and merged cells in vendor templates. </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>Fuzzy matching & reconciliation (explain simply)</strong><br>- Primary match: exact NPWP or employee_id.<br>- Secondary match: name similarity + salary closeness. Use a threshold (e.g., name_similarity ≥ 0.90 and salary_ratio ≤ 0.05) to auto-suggest matches; otherwise send to reviewer.<br>- Always include <code>score_breakdown</code> (why candidate matched) so humans can decide. </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>Validation rules: examples you should implement locally</strong><br>- <code>npwp_required</code> for employees who are taxpayers.<br>- <code>tax_rate_allowed</code> should be in allowed set (0.5, 2.5, 5.0, etc.).<br>- <code>tax_calculation_check</code> ensures <code>round(dpp*tax_rate/100,2)</code> equals <code>tax_withheld</code> within tolerance.<br>- <code>duplicate_invoice_check</code> finds repeated invoice_ref. </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>Handling missing NPWP rows (policy decision)</strong><br>- Option 1: flag and require manual entry before submission.<br>- Option 2: if allowed, submit with NPWP blank but mark in your ledger and include reviewer note. Check PJAP/DJP rules — some platforms allow non-NPWP but it may require extra fields. </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>Testing & QA before first production run</strong><br>1. Create three test files: small (5 rows), mid (100 rows), and large (1k+ rows).<br>2. Run full pipeline: parse → validate → reconcile → export. Compare checksums and counts. <br>3. Review validation output and fix rules. <br>4. Run a sandbox upload (PJAP sandbox or DJP sandbox if available).<br>5. Only after green tests run production upload. </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>CI & automation checks (if you have a dev pipeline)</strong><br>- <code>rules_cli_check(config/validation_rules.yaml)</code> should be run in CI.<br>- Parser unit tests (<code>tests/test_parsers.py</code>) must pass.<br>- Keep canonical sample dataset and require sign-off if validation outputs change. </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>Operational runbook (step-by-step every monthly run)</strong><br>1. Pull payroll export from payroll system. 2. Save immutable copy and compute checksum. 3. Run normalization script (Power Query/Python). 4. Run local validator (report errors/warnings). 5. Fix rows and rerun until no blocking errors. 6. Upload to PJAP/DJP. 7. Fix portal-flagged rows and re-upload if needed. 8. Authorize and submit. 9. Download BPE and archive. 10. Reconcile totals and mark accounting entries. 11. Keep all artifacts in named folder: <code>/archive/YYYY-MM/</code> with checksum, mapping profile, validation logs, receipt files. </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>If upload is rejected and you don't understand the message</strong><br>- Copy the exact error text and failing row(s).<br>- Compare the row as uploaded vs your local normalized row (often CSV quoting or encoding changed during edit).<br>- If platform error seems wrong, contact support with sample rows + file checksum + screenshots. </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>When to contact provider/DJP support (be specific)</strong><br>- Platform reports inconsistent validation (same data accepted earlier then rejected now).<br>- You get a server error or timeout that prevents upload completion. <br>- BPE not returned after accepted status. Provide: job_id, checksum, sample row, validation file. </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>Record retention & compliance (basic policy)</strong><br>- Keep originals + normalized file + receipts for statutory retention (e.g., 5–10 years depending on jurisdiction).<br>- Protect PII (encrypt, restrict access).<br>- Log who did what (uploader, reviewer, approver) and when — store this in audit log. </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>Simple checklist summary you can print & use</strong><br>1. Copy original export and compute checksum. 2. Normalize NPWP and numbers. 3. Map columns to template. 4. Run local validation (required fields, NPWP length, tax math). 5. Fix errors & re-run. 6. Upload to provider. 7. Fix portal errors if any. 8. Submit & save receipt. 9. Reconcile totals. 10. Archive artifacts. </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>Troubleshooting quick reference (do this first)</strong><br>- Wrong NPWP length: check Excel numeric formatting (set Text), re-export to CSV.<br>- Decimal wrong: re-export CSV with dot decimal, ensure Excel locale doesn't change delimiter.<br>- Missing rows after upload: check if CSV has header mismatch or extra hidden columns — open CSV in a plain text editor and verify row count. </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>Optional improvements if you want automation later</strong><br>- Build a small script (Python) or Power Query template that normalizes and validates in one click. <br>- Add unit tests for parsers and rules and run in CI. <br>- Use <code>file_checksum + idempotency_key</code> to prevent duplicate submissions. <br>- Keep mapping profiles per vendor under <code>config/mapping_profiles/</code> with samples and tests. </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>If you want me to do a specific next thing for you</strong><br>- I can generate a PJAP-style Excel template (I will need your canonical column list).<br>- I can produce a Power Query or Python script to convert your payroll export to DJP/PJAP CSV (upload a small sample export file or paste 5–10 rows).<br>- I can produce a DJP-compliant CSV/XML example from a sample dataset (paste 5–10 rows). </td></tr><tr><td data-label="e-Bupot — Import Guidance"> <strong>Final plain-language tips</strong><br>- Test once with a small set (5 rows) and see exactly what the portal returns — that saves hours. <br>- Never edit the original file directly — always work on a copy and keep checksums. <br>- When in doubt, mark the row for manual review instead of guessing NPWP or amounts. </td></tr></tbody></table></div><div class="row-count">Rows: 43</div></div><div class="table-caption" id="Table2" data-table="Docu_0167_02" style="margin-top:2mm;margin-left:3mm;"><strong>Table 2</strong></div>
<div class="table-wrapper" data-table-id="table-2"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown — main.py"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown — main.py</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — main.py"> <strong>File-level responsibilities</strong><br><br>This module is the single authoritative assembly point for the e-Bupot service ASGI application. Responsibilities: deterministic wiring of components (parsers, rule engine, reconciler, exporters), import-time safety (no network I/O at import), clear operational knobs (settings.strict, settings.readonly), secrets handling (no secrets in globals), lifecycle semantics (startup/shutdown order for connectors), and observability contracts (structured logs, request tracing, audit hooks). Keep this file narrowly focused on composition: move heavy logic into small modules and expose factories for unit tests. Document runtime modes (dev/demo/production) and how they alter behavior. </td></tr><tr><td data-label="Technical Breakdown — main.py"> <strong>create_app(settings: Settings) -> FastAPI</strong><br><br>Factory used by tests and ASGI server. Must only perform synchronous wiring and validation of the Settings object. Responsibilities: validate non-secret settings surface, attach config and resource factories to <code>app.state</code> (file_store_factory, rules_loader_factory, matcher_factory, audit_client_factory), register middleware and routes using small helper functions, and attach <code>runtime_fingerprint</code> and <code>rules_version</code> to <code>app.state</code>. Avoid starting background work or opening connections — expose connect()/close() hooks on factories. Unit tests should build a minimal Settings and assert <code>create_app</code> is idempotent and returns a FastAPI instance with expected endpoints. </td></tr><tr><td data-label="Technical Breakdown — main.py"> <strong>register_middlewares(app, settings) -> None</strong><br><br>Register ordered, idempotent middleware: (1) inbound trace/request-id normalization, (2) request size & content-type guards (protect CSV/XLSX parsing endpoints), (3) basic auth/session check (if enabled), (4) structured request/response logging with sampling and PII redaction rules, (5) rate limiting for public endpoints, (6) CORS (restrictive by default), (7) GZip compression, (8) error mapping middleware as last resort. Middleware registration must be guarded by <code>app.state._registered_middleware</code>. When logging file payloads or sample rows, apply redaction and sampling. Provide <code>middleware_mode</code> stubs for tests. </td></tr><tr><td data-label="Technical Breakdown — main.py"> <strong>register_routes(app) -> None</strong><br><br>Organize routes by surface: <code>api.v1.*</code> (ingest, validate, reconcile, export), <code>internal.*</code> (health, metrics, admin), and <code>ui.*</code> (static demo UI). Routers must be pure definitions (no side effects). Gate admin/debug routes behind explicit flags. Use dependency injection for user identity and permission scopes rather than inline checks. Include concise example payloads in the router docstrings for reviewers. Ensure OpenAPI only documents public routes; hide internal/admin surfaces. </td></tr><tr><td data-label="Technical Breakdown — main.py"> <strong>init_extensions(app, settings) -> None</strong><br><br>Register client factories on <code>app.state</code>—file_store_factory (local FS or S3), rules_loader (loads YAML/JSON rulesets), audit_client_factory (sqlite or CSV writer), parse_worker_factory (optional). Factories must not open persistent connections. Each factory must expose connect()/aclose() methods invoked by startup/shutdown handlers. Populate <code>app.state._extensions</code> mapping and supply NullClient defaults when config incomplete and <code>settings.strict</code> is false. Unit tests should assert presence of these factories and that startup calls connect() on them. </td></tr><tr><td data-label="Technical Breakdown — main.py"> <strong>configure_logging(app, settings) -> None</strong><br><br>Set structured JSON logging with redaction filter for PII (NPWP, bank account, employee name by pattern). Integrate trace context fields (trace_id, request_id, service). Use a namespaced logger (<code>e_bupot</code>) rather than root-level modifications. Provide a <code>dry_run</code> logging handler for tests (in-memory capture). Document how to extend the redaction list and provide tests ensuring NPWP-like values are redacted. </td></tr><tr><td data-label="Technical Breakdown — main.py"> <strong>register_startup_handlers(app) -> None</strong><br><br>Attach timeboxed startup handlers that: validate <code>config/validation_rules.yaml</code> presence and checksum, instantiate rules index (precompile regexes, thresholds), create/verify file store path or S3 bucket, and warm small sample parses using <code>sample_data</code> (if settings.startup_mode == "full"). Handlers must use asyncio.wait_for with configurable timeouts and respect <code>settings.strict</code>. On critical failure with <code>strict=True</code>, raise to abort startup; otherwise mark <code>app.state._startup_degraded = True</code> and log diagnostics. Tests should simulate rule parsing errors and assert strict vs lenient behavior. </td></tr><tr><td data-label="Technical Breakdown — main.py"> <strong>register_shutdown_handlers(app) -> None</strong><br><br>Close resources in reverse order: stop parsing/reconciliation workers, flush and close audit sink, close file store connections, and ensure temporary files are cleaned. Do not re-raise exceptions during shutdown; log and continue. Ensure shutdown respects <code>settings.shutdown_grace_period</code>. Integration tests must exercise TestClient lifespan and assert cleanup (no orphan temp files). </td></tr><tr><td data-label="Technical Breakdown — main.py"> <strong>configure_error_handlers(app) -> None</strong><br><br>Map domain exceptions to stable error envelopes <code>{code, message, trace_id, hint}</code>. Examples: <code>ParseError -&gt; 400</code> (with hint about file format), <code>ValidationFailure -&gt; 422</code> (with aggregated rule IDs), <code>ReconciliationAmbiguous -&gt; 409</code>, and generic <code>Exception -&gt; 500</code>. Scrub PII from error messages and avoid echoing file contents. Provide telemetry hooks to sample error traces. Tests should raise each exception via a route to verify status codes and sanitized payloads. </td></tr><tr><td data-label="Technical Breakdown — main.py"> <strong>make_health_handler(app) -> Callable</strong><br><br>Provide two handlers: <code>/health</code> (liveness/basic) and <code>/deep-health</code> (readiness). <code>/health</code> returns ok if process alive and essential state keys present. <code>/deep-health</code> performs short async checks: readable rule file, write test to file store, and DB/audit sink connectivity check (with strict timeout). Include <code>runtime_fingerprint</code>, <code>rules_version</code>, and <code>startup_degraded</code> in responses. Deep-health should be protected in shared environments. Tests should mock health check callables and assert aggregate statuses. </td></tr><tr><td data-label="Technical Breakdown — main.py"> <strong>start_background_workers(app) -> None</strong><br><br>MVP avoids heavy in-process workers. If enabled for demos, start a single-threaded parse/reconcile worker with careful lifecycle management: attach to <code>app.state._workers</code>, support graceful cancellation, and ensure work is idempotent (idempotency tokens for ingested files). Implement basic retry/backoff and a bounded queue. Tests should use an in-memory queue to assert worker processes simple jobs. </td></tr><tr><td data-label="Technical Breakdown — main.py"> <strong>get_uvicorn_config(settings) -> Dict[str,Any]</strong><br><br>Encapsulate ASGI server configuration (host/port, workers, limit_concurrency, proxy_headers). Validate numeric ranges and default to safe production-aware values. Document recommended production deployment (systemd / k8s) and health probe wiring. Tests should verify config generation for dev and prod fixtures. </td></tr><tr><td data-label="Technical Breakdown — main.py"> <strong>settings_cli_main(argv: Optional[List[str]] = None) -> int</strong><br><br>CLI wrapper for local dev and CI: parse env overrides, support <code>--check</code> to validate rules and file store write tests, and <code>--reload</code> for local development. When <code>--check</code> is used, exit non-zero on config or rule validation errors. Tests should exercise <code>--check</code> behavior without starting network listeners. </td></tr><tr><td data-label="Technical Breakdown — main.py"> <strong>Implementation patterns & guardrails</strong><br><br>— <strong>Lazy imports</strong>: heavy deps (pandas, openpyxl, pyxlsb, pdfplumber) imported inside factories or functions.<br>— <strong>Idempotence</strong>: registration helpers check <code>app.state._registered</code> sets.<br>— <strong>PII handling</strong>: treat NPWP, names, bank accounts as sensitive—redact in logs and audit outputs unless <code>settings.debug_pii = True</code> and environment is local.<br>— <strong>Externalized rules</strong>: load rules from <code>config/validation_rules.yaml</code> with versioning; never hardcode rule logic in code.<br>— <strong>File handling safety</strong>: write uploads to atomic temp locations, validate MIME + magic bytes before parsing, enforce per-file size limits, and cleanup on error paths. </td></tr><tr><td data-label="Technical Breakdown — main.py"> <strong>Recommended tests & CI checks</strong><br><br>1. <strong>Unit</strong>: pure helpers (rule loader, NPWP validator, chunkers).<br>2. <strong>Integration (fast)</strong>: TestClient lifecycle confirms startup/shutdown handlers and extension connect/close calls.<br>3. <strong>Parser contract</strong>: sample ebupot variants → canonical DF assertions. <br>4. <strong>Validator contract</strong>: rules trigger expected errors/warnings and auto-fixes. <br>5. <strong>Reconcile flow</strong>: sample payroll + ebupot → matched/mismatched records with thresholds. <br>6. <strong>Security</strong>: tests ensuring PII not present in logs or responses. <br>7. <strong>E2E smoke</strong>: containerized run ingest→validate→reconcile→export with sample data. Automate in CI gating. </td></tr><tr><td data-label="Technical Breakdown — main.py"> <strong>Operational & security checklist</strong><br><br>Before production: <code>strict=True</code>, CORS locked to allowed origins, rate limits configured, logging redaction rules validated, rule files versioned and signed (or hashed), audit sink durability verified, backups for <code>outputs/</code> tested, and health endpoints protected as appropriate. Ensure file store quotas and retention policy are documented. Run <code>settings_cli_main --check</code> in CI to validate rule syntax and minimal external checks. </td></tr><tr><td data-label="Technical Breakdown — main.py"> <strong>Maintenance & developer notes</strong><br><br>— When adding a new parser, add mapping profile under <code>config/mapping_profiles</code> and a small unit test with representative sample.<br>— When modifying validation rules, bump <code>rules_version</code> and include migration notes in <code>docs/RULES_SPEC.md</code>.<br>— Keep <code>main.py</code> small: complex behaviors should live in <code>parsers/</code>, <code>rules/</code>, <code>reconcile/</code>, or <code>services/</code> modules. Provide <code>build_test_app</code> helper to inject fake factories and dry-run logging for unit tests. </td></tr></tbody></table></div><div class="row-count">Rows: 17</div></div><div class="table-caption" id="Table3" data-table="Docu_0167_03" style="margin-top:2mm;margin-left:3mm;"><strong>Table 3</strong></div>
<div class="table-wrapper" data-table-id="table-3"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown — endpoints.py"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown — endpoints.py</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — endpoints.py"> <strong>File-level responsibilities</strong><br><br>This module implements the public API surface for the MVP: file ingestion, validation, reconciliation, export, and light orchestration endpoints. It translates HTTP requests into service-layer calls (file_service, rules_loader, validator, reconciler, exporter), enforces request-level guards (auth, size limits, content-type), and ensures responses follow a stable schema with sanitized error envelopes. Keep business logic out of this file: handlers should be thin adapters that validate input, call injected services, trigger background work where appropriate, and attach audit/context metadata. Document each endpoint's contract, side-effects, and expected latency class (sync vs async/background). </td></tr><tr><td data-label="Technical Breakdown — endpoints.py"> <strong>Design principles</strong><br><br>— Thin controllers: handlers should orchestrate services but not implement parsing/matching logic. <br>— Dependency injection: accept service factories/dependencies via FastAPI <code>Depends</code> so tests can inject fakes. <br>— Idempotency and safe retries: support idempotency tokens for ingest/export endpoints. <br>— Fail-fast validation: validate upload headers and basic schema before writing to storage. <br>— Explicit size & type guards: enforce per-file size and MIME/magic checks to avoid DoS. </td></tr><tr><td data-label="Technical Breakdown — endpoints.py"> <strong>Primary endpoints (recommended signatures)</strong><br><br>1. <code>POST /api/v1/ingest</code> — multipart/form-data file upload OR JSON <code>{&quot;source_type&quot;:&quot;url&quot;,&quot;source&quot;:&quot;...&quot;}</code>. <br>• Responsibilities: validate content-type, enforce size, write atomic temp file via file_service, create ingest job entry, optionally trigger immediate validation (sync) or background processing (async). <br>• Response: <code>{&quot;status&quot;:&quot;accepted&quot;,&quot;job_id&quot;:&quot;...&quot;, &quot;idempotency_key&quot;:&quot;...&quot;}</code> (202 when background). <br><br>2. <code>POST /api/v1/validate</code> — body: <code>{ &quot;file_id&quot;: &quot;...&quot;, &quot;rules_version&quot;: &quot;...&quot; , &quot;auto_fix&quot;: false }</code>. <br>• Responsibilities: load file via file_service, call rules_loader + validator, return structured validation result (errors/warnings/auto_fixes). <br>• Response: <code>{&quot;validated&quot;: true, &quot;errors&quot;: [...], &quot;warnings&quot;:[...], &quot;auto_fixes&quot;:[...]}</code>. <br><br>3. <code>POST /api/v1/reconcile</code> — body: <code>{ &quot;ebupot_file_id&quot;: &quot;...&quot;, &quot;payroll_file_id&quot;:&quot;...&quot;, &quot;thresholds&quot;: {...} }</code>. <br>• Responsibilities: orchestrate reconciler.match and reconciler.reconcile flows, stream progress or return job id for long runs. <br>• Response: <code>{&quot;job_id&quot;:&quot;...&quot;, &quot;summary&quot;:{&quot;matched&quot;:n,&quot;mismatched&quot;:m}}</code> or streamed JSON lines for progress. <br><br>4. <code>GET /api/v1/export/{job_id}</code> — returns cleaned e-Bupot file or reconciliation report. <br>• Responsibilities: authorize access, support range/resume where large, set correct content-disposition, record audit event. <br>• Response: streamed file bytes with <code>application/vnd.openxmlformats-officedocument.spreadsheetml.sheet</code> or <code>text/csv</code>. </td></tr><tr><td data-label="Technical Breakdown — endpoints.py"> <strong>Request/response models</strong><br><br>Define Pydantic schemas in <code>schemas.py</code> and reuse across handlers. Keep response envelopes stable: <code>{ &quot;status&quot;: &quot;ok|error&quot;, &quot;data&quot;: {...}, &quot;meta&quot;: {...} }</code>. Always include <code>trace_id</code> in meta for correlation. Validation results should include <code>rule_id</code>, <code>severity</code>, <code>message</code>, <code>suggested_fix</code>, and <code>meta</code> (source file, page/row). </td></tr><tr><td data-label="Technical Breakdown — endpoints.py"> <strong>Dependency injection & service usage</strong><br><br>Handlers must obtain services via <code>Depends</code> (file_service, validator, reconciler, exporter, audit_service). Do not import concrete implementations directly. Provide <code>get_test_file_service()</code> in tests to inject in-memory stores. Services must be called with explicit timeouts and cancellation-awareness. </td></tr><tr><td data-label="Technical Breakdown — endpoints.py"> <strong>File upload & parsing guards</strong><br><br>— Enforce <code>MAX_UPLOAD_SIZE</code> from settings. <br>— Validate MIME type and check magic bytes before persisting. <br>— Store uploads to atomic temp file path and only move to final location after basic validation. <br>— Support chunked/resumable uploads for large files (optional MVP but design hooks). <br>— Compute and return content checksum (sha256) so clients can detect duplicate uploads; use as idempotency key when provided. </td></tr><tr><td data-label="Technical Breakdown — endpoints.py"> <strong>Idempotency & job management</strong><br><br>Accept <code>Idempotency-Key</code> header for <code>/ingest</code> and <code>/export</code> to make operations retry-safe. Persist job state (queued/running/failed/success) and ensure repeated requests with same idempotency key return previous result rather than reprocessing. Record source checksum and file metadata in job record. </td></tr><tr><td data-label="Technical Breakdown — endpoints.py"> <strong>Long-running operations</strong><br><br>Prefer background tasks for heavy operations (reconciliation on large corpuses). For MVP use FastAPI <code>BackgroundTasks</code> or a simple in-process queue with limited concurrency. For long tasks return <code>202 Accepted</code> with job id and provide <code>GET /api/v1/jobs/{job_id}</code> to poll status and <code>GET /api/v1/export/{job_id}</code> to fetch results when complete. Ensure background tasks are resilient to restarts by persisting job metadata. </td></tr><tr><td data-label="Technical Breakdown — endpoints.py"> <strong>Streaming & large responses</strong><br><br>Exports must support streaming to avoid large memory spikes. Use <code>StreamingResponse</code> for CSV/XLSX and set appropriate headers. Support <code>Range</code>/resume if file sizes justify it. For JSON progress updates use <code>application/x-ndjson</code>. </td></tr><tr><td data-label="Technical Breakdown — endpoints.py"> <strong>Security & rate limiting</strong><br><br>— Require authentication for all endpoints; use dependency-injected auth scopes. <br>— Enforce role-based access (uploader vs reviewer vs admin). <br>— Rate-limit public endpoints per IP and per API key with conservative defaults. <br>— Sanitize and validate filenames, avoid path traversal. <br>— Restrict file types and block executable content. </td></tr><tr><td data-label="Technical Breakdown — endpoints.py"> <strong>Error handling & response mapping</strong><br><br>Map domain errors to consistent HTTP responses: <code>ParseError -&gt; 400</code>, <code>ValidationFailure -&gt; 422</code>, <code>ReconciliationAmbiguous -&gt; 409</code>, <code>NotFound -&gt; 404</code>, <code>AuthError -&gt; 401/403</code>, <code>StorageError -&gt; 503</code> (retryable). Use the shared error envelope <code>{code, message, trace_id, hint}</code> and ensure PII scrubbed. Log full stack traces only in dev. </td></tr><tr><td data-label="Technical Breakdown — endpoints.py"> <strong>Audit & observability</strong><br><br>Every mutating request must emit an audit event recorded by <code>audit_service</code> including user_id, action, file_id(s), job_id, timestamp, outcome, and sample sanitized metadata (row counts, failure counts). Attach <code>trace_id</code> and route metadata. Emit metrics for request durations, validation error rates, and job queue depth. </td></tr><tr><td data-label="Technical Breakdown — endpoints.py"> <strong>Testing guidance</strong><br><br>— Unit tests: handler input validation, idempotency behavior, and error mapping using FastAPI TestClient with injected fake services. <br>— Integration tests (fast): ingest small sample files → validate → reconcile → export using temp file_store. <br>— Security tests: ensure endpoints reject invalid auth and that PII is not returned in errors. <br>— Load tests (optional): simulate concurrent ingest requests to validate rate limiting and size guards. </td></tr><tr><td data-label="Technical Breakdown — endpoints.py"> <strong>Performance & resource considerations</strong><br><br>— Limit concurrent file parsing jobs to avoid memory exhaustion. <br>— Use streaming parses where possible (pandas chunking or iterative CSV readers). <br>— Set per-request timeouts and gracefully cancel tasks on client disconnect. <br>— Monitor temp storage usage and enforce retention/cleanup policy. </td></tr><tr><td data-label="Technical Breakdown — endpoints.py"> <strong>Example handler pseudocode (best-practice pattern)</strong><br><br>``<code>py\n@router.post(&#x27;/ingest&#x27;)\nasync def ingest(file: UploadFile = File(...), deps=Depends(deps)):\n    validate_content_type(file)\n    checksum = await compute_checksum(file)\n    job = await file_service.save_temp(file, checksum=checksum)\n    audit_service.record(&#x27;ingest.accepted&#x27;, user, job.id, meta)\n    if settings.sync_ingest:\n        result = await validator.validate(job.file_id)\n        return JSONResponse({&#x27;status&#x27;:&#x27;ok&#x27;,&#x27;result&#x27;:result})\n    else:\n        await jobs.enqueue(&#x27;validate&#x27;, job.id)\n        return JSONResponse(status_code=202, content={&#x27;job_id&#x27;: job.id})\n</code>`` </td></tr><tr><td data-label="Technical Breakdown — endpoints.py"> <strong>Operational notes & guardrails</strong><br><br>— Limit payload logging; when logging sample rows apply deterministic sampling+redaction. <br>— Ensure all endpoints are idempotent-friendly and safe to retry. <br>— Provide clear client error messages with actionable hints (e.g., expected column names or rule_id). <br>— Document API contracts and include example curl snippets in docstrings. </td></tr></tbody></table></div><div class="row-count">Rows: 16</div></div><div class="table-caption" id="Table4" data-table="Docu_0167_04" style="margin-top:2mm;margin-left:3mm;"><strong>Table 4</strong></div>
<div class="table-wrapper" data-table-id="table-4"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown — schemas.py"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown — schemas.py</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — schemas.py"> <strong>File-level responsibilities</strong><br><br>Contain stable Pydantic request/response models and shared serialization helpers used by API handlers. Provide a single source of truth for the HTTP contract: types, field names, example payloads, validation rules, and JSON Schema generation. Keep no business logic here — only validation, coercion, and lightweight transformation helpers (e.g., <code>to_public_dict()</code>, <code>mask_pii()</code>). Document model versions and migration guidance at the top of the file. </td></tr><tr><td data-label="Technical Breakdown — schemas.py"> <strong>Design principles</strong><br><br>— Explicit: prefer precise field types over <code>Any</code>.<br>— Minimal: only include fields required by the contract.<br>— Stable: maintain backward-compatible JSON names; add new optional fields rather than renaming.<br>— Secure: models used for responses must provide <code>.to_safe_dict()</code> that redacts PII (NPWP, bank account, personal names) by default. </td></tr><tr><td data-label="Technical Breakdown — schemas.py"> <strong>Model naming & versioning</strong><br><br>Use <code>V1</code> suffix for top-level API models (e.g., <code>IngestRequestV1</code>, <code>ValidateResponseV1</code>). Add docstring header with <code>__api_version__ = &quot;v1&quot;</code> and maintain changelog comments. When introducing breaking changes, add <code>V2</code> models and keep V1 alive for a deprecation period. </td></tr><tr><td data-label="Technical Breakdown — schemas.py"> <strong>Primary request models (examples)</strong><br><br>``<code>py\nclass IngestRequestV1(BaseModel):\n    idempotency_key: Optional[str]\n    source_type: Literal[&#x27;upload&#x27;,&#x27;url&#x27;]\n    source_url: Optional[AnyUrl]\n    auto_validate: bool = True\n\nclass ValidateRequestV1(BaseModel):\n    file_id: str\n    rules_version: Optional[str]\n    auto_fix: bool = False\n\nclass ReconcileRequestV1(BaseModel):\n    ebupot_file_id: str\n    payroll_file_id: str\n    thresholds: Optional[Dict[str, float]] = Field(default_factory=lambda: {&#x27;name_similarity&#x27;:0.85})\n    max_candidates: conint(ge=1, le=50) = 5\n</code>`` </td></tr><tr><td data-label="Technical Breakdown — schemas.py"> <strong>Primary response models (examples)</strong><br><br>``<code>py\nclass ErrorItem(BaseModel):\n    rule_id: str\n    severity: Literal[&#x27;error&#x27;,&#x27;warning&#x27;,&#x27;info&#x27;]\n    message: str\n    suggested_fix: Optional[str]\n    meta: Optional[Dict[str, Any]]\n\nclass ValidateResponseV1(BaseModel):\n    validated: bool\n    errors: List[ErrorItem] = []\n    warnings: List[ErrorItem] = []\n    auto_fixes: List[Dict[str, Any]] = []\n\nclass ReconcileSummary(BaseModel):\n    matched: int\n    mismatched: int\n    missing_in_payroll: int\n    missing_in_ebupot: int\n\nclass ReconcileResponseV1(BaseModel):\n    job_id: str\n    summary: ReconcileSummary\n    preview: Optional[List[Dict[str,Any]]] = None\n</code>`` </td></tr><tr><td data-label="Technical Breakdown — schemas.py"> <strong>Job & status models</strong><br><br>Provide compact models for job polling and streaming: <code>JobStatusV1</code> with <code>job_id</code>, <code>status</code> (queued/running/success/failed), <code>progress</code> (0-100), <code>started_at</code>/<code>finished_at</code>, and <code>result_meta</code>. Use <code>condecimal</code> / <code>confloat</code> for numeric constraints. </td></tr><tr><td data-label="Technical Breakdown — schemas.py"> <strong>Error envelope</strong><br><br>Define a stable top-level error schema used across endpoints: <code>ErrorEnvelopeV1 = { code: str, message: str, trace_id: str, hint?: str }</code>. Ensure FastAPI exception handlers use this shape for all non-2xx responses. Responses must not include raw PII — use model <code>.to_safe_dict()</code> for output. </td></tr><tr><td data-label="Technical Breakdown — schemas.py"> <strong>Field-level validation rules</strong><br><br>— <code>file_id</code>: regex <code>^[a-f0-9\\-]{8,64}$</code>.<br>— <code>idempotency_key</code>: max length 128, strip whitespace.<br>— <code>thresholds.name_similarity</code>: float between 0.0 and 1.0.<br>— <code>max_candidates</code>: integer 1..50. <br>Implement these with Pydantic validators and <code>Field(..., example=...)</code>. </td></tr><tr><td data-label="Technical Breakdown — schemas.py"> <strong>PII redaction helpers</strong><br><br>Export <code>mask_npwp(value: str) -&gt; str</code> and <code>mask_name(value: str) -&gt; str</code> used by <code>BaseResponseModel.to_safe_dict()</code>. Default response serialization in endpoints should call <code>to_safe_dict()</code> unless <code>settings.debug_pii=True</code>. Include unit tests ensuring redaction patterns. </td></tr><tr><td data-label="Technical Breakdown — schemas.py"> <strong>Schema generation & docs</strong><br><br>Expose <code>get_openapi_components()</code> that returns reusable component schemas for the OpenAPI generator. Provide example payloads in each model's <code>schema_extra</code> to improve generated docs. Hide internal-only models from OpenAPI (set <code>exclude_from_schema=True</code>) where applicable. </td></tr><tr><td data-label="Technical Breakdown — schemas.py"> <strong>Inter-model transformations</strong><br><br>Implement small helpers for conversions used by the API layer: <code>validation_result_from_validator()</code> converts internal validator output to <code>ValidateResponseV1</code>; <code>reconcile_preview_from_reconciler()</code> builds the <code>preview</code> list with limited fields and redaction. Keep these pure and testable. </td></tr><tr><td data-label="Technical Breakdown — schemas.py"> <strong>Dependency & reuse guidance</strong><br><br>Schemas must be imported by endpoints only — not by low-level services. Services should return plain dicts or domain dataclasses; adapter functions in <code>schemas.py</code> convert them to transport models. This prevents tight coupling and accidental import cycles. </td></tr><tr><td data-label="Technical Breakdown — schemas.py"> <strong>Testing guidance</strong><br><br>— Unit tests for each Pydantic model: validate accepted/rejected values, <code>schema()</code> shape, <code>schema_extra</code> examples.<br>— Tests for redaction helpers ensuring NPWP/name patterns are masked.<br>— Integration tests asserting <code>TestClient</code> responses conform to schemas using <code>response.json()</code> validation. </td></tr><tr><td data-label="Technical Breakdown — schemas.py"> <strong>Extensibility & backward compatibility</strong><br><br>When adding new optional fields, mark them <code>Optional</code> and include <code>deprecated</code> info in <code>schema_extra</code>. For breaking contract changes, add <code>V2</code> models and keep V1 routes until deprecation. Provide a short migration checklist in comments. </td></tr><tr><td data-label="Technical Breakdown — schemas.py"> <strong>Performance & payload size caution</strong><br><br>Avoid embedding large arrays or raw file contents in response models. Use <code>result_meta</code> with references (<code>file_id</code>, <code>preview_rows</code>) and provide export endpoints for large payloads. Limit <code>preview</code> size by default (e.g., 10 rows). </td></tr><tr><td data-label="Technical Breakdown — schemas.py"> <strong>Example minimal test snippet</strong><br><br>``<code>py\ndef test_validate_response_schema():\n    resp = ValidateResponseV1(validated=True, errors=[], warnings=[], auto_fixes=[])\n    data = resp.dict()\n    assert &#x27;validated&#x27; in data and data[&#x27;validated&#x27;] is True\n</code>`` </td></tr></tbody></table></div><div class="row-count">Rows: 16</div></div><div class="table-caption" id="Table5" data-table="Docu_0167_05" style="margin-top:2mm;margin-left:3mm;"><strong>Table 5</strong></div>
<div class="table-wrapper" data-table-id="table-5"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown — config.py"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown — config.py</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — config.py"> <strong>File-level responsibilities</strong><br><br>Provide a single, well-tested Settings abstraction for the application: load configuration from environment files and process environment, validate and coerce values, offer typed access to configuration values across the codebase, expose safe/redacted views for logging, and centralize feature flags and operational knobs. This module must contain no side-effects (no network I/O, no DB/file access) at import time. Prefer <code>pydantic.BaseSettings</code> (or equivalent) for validation and secrets handling. Document each setting with purpose, allowed values, and security implications. </td></tr><tr><td data-label="Technical Breakdown — config.py"> <strong>Design principles</strong><br><br>— Single source of truth: all config keys referenced elsewhere must be defined here.<br>— Safe defaults: choose conservative safe defaults (small ports, local storage, strict=False only for dev).<br>— Fail-fast validation: validate critical config on load; provide <code>validate_settings()</code> for CI and startup checks.<br>— Secrets safety: do not expose secrets in <code>repr()</code> or logs; provide <code>to_safe_dict()</code> that redacts sensitive fields.<br>— Immutable runtime view: once constructed, Settings should be treated as immutable by the app. </td></tr><tr><td data-label="Technical Breakdown — config.py"> <strong>Primary API surface</strong><br><br><code>class Settings(BaseSettings)</code> — typed attributes for all config values.<br><code>def get_settings() -&gt; Settings</code> — singleton accessor used across the app (idempotent, lightweight).<br><code>def validate_settings(settings: Settings) -&gt; None</code> — performs cross-field invariants and raises descriptive errors on invalid combos.<br><code>def to_safe_dict(settings: Settings) -&gt; dict</code> — returns a redacted dict for logging/telemetry.<br><code>def load_env_file(path: Optional[str]) -&gt; None</code> — helper to load <code>.env</code> into os.environ for local dev (test-friendly, idempotent). </td></tr><tr><td data-label="Technical Breakdown — config.py"> <strong>Key settings & examples</strong><br><br>Provide typed fields with examples and constraints. Important example keys (non-exhaustive):<br>— <code>ENV: Literal[&#x27;dev&#x27;,&#x27;staging&#x27;,&#x27;prod&#x27;] = &#x27;dev&#x27;</code> — runtime environment.<br>— <code>HOST: str = &#x27;0.0.0.0&#x27;</code> and <code>PORT: int = 8000</code> — network binding (validate 1..65535).<br>— <code>STRICT: bool = True</code> — whether startup should fail on missing external clients (default True for prod).<br>— <code>MAX_UPLOAD_SIZE_BYTES: int = 10_000_000</code> — enforce upload size guard.<br>— <code>FILE_STORE_TYPE: Literal[&#x27;local&#x27;,&#x27;s3&#x27;] = &#x27;local&#x27;</code> and <code>FILE_STORE_PATH: str = &#x27;./uploads&#x27;</code> — storage backend selection.<br>— <code>RULES_PATH: str = &#x27;./config/validation_rules.yaml&#x27;</code> — required rules file path.<br>— <code>LOG_LEVEL: str = &#x27;INFO&#x27;</code> — validated against accepted levels.<br>— <code>ALLOWED_ORIGINS: List[str] = []</code> — CORS configuration; empty by default (restrictive).<br>— <code>RATE_LIMIT_RPS: float = 1.0</code> — default conservative API rate limit.<br>— <code>AUDIT_SINK: Literal[&#x27;sqlite&#x27;,&#x27;csv&#x27;,&#x27;remote&#x27;] = &#x27;sqlite&#x27;</code> and <code>AUDIT_DSN: Optional[str] = None</code>.<br>— <code>SENTRY_DSN: Optional[SecretStr] = None</code> — secret typed and redacted. </td></tr><tr><td data-label="Technical Breakdown — config.py"> <strong>Validation & cross-field invariants</strong><br><br>Implement <code>@root_validator</code> or <code>validate_settings()</code> to enforce rules such as:<br>— If <code>FILE_STORE_TYPE == &#x27;s3&#x27;</code> then <code>AWS_BUCKET</code> and <code>AWS_REGION</code> must be present.<br>— <code>MAX_UPLOAD_SIZE_BYTES</code> is within allowed bounds (e.g., 1MB..100MB).<br>— <code>ENV == &#x27;prod&#x27;</code> implies <code>STRICT == True</code> and <code>DEBUG_PII == False</code>.<br>— <code>ALLOWED_ORIGINS</code> cannot be <code>[&quot;*&quot;]</code> when <code>ENV == &#x27;prod&#x27;</code>.<br>Return clear, actionable error messages (which CI <code>--check</code> will surface). </td></tr><tr><td data-label="Technical Breakdown — config.py"> <strong>Secrets handling & redaction</strong><br><br>— Type sensitive fields as <code>SecretStr</code>/<code>SecretBytes</code> where available.<br>— <code>to_safe_dict()</code> must replace secrets with <code>&lt;redacted&gt;</code> and mask NPWP-like values if present in config (e.g., test NPWP in sample config).<br>— Never log raw <code>.env</code> file contents. Provide a <code>reveal_secret(name: str) -&gt; str</code> accessor for code that actually needs the secret (documented use and test coverage) and ensure it is only used at startup handlers where usage is timeboxed. </td></tr><tr><td data-label="Technical Breakdown — config.py"> <strong>Environment loading order</strong><br><br>Load precedence: explicit args/CLI overrides > environment variables > <code>.env</code> file (if present) > defaults. Provide helper <code>load_env_file(path)</code> to read <code>.env</code> for local dev; keep the function idempotent and testable. Document how CI should inject secrets (preferred: environment variables or secret manager; <code>.env</code> only for local dev). </td></tr><tr><td data-label="Technical Breakdown — config.py"> <strong>Runtime feature flags & operational knobs</strong><br><br>Expose feature flags to gate dev-only behavior: <code>SYNC_INGEST</code>, <code>ENABLE_DEMO_UI</code>, <code>DEBUG_PII</code>, <code>STARTUP_MODE</code> (<code>&#x27;fast&#x27;|&#x27;full&#x27;</code>), <code>ALLOW_INPROCESS_WORKERS</code>. Document safe production defaults and the impact of toggling each flag. Use conservative defaults (e.g., <code>ALLOW_INPROCESS_WORKERS=False</code> in prod). </td></tr><tr><td data-label="Technical Breakdown — config.py"> <strong>Type hints & constraints</strong><br><br>Use Pydantic constrained types (e.g., <code>conint</code>, <code>confloat</code>, <code>AnyUrl</code>) for robust validation. Provide <code>schema_extra</code> examples so OpenAPI and docs show valid values for runtime CLI checks. </td></tr><tr><td data-label="Technical Breakdown — config.py"> <strong>Singleton accessor & test hooks</strong><br><br>Provide <code>get_settings()</code> which caches the Settings instance. For tests, allow <code>override_settings()</code> context manager or <code>build_test_settings()</code> factory so unit tests can inject ephemeral config without touching process env. Ensure <code>get_settings()</code> is safe for import-time usage but does not perform side effects like file IO. </td></tr><tr><td data-label="Technical Breakdown — config.py"> <strong>CLI & CI integration</strong><br><br>Expose <code>settings_cli_check()</code> used by CI to validate configuration and rule presence. This function should: load settings, call <code>validate_settings()</code>, check presence/readability of <code>RULES_PATH</code>, check <code>FILE_STORE_PATH</code> writability (only if explicitly requested via <code>--check-storage</code>), and return non-zero exit code on failures. Keep checks fast and idempotent. </td></tr><tr><td data-label="Technical Breakdown — config.py"> <strong>Documentation & examples</strong><br><br>At the top of the file include a commented example <code>.env</code> and short explanation of recommended secrets injection (CI via env vars or secret manager). Provide explicit examples of how <code>STRICT</code>, <code>ENV</code>, and <code>ALLOWED_ORIGINS</code> interact. Include a small usage snippet for tests: <code>with override_settings({&#x27;ENV&#x27;:&#x27;dev&#x27;,&#x27;STRICT&#x27;:False}): ...</code>. </td></tr><tr><td data-label="Technical Breakdown — config.py"> <strong>Testing guidance</strong><br><br>— Unit tests: validate each field validation (good & bad values), <code>to_safe_dict()</code> redaction, and <code>validate_settings()</code> cross-field rules.<br>— Integration/Ci: <code>settings_cli_check()</code> run as a pre-merge gate; include a test that runs the check against sample <code>config/validation_rules.yaml</code> and <code>sample_data</code> (fast, read-only by default).<br>— Edge cases: test missing optional secrets, invalid ports, wildcard CORS in prod, and oversized <code>MAX_UPLOAD_SIZE_BYTES</code>. </td></tr><tr><td data-label="Technical Breakdown — config.py"> <strong>Operational guardrails</strong><br><br>— Avoid embedding defaults that could cause data loss (e.g., do not default <code>FILE_STORE_PATH</code> to <code>/tmp</code> in production).<br>— Ensure <code>ENV=&#x27;prod&#x27;</code> causes stricter defaults and more conservative limits.<br>— Document how to rotate secrets and how settings changes require application restart. </td></tr><tr><td data-label="Technical Breakdown — config.py"> <strong>Performance & safety concerns</strong><br><br>Config parsing is cheap; avoid expensive transformations in property getters. If derived values are required (e.g., computed upload quota per user), compute them lazily and document caching semantics. </td></tr><tr><td data-label="Technical Breakdown — config.py"> <strong>Examples & snippets</strong><br><br>Provide an example Settings snippet in comments for reviewers; show how <code>to_safe_dict()</code> is used for logging startup config without leaking secrets. Example <code>__repr__</code> should be brief and safe. </td></tr><tr><td data-label="Technical Breakdown — config.py"> <strong>Extensibility & migrations</strong><br><br>Add new settings by appending to Settings with a clear <code>schema_extra</code> example and update <code>settings_cli_check()</code> so CI catches misconfigurations. When removing or renaming keys, provide migration notes and deprecation warnings (log at startup when deprecated keys appear). </td></tr></tbody></table></div><div class="row-count">Rows: 17</div></div><div class="table-caption" id="Table6" data-table="Docu_0167_06" style="margin-top:2mm;margin-left:3mm;"><strong>Table 6</strong></div>
<div class="table-wrapper" data-table-id="table-6"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown — logger.py"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown — logger.py</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — logger.py"> <strong>File-level responsibilities</strong><br><br>Provide centralized, idempotent logging configuration for the application. Responsibilities: structured logging (JSON-lines in prod), PII redaction, integration with trace/request context (trace_id, request_id), non-blocking handler setup (QueueHandler/QueueListener), test-friendly in-memory capture, and safe defaults. This module should expose a small API for application wiring (<code>configure_logging(settings)</code>), runtime helpers (<code>get_logger(name)</code>), and test hooks (<code>capture_logs()</code>), and must avoid side effects at import time. Document the redaction rules, sampling strategy, and how to extend sinks (Sentry/OTel/remote). </td></tr><tr><td data-label="Technical Breakdown — logger.py"> <strong>Design principles</strong><br><br>— Idempotence: calling <code>configure_logging</code> multiple times is a no-op after first successful run.<br>— Minimal import-time behavior: do not touch handlers/loggers at module import; perform all setup during <code>configure_logging</code>.<br>— Secure by default: redact sensitive keys (NPWP, account numbers, tokens) and never log raw secrets unless <code>settings.debug_pii=True</code> and explicitly enabled.<br>— Observability-first: include trace_id, request_id, service, env, and runtime_fingerprint on each record. </td></tr><tr><td data-label="Technical Breakdown — logger.py"> <strong>Primary API</strong><br><br>``<code>py\ndef configure_logging(settings: Settings) -&gt; None: ...\ndef get_logger(name: str = &quot;e_bupot&quot;) -&gt; logging.Logger: ...\n@contextmanager\ndef capture_logs(level=logging.DEBUG): ...  # test helper\n</code>`` </td></tr><tr><td data-label="Technical Breakdown — logger.py"> <strong>configure_logging(settings)</strong><br><br>Idempotently set root and namespaced logger configuration: level, formatters, handlers. Preferred production format: JSON lines with fields <code>{ts, level, logger, msg, trace_id, request_id, service, env, pid, thread, extra...}</code>. Use a <code>JsonLogFormatter</code> for structured output and a <code>RedactionFilter</code> that replaces configured sensitive keys/patterns with <code>&quot;&lt;redacted&gt;&quot;</code>. Attach a <code>QueueHandler</code> to minimize blocking; run a <code>QueueListener</code> in startup handlers or background tasks. Provide <code>dry_run</code> option for tests to attach <code>MemoryHandler</code>. Validate <code>LOG_LEVEL</code> and fallback gracefully. Ensure UTC timestamps and ISO8601 formatting. </td></tr><tr><td data-label="Technical Breakdown — logger.py"> <strong>Trace/context integration</strong><br><br>Provide utilities to extract trace_id/request_id from context (thread-local or contextvars) and attach them automatically to records via a <code>ContextFilter</code> or <code>logging.LoggerAdapter</code>. Document integration points with ASGI middleware that sets contextvars for <code>trace_id</code> and <code>request_id</code>. Ensure logs emitted outside request context still include <code>runtime_fingerprint</code> and <code>service</code> fields. </td></tr><tr><td data-label="Technical Breakdown — logger.py"> <strong>RedactionFilter</strong><br><br>Implement a <code>logging.Filter</code> that inspects record.args and record.extra (if present) and redacts values matching configured patterns: NPWP regex, account numbers, JWT-like tokens, and keys listed in <code>settings.REDACTION_KEYS</code>. Provide an allowlist mechanism (fields safe to log). Keep redaction deterministic and well-tested. </td></tr><tr><td data-label="Technical Breakdown — logger.py"> <strong>Formatter choices</strong><br><br>— <code>JsonLogFormatter</code> for production (structured fields, no pretty printing).<br>— <code>HumanReadableFormatter</code> for dev (colored/simple text) toggled by <code>ENV == &#x27;dev&#x27;</code>.<br>— <code>MemoryFormatter</code> for tests (plain text captured by MemoryHandler). </td></tr><tr><td data-label="Technical Breakdown — logger.py"> <strong>Non-blocking & resilient handlers</strong><br><br>Use <code>QueueHandler</code> + background <code>QueueListener</code> to send logs to IO-bound sinks (file, remote). For external sinks (Sentry/HTTP), use asynchronous background forwarding with bounded queues and backpressure policies (drop or sample when full). Ensure handler initialization is tolerant to transient errors (exponential backoff) and does not block application startup. Provide a fallback file handler on disk if remote sink initialization repeatedly fails. </td></tr><tr><td data-label="Technical Breakdown — logger.py"> <strong>Test hooks & dry-run mode</strong><br><br><code>capture_logs()</code> context manager returns captured records for assertions. <code>configure_logging(settings, dry_run=True)</code> attaches an in-memory <code>MemoryHandler</code> so unit tests can validate redaction and structured keys. Provide helper <code>assert_log_contains(record_list, key, value_substr)</code> for tests. </td></tr><tr><td data-label="Technical Breakdown — logger.py"> <strong>Sentry / OTel / External sinks</strong><br><br>Provide adapter functions to wire Sentry or OTel exporters without importing their heavy SDKs at module import. Example: <code>attach_sentry(dsn, options)</code> performs lazy import and registers an integration with the <code>QueueListener</code>. Document that attaching remote sinks should be done in startup handlers, not at import. Ensure sensitive fields are excluded from Sentry breadcrumbs by default. </td></tr><tr><td data-label="Technical Breakdown — logger.py"> <strong>Sampling & rate limiting</strong><br><br>Implement log-sampling for verbose debug events (e.g., large CSV row dumps). Sampling policy should be configurable (<code>LOG_SAMPLING_RATE</code>) and deterministic for reproducible debugging. Provide a <code>sampled_log(record, rate)</code> helper used by various modules before emitting large payloads. </td></tr><tr><td data-label="Technical Breakdown — logger.py"> <strong>Metrics & telemetry</strong><br><br>Emit metrics for log emission errors, dropped log events, and queue depth. Expose these via a simple counter interface so the telemetry subsystem can scrape them. Record startup time of logging subsystem and number of handlers attached. </td></tr><tr><td data-label="Technical Breakdown — logger.py"> <strong>Error handling & safety</strong><br><br>Avoid raising exceptions from formatters or filters — catch and fallback to a safe plain-text representation. Ensure redaction never raises on unexpected types. In case of formatter failure, emit a minimal fallback record with <code>level=ERROR</code> and <code>msg=&quot;logging failure&quot;</code> plus sanitized exception info. </td></tr><tr><td data-label="Technical Breakdown — logger.py"> <strong>Performance considerations</strong><br><br>Keep logging overhead low on the hot path: avoid expensive serialization or regex operations per-record when possible (use compiled patterns cached on module-level and conditional checks for debug-level messages). Use <code>LoggerAdapter</code> to attach static context cheaply. Ensure the queue and listener are sized to absorb bursts without growing unbounded. </td></tr><tr><td data-label="Technical Breakdown — logger.py"> <strong>Testing guidance</strong><br><br>— Unit tests: redaction patterns, JSON formatter output keys, context filter attaching trace_id, idempotent configure_logging behavior.<br>— Integration tests: start app with <code>configure_logging(dry_run=True)</code> and assert memory handler captured expected structured fields for sample requests.<br>— Failure tests: simulate sink initialization error and assert fallback file handler is used and no exception propagates. Include CI checks to run log-related tests. </td></tr><tr><td data-label="Technical Breakdown — logger.py"> <strong>Usage example (summary)</strong><br><br>1. Call <code>configure_logging(settings)</code> during app <code>create_app</code> (but before app startup handlers that may use logging).<br>2. ASGI middleware populates trace/request contextvars.<br>3. Application code obtains loggers via <code>get_logger(__name__)</code> and emits structured logs.<br>4. Tests use <code>capture_logs()</code> to assert redaction and presence of <code>trace_id</code>. </td></tr><tr><td data-label="Technical Breakdown — logger.py"> <strong>Extensibility & guardrails</strong><br><br>— When adding new redaction keys, document rationale and add unit tests. <br>— Avoid adding heavy SDK imports at module top-level; require lazy import in attach functions. <br>— Keep this module focused on logging concerns only — do not mix audit/event persistence; those belong in <code>services/audit_service.py</code>. </td></tr></tbody></table></div><div class="row-count">Rows: 17</div></div><div class="table-caption" id="Table7" data-table="Docu_0167_07" style="margin-top:2mm;margin-left:3mm;"><strong>Table 7</strong></div>
<div class="table-wrapper" data-table-id="table-7"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown — exceptions.py"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown — exceptions.py</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — exceptions.py"> <strong>File-level responsibilities</strong><br><br>Define the domain-specific exception hierarchy used across the application and provide utilities to map those exceptions to stable HTTP error envelopes. Responsibilities: clear typed exceptions (parse, validation, reconciliation, storage, auth, rate-limit), safe-to-log error representations (no PII), conversion helpers for API error handlers, and a small set of predicates/inspectors used by tests and middleware. Keep exceptions lightweight (no heavy logic) and serializable. Document each exception class with typical raise sites and suggested remediation/hints. </td></tr><tr><td data-label="Technical Breakdown — exceptions.py"> <strong>Design principles</strong><br><br>— Explicit & typed: prefer named classes over generic Exception with codes.<br>— Transportable: exceptions expose <code>to_dict()</code> for building API envelopes and audit events.<br>— PII-safe: messages should avoid embedding sensitive user data; provide <code>meta</code> for structured, non-sensitive diagnostic info.<br>— Mapping-friendly: include stable <code>code</code> and <code>http_status</code> attributes to simplify mapping in <code>configure_error_handlers</code>. </td></tr><tr><td data-label="Technical Breakdown — exceptions.py"> <strong>Primary exception classes (recommended)</strong><br><br>``<code>py\nclass AppError(Exception):\n    code: str = &#x27;app.error&#x27;\n    http_status: int = 500\n    severity: str = &#x27;error&#x27;\n    def __init__(self, message: str, *, meta: Optional[dict]=None): ...\n    def to_dict(self): return {&#x27;code&#x27;: self.code, &#x27;message&#x27;: self.message, &#x27;meta&#x27;: self.meta}\n\nclass ParseError(AppError): code=&#x27;parse.error&#x27;; http_status=400\nclass ValidationFailure(AppError): code=&#x27;validation.failure&#x27;; http_status=422\nclass ReconciliationAmbiguous(AppError): code=&#x27;reconcile.ambiguous&#x27;; http_status=409\nclass NotFound(AppError): code=&#x27;not_found&#x27;; http_status=404\nclass AuthError(AppError): code=&#x27;auth.error&#x27;; http_status=401\nclass PermissionDenied(AuthError): code=&#x27;auth.permission_denied&#x27;; http_status=403\nclass RateLimitExceeded(AppError): code=&#x27;rate_limit.exceeded&#x27;; http_status=429\nclass StorageError(AppError): code=&#x27;storage.error&#x27;; http_status=503\nclass ExternalDependencyError(AppError): code=&#x27;external.dependency&#x27;; http_status=502\n</code>`` </td></tr><tr><td data-label="Technical Breakdown — exceptions.py"> <strong>Exception shape & fields</strong><br><br>Each exception instance should carry: <code>message</code> (user-facing short string), <code>code</code> (machine identifier), <code>http_status</code> (mapped status), <code>meta</code> (non-PII diagnostics such as <code>file_id</code>, <code>rule_id</code>, <code>row</code>), and optional <code>hint</code> for actionable remediation. Avoid including raw file snippets or personal identifiers in <code>message</code> or <code>meta</code>. Provide <code>as_http_response(trace_id)</code> helper that returns the canonical error envelope. </td></tr><tr><td data-label="Technical Breakdown — exceptions.py"> <strong>Error envelope & serialization</strong><br><br>Standard error envelope used by API handlers: <code>{ &quot;error&quot;: { &quot;code&quot;: &quot;&lt;machine&gt;&quot;, &quot;message&quot;: &quot;&lt;user-facing&gt;&quot;, &quot;hint&quot;: &quot;&lt;optional&gt;&quot;, &quot;meta&quot;: {...} }, &quot;trace_id&quot;: &quot;&lt;id&gt;&quot; }</code>. Use <code>exc.to_dict()</code> to populate <code>error</code> and <code>trace_id</code> from request context. Ensure <code>meta</code> fields are scrubbed by <code>mask_pii()</code> before serialization. </td></tr><tr><td data-label="Technical Breakdown — exceptions.py"> <strong>Mapping to HTTP responses</strong><br><br>Provide a single mapping helper used by error middleware: <code>map_exception_to_response(exc, trace_id)</code> that: 1) detects <code>AppError</code> subclasses and uses their <code>http_status</code>, 2) for unknown exceptions returns 500 with a generic <code>app.unexpected_error</code> code, 3) logs full exception details at appropriate severity (info for client errors, error/critical for server errors) while ensuring PII is redacted. Include sample unit tests asserting mapping for each class. </td></tr><tr><td data-label="Technical Breakdown — exceptions.py"> <strong>Helper utilities</strong><br><br>— <code>is_client_error(exc) -&gt; bool</code> (400–499).<br>— <code>wrap_exception(func)</code> decorator to convert uncaught exceptions into <code>AppError</code> with contextual meta (used in service boundary wrappers).<br>— <code>from_validation_errors(errors)</code> to build <code>ValidationFailure</code> containing aggregated <code>rule_id</code> list. </td></tr><tr><td data-label="Technical Breakdown — exceptions.py"> <strong>Logging & telemetry guidance</strong><br><br>When an <code>AppError</code> is raised: log a sanitized structured event including <code>code</code>, <code>http_status</code>, <code>meta</code>, <code>trace_id</code>, and <code>user_id</code> (if available and non-PII). For unexpected exceptions, capture stack trace (safely) and emit a sampled telemetry event. Avoid logging full uploaded file contents; include <code>file_id</code> only. Provide helpers for audit events: <code>to_audit_event(exc, user)</code> that returns minimal fields appropriate for the audit sink. </td></tr><tr><td data-label="Technical Breakdown — exceptions.py"> <strong>Usage patterns & guardrails</strong><br><br>— Raise <code>ParseError</code> when input file format is invalid (include <code>meta={&#x27;row&#x27;:n,&#x27;col&#x27;:&#x27;NPWP&#x27;}</code> where helpful).<br>— Raise <code>ValidationFailure</code> when rules detect errors; include <code>meta={&#x27;error_count&#x27;:n,&#x27;failed_rules&#x27;:[...]} </code>.<br>— Raise <code>ReconciliationAmbiguous</code> when multiple payroll candidates meet threshold; include top candidates in <code>meta</code> but without PII (use hashed ids).<br>— Raise <code>StorageError</code> for transient write failures; include retry hint. </td></tr><tr><td data-label="Technical Breakdown — exceptions.py"> <strong>Testing guidance</strong><br><br>— Unit tests: instantiate each exception class, assert <code>to_dict()</code> shape and <code>http_status</code> mapping.<br>— Integration tests: simulate service raising exceptions and assert API returns correct envelope and status code; ensure PII not present in response. <br>— Chaos tests: ensure middleware maps unknown exceptions to 500 and that error logging occurs (captured via <code>capture_logs</code>). </td></tr><tr><td data-label="Technical Breakdown — exceptions.py"> <strong>Backward compatibility & versioning</strong><br><br>When changing exception <code>code</code> values, add aliases or deprecation notes. Keep <code>http_status</code> stable for existing codes to avoid client breakage. Document any new exception classes in <code>docs/</code> and update API error reference. </td></tr><tr><td data-label="Technical Breakdown — exceptions.py"> <strong>Examples</strong><br><br>``<code>py\n# raising in parser\nif not valid_header:\n    raise ParseError(&#x27;Invalid CSV header&#x27;, meta={&#x27;expected&#x27;: [&#x27;NPWP&#x27;,&#x27;Name&#x27;,&#x27;Amount&#x27;]})\n\n# wrapping validation results\nif errors:\n    raise ValidationFailure(&#x27;Validation failed&#x27;, meta={&#x27;error_count&#x27;: len(errors), &#x27;failed_rules&#x27;: [e.id for e in errors]})\n</code>`` </td></tr><tr><td data-label="Technical Breakdown — exceptions.py"> <strong>Operational notes</strong><br><br>— Keep exception messages concise and actionable.<br>— Never include raw NPWP or employee names in exception messages returned to API clients.<br>— Use meta for debug-only data and ensure middleware strips sensitive meta from outward responses unless <code>settings.debug_pii=True</code> and caller is authorized. </td></tr></tbody></table></div><div class="row-count">Rows: 13</div></div><div class="table-caption" id="Table8" data-table="Docu_0167_08" style="margin-top:2mm;margin-left:3mm;"><strong>Table 8</strong></div>
<div class="table-wrapper" data-table-id="table-8"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown — ebupot_parser.py"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown — ebupot_parser.py</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — ebupot_parser.py"> <strong>File-level responsibilities</strong><br><br>Canonicalize heterogeneous e-Bupot exports (CSV, XLSX, HTML) into a single, predictable in-memory representation (pandas.DataFrame or domain dataclass list) used by validator and reconciler. Responsibilities: file-type detection, safe ingestion (size/magic checks), mapping-profile-driven column normalization, robust type/coercion rules, NPWP/name normalization, minimal PII handling (masking for logs), and rich per-row metadata (source, sheet/page, row_index, raw_values). Keep parsing pure and side-effect light: writing/reading storage belongs to <code>file_service</code>. </td></tr><tr><td data-label="Technical Breakdown — ebupot_parser.py"> <strong>Design goals & constraints</strong><br><br>— Deterministic output: same input → same canonical rows (stable ordering, stable IDs).<br>— Loss-minimizing: preserve original raw values in <code>raw</code> column for auditability.<br>— Safe-by-default: enforce max file size, check magic bytes, validate encoding (UTF-8 fallback), and fail with <code>ParseError</code> for malformed files rather than producing silent corruption.<br>— Extensible mapping: mapping profiles live in <code>config/mapping_profiles/</code> and map common vendor/ERP column names to canonical fields. </td></tr><tr><td data-label="Technical Breakdown — ebupot_parser.py"> <strong>Canonical schema (example)</strong><br><br><code>canonical_columns = [ &#x27;row_id&#x27;, &#x27;npwp&#x27;, &#x27;name&#x27;, &#x27;tax_period&#x27;, &#x27;gross_amount&#x27;, &#x27;tax_base&#x27;, &#x27;tax_withheld&#x27;, &#x27;gross_up_flag&#x27;, &#x27;notes&#x27;, &#x27;source_file&#x27;, &#x27;source_sheet&#x27;, &#x27;source_row&#x27;, &#x27;raw&#x27; ]</code><br>— <code>row_id</code>: stable UUID based on file checksum + row index.<br>— <code>raw</code>: JSON/dict of original parsed fields. </td></tr><tr><td data-label="Technical Breakdown — ebupot_parser.py"> <strong>Primary public functions (signatures)</strong><br><br>``<code>py\ndef detect_file_type(file_path: str) -&gt; Literal[&#x27;csv&#x27;,&#x27;xlsx&#x27;,&#x27;html&#x27;]:\n\ndef parse_ebupot(file_path: str, mapping_profile: Optional[str]=None, *, max_rows: Optional[int]=None) -&gt; Tuple[pd.DataFrame, ParseMetadata]:\n\ndef normalize_row(row: Dict[str,Any]) -&gt; Dict[str,Any]:\n\ndef infer_mapping_from_header(header: List[str]) -&gt; str:  # returns profile id\n</code>`` </td></tr><tr><td data-label="Technical Breakdown — ebupot_parser.py"> <strong>Mapping profiles & header mapping</strong><br><br>— Profiles are YAML files describing source-column → canonical-field mapping, expected data types, required columns, and example header regexes.<br>— <code>infer_mapping_from_header</code> selects best-matching profile by header token similarity (tokenize, lowercase, fuzzy ratio) and returns a confidence score. If confidence < threshold, parser returns a <code>ParseWarning</code> and uses a fallback guessing strategy. </td></tr><tr><td data-label="Technical Breakdown — ebupot_parser.py"> <strong>CSV parsing rules</strong><br><br>— Use <code>csv</code> + <code>pandas.read_csv</code> with explicit <code>engine=&#x27;python&#x27;</code> fallback; detect delimiter via <code>sniff</code>.<br>— Validate magic bytes and BOM; attempt <code>utf-8</code>, then <code>latin-1</code> fallback with explicit logging. <br>— Enforce <code>MAX_UPLOAD_SIZE</code> and <code>MAX_ROWS</code> early. <br>— Preserve original column names in <code>raw</code> and record <code>encoding</code> and <code>delimiter</code> in metadata. </td></tr><tr><td data-label="Technical Breakdown — ebupot_parser.py"> <strong>XLSX parsing rules</strong><br><br>— Use <code>openpyxl</code>/<code>pandas.read_excel</code> with <code>engine=&#x27;openpyxl&#x27;</code> for small files and <code>pyxlsb</code> for binary XLSB if needed (lazy import).<br>— Iterate sheets: attempt to detect target sheet by header matching; include <code>source_sheet</code> in each row's metadata. <br>— Normalize Excel date types (xlrd/pandas) to ISO <code>YYYY-MM-DD</code>. </td></tr><tr><td data-label="Technical Breakdown — ebupot_parser.py"> <strong>HTML parsing rules</strong><br><br>— Use <code>beautifulsoup</code> to find <code>&lt;table&gt;</code> elements, prefer those that include NPWP-like patterns in cells. <br>— Convert table rows to rows; normalize nested tags (strip <code>&lt;br&gt;</code>, handle <code>&lt;td colspan&gt;</code> by expansion). <br>— Record original HTML snippet in <code>raw</code>. </td></tr><tr><td data-label="Technical Breakdown — ebupot_parser.py"> <strong>Normalization & coercion</strong><br><br>— NPWP: normalize to digits-only canonical form; validate with regex <code>^\d{15}$</code> (or configurable), return <code>npwp_valid: bool</code>. <br>— Numbers: remove thousands separators, coerce to Decimal with fixed precision; report parse failures per-row. <br>— Dates: parse with <code>dateutil.parser</code> and enforce <code>tax_period</code> format; on ambiguous day/month, prefer ISO-style or fail with <code>ParseWarning</code> for manual review. </td></tr><tr><td data-label="Technical Breakdown — ebupot_parser.py"> <strong>Row-level metadata & auditability</strong><br><br>— Attach <code>source_file</code>, <code>source_sheet</code>, <code>source_row</code>, <code>checksum</code>, <code>ingest_timestamp</code>, and <code>raw</code> dictionary.<br>— Compute <code>row_id = sha256(file_checksum + &#x27;:&#x27; + str(source_row))</code> to allow idempotent reprocessing. </td></tr><tr><td data-label="Technical Breakdown — ebupot_parser.py"> <strong>Error handling & partial failures</strong><br><br>— Failures: raise <code>ParseError</code> for fatal conditions (invalid file type, unreadable file).<br>— Non-fatal row issues: return <code>ParseResult</code> containing <code>rows</code> plus <code>errors</code> and <code>warnings</code> (with <code>rule_id</code> and <code>row_id</code>) so caller can decide to stop or continue. <br>— Provide <code>strict</code> flag: in strict mode any row error escalates to <code>ParseError</code>. </td></tr><tr><td data-label="Technical Breakdown — ebupot_parser.py"> <strong>PII & logging</strong><br><br>— Never log full NPWP/name values; use <code>mask_npwp()</code> and <code>mask_name()</code> helpers from <code>schemas</code>/<code>logger</code>. <br>— Audit-level outputs retain raw values but are stored only via <code>audit_service</code> with controlled retention; parsing logs should include only <code>file_id</code>, <code>row_count</code>, and <code>error_counts</code>. </td></tr><tr><td data-label="Technical Breakdown — ebupot_parser.py"> <strong>Performance & memory</strong><br><br>— For large CSVs, support streaming parse with chunked <code>pandas.read_csv(chunksize=...)</code> and yield canonicalized batches; keep API that returns a generator for the service layer. <br>— Avoid loading entire Excel workbook into memory when not necessary; parse sheets one at a time. <br>— Use vectorized coercions where possible (pandas <code>.to_datetime</code>, <code>.astype</code>) and fall back to row-wise for tricky cases. </td></tr><tr><td data-label="Technical Breakdown — ebupot_parser.py"> <strong>Security & safety</strong><br><br>— Reject files exceeding <code>MAX_UPLOAD_SIZE</code>. <br>— Validate MIME + magic bytes before parsing. <br>— Sanitize HTML to avoid script tags and do not execute any content. <br>— Use temporary directories with atomic moves for intermediate files; ensure cleanup on exceptions. </td></tr><tr><td data-label="Technical Breakdown — ebupot_parser.py"> <strong>Integration points</strong><br><br>— Returns a <code>ParseResult</code> consumed by <code>rules/validator.py</code> (validator expects canonical column names and <code>meta</code> fields). <br>— Works with <code>file_service</code> for reads/writes and with <code>audit_service</code> to record parse outcomes. <br>— Expose <code>infer_mapping_from_header</code> for UI to present mapping suggestions when upload headers are ambiguous. </td></tr><tr><td data-label="Technical Breakdown — ebupot_parser.py"> <strong>Testing guidance</strong><br><br>— Unit tests: canonicalization of representative CSV/XLSX/HTML samples in <code>tests/test_parsers.py</code>, NPWP normalization edge cases, date parsing ambiguous cases, and mapping-profile inference. <br>— Integration tests: full ingest → parse → validate pipeline using <code>sample_data/ebupot_example.*</code>. <br>— Fuzz tests: malformed CSVs, unexpected encodings, and Excel files with merged cells. </td></tr><tr><td data-label="Technical Breakdown — ebupot_parser.py"> <strong>Example behaviors / pseudocode</strong><br><br>``<code>py\ndef parse_ebupot(path, profile=None):\n    ftype = detect_file_type(path)\n    if ftype == &#x27;csv&#x27;: df = parse_csv(path)\n    elif ftype == &#x27;xlsx&#x27;: df = parse_xlsx(path)\n    else: df = parse_html(path)\n    profile = profile or infer_mapping_from_header(list(df.columns))\n    canonical_rows = [ normalize_row(map_row(r, profile)) for r in df.iterrows() ]\n    return ParseResult(rows=canonical_rows, errors=errors, warnings=warnings)\n</code>`` </td></tr><tr><td data-label="Technical Breakdown — ebupot_parser.py"> <strong>Extensibility notes</strong><br><br>— When adding new vendor formats, add a mapping profile under <code>config/mapping_profiles/</code> and a unit test with a small sample file. <br>— Consider adding a small interactive header-mapping UI later that uses <code>infer_mapping_from_header</code> suggestions and allows manual column mapping. </td></tr></tbody></table></div><div class="row-count">Rows: 18</div></div><div class="table-caption" id="Table9" data-table="Docu_0167_09" style="margin-top:2mm;margin-left:3mm;"><strong>Table 9</strong></div>
<div class="table-wrapper" data-table-id="table-9"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown — payroll_parser.py"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown — payroll_parser.py</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — payroll_parser.py"> <strong>File-level responsibilities</strong><br><br>Parse payroll workbooks (XLSX, XLS, CSV) into the canonical internal representation consumed by validation and reconciliation. Responsibilities: file-type detection, secure ingestion, mapping-profile driven column normalization, consistent employee identity canonicalization, robust numeric/date coercion, per-row provenance metadata, gentle handling of vendor/ERP variants, and producing streaming-friendly outputs for large payrolls. Keep parsing pure (no persistence) and return structured <code>ParseResult</code> objects for downstream services. </td></tr><tr><td data-label="Technical Breakdown — payroll_parser.py"> <strong>Design goals & constraints</strong><br><br>— Deterministic canonical output (stable row_id generation).<br>— Preserve raw input for auditability while exposing normalized fields for business logic.<br>— Fail fast on structural faults (missing required columns) but provide non-fatal row-level warnings for recoverable issues.<br>— Mapping profiles externalized under <code>config/mapping_profiles/</code> for easy vendor additions. </td></tr><tr><td data-label="Technical Breakdown — payroll_parser.py"> <strong>Canonical payroll schema (example)</strong><br><br><code>[&#x27;row_id&#x27;,&#x27;employee_id&#x27;,&#x27;npwp&#x27;,&#x27;name&#x27;,&#x27;period_start&#x27;,&#x27;period_end&#x27;,&#x27;pay_date&#x27;,&#x27;gross_salary&#x27;,&#x27;taxable_income&#x27;,&#x27;total_deductions&#x27;,&#x27;net_pay&#x27;,&#x27;department&#x27;,&#x27;bank_account&#x27;,&#x27;notes&#x27;,&#x27;source_file&#x27;,&#x27;source_sheet&#x27;,&#x27;source_row&#x27;,&#x27;raw&#x27;]</code><br>— <code>row_id</code>: stable SHA256(file_checksum + ':' + row_index).<br>— <code>raw</code>: original column values dict. </td></tr><tr><td data-label="Technical Breakdown — payroll_parser.py"> <strong>Primary public functions (signatures)</strong><br><br>``<code>py\ndef detect_payroll_file_type(file_path: str) -&gt; Literal[&#x27;csv&#x27;,&#x27;xlsx&#x27;,&#x27;xls&#x27;]:\n\ndef parse_payroll(file_path: str, mapping_profile: Optional[str]=None, *, max_rows: Optional[int]=None, strict: bool = False) -&gt; ParseResult:\n\ndef normalize_employee_identity(record: Dict[str,Any]) -&gt; Dict[str,Any]:\n\ndef infer_mapping_from_header(header: List[str]) -&gt; Tuple[str,float]\n</code>`` </td></tr><tr><td data-label="Technical Breakdown — payroll_parser.py"> <strong>Mapping profiles & header inference</strong><br><br>— Profiles describe column name aliases, required fields, preferred data types, and header regexes.<br>— <code>infer_mapping_from_header</code> returns best-match profile and confidence; if below threshold return a <code>ParseWarning</code> and allow caller/UI to request manual mapping. </td></tr><tr><td data-label="Technical Breakdown — payroll_parser.py"> <strong>Excel & CSV parsing rules</strong><br><br>— Prefer <code>pandas.read_excel</code> (openpyxl) for XLSX and <code>read_csv</code> with <code>sniffer</code> for CSVs; lazy-import heavy libs inside functions.<br>— Iterate sheets for multi-sheet payrolls; detect payroll sheet by matching required column aliases.<br>— Convert Excel serial dates and mixed-date cells to ISO <code>YYYY-MM-DD</code> consistently. </td></tr><tr><td data-label="Technical Breakdown — payroll_parser.py"> <strong>Identity normalization</strong><br><br>— <code>employee_id</code>: normalize as string, strip leading zeros unless semantically significant; preserve original in <code>raw</code>.<br>— <code>npwp</code>: digits-only canonicalization and <code>npwp_valid</code> boolean; tolerate formatted NPWP (dots/dashes) and empty NPWP for contractor rows.<br>— <code>name</code>: unicode-normalize, trim whitespace, collapse internal whitespace; optionally produce <code>name_key</code> (soundex/normalized) to help fuzzy matching. </td></tr><tr><td data-label="Technical Breakdown — payroll_parser.py"> <strong>Numeric & currency handling</strong><br><br>— Remove thousands separators and currency symbols before coercion to <code>Decimal</code> with controlled precision.<br>— Preserve original currency if present; convert only when a conversion rate is provided (MVP: fail if mixed currencies unless user-specified).<br>— Record parse failures per-field as row-level warnings. </td></tr><tr><td data-label="Technical Breakdown — payroll_parser.py"> <strong>Period & date handling</strong><br><br>— Support payroll period columns in multiple forms (single <code>period</code> or <code>start</code>/<code>end</code>); normalize to <code>period_start</code>/<code>period_end</code> ISO dates.<br>— <code>pay_date</code>: prefer explicit pay/run date; if missing infer from <code>period_end</code> but mark inference in metadata. </td></tr><tr><td data-label="Technical Breakdown — payroll_parser.py"> <strong>Row metadata & auditability</strong><br><br>— Attach <code>source_file</code>, <code>source_sheet</code>, <code>source_row</code>, <code>file_checksum</code>, <code>ingest_timestamp</code>, and <code>raw</code> dictionary.<br>— Provide <code>canonical_id</code> for each employee row to help stable reconciliation across runs. </td></tr><tr><td data-label="Technical Breakdown — payroll_parser.py"> <strong>Error handling & strictness modes</strong><br><br>— Fatal errors: unreadable file, missing required columns in chosen mapping profile => raise <code>ParseError</code>.<br>— Non-fatal per-row issues: return <code>ParseResult</code> containing <code>rows</code>, <code>warnings</code>, and <code>errors</code> lists. In <code>strict=True</code> escalate row errors into <code>ParseError</code> to force manual correction. </td></tr><tr><td data-label="Technical Breakdown — payroll_parser.py"> <strong>Large file & streaming support</strong><br><br>— Support chunked parsing via <code>yield</code> generator of canonical batches (<code>Iterator[ParseBatch]</code>) for large payrolls; consumer can process batches to avoid loading entire DF into memory.<br>— Provide a <code>preview(rows=10)</code> mode for UI quick inspection. </td></tr><tr><td data-label="Technical Breakdown — payroll_parser.py"> <strong>PII & logging</strong><br><br>— Never log raw NPWP or full employee names; use <code>mask_npwp()</code> and <code>mask_name()</code> helpers when emitting logs. Audit records may store raw values but must go through <code>audit_service</code> with retention policy and access controls. </td></tr><tr><td data-label="Technical Breakdown — payroll_parser.py"> <strong>Integration points</strong><br><br>— Output consumed by <code>rules/validator.py</code> and <code>reconcile/matcher.py</code>.<br>— Works with <code>file_service</code> for reading uploaded files and <code>audit_service</code> for recording parse outcomes and warnings.<br>— <code>infer_mapping_from_header</code> callable used by UI mapping assistant. </td></tr><tr><td data-label="Technical Breakdown — payroll_parser.py"> <strong>Security & safety</strong><br><br>— Enforce <code>MAX_UPLOAD_SIZE</code> and verify MIME+magic bytes before parsing.<br>— Use temp directories with atomic move for intermediate files; ensure cleanup on exceptions. Sanitize any filenames used in logs to avoid path disclosure and injection. </td></tr><tr><td data-label="Technical Breakdown — payroll_parser.py"> <strong>Testing guidance</strong><br><br>— Unit tests: various vendor sample files mapping to canonical schema, NPWP edge-cases, numeric parsing with different separators, date ambiguities, and mapping-profile inference. <br>— Integration tests: full ingest → parse → validate → reconcile flow with <code>sample_data/payroll_example.xlsx</code>. <br>— Performance tests: streaming parse on large synthetic payroll (10k+ rows) to verify memory usage. </td></tr><tr><td data-label="Technical Breakdown — payroll_parser.py"> <strong>Example pseudocode</strong><br><br>``<code>py\ndef parse_payroll(path, profile=None, strict=False):\n    ftype = detect_payroll_file_type(path)\n    df = read_file_to_df(path, ftype)\n    profile = profile or infer_mapping_from_header(df.columns)\n    if missing_required_columns(profile):\n        raise ParseError(...)\n    rows = []\n    for idx, raw_row in df.iterrows():\n        canon = normalize_employee_identity(map_row(raw_row, profile))\n        row_id = make_row_id(file_checksum, idx)\n        rows.append({**canon, &#x27;row_id&#x27;: row_id, &#x27;raw&#x27;: raw_row.to_dict()})\n    return ParseResult(rows=rows, warnings=warnings, errors=errors)\n</code>`` </td></tr><tr><td data-label="Technical Breakdown — payroll_parser.py"> <strong>Extensibility notes</strong><br><br>— Add new mapping profiles under <code>config/mapping_profiles/</code> and accompanying sample files and tests.<br>— If adding contractor/benefit-specific columns, update canonical schema and validator rule references. </td></tr></tbody></table></div><div class="row-count">Rows: 18</div></div><div class="table-caption" id="Table10" data-table="Docu_0167_10" style="margin-top:2mm;margin-left:3mm;"><strong>Table 10</strong></div>
<div class="table-wrapper" data-table-id="table-10"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown — html_parser.py"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown — html_parser.py</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — html_parser.py"> <strong>File-level responsibilities</strong><br><br>Safely extract tabular e-Bupot data from arbitrary HTML files/exports and convert them to the canonical row representation used by the rest of the pipeline. Responsibilities: HTML sanitization, robust table detection and ranking, handling complex table structures (colspan/rowspan), header inference, mapping to canonical columns (via mapping profiles), loss-minimizing raw capture, and producing per-row provenance metadata. Keep I/O out of this module — accept file paths or bytes and return pure parse results (DataFrame / list of dicts + ParseMetadata). </td></tr><tr><td data-label="Technical Breakdown — html_parser.py"> <strong>Design goals & constraints</strong><br><br>— Safety: never execute or evaluate HTML/JS. Sanitize inputs and strip scripts/styles. <br>— Determinism: same input → same canonical rows and stable row_ids. <br>— Auditability: preserve original HTML snippet for each table row in <code>raw</code>. <br>— Resilience: tolerate malformed HTML, nested tables, missing headers, merged cells, and inconsistent rows. <br>— Interoperability: output matches canonical schema expected by <code>validator</code> and <code>reconciler</code>. </td></tr><tr><td data-label="Technical Breakdown — html_parser.py"> <strong>Canonical outputs</strong><br><br>Return <code>ParseResult</code> containing: <code>rows: List[Dict]</code> with canonical fields (<code>row_id</code>, <code>npwp</code>, <code>name</code>, <code>tax_period</code>, <code>gross_amount</code>, etc.), <code>errors</code>, <code>warnings</code>, and <code>meta</code> including <code>source_table_index</code>, <code>table_score</code>, and <code>file_checksum</code>. Each row must include <code>raw_html_snippet</code> and <code>source_row</code> index. </td></tr><tr><td data-label="Technical Breakdown — html_parser.py"> <strong>Primary public functions (signatures)</strong><br><br>``<code>py\ndef extract_tables_from_html(html: Union[str, bytes]) -&gt; List[BeautifulSoup.table]\n\ndef score_table(table: Tag) -&gt; float\n\ndef parse_table(table: Tag, mapping_profile: Optional[str]=None) -&gt; ParseResult\n\ndef parse_html_file(path: str, mapping_hint: Optional[str]=None) -&gt; ParseResult\n\ndef infer_header_from_table(table: Tag) -&gt; List[str]\n</code>`` </td></tr><tr><td data-label="Technical Breakdown — html_parser.py"> <strong>Table detection & ranking</strong><br><br>— Use BeautifulSoup to parse and <code>soup.find_all(&#x27;table&#x27;)</code>. <br>— Score tables using heuristics: presence of NPWP-like cell patterns, column count, header-like first row (th), proportion of numeric cells, and average cell length. <br>— Return tables ordered by score so caller/UI can present top candidates when ambiguous. If no table matches minimum threshold, return a <code>ParseWarning</code>. </td></tr><tr><td data-label="Technical Breakdown — html_parser.py"> <strong>Header inference</strong><br><br>— Prefer explicit <code>&lt;th&gt;</code> rows; otherwise use first non-empty <code>&lt;tr&gt;</code> and apply header-normalization heuristics (tokenize, lower-case, strip punctuation). <br>— Detect multi-row headers and flatten them (concatenate with <code>|</code> or choose most specific non-empty token). <br>— Provide header confidence score; if low, include <code>header_suspect=True</code> in metadata. </td></tr><tr><td data-label="Technical Breakdown — html_parser.py"> <strong>Colspan/rowspan handling</strong><br><br>— Normalize merged cells by expanding them into multiple logical cells: build an index grid, place cell text according to colspan/rowspan offsets, and fill missing cells with <code>None</code>. <br>— Preserve original merged sematics in <code>raw</code> for audit. </td></tr><tr><td data-label="Technical Breakdown — html_parser.py"> <strong>Cell cleaning & coercion</strong><br><br>— Strip inner tags, collapse whitespace, convert HTML entities, and normalize decimal separators. <br>— Detect and normalize NPWP patterns (digits + separators) to canonical form, marking <code>npwp_valid</code>. <br>— Convert numeric-looking cells to <code>Decimal</code> only after mapping profile declares field type; otherwise keep as raw string and warn. </td></tr><tr><td data-label="Technical Breakdown — html_parser.py"> <strong>Mapping & normalization</strong><br><br>— Use mapping profiles (same mechanism as CSV/XLSX parsers) to map inferred/explicit headers to canonical fields. <br>— When header names are ambiguous, run fuzzy-match against profile aliases and include <code>mapping_confidence</code> per field. <br>— For rows with missing canonical fields (e.g., NPWP absent), include them with <code>None</code> and produce a <code>warning</code> row-level item rather than failing the whole parse (unless <code>strict=True</code>). </td></tr><tr><td data-label="Technical Breakdown — html_parser.py"> <strong>Handling noisy HTML</strong><br><br>— Skip tables that are layout-only (detected via many empty cells, lots of rowspan/colspan for layout) or have fewer columns than a configured minimum. <br>— Strip inline scripts/styles and remove comment nodes. <br>— If tables contain nested tables, attempt to flatten by extracting the most table-like descendant or parse nested tables separately and attach <code>parent_table_index</code>. </td></tr><tr><td data-label="Technical Breakdown — html_parser.py"> <strong>Performance & memory</strong><br><br>— Parse HTML incrementally when possible (streaming parsers for very large files), but default to BeautifulSoup for robustness in MVP. <br>— Limit parsing of extremely large HTML by <code>MAX_UPLOAD_SIZE</code> and fail early with <code>ParseError</code>. </td></tr><tr><td data-label="Technical Breakdown — html_parser.py"> <strong>Security considerations</strong><br><br>— Do not call <code>soup.prettify()</code> on untrusted HTML for logs (can expose huge payloads). <br>— Sanitize any attributes before including in <code>raw</code> metadata to avoid injection when rendering in a UI. <br>— Ensure file writes for temporary HTML are atomic and permission-restricted when used by higher layers. </td></tr><tr><td data-label="Technical Breakdown — html_parser.py"> <strong>Error handling & strictness modes</strong><br><br>— Fatal: unreadable bytes, unsupported encoding, or missing table and <code>must_have_table=True</code> => raise <code>ParseError</code>. <br>— Non-fatal: malformed rows, inconsistent column counts, header uncertainty => return <code>ParseResult</code> with <code>warnings</code> and row-level <code>errors</code>. <code>strict=True</code> promotes these to <code>ParseError</code>. </td></tr><tr><td data-label="Technical Breakdown — html_parser.py"> <strong>Integration points</strong><br><br>— Returns <code>ParseResult</code> used directly by <code>rules/validator.py</code>. <br>— <code>infer_header_from_table</code> and <code>score_table</code> are exposed for UI mapping assistant to show candidate tables and suggested header mappings. <br>— Works with <code>file_service</code> for reading HTML bytes and with <code>audit_service</code> to record parse outcomes. </td></tr><tr><td data-label="Technical Breakdown — html_parser.py"> <strong>Testing guidance</strong><br><br>— Unit tests: varied HTML samples: simple single-table e-Bupot, multi-table pages, nested tables, merged cells, malformed HTML, different encodings, and tables where NPWP appears in different columns. <br>— Edge tests: missing headers, multiple candidate tables, and tables with numeric columns using commas as decimal separators. <br>— Integration tests: ingest HTML → parse → validate → reconcile flow using <code>sample_data/ebupot_example.html</code>. </td></tr><tr><td data-label="Technical Breakdown — html_parser.py"> <strong>Pseudocode snippet</strong><br><br>``<code>py\ndef parse_html_file(path):\n    html = read_bytes(path)\n    soup = BeautifulSoup(html, &#x27;lxml&#x27;)\n    tables = soup.find_all(&#x27;table&#x27;)\n    scored = sorted([(score_table(t), idx, t) for idx,t in enumerate(tables)], reverse=True)\n    results = ParseResult(rows=[], errors=[], warnings=[])\n    for score, idx, table in scored[:max_candidates]:\n        header = infer_header_from_table(table)\n        df_rows, row_meta = parse_table(table, header)\n        results.rows.extend(df_rows)\n        results.meta[&#x27;tables&#x27;].append({&#x27;index&#x27;: idx, &#x27;score&#x27;: score})\n    return results\n</code>`` </td></tr><tr><td data-label="Technical Breakdown — html_parser.py"> <strong>Extensibility notes</strong><br><br>— Add heuristic plug-ins to <code>score_table</code> for new vendor patterns without changing core parsing logic. <br>— If HTML sources include embedded CSV/XLSX links, add a pre-scan that extracts and hands off to the more suitable parser. <br>— Later: add a small interactive mapping UI that shows detected tables and allows column remapping using functions exported from this module. </td></tr></tbody></table></div><div class="row-count">Rows: 17</div></div><div class="table-caption" id="Table11" data-table="Docu_0167_11" style="margin-top:2mm;margin-left:3mm;"><strong>Table 11</strong></div>
<div class="table-wrapper" data-table-id="table-11"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown — rules_loader.py"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown — rules_loader.py</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — rules_loader.py"> <strong>File-level responsibilities</strong><br><br>Load, validate, version, and provide efficient access to validation/reconciliation rules used by the validator and reconciler. Responsibilities: canonicalize rule format (JSON/YAML → internal dataclasses), enforce schema & semantic invariants, compute and expose ruleset version/checksum, cache compiled rule indices for fast evaluation, support safe hot-reload in dev, provide a <code>--check</code> entrypoint for CI, and expose read-only accessors for runtime code. Keep IO isolated and side-effects explicit (load only when called; no network I/O at import). </td></tr><tr><td data-label="Technical Breakdown — rules_loader.py"> <strong>Design principles</strong><br><br>— Single source of truth: rules are authoritative and externalized (<code>config/validation_rules.yaml</code>).<br>— Validate early & loudly: fail fast on syntactic/semantic errors; provide clear messages for operators.<br>— Immutable runtime view: once loaded for a process, return an immutable ruleset unless an explicit reload is requested.<br>— Efficient lookups: compile indices (by rule_id, by field, by severity) to avoid scanning rules at validation time. </td></tr><tr><td data-label="Technical Breakdown — rules_loader.py"> <strong>Primary public API (recommended)</strong><br><br>``<code>py\nclass Ruleset(NamedTuple):\n    version: str\n    checksum: str\n    loaded_at: datetime\n    raw: dict\n    rules_by_id: Mapping[str, Rule]\n    rules_by_field: Mapping[str, List[Rule]]\n\ndef load_rules(path: str, *, validate: bool = True) -&gt; Ruleset\n\ndef validate_ruleset_schema(raw: dict) -&gt; None  # raises SchemaError\n\ndef get_ruleset() -&gt; Ruleset  # returns cached instance or raises if not loaded\n\ndef reload_rules(path: str) -&gt; Ruleset  # dev-only hot-reload with safety checks\n\ndef rules_cli_check(path: str) -&gt; int  # exit-code friendly for CI\n</code>`` </td></tr><tr><td data-label="Technical Breakdown — rules_loader.py"> <strong>Rule schema & semantics</strong><br><br>Rules file should declare metadata (<code>version</code>, <code>effective_date</code>, <code>description</code>) and a list of rule objects with fields: <code>rule_id</code> (stable string), <code>severity</code> (<code>error|warning|info</code>), <code>field_pattern</code> (column name or regex), <code>condition</code> (DSL or JSON Logic), <code>message</code> (user-facing), <code>suggested_fix</code> (optional), <code>examples</code> (optional), and <code>applies_to</code> (ebupot/payroll/both). Define JSON Schema for this structure and validate on load. </td></tr><tr><td data-label="Technical Breakdown — rules_loader.py"> <strong>Condition expression options</strong><br><br>Support one of (documented) condition representations: JSON Logic, a minimal domain DSL, or simple Python-safe expressions compiled with <code>ast</code> and restricted primitives. Prefer JSON Logic for portability; provide a small, well-tested adapter that compiles conditions into fast callables. Document allowed operators and deny arbitrary code execution. </td></tr><tr><td data-label="Technical Breakdown — rules_loader.py"> <strong>Compilation & indices</strong><br><br>On load: validate schema → canonicalize rule fields → compile conditions to callables → build <code>rules_by_id</code> and <code>rules_by_field</code> indices and a <code>global_rules</code> list. Precompute small metadata (estimated cost, affected columns, sample messages) to speed validator planning. Cache the Ruleset object and expose it via <code>get_ruleset()</code>. </td></tr><tr><td data-label="Technical Breakdown — rules_loader.py"> <strong>Versioning & checksums</strong><br><br>Compute a stable <code>checksum</code> (sha256) over normalized rule file (sorted keys, normalized whitespace) and set <code>version</code> from explicit <code>version</code> field or fallback to <code>checksum[:12]</code>. Expose both values to API and health endpoints so operators can confirm the active ruleset. </td></tr><tr><td data-label="Technical Breakdown — rules_loader.py"> <strong>Hot-reload behavior (dev only)</strong><br><br>Provide <code>reload_rules()</code> that atomically replaces cached ruleset after validation/compilation. Protect hot-reload with <code>settings.startup_mode</code> guard and a <code>reload_token</code> confirmation to avoid accidental production reloads. Emit structured audit events via <code>audit_service</code> on reloads (who triggered, old->new checksum). </td></tr><tr><td data-label="Technical Breakdown — rules_loader.py"> <strong>Schema validation & error reporting</strong><br><br>Use <code>jsonschema</code> (or pydantic) to validate rule file shape and additional semantic checks: duplicate <code>rule_id</code>, overlapping contradictory rules, impossible conditions, unresolved references to fields. When errors occur, produce developer-friendly diagnostics: file path, rule_id, location (line/offset), and suggested remediation. <code>rules_cli_check()</code> should return non-zero and print structured JSON diagnostics for CI. </td></tr><tr><td data-label="Technical Breakdown — rules_loader.py"> <strong>Security & safe evaluation</strong><br><br>— Never execute raw code from rule files. If supporting expressions, compile them into a safe subset using <code>ast</code> or use JSON Logic. <br>— Limit resource usage of compiled conditions (no long-running loops) and avoid expensive operations per-row (if needed, mark as heavy and require validator to batch). <br>— Sanitize messages and examples to avoid accidental PII leakage. </td></tr><tr><td data-label="Technical Breakdown — rules_loader.py"> <strong>Integration points</strong><br><br>— <code>rules/validator.py</code> calls <code>get_ruleset()</code> and uses <code>rules_by_field</code> to fetch candidate rules for a row/column. <br>— <code>api</code> and health endpoints read <code>rules_version</code>/<code>checksum</code> from <code>get_ruleset()</code> for diagnostics. <br>— <code>audit_service</code> records ruleset load/reload events and <code>rules_cli_check()</code> output. </td></tr><tr><td data-label="Technical Breakdown — rules_loader.py"> <strong>Error & conflict resolution strategies</strong><br><br>Detect and report rule conflicts (two <code>error</code> rules that mutually exclude each other) with a <code>ConflictWarning</code>. Provide a <code>severity_override</code> policy in settings for emergency mode (treat all warnings as errors or vice versa). Document how validator should choose between overlapping rules (explicit ordering, rule <code>priority</code> field). </td></tr><tr><td data-label="Technical Breakdown — rules_loader.py"> <strong>Performance considerations</strong><br><br>— Keep compiled callables lightweight and pure; avoid per-row reparsing of condition expressions. <br>— Cache compiled ruleset in memory and allow reusing across validator threads/processes. <br>— For very large rule sets, provide lazy compilation of less-used rules and profiling hooks to find hot rules. </td></tr><tr><td data-label="Technical Breakdown — rules_loader.py"> <strong>Testing guidance</strong><br><br>— Unit tests: schema validation, checksum/version determinism, condition compilation/correctness, duplicate rule detection, and safe-failure modes. <br>— Integration tests: <code>rules_cli_check()</code> against good and bad sample rule files; validator interaction tests using small canonical rulesets. <br>— Fuzz tests: malformed JSON/YAML, rules with unsupported operators, and large rule collections. </td></tr><tr><td data-label="Technical Breakdown — rules_loader.py"> <strong>CLI & CI integration</strong><br><br><code>rules_cli_check(path)</code> returns 0 on clean file; non-zero with JSON diagnostic output on failure. Use in CI pipeline as <code>python -m app.rules.rules_loader --check config/validation_rules.yaml</code>. Provide machine-friendly <code>--format json|text</code> output. </td></tr><tr><td data-label="Technical Breakdown — rules_loader.py"> <strong>Documentation & operational notes</strong><br><br>— Keep authoritative rule authoring guide in <code>docs/RULES_SPEC.md</code> with examples, allowed operators, and migration recommendations. <br>— When updating rules in production, perform canary rollout (load new rules in a staging process and run sample dataset checks). <br>— Log rule loads with <code>runtime_fingerprint</code>, <code>rules_checksum</code>, and operator identity. </td></tr><tr><td data-label="Technical Breakdown — rules_loader.py"> <strong>Extensibility</strong><br><br>— Support additional rule attributes later: <code>deprecated</code>, <code>effective_end</code>, <code>apply_scope</code> (per-entity), and <code>throttle</code> (to limit expensive rule execution). <br>— Provide a UI-friendly JSON export of rule metadata for editors and reviewers. </td></tr></tbody></table></div><div class="row-count">Rows: 17</div></div><div class="table-caption" id="Table12" data-table="Docu_0167_12" style="margin-top:2mm;margin-left:3mm;"><strong>Table 12</strong></div>
<div class="table-wrapper" data-table-id="table-12"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown — validator.py"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown — validator.py</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — validator.py"> <strong>File-level responsibilities</strong><br><br>Evaluate the canonicalized rows (from parsers) against the compiled ruleset and produce deterministic validation outcomes suitable for API responses, audit records, and automated fixes. Responsibilities: orchestrate rule selection, execute compiled conditions safely and efficiently, categorize findings (error/warning/info), support configurable auto-fixes, aggregate per-file and per-row results, expose a compact, stable result schema, and provide instrumentation hooks (metrics, timings). Keep business logic focused here; rule authoring and compilation belong to <code>rules_loader</code>. No I/O beyond reading passed objects and returning results. </td></tr><tr><td data-label="Technical Breakdown — validator.py"> <strong>Design principles</strong><br><br>— Pure functions where possible: validation functions accept domain objects and return structured results.<br>— Safe evaluation: never execute arbitrary code from rules; use compiled, sandboxed callables (produced by <code>rules_loader</code>) or JSON Logic adapters.<br>— Predictable ordering: deterministic rule application order (priority -> rule_id) to make results reproducible.<br>— Minimal per-row allocations: support vectorized and batch checks to scale to large files. </td></tr><tr><td data-label="Technical Breakdown — validator.py"> <strong>Primary types & return shapes</strong><br><br><code>ValidationResult</code> (dataclass): <code>{ validated: bool, errors: List[ValidationItem], warnings: List[ValidationItem], fixes_applied: List[FixItem], stats: {rows, errors, warnings, duration_ms}, per_row: Optional[Dict[row_id, List[ValidationItem]]} }</code><br><code>ValidationItem</code>: <code>{rule_id, severity, message, suggested_fix, field, row_id, meta, score?}</code><br><code>FixItem</code>: <code>{row_id, field, old_value, new_value, rule_id, applied_by}</code> </td></tr><tr><td data-label="Technical Breakdown — validator.py"> <strong>Primary public API (recommended)</strong><br><br>``<code>py\ndef validate_rows(rows: Iterable[Dict], ruleset: Ruleset, *, thresholds: Optional[dict]=None, max_rules_per_row: int=50, auto_fix: bool=False, strict: bool=False) -&gt; ValidationResult\n\ndef validate_row(row: Dict, candidate_rules: List[Rule], compiled_context: CompilerContext) -&gt; List[ValidationItem]\n\ndef apply_auto_fixes(rows: List[Dict], fixes: List[FixItem]) -&gt; List[Dict]\n\ndef summarize_results(result: ValidationResult) -&gt; dict\n</code>`` </td></tr><tr><td data-label="Technical Breakdown — validator.py"> <strong>Rule selection strategy</strong><br><br>— Use <code>ruleset.rules_by_field</code> to fetch candidate rules for fields present in the row, plus <code>global_rules</code>. <br>— Short-circuit: skip rules whose <code>applies_to</code> does not match the source (ebupot vs payroll). <br>— Enforce per-row <code>max_rules_per_row</code> and a short-circuit on first matching high-severity <code>error</code> when <code>settings.short_circuit_on_error=True</code>. </td></tr><tr><td data-label="Technical Breakdown — validator.py"> <strong>Condition execution & safety</strong><br><br>— Execute precompiled, sandboxed callables provided by <code>rules_loader</code> that accept a stable <code>RowContext</code> (limited API: field accessors, safe helpers like <code>decimal_compare</code>, <code>date_diff</code>, <code>in_list</code>).<br>— Protect each rule execution with a per-call timeout and exception handling; failing rule evaluation becomes a <code>ValidationItem</code> with severity <code>warning</code> or a <code>RuleEvaluationFailure</code> logged and surfaced as <code>meta</code>. Do not allow IO, filesystem, or network access in rule execution. </td></tr><tr><td data-label="Technical Breakdown — validator.py"> <strong>Auto-fix semantics</strong><br><br>— Rules may include <code>auto_fix</code> metadata with deterministic transformations (e.g., normalize NPWP, coerce numeric). <br>— <code>apply_auto_fixes</code> runs only after all validations are evaluated (so fixes don't mask other rule detections unexpectedly), records <code>FixItem</code> entries, and returns modified rows. <br>— Auto-fix must be idempotent and reversible where possible (record old_value). Respect <code>auto_fix</code> flag at call-time and <code>settings.strict</code> to decide whether an auto-fix failure is fatal. </td></tr><tr><td data-label="Technical Breakdown — validator.py"> <strong>Batching & performance</strong><br><br>— Support processing in batches (iterator input) to reduce memory pressure. <br>— Use vectorized checks for simple rules (presence, regex match, numeric range) using pandas when row set is a DataFrame; fall back to per-row evaluation for complex conditions. <br>— Cache compiled rule callables and helper objects; precompute rule candidates per-batch to avoid per-row lookups. <br>— Instrument metrics: <code>validation_latency_ms</code>, <code>rules_executions</code>, <code>rules_failed</code>, <code>rows_per_second</code>. </td></tr><tr><td data-label="Technical Breakdown — validator.py"> <strong>Aggregation & output</strong><br><br>— Return per-row items plus aggregated summary (<code>errors</code>, <code>warnings</code>, counts). <br>— Provide a <code>to_transport()</code> helper that converts internal <code>ValidationResult</code> into <code>schemas.ValidateResponseV1</code> shape with safe redaction (<code>to_safe_dict()</code>) to ensure API layer uses sanitized payloads. </td></tr><tr><td data-label="Technical Breakdown — validator.py"> <strong>Error handling & strict modes</strong><br><br>— Row-level rule evaluation errors: record as <code>ValidationItem</code> with <code>severity=warning</code> and <code>meta={&#x27;rule_exec_error&#x27;: str(...)}</code>; do not abort whole job unless <code>strict=True</code> and the error is critical. <br>— Fatal conditions (invalid ruleset, corrupted row structure): raise <code>ValidationFailure</code> (domain exception) with clear <code>meta</code>. <br>— Allow <code>settings.fail_on_first_error</code> toggle for interactive mode. </td></tr><tr><td data-label="Technical Breakdown — validator.py"> <strong>Audit & observability hooks</strong><br><br>— Emit structured audit events via <code>audit_service</code> for aggregated validation results (counts, top failed rules, sample row_ids). <br>— Provide per-row sampling (configurable) that sends full sanitized context to audit sink for postmortem. <br>— Ensure trace_id is propagated in events and metric tags. </td></tr><tr><td data-label="Technical Breakdown — validator.py"> <strong>Concurrency & safety</strong><br><br>— Validator functions must be thread-safe; avoid global mutable state. <br>— For parallel evaluation, partition batches and ensure compiled rule callables are re-entrant. <br>— Use a bounded worker pool for parallel per-row evaluations to avoid CPU oversubscription. </td></tr><tr><td data-label="Technical Breakdown — validator.py"> <strong>Integration points</strong><br><br>— Inputs: canonical rows (parsers), compiled <code>Ruleset</code> (rules_loader). <br>— Outputs: <code>ValidationResult</code> to <code>api</code> layer and <code>exporter</code>, <code>FixItem</code>s to <code>reconciler</code> when reconciliation can use canonicalized corrected rows, and audit events to <code>audit_service</code>. <br>— Health: validator readiness depends on <code>ruleset</code> loaded; expose <code>rules_version</code> in health metadata. </td></tr><tr><td data-label="Technical Breakdown — validator.py"> <strong>Testing guidance</strong><br><br>— Unit tests: single-row checks for every rule type (presence, regex, numeric, date logic), auto-fix correctness and idempotency, rule evaluation failure handling. <br>— Integration tests: batch validation with sample ebupot/payroll files asserting expected error counts and sample messages. <br>— Regression tests: a canonical dataset with assertions protected in CI to detect accidental rule-result changes. Include performance smoke tests to assert throughput. </td></tr><tr><td data-label="Technical Breakdown — validator.py"> <strong>Example pseudocode</strong><br><br>``<code>py\ndef validate_rows(rows, ruleset, auto_fix=False):\n    compiled = compile_context(ruleset)\n    summary = Summary()\n    for batch in chunk(rows, size=settings.batch_size):\n        candidates = preselect_rules(batch, ruleset)\n        for row in batch:\n            items = validate_row(row, candidates[row[&#x27;row_id&#x27;]], compiled)\n            summary.collect(items)\n            if auto_fix:\n                fixes = plan_fixes(items)\n                apply_auto_fixes([row], fixes)\n    return build_validation_result(summary)\n</code>`` </td></tr><tr><td data-label="Technical Breakdown — validator.py"> <strong>Extensibility notes</strong><br><br>— Add specialized vectorized rule evaluators (e.g., tax-sum invariants) as separate modules and register them via plugin hooks. <br>— Support <code>explain</code> mode that returns rule-evaluation traces for a row (useful for reviewer UI) but gate it behind dev/debug settings to avoid heavy payloads. <br>— Keep rule-evaluation helpers small and testable; avoid embedding domain heuristics directly here — prefer small helper modules imported by the validator. </td></tr></tbody></table></div><div class="row-count">Rows: 16</div></div><div class="table-caption" id="Table13" data-table="Docu_0167_13" style="margin-top:2mm;margin-left:3mm;"><strong>Table 13</strong></div>
<div class="table-wrapper" data-table-id="table-13"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown — matcher.py"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown — matcher.py</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — matcher.py"> <strong>File-level responsibilities</strong><br><br>Produce deterministic candidate matches between ebupot rows and payroll rows. Responsibilities: fast candidate generation, multi-signal scoring, configurable thresholds, tie-breaking, deterministic ordering, privacy-safe output (no raw PII in logs), and explainable match reasons for reviewer UI. Keep this module focused on matching logic and scoring; persistence, orchestration, and exports belong to <code>reconciler.py</code> / services. Provide small, testable functions and avoid side-effects at import time. </td></tr><tr><td data-label="Technical Breakdown — matcher.py"> <strong>Design goals & constraints</strong><br><br>— Precision-first: prefer high-precision matches (exact NPWP/employee_id) and fall back to fuzzy methods only when configured.<br>— Determinism: identical inputs → identical candidate order and scores (seed any non-deterministic components).<br>— Explainability: every match must include a traceable score breakdown (signals and weights) and <code>match_reason</code> tokens.<br>— Performance: candidate pre-filtering to avoid O(n²) comparisons for large files (indexing, blocking). </td></tr><tr><td data-label="Technical Breakdown — matcher.py"> <strong>Primary public API (recommended)</strong><br><br>``<code>py\ndef build_indices(payroll_rows: Iterable[Dict], *, settings) -&gt; Indices\n\ndef generate_candidates(eb_row: Dict, indices: Indices, top_k: int = 10) -&gt; List[Candidate]\n\ndef score_candidate(eb_row: Dict, payroll_row: Dict, weights: Optional[Dict]=None) -&gt; CandidateScore\n\ndef match_batch(eb_rows: Iterable[Dict], payroll_rows: Iterable[Dict], *, settings) -&gt; Iterator[MatchResult]\n\ndef explain_candidate(candidate: Candidate) -&gt; Dict  # returns score breakdown and matched fields\n</code>`` </td></tr><tr><td data-label="Technical Breakdown — matcher.py"> <strong>Indexing & blocking strategies</strong><br><br>Build lightweight indices to reduce comparisons: exact maps (<code>npwp_map</code>, <code>employee_id_map</code>), token-based inverted index on normalized names (<code>name_token_index</code>), hashed phonetic index (<code>soundex_key</code> or <code>metaphone_key</code>), and numeric ranges for amount-based blocking (gross salary buckets). Provide a <code>blocking_strategy</code> abstraction to allow multiple strategies and allow configuration of which to run and in what order. Indices must be memory-efficient and support incremental updates for streaming runs. </td></tr><tr><td data-label="Technical Breakdown — matcher.py"> <strong>Signals & features used for scoring</strong><br><br>Typical signals (each normalized and documented):<br>— <code>npwp_exact</code> (bool) — highest weight.<br>— <code>employee_id_exact</code> (bool).<br>— <code>name_similarity</code> (float 0..1) — token-set ratio / Jaro-Winkler.<br>— <code>name_phonetic_match</code> (bool) — metaphone/soundex equality.<br>— <code>date_overlap</code> (bool/score) — payroll period vs tax period.<br>— <code>salary_ratio</code> (float closeness) — relative difference between gross or taxable amounts.<br>— <code>company_unit</code> (bool) — department or branch match if available.<br>— <code>row_proximity</code> (metadata) — same file import batch hints.<br>Signals are numeric/floating; normalize to consistent ranges for weighted sum. </td></tr><tr><td data-label="Technical Breakdown — matcher.py"> <strong>Scoring model & configuration</strong><br><br>Score = Σ(weight_i × feature_i) + bias. Provide default weights that emphasize <code>npwp_exact</code> and <code>employee_id_exact</code>. Make weight set configurable via <code>settings</code> or UI per-run. Support both linear scoring and pluggable comparator (e.g., logistic model) for later improvements. Expose a function to compute normalized confidence [0..1] from raw score. </td></tr><tr><td data-label="Technical Breakdown — matcher.py"> <strong>Thresholding & decisions</strong><br><br>Provide three outcome classes for each eb_row: <code>MATCH (single high-confidence)</code>, <code>AMBIGUOUS (multiple candidates above lower threshold)</code>, <code>NO_MATCH (none above threshold)</code>. Configurable thresholds: <code>match_threshold</code> and <code>ambiguous_threshold</code>. Support <code>secondary_checks</code> (e.g., require salary_ratio < X for final match) to reduce false positives. </td></tr><tr><td data-label="Technical Breakdown — matcher.py"> <strong>Tie-breaking & deterministic order</strong><br><br>When multiple candidates have equal scores, break ties with deterministic rules: prefer exact NPWP, then higher name_similarity, then smaller salary_ratio, then lower payroll_row_id (stable). Document tie-break order and tests to guarantee determinism. </td></tr><tr><td data-label="Technical Breakdown — matcher.py"> <strong>Explainability & UI payloads</strong><br><br><code>Candidate</code> objects include <code>score</code>, <code>confidence</code>, <code>matched_fields</code> (list), and <code>score_breakdown</code> (per-feature contribution). <code>explain_candidate()</code> returns human-friendly tokens (e.g., "NPWP exact", "name similarity 0.92", "salary difference 3%") for review UI. Limit returned PII in UI payloads unless user has permission; use hashed IDs for audit when needed. </td></tr><tr><td data-label="Technical Breakdown — matcher.py"> <strong>Auto-accept / auto-reject rules</strong><br><br>Support fast-path rules: e.g., if <code>npwp_exact</code> and <code>salary_ratio &lt; 0.02</code> then <code>auto_accept=True</code>. These rules are configurable and separate from scoring thresholds; log auto actions and allow reviewer override. Auto-reject for clear conflicts (same NPWP but contradictory payroll entity) should surface high-severity warnings. </td></tr><tr><td data-label="Technical Breakdown — matcher.py"> <strong>Fuzzy matching details</strong><br><br>— Name similarity: token_set_ratio (fuzzywuzzy-like) and Jaro-Winkler for short names; normalize diacritics and common honorifics.<br>— Phonetic keys: metaphone or Double Metaphone for Indonesian names; store fallback for languages where phonetics are less useful.<br>— Configurable similarity functions with deterministic behavior (no randomized seeding). </td></tr><tr><td data-label="Technical Breakdown — matcher.py"> <strong>Privacy & logging</strong><br><br>Never log raw NPWP or full names. When logging, record only masked values (e.g., <code>xxx-xx-1234</code>) or hashed identifiers. Audit entries include <code>match_decision</code>, <code>matched_candidate_id</code>, <code>score</code>, <code>rules_version</code>, and <code>trace_id</code>. Store minimal sanitized data for reproducibility. </td></tr><tr><td data-label="Technical Breakdown — matcher.py"> <strong>Performance & scaling</strong><br><br>— Precompute indices once per job; reuse across batch matching. <br>— Use vectorized operations for name token intersection when possible; fallback to efficient C-accelerated libs (python-levenshtein) with lazy import. <br>— For very large corpora, support sharding by payroll unit or hash-range and run matching per-shard. <br>— Provide <code>max_candidates_per_row</code> guard to avoid explosion. </td></tr><tr><td data-label="Technical Breakdown — matcher.py"> <strong>Testing & quality checks</strong><br><br>— Unit tests: individual signal calculators, index building, deterministic tie-breaking, scoring math with weights, and explain payload shape. <br>— Integration tests: sample payroll + ebupot datasets asserting expected matches/ambiguous/no-match counts and stable results across repeated runs. <br>— Edge cases: identical names, missing NPWP, duplicates in payroll, contractors without NPWP, and international name variations. </td></tr><tr><td data-label="Technical Breakdown — matcher.py"> <strong>Integration with reconciler</strong><br><br><code>reconciler.py</code> calls <code>build_indices()</code> then <code>match_batch()</code>; it receives <code>MatchResult</code> items and applies reconciliation logic (apply fixes, produce reports). Matcher must be agnostic to persistence and return lightweight serializable objects for the reconciler to consume. Provide clear error signalling for inability to match (raise <code>ReconciliationAmbiguous</code> only at higher orchestration layer when appropriate). </td></tr><tr><td data-label="Technical Breakdown — matcher.py"> <strong>Observability & metrics</strong><br><br>Emit metrics: <code>matcher.rows_processed</code>, <code>matcher.avg_candidates_per_row</code>, <code>matcher.match_rate</code>, <code>matcher.ambiguous_rate</code>, and histogram of <code>candidate_confidence</code>. Record <code>rules_version</code> and <code>runtime_fingerprint</code> as tags. </td></tr><tr><td data-label="Technical Breakdown — matcher.py"> <strong>Example pseudocode</strong><br><br>``<code>py\nindices = build_indices(payroll_rows, settings)\nfor eb in eb_rows:\n    candidates = generate_candidates(eb, indices, top_k=10)\n    scored = [score_candidate(eb, p, weights) for p in candidates]\n    scored.sort(key=lambda s: (-s.confidence, s.payroll_row_id))\n    if scored and scored[0].confidence &gt;= settings.match_threshold:\n        yield MatchResult(eb_row_id=eb.row_id, decision=&#x27;MATCH&#x27;, best=scored[0], candidates=scored[:5])\n    elif any(s.confidence &gt;= settings.ambiguous_threshold for s in scored):\n        yield MatchResult(..., decision=&#x27;AMBIGUOUS&#x27;, candidates=scored[:10])\n    else:\n        yield MatchResult(..., decision=&#x27;NO_MATCH&#x27;)\n</code>`` </td></tr><tr><td data-label="Technical Breakdown — matcher.py"> <strong>Extensibility</strong><br><br>— Allow plugin comparator functions (ML model scorer) behind a clear interface for later upgrade. <br>— Add custom blocking strategies (business-key based) per customer profile. <br>— Expose a small CLI to run matching on sample files for debugging and CI smoke tests. </td></tr></tbody></table></div><div class="row-count">Rows: 18</div></div><div class="table-caption" id="Table14" data-table="Docu_0167_14" style="margin-top:2mm;margin-left:3mm;"><strong>Table 14</strong></div>
<div class="table-wrapper" data-table-id="table-14"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown — reconciler.py"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown — reconciler.py</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — reconciler.py"> <strong>File-level responsibilities</strong><br><br>Orchestrate the end-to-end reconciliation workflow: take parsed & validated ebupot and payroll rows, call the matcher to produce candidate matches, apply reconciliation rules and auto-fixes, surface ambiguous cases for human review, produce final cleaned exports and reconciliation reports, and emit audit events. This module coordinates business logic (decisioning, conflict resolution, approval gating), persistence of job state/results, and safe side-effects (exports, audit). Keep reconciliation logic testable and side-effect-free where possible: side-effects (writes, notifications) should be encapsulated in services. </td></tr><tr><td data-label="Technical Breakdown — reconciler.py"> <strong>Primary concepts & types</strong><br><br>— <code>ReconcileJob</code> (domain): job_id, submitter, inputs (file_ids), settings, status (queued/running/needs_review/success/failed), created_at, updated_at.<br>— <code>MatchResult</code> (from matcher): eb_row_id, decision (MATCH/AMBIGUOUS/NO_MATCH), candidates, best_candidate.<br>— <code>ReconcileDecision</code>: canonical representation for each eb_row: final_status (accepted/rejected/manual), matched_payroll_row_id, applied_fixes, confidence, reason_codes.<br>— <code>ReconciliationResult</code>: job-level aggregation (counts, lists), per-row decisions, export_paths, audit_refs. </td></tr><tr><td data-label="Technical Breakdown — reconciler.py"> <strong>Primary public functions (recommended)</strong><br><br>``<code>py\ndef start_reconcile_job(job_request: ReconcileRequest) -&gt; ReconcileJob\n\ndef run_reconcile(job_id: str) -&gt; ReconciliationResult\n\ndef reconcile_batch(job: ReconcileJob, eb_rows: Iterator, payroll_rows: Iterator) -&gt; Iterator[ReconcileDecision]\n\ndef apply_decision(decision: ReconcileDecision, *, persist: bool=True) -&gt; None\n\ndef finalize_job(job_id: str, result: ReconciliationResult) -&gt; None\n</code>`` </td></tr><tr><td data-label="Technical Breakdown — reconciler.py"> <strong>High-level workflow</strong><br><br>1. Validate job request and persist <code>ReconcileJob</code> record (idempotency keyed by checksum/idempotency_key).<br>2. Load canonical rows (streaming where possible).<br>3. Build indices via <code>matcher.build_indices()</code>.<br>4. Iterate eb_rows → <code>matcher.generate_candidates()</code> → score & convert to <code>MatchResult</code>.<br>5. For each <code>MatchResult</code> produce <code>ReconcileDecision</code> using rules: auto-accept, auto-reject, ambiguous (needs review), or no-match (create task).<br>6. Optionally apply <code>validator.apply_auto_fixes()</code> before finalization.<br>7. Persist per-row decisions and interim artifacts; emit audit events.<br>8. When human review required, pause job with <code>needs_review</code> status and expose review endpoints. After approval, <code>finalize_job</code> generates exports and reconciliation report(s). </td></tr><tr><td data-label="Technical Breakdown — reconciler.py"> <strong>Decision rules & priority</strong><br><br>— Use explicit precedence: exact NPWP + salary check → auto-accept; ambiguous high-similarity → needs_review; conflicting high-confidence matches → escalate (needs_review).<br>— Allow <code>ruleset</code> priorities and <code>settings</code> overrides (e.g., treat warnings as errors in strict mode).<br>— Keep rule decision paths deterministic and log rationale (rule_ids, score_breakdown) for each decision. </td></tr><tr><td data-label="Technical Breakdown — reconciler.py"> <strong>Auto-fixes & transformations</strong><br><br>— Support deterministic, idempotent auto-fixes (normalize NPWP, coerce numeric formats, fill inferred fields).<br>— Record <code>FixItem</code> with <code>old_value</code> and <code>new_value</code> and who/what applied it (auto vs reviewer).<br>— Auto-fixes must be reversible (store old values) and recorded in audit for rollback. </td></tr><tr><td data-label="Technical Breakdown — reconciler.py"> <strong>Human review / approval flow</strong><br><br>— Provide a <code>needs_review</code> job state and an API surface for reviewers to inspect candidate rows with <code>explain_candidate()</code> output and suggested fixes. <br>— Review actions: accept suggested match & fixes, reject & select alternate candidate, mark for manual correction, or flag as not-applicable. <br>— Support batch-review operations (accept top-k, accept by filter). Ensure review actions are idempotent and recorded. </td></tr><tr><td data-label="Technical Breakdown — reconciler.py"> <strong>Persistence & idempotency</strong><br><br>— Persist job state and decisions to durable store (sqlite for MVP, optional remote DB later). <br>— Use idempotency-key + checksums to detect duplicate ingest or re-run requests and return existing job instead of duplicating work. <br>— Support resume semantics: jobs should resume from last committed checkpoint to survive process restarts. </td></tr><tr><td data-label="Technical Breakdown — reconciler.py"> <strong>Transactions & atomicity</strong><br><br>— Group multi-step writes (persist decision + audit + export pointer) in a transaction to keep state consistent. <br>— For file exports, write to a temp path and perform atomic move into <code>outputs/</code> on success. <br>— Ensure partial failures (export fail, disk full) do not mark job <code>success</code>; mark <code>failed</code> with clear <code>meta</code> for operator action. </td></tr><tr><td data-label="Technical Breakdown — reconciler.py"> <strong>Outputs & report formats</strong><br><br>— CSV/XLSX cleaned e-Bupot export (compliant column order).<br>— Reconciliation report (Excel) with sheets: summary, matched, mismatched, suggested-fixes, audit-log. <br>— Machine-readable JSON/NDJSON for downstream consumption (job metadata, per-row decisions).<br>— Expose signed URLs or local paths via <code>exporter</code> and record in <code>ReconciliationResult</code>. </td></tr><tr><td data-label="Technical Breakdown — reconciler.py"> <strong>Audit & observability</strong><br><br>— Emit audit events for job lifecycle, per-row decisions, auto-fixes applied, and reviewer actions via <code>audit_service</code>. Include <code>trace_id</code>, <code>user_id</code>, <code>job_id</code>, <code>rules_checksum</code>. <br>— Metrics to emit: <code>reconcile.jobs_started</code>, <code>jobs_succeeded</code>, <code>rows_processed</code>, <code>match_rate</code>, <code>ambiguous_rate</code>, <code>avg_decision_time</code>. <br>— Log rationale for automated decisions and summary counts (no PII in logs). </td></tr><tr><td data-label="Technical Breakdown — reconciler.py"> <strong>Error handling & retries</strong><br><br>— Distinguish transient failures (IO, temporary DB lock) from fatal business errors (invalid file). <br>— Implement exponential backoff for transient operations and mark job <code>degraded</code> if external services unreachable. <br>— Provide actionable error messages and operator hints in job <code>meta</code>. </td></tr><tr><td data-label="Technical Breakdown — reconciler.py"> <strong>Security & PII handling</strong><br><br>— Never include raw NPWP, full names, or bank details in logs; use masked values or stable hashes when needed for correlation. <br>— Restrict export downloads to authorized users and generate short-lived signed links if exposing over HTTP. <br>— Ensure stored audit logs containing raw PII are protected (access controls) and their retention follows policy. </td></tr><tr><td data-label="Technical Breakdown — reconciler.py"> <strong>Performance & scaling</strong><br><br>— Stream inputs and persist decisions incrementally to avoid high memory usage. <br>— Limit concurrency with bounded worker pools for match scoring and fix application. <br>— For large datasets, support sharding by payroll unit or hash ranges and reconcile shards in parallel. <br>— Provide <code>preview</code> mode for quick checks (process small sample instead of full job). </td></tr><tr><td data-label="Technical Breakdown — reconciler.py"> <strong>Testing guidance</strong><br><br>— Unit tests: decision logic for each rule path, auto-fix idempotency, transaction semantics (commit/rollback), and resume behavior from checkpoint. <br>— Integration tests: full flow ingest→match→reconcile→export with sample datasets ensuring exported file structure & counts match expectations. <br>— Acceptance tests: human-review flows (accept/reject), idempotent re-run behavior, and failure injection tests (disk full, DB lock) to verify graceful handling. </td></tr><tr><td data-label="Technical Breakdown — reconciler.py"> <strong>APIs & integration points</strong><br><br>— Called by <code>api.v1</code> endpoints for <code>POST /reconcile</code> and job polling endpoints. <br>— Uses <code>matcher</code>, <code>validator</code> (for pre/post checks), <code>exporter</code> (for outputs), <code>file_service</code> (for reads/writes), and <code>audit_service</code>. <br>— Exposes small helper functions for UI to fetch per-row explain data and to apply reviewer decisions. </td></tr><tr><td data-label="Technical Breakdown — reconciler.py"> <strong>Operational guardrails</strong><br><br>— Provide a <code>dry_run</code> flag to produce reports without writing exports or mutating persistent state (useful for previews and CI). <br>— Provide <code>max_rows</code> and <code>max_runtime</code> limits to prevent runaway jobs; surface helpful messages when thresholds reached. <br>— Document runbook for manual remediation: how to re-run job from checkpoint, how to rehydrate audit logs, and how to rerun after changing rules (rules_version mismatches must be recorded). </td></tr></tbody></table></div><div class="row-count">Rows: 17</div></div><div class="table-caption" id="Table15" data-table="Docu_0167_15" style="margin-top:2mm;margin-left:3mm;"><strong>Table 15</strong></div>
<div class="table-wrapper" data-table-id="table-15"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown — exporter.py"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown — exporter.py</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — exporter.py"> <strong>File-level responsibilities</strong><br><br>Create, format, and deliver final artifacts produced by the reconciliation pipeline. Responsibilities: convert canonical rows and reconciliation decisions into consumer-ready exports (cleaned e-Bupot CSV/XLSX, reconciliation Excel report, NDJSON/JSON summary), stream large exports without excessive memory use, perform atomic writes and checksum generation, produce human-readable summary sheets, attach audit metadata, and provide secure download pointers (local path or signed URL via <code>file_service</code>). Keep business logic minimal: formatting, packaging, and safety checks only. Side-effecting operations (writes, uploads) should be explicit and testable. </td></tr><tr><td data-label="Technical Breakdown — exporter.py"> <strong>Design principles</strong><br><br>— Deterministic outputs: same input + rules_version → same export file checksum. <br>— Atomicity: write to temp paths and move atomically to outputs/ on success. <br>— Streaming: support streaming responses for large CSV/XLSX to avoid OOM. <br>— Traceability: include <code>export_metadata.json</code> with <code>job_id</code>, <code>rules_checksum</code>, <code>created_at</code>, <code>row_counts</code>, and producer <code>runtime_fingerprint</code>. <br>— Safe defaults: produce RFC-compliant CSV (UTF-8 with BOM optional), XLSX with typed columns, and NDJSON for machine consumption. </td></tr><tr><td data-label="Technical Breakdown — exporter.py"> <strong>Primary public API (recommended)</strong><br><br>``<code>py\ndef export_cleaned_ebupot(rows: Iterable[Dict], output_path: str, format: Literal[&#x27;csv&#x27;,&#x27;xlsx&#x27;]=&#x27;csv&#x27;, *, columns: Optional[List[str]]=None, stream: bool=False) -&gt; ExportResult\ndef export_reconciliation_report(decisions: Iterable[ReconcileDecision], output_path: str, *, include_sheets: Sequence[str]=(&#x27;summary&#x27;,&#x27;matched&#x27;,&#x27;mismatched&#x27;,&#x27;fixes&#x27;,&#x27;audit&#x27;), preview_rows:int=50) -&gt; ExportResult\ndef stream_export_csv(rows_iter: Iterator[Dict], fieldnames: List[str]) -&gt; StreamingResponse\ndef generate_export_package(job_id: str, artifacts: Dict[str,str]) -&gt; str  # returns package path (zip)\ndef compute_export_checksum(path: str) -&gt; str\ndef prepare_export_paths(job_id: str) -&gt; Dict[str,str]\n</code>`` </td></tr><tr><td data-label="Technical Breakdown — exporter.py"> <strong>Export artifact types</strong><br><br>— <code>cleaned_ebupot.{csv|xlsx}</code>: canonical columns, strictly ordered, validated against <code>rules/exports_spec</code> if present. <br>— <code>reconciliation_report.xlsx</code>: multi-sheet Excel with summary, matched, ambiguous, missing, suggested fixes, and audit-log (timestamps and non-PII). <br>— <code>reconciliation_summary.json</code> / <code>ndjson</code> per-row decisions. <br>— <code>export_metadata.json</code>: provenance (rules_checksum, settings, runtime_fingerprint). <br>— Optional <code>package.zip</code> containing all above. </td></tr><tr><td data-label="Technical Breakdown — exporter.py"> <strong>Formatting rules & compliance</strong><br><br>— CSV: UTF-8 (BOM optional), quoted where necessary, newline <code>\n</code>. Provide <code>dialect</code> options for downstream systems. <br>— XLSX: use typed cells (dates as ISO date types, decimals as numeric). Ensure column headers and required order follow e-Bupot spec. <br>— Schema validation: before finalizing, validate exported CSV/XLSX contains required columns and types; failing validation should abort export and surface detailed diagnostics. </td></tr><tr><td data-label="Technical Breakdown — exporter.py"> <strong>Streaming & memory management</strong><br><br>— Implement <code>stream_export_csv</code> that writes rows progressively to response or temporary file using buffered writer and flush checkpoints. <br>— For XLSX use <code>openpyxl</code>/<code>xlsxwriter</code> in streaming mode or write CSV and convert to XLSX via small in-memory buffers only for small jobs; for large jobs prefer streaming CSV and document XLSX limitations. <br>— Support chunked write sizes and temporary file rotation for very large exports. </td></tr><tr><td data-label="Technical Breakdown — exporter.py"> <strong>Atomic write & durability</strong><br><br>— Write to <code>tmp/&lt;job_id&gt;.&lt;ext&gt;.part</code> then atomically <code>os.replace()</code> to <code>outputs/&lt;finalname&gt;</code>. <br>— After successful write compute checksum and write alongside <code>export_metadata.json</code>. <br>— Provide configurable retention/cleanup behavior and integrate with file_service for persistence (local or S3). </td></tr><tr><td data-label="Technical Breakdown — exporter.py"> <strong>Security & PII handling</strong><br><br>— Exports may contain PII; by default redact or mask sensitive fields in human-facing report sheets unless <code>settings.allow_full_pii_export=True</code> and caller authorized. <br>— When returning download URLs, use short-lived signed URLs if using remote storage. <br>— Ensure directory permissions limit access; do not expose raw filesystem paths to unprivileged users. </td></tr><tr><td data-label="Technical Breakdown — exporter.py"> <strong>Audit & metadata</strong><br><br>— Record export actions via <code>audit_service</code> with <code>job_id</code>, <code>user_id</code>, artifact paths, <code>rules_checksum</code>, counts (rows exported, matched, ambiguous), and <code>export_checksum</code>. <br>— Attach <code>created_by</code> and <code>created_at</code> fields in <code>export_metadata.json</code>. </td></tr><tr><td data-label="Technical Breakdown — exporter.py"> <strong>Retries & resumability</strong><br><br>— For transient failures during writing (disk full, network error), implement coherent retry/backoff for remote writes; for local writes ensure partial <code>.part</code> files are cleaned and job marked <code>failed</code> with diagnostics. <br>— Support resumable upload hooks when uploading to remote stores (S3 multipart) through <code>file_service</code>. </td></tr><tr><td data-label="Technical Breakdown — exporter.py"> <strong>Edge cases & governance</strong><br><br>— Enforce a maximum export size threshold (<code>MAX_EXPORT_ROWS</code>) and a configurable <code>preview_mode</code> that outputs first N rows for UI. <br>— If export schema changes between runs, add <code>schema_version</code> to metadata and avoid overwriting previous exports unless explicit <code>overwrite=True</code>. </td></tr><tr><td data-label="Technical Breakdown — exporter.py"> <strong>Integration & wiring</strong><br><br>— Caller (reconciler) provides canonical rows / decisions plus job context. <br>— Use <code>file_service</code> to persist or upload final artifacts; exporter should not directly manage low-level storage beyond temp-file handling. <br>— Use <code>audit_service</code> to log completion and to attach metadata to job records. </td></tr><tr><td data-label="Technical Breakdown — exporter.py"> <strong>Testing guidance</strong><br><br>— Unit tests: CSV/XLSX generation for small canonical inputs, checksum determinism tests, atomic write semantics (simulate .part to final move), redaction behavior, and metadata correctness. <br>— Integration tests: full pipeline run producing artifacts, validate schema of outputs, and confirm audit events are emitted. <br>— Streaming tests: simulate large dataset using generator and assert memory stays bounded and streaming response is valid. </td></tr><tr><td data-label="Technical Breakdown — exporter.py"> <strong>Performance & monitoring</strong><br><br>— Instrument export durations, bytes written, row throughput, and checksum calculation time. <br>— Monitor temp disk usage and queue length for concurrent exports. <br>— Provide simple throttling for concurrent export jobs in settings. </td></tr><tr><td data-label="Technical Breakdown — exporter.py"> <strong>Error reporting & user feedback</strong><br><br>— On failure return structured diagnostics: <code>{error_code, message, hint, artifact_partial_path?, trace_id}</code>. <br>— Provide actionable hints (e.g., increase MAX_EXPORT_ROWS, check storage quotas) and include sample rows that triggered schema validation failures (sanitized). </td></tr><tr><td data-label="Technical Breakdown — exporter.py"> <strong>Extensibility</strong><br><br>— Exporter should be pluggable to add new formats (parquet, database load file) by implementing small <code>FormatWriter</code> interface. <br>— Allow custom report sheets to be injected by domain rules (e.g., tax-year-specific summary). </td></tr><tr><td data-label="Technical Breakdown — exporter.py"> <strong>Example pseudocode (summary)</strong><br><br>``<code>py\npaths = prepare_export_paths(job_id)\nwith atomic_temp_writer(paths[&#x27;cleaned_csv&#x27;]) as tmp:\n    for row in rows_iter:\n        write_csv_row(tmp, row)\nchecksum = compute_export_checksum(paths[&#x27;cleaned_csv&#x27;])\nwrite_metadata(paths[&#x27;export_meta&#x27;], {..., &#x27;checksum&#x27;: checksum})\nfile_service.persist(paths[&#x27;cleaned_csv&#x27;], dest)\naudit_service.record(&#x27;export.completed&#x27;, job_id, meta)\nreturn ExportResult(paths=..., checksum=checksum)\n</code>`` </td></tr></tbody></table></div><div class="row-count">Rows: 17</div></div><div class="table-caption" id="Table16" data-table="Docu_0167_16" style="margin-top:2mm;margin-left:3mm;"><strong>Table 16</strong></div>
<div class="table-wrapper" data-table-id="table-16"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown — audit_service.py"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown — audit_service.py</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — audit_service.py"> <strong>File-level responsibilities</strong><br><br>Provide a reliable, tamper-evident audit event service for all important actions (file uploads, validation results, reconciliation decisions, exports, config/rules changes, reviewer actions). Responsibilities: ingest audit events, normalize and sanitize payloads (remove PII unless explicitly allowed), persist events durably, support queries/exports for compliance, emit real-time forwards to telemetry or SIEM, and expose a small async-friendly API used by handlers and services. Keep side-effects explicit (connect/close) and allow pluggable backends (sqlite/local file/remote). </td></tr><tr><td data-label="Technical Breakdown — audit_service.py"> <strong>Design principles & guardrails</strong><br><br>— Immutable events: once written an event is append-only and versioned. <br>— Minimal PII in indexable fields; full PII allowed only in protected storage and with explicit policy flags. <br>— Idempotency: accept <code>event_id</code> or checksum to deduplicate repeated writes. <br>— Performance: non-blocking ingestion (batch or buffered writes) and backpressure policies to avoid blocking business flows. <br>— Auditability: include checksum, digital signature (optional), and <code>rules_version</code>/<code>app_runtime_fingerprint</code> in metadata for reproducibility. </td></tr><tr><td data-label="Technical Breakdown — audit_service.py"> <strong>Primary public API (recommended signatures)</strong><br><br>``<code>py\nclass AuditService:\n    async def connect(self) -&gt; None\n    async def aclose(self) -&gt; None\n    async def record(self, event: AuditEvent, *, sync: bool = False) -&gt; AuditRecord\n    async def record_batch(self, events: Iterable[AuditEvent]) -&gt; List[AuditRecord]\n    async def query(self, *, start_ts: Optional[datetime]=None, end_ts: Optional[datetime]=None, event_type: Optional[str]=None, job_id:Optional[str]=None, limit:int=100, cursor:Optional[str]=None) -&gt; AuditQueryResult\n    def export(self, query_params, format: Literal[&#x27;ndjson&#x27;,&#x27;csv&#x27;]) -&gt; str  # returns path or stream\n    def health_check(self) -&gt; dict\n</code>`` </td></tr><tr><td data-label="Technical Breakdown — audit_service.py"> <strong>Event shape & schema</strong><br><br><code>AuditEvent</code> (input schema): <code>{ event_id?: str, event_type: str, timestamp: ISO8601, actor_id?: str (hashed/masked), actor_role?: str, job_id?: str, file_id?: str, trace_id?: str, severity?: &#x27;info&#x27;|&#x27;warn&#x27;|&#x27;error&#x27;|&#x27;audit&#x27;, payload: dict (sanitized), rules_version?: str, app_fingerprint?: str }</code>.<br><code>AuditRecord</code> (persisted): extends <code>AuditEvent</code> with <code>persisted_at</code>, <code>storage_path</code>, <code>checksum</code>, <code>sequence_id</code>. Use strict JSON Schema validation on ingest and provide helpful error messages. </td></tr><tr><td data-label="Technical Breakdown — audit_service.py"> <strong>Sanitization & PII handling</strong><br><br>— Before persisting, run <code>sanitize_audit_payload()</code> to remove or mask NPWP, bank account, full names, and other sensitive fields unless <code>allow_sensitive=True</code> and caller is authorized. <br>— Provide <code>redaction_level</code> option (minimal, standard, full) controlled by settings and per-event override for exceptional ops (documented and audited). <br>— Keep a secure, access-controlled store for PII-inclusive raw payloads if required by compliance; record pointers in the main audit index. </td></tr><tr><td data-label="Technical Breakdown — audit_service.py"> <strong>Backends & persistence</strong><br><br>Support pluggable backends via <code>AuditBackend</code> interface:<br>— <code>SqliteAuditBackend</code> (MVP): append-only table with JSON payload column, indexed by <code>event_type</code>, <code>timestamp</code>, <code>job_id</code>, <code>file_id</code>. Use WAL mode and periodic VACUUM/compaction policies. <br>— <code>FileAuditBackend</code>: NDJSON files partitioned by date (useful for S3 archival). <br>— <code>RemoteAuditBackend</code>: forward to remote logging/ELK/Splunk/OTel exporter with local buffering and fallback. <br>— Provide <code>NullAuditBackend</code> for tests. </td></tr><tr><td data-label="Technical Breakdown — audit_service.py"> <strong>Durability, consistency & ordering</strong><br><br>— Use monotonic <code>sequence_id</code> per backend instance for ordering. <br>— For SQLite use transactions; for remote backends persist to local durable queue first then forward. <br>— Record <code>event_checksum</code> (sha256 of canonical JSON) and include it in <code>AuditRecord</code>. Optionally sign events with a service key when <code>settings.audit_signing=True</code>. </td></tr><tr><td data-label="Technical Breakdown — audit_service.py"> <strong>Ingestion semantics</strong><br><br>— <code>record(event, sync=False)</code>: default async, returns immediately after enqueue; <code>sync=True</code> forces durable write and returns <code>AuditRecord</code>. <br>— <code>record_batch</code> supports bulk ingestion with idempotency checks and transactional semantics for the batch where backend supports it. <br>— Enforce max payload size and reject overly large events with clear diagnostics. </td></tr><tr><td data-label="Technical Breakdown — audit_service.py"> <strong>Query & export features</strong><br><br>— Provide cursor-based pagination for queries. <br>— Allow filter by time range, event_type, actor_id (hashed), job_id, file_id, and severity. <br>— Export supports streaming NDJSON or CSV with optional redaction. Exports must append <code>export_metadata.json</code> describing query parameters, rules_version, and checksum. </td></tr><tr><td data-label="Technical Breakdown — audit_service.py"> <strong>Real-time forwarding & integration</strong><br><br>— Optionally forward events to telemetry (OTel) or SIEM using background workers; failures to forward should not block ingestion—use a bounded retry queue. <br>— Expose application hookpoints for services to subscribe to events (in-process callback or external webhook) for live dashboards. </br>— Provide a lightweight webhook signer/verifier for external consumers. </td></tr><tr><td data-label="Technical Breakdown — audit_service.py"> <strong>Access control & protection</strong><br><br>— Ensure read/export endpoints require appropriate roles (auditor/admin). <br>— Protect raw PII stores with ACLs and log every access to PII-inclusive records in audit (meta-audit). <br>— Rotate signing keys and provide key-id metadata in event records when signing is enabled. </td></tr><tr><td data-label="Technical Breakdown — audit_service.py"> <strong>Retention, compaction & archival</strong><br><br>— Implement configurable retention policies: keep <code>index</code> (summary) indefinitely and raw payloads for a shorter window (configurable). <br>— Provide compaction job to collapse redundant or low-value events (policy-driven). <br>— Support archival to S3/OBJECT storage in compressed NDJSON partitions and verify checksums after upload. </td></tr><tr><td data-label="Technical Breakdown — audit_service.py"> <strong>Operational & health endpoints</strong><br><br>— <code>health_check()</code> returns backend connectivity, queue depth, last persisted sequence_id, and recent failure counts. <br>— Provide admin APIs for <code>reindex</code>, <code>repair_checksums</code>, and <code>prune</code> (delete/archive) guarded by settings and admin auth. </td></tr><tr><td data-label="Technical Breakdown — audit_service.py"> <strong>Observability & metrics</strong><br><br>— Emit metrics: <code>audit.events.ingested</code>, <code>audit.events.failed</code>, <code>audit.queue.depth</code>, <code>audit.export.duration</code>, <code>audit.forward_failures</code>. Tag metrics with <code>env</code>, <code>rules_version</code>, and <code>service</code>. Provide counters for redaction_count and PII_access_count. </td></tr><tr><td data-label="Technical Breakdown — audit_service.py"> <strong>Testing guidance</strong><br><br>— Unit tests: schema validation, sanitization/redaction logic, idempotency, event checksum determinism, API error handling. <br>— Integration tests: backend persistence (Sqlite), batch writes, query pagination, export correctness with redaction, and health-check behavior. <br>— Failure tests: simulate backend outage and assert local buffering + retry semantics. <br>— Security tests: verify PII is not present in indexable fields and that PII access is audited. </td></tr><tr><td data-label="Technical Breakdown — audit_service.py"> <strong>Performance & scalability</strong><br><br>— For high-throughput, use local durable queue + background batch writer to backend, size-bounded to avoid memory growth. <br>— Use prepared statements and indices in SQL backend; shard NDJSON files by date/job for fast reads. <br>— Benchmark ingestion path and set safe defaults for batch size and flush interval. </td></tr><tr><td data-label="Technical Breakdown — audit_service.py"> <strong>Sample usage patterns</strong><br><br>— <code>await audit_service.record({&#x27;event_type&#x27;:&#x27;file.upload&#x27;,&#x27;file_id&#x27;:fid,&#x27;actor_id&#x27;:hash(actor),&#x27;payload&#x27;:sanitized_meta,&#x27;trace_id&#x27;:trace})</code> called by <code>/ingest</code> handler. <br>— <code>audit_service.record_batch([...])</code> used by validator/reconciler to persist per-row summaries after batch validation. <br>— <code>exporter</code> calls <code>audit_service.record</code> with export completion metadata and <code>export_checksum</code>. </td></tr><tr><td data-label="Technical Breakdown — audit_service.py"> <strong>Extensibility & upgrade notes</strong><br><br>— Add new storage backends by implementing <code>AuditBackend</code> and providing migration helpers. <br>— When changing event schema, support <code>event_schema_version</code> and provide a migration/compatibility layer; never overwrite existing events. <br>— Maintain <code>docs/AUDIT_SPEC.md</code> describing event types, required fields, and redaction policy for auditors and compliance teams. </td></tr></tbody></table></div><div class="row-count">Rows: 18</div></div><div class="table-caption" id="Table17" data-table="Docu_0167_17" style="margin-top:2mm;margin-left:3mm;"><strong>Table 17</strong></div>
<div class="table-wrapper" data-table-id="table-17"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown — file_service.py"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown — file_service.py</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — file_service.py"> <strong>File-level responsibilities</strong><br><br>Abstract and encapsulate all file I/O and storage concerns for uploads, temporary staging, final artifact persistence, and secure downloads. Provide a stable, testable API that the rest of the application (API handlers, parsers, exporter, reconciler, audit) calls to read/write files without knowing storage backend details. Support multiple backends (local filesystem, S3-compatible) via a factory pattern, ensure atomic writes and content checksums, implement retention/cleanup policies, and provide safe utilities for presigned URLs and streaming reads/writes. This module must not perform network I/O at import time and should expose factories that return clients with explicit connect()/close() methods invoked by startup/shutdown handlers. </td></tr><tr><td data-label="Technical Breakdown — file_service.py"> <strong>Design principles & guardrails</strong><br><br>— <strong>Backend-agnostic API:</strong> callers use the same methods regardless of <code>FILE_STORE_TYPE</code>.<br>— <strong>Atomicity:</strong> write to <code>.part</code> temp files and <code>os.replace()</code> to final paths when complete.<br>— <strong>Idempotency:</strong> compute and persist checksums (sha256) for uploaded contents and use them for dedup/idempotency keys.<br>— <strong>PII safety:</strong> when storing files that contain PII, ensure access controls are enforced by storage layer and metadata is minimal in logs (use masked filenames or internal IDs).<br>— <strong>Resource limits:</strong> enforce <code>MAX_UPLOAD_SIZE</code>, per-user quotas, and temp storage limits; return clear errors when limits are hit. </td></tr><tr><td data-label="Technical Breakdown — file_service.py"> <strong>Primary responsibilities</strong><br><br>1. Accept uploaded file streams and persist them safely to temp storage.<br>2. Validate MIME + magic bytes and enforce size limits before/while writing.<br>3. Compute and return content checksum and metadata (size, mime, detected encoding).<br>4. Promote temp files to permanent store (local <code>outputs/</code> or remote) atomically.<br>5. Provide streaming read API for downstream consumers (parsers, exporters) with content-length and checksum support.<br>6. Provide secure download helpers: presigned URLs (for S3) or tokenized ephemeral download links for local store.<br>7. Cleanup temp files and manage retention policy for outputs.<br>8. Provide small admin helpers: list_files(prefix), delete_file(path/id), get_metadata(file_id). </td></tr><tr><td data-label="Technical Breakdown — file_service.py"> <strong>Primary public API (recommended signatures)</strong><br><br>``<code>py\ndef build_file_service(settings: Settings) -&gt; FileService  # factory\n\nclass FileService:\n    async def connect(self) -&gt; None\n    async def aclose(self) -&gt; None\n    def save_upload(self, stream: BinaryIO, filename: str, *, max_size: Optional[int]=None, detect_mime:bool=True) -&gt; FileRecord\n    def save_upload_chunked(self, chunk_iter: Iterator[bytes], filename: str, ...) -&gt; FileRecord\n    def read_stream(self, file_id: str) -&gt; Iterator[bytes]\n    def open_file(self, file_id: str) -&gt; BinaryIO  # context manager\n    def promote_to_store(self, temp_file_id: str, dest_path: Optional[str]=None, overwrite: bool=False) -&gt; FileRecord\n    def compute_checksum(self, file_path_or_stream) -&gt; str\n    def get_presigned_download(self, file_id: str, expires_in: int=3600) -&gt; str\n    def list_files(prefix: Optional[str]=None) -&gt; List[FileRecord]\n    def delete_file(self, file_id: str) -&gt; None\n    def cleanup_temp(self, older_than_seconds: int) -&gt; int  # returns deleted count\n</code>`` </td></tr><tr><td data-label="Technical Breakdown — file_service.py"> <strong>FileRecord structure</strong><br><br><code>FileRecord</code> (dataclass): <code>{ file_id, path, filename, size, checksum, mime_type, encoding, created_at, storage_backend, meta }</code>. Use stable <code>file_id</code> (UUID or checksum-based) and avoid embedding user PII in <code>file_id</code> or <code>path</code>. </td></tr><tr><td data-label="Technical Breakdown — file_service.py"> <strong>Backend implementations</strong><br><br>Provide small adapter classes implementing the same interface:<br>— <code>LocalFileStore</code> — writes to configurable base path, enforces POSIX-safe filenames, handles temp <code>.part</code> writes, atomic rename, and permission setting.<br>— <code>S3FileStore</code> — lazy-imports boto3/client on connect, supports multipart upload for large files, presigned URL generation, and server-side encryption options. <br>— <code>NullFileStore</code> — test stub that stores files in memory (for unit tests). </td></tr><tr><td data-label="Technical Breakdown — file_service.py"> <strong>Safety & validation</strong><br><br>— Validate MIME via magic bytes (python-magic) if available; fall back to extension-based guesses. <br>— Enforce <code>MAX_UPLOAD_SIZE</code> while streaming writes; abort and cleanup on violation. <br>— Sanitize filenames to prevent path traversal and limit name length. <br>— When saving, compute sha256 incrementally to avoid double-reading large files. </td></tr><tr><td data-label="Technical Breakdown — file_service.py"> <strong>Atomic & resumable uploads</strong><br><br>— For local store: write to <code>tmp/{file_id}.part</code> then <code>os.replace()</code> to <code>final/{file_id}</code>. <br>— For remote store (S3): support multipart uploads and allow resuming by keeping multipart state in temporary metadata. <br>— Provide idempotency via <code>Idempotency-Key</code> + checksum: if checksum already exists in store, return existing <code>FileRecord</code> and avoid duplicate writes. </td></tr><tr><td data-label="Technical Breakdown — file_service.py"> <strong>Streaming read/write patterns</strong><br><br>— <code>read_stream(file_id)</code> yields chunks (configurable chunk size) and supports <code>Range</code> reads for large files and resume. <br>— <code>stream_upload(chunk_iter)</code> accepts an iterator of bytes (useful when receiving multipart/form-data in streaming mode). <br>— Ensure context managers close file handles properly and that exceptions during streaming trigger cleanup. </td></tr><tr><td data-label="Technical Breakdown — file_service.py"> <strong>Presigned URLs & secure downloads</strong><br><br>— For S3: return standard presigned GET URLs with configurable expiry. <br>— For local store: generate ephemeral signed tokens (HMAC of file_id + expiry + secret), provide <code>GET /download/{token}</code> endpoint in API layer that validates token and streams file. Document token format and provide rotateable secret via <code>config</code>. </td></tr><tr><td data-label="Technical Breakdown — file_service.py"> <strong>Metadata & audit integration</strong><br><br>— Persist minimal metadata alongside files (JSON sidecar or metadata store) with <code>file_id</code>, checksum, storage path, uploader_id, and job_id. <br>— Integrate with <code>audit_service</code> (call <code>audit_service.record(&#x27;file.upload&#x27;, meta)</code>) after successful promotion. Keep metadata reads cheap and avoid exposing PII in metadata fields returned to public endpoints. </td></tr><tr><td data-label="Technical Breakdown — file_service.py"> <strong>Retention & cleanup policies</strong><br><br>— Implement <code>cleanup_temp()</code> for temp upload garbage collection and <code>prune_outputs(age_days)</code> for old exports. <br>— Provide configuration for retention windows and a dry-run mode for admin checks. <br>— Ensure cleanup is safe: mark files for deletion first and only delete after audit/event recording. </td></tr><tr><td data-label="Technical Breakdown — file_service.py"> <strong>Error handling & retries</strong><br><br>— Raise domain exceptions (<code>StorageError</code>, <code>ParseError</code> when magic check fails) defined in <code>core/exceptions.py</code>. <br>— For transient remote failures (network/S3 throttling), implement retry with exponential backoff and idempotency keys for safe retries. <br>— Ensure partial <code>.part</code> or multipart state is cleaned on unrecoverable failures. </td></tr><tr><td data-label="Technical Breakdown — file_service.py"> <strong>Performance & scaling</strong><br><br>— Use chunked streaming to keep memory bounded. <br>— For local storage on high-throughput systems, ensure limiting concurrent writes and use background cleanup to avoid disk exhaustion. <br>— For S3, use multipart uploads for large files and parallelize part uploads when appropriate. <br>— Expose metrics: <code>file_service.upload_bytes</code>, <code>upload_time_ms</code>, <code>active_uploads</code>, <code>failed_uploads</code>, <code>temp_cleanup_count</code>. </td></tr><tr><td data-label="Technical Breakdown — file_service.py"> <strong>Testing guidance</strong><br><br>— Unit tests: LocalFileStore: save/read/atomic move, checksum correctness, temp cleanup behavior, filename sanitization, idempotency handling with duplicate uploads. <br>— Integration tests (fast): S3FileStore mocked boto3 using moto or a fake client; test multipart flows and presigned URL generation. <br>— Failure tests: interrupted streams, size limit exceeded, invalid magic bytes. <br>— Security tests: ensure download token validation rejects expired or tampered tokens. </td></tr><tr><td data-label="Technical Breakdown — file_service.py"> <strong>Admin & operational helpers</strong><br><br>— <code>list_temp_files()</code> and <code>list_exports()</code> for operators. <br>— <code>verify_all_checksums()</code> to validate persisted checksums vs actual content (useful after migration). <br>— <code>migrate_files(local_path, s3_bucket)</code> helper for manual migration with progress reporting and dry-run. </td></tr><tr><td data-label="Technical Breakdown — file_service.py"> <strong>Integration examples (usage patterns)</strong><br><br>— <strong>Ingest /ingest handler:</strong> call <code>file_service.save_upload(stream, filename)</code> → returns <code>FileRecord</code> → pass file path or stream to parser. <br>— <strong>Parser:</strong> call <code>file_service.open_file(file_id)</code> to get a file-like object for <code>pandas</code>/BeautifulSoup. <br>— <strong>Exporter:</strong> call <code>file_service.prepare_paths(job_id)</code> then <code>exporter</code> writes to temp paths and calls <code>file_service.promote_to_store(temp_id, dest_path)</code> to persist final artifacts. <br>— <strong>Download endpoint:</strong> call <code>file_service.get_presigned_download(file_id)</code> or validate token and stream via <code>read_stream()</code>. </td></tr><tr><td data-label="Technical Breakdown — file_service.py"> <strong>Extensibility</strong><br><br>— Add new backends by implementing the <code>FileStore</code> interface (e.g., Azure Blob, GCS). <br>— Add storage-tiering policy (hot/cold) later; expose TTLs per-namespace. <br>— Consider a small metadata DB (sqlite) for high-concurrency metadata operations rather than only sidecar JSON files. </td></tr></tbody></table></div><div class="row-count">Rows: 18</div></div><script src="assets/xlsx.full.min.js?v=1758605028" defer></script>
<script src="assets/script.js?v=1759748863" defer></script>
<script src="assets/worker.js?v=1758331710" defer></script>
<script>
(function(){
  const template = "{table}_{date}";
  const userVal = "";
  const hrefPrefix = "assets";
  function formatName(tableName) {
    const date = (new Date()).toISOString().slice(0,10);
    return template.replace('{table}', tableName).replace('{date}', date).replace('{user}', userVal);
  }
  const btn = document.getElementById('exportBtn');
  if(btn) {
    btn.addEventListener('click', async function() {
      try {
        const html = document.documentElement.outerHTML;
        if(html.length > 2000000) { alert('Export refused: html too large'); return; }
        if(window.Worker) {
          const workerUrl = (function(){ try{ return hrefPrefix + '/worker.js'; }catch(e){ return null; } })();
          if(workerUrl) {
            try {
              const worker = new Worker(workerUrl);
              worker.postMessage({html: html, format: 'pdf'});
              worker.onmessage = function(e) { console.log('worker:', e.data); alert('Worker replied: '+(e.data.msg||e.data.status)); };
            } catch(err) { console.warn('Worker create failed', err); alert('Worker not available'); }
          } else { alert('Worker not available'); }
        } else { alert('Export worker not supported in this environment.'); }
      } catch(err) { console.warn('Export failed', err); alert('Export worker not available. See console for details.'); }
    });
  }
})();
window.addEventListener('load', function(){ try{ document.querySelectorAll('.table-wrapper').forEach(function(e){ e.style.opacity='1'; }); }catch(e){} });
</script>
</div>
</body>
</html>