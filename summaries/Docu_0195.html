<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='utf-8'><meta name='viewport' content='width=device-width,initial-scale=1'>
<title>Tables Viewer v2.1</title>
<style>:root{--main-header-height:56px;} .table-wrapper{opacity:0;min-height:24px;transition:opacity .12s linear}</style>
<link rel="stylesheet" href="assets/style.css?v=1769960840">
<link rel="stylesheet" href="assets/overrides.css?v=1771316624">
</head><body>
<div id="tables-viewer" role="region" aria-label="Tables viewer">
<div id="stickyMainHeader">
<div id="tv-header">
<div><h1>Tables Viewer v2.1</h1></div>
<div style="display:flex;gap:8px;align-items:center;">
<input id="searchBox" class="tv-search" type="search" placeholder="Search" aria-label="Search tables" />
<button id="modeBtn" type="button" onclick="toggleMode()" aria-label="Toggle theme">Theme</button>
<button id="toggleAllBtn" type="button" aria-label="Toggle all" onclick="toggleAllTables()">Collapse All Tables</button>
<button id="copyAllPlainBtn" class="copy-btn" type="button" onclick="copyAllTablesPlain()" aria-label="Copy all tables as plain text">Copy All Tables (Plain Text)</button>
<button id="copyAllTablesBtn" class="copy-all-btn" type="button" onclick="copyAllTablesMarkdown()" aria-label="Copy All Tables (Markdown)">Copy All Tables (Markdown)</button>
<button id="copyAllMdBtn" style="display:none" aria-hidden="true"></button>
<button id="resetAllBtn" type="button" onclick="resetAllTables()" aria-label="Reset All Tables">Reset All Tables</button>
</div></div>
<script>
(function(){
  function ensureDelegation(){
    try {
      var vis = document.getElementById('copyAllTablesBtn');
      var alias = document.getElementById('copyAllMdBtn');
      if(!vis || !alias) return;
      alias.addEventListener = function(type, listener, options){
        vis.addEventListener(type, listener, options);
      };
      alias.removeEventListener = function(type, listener, options){
        vis.removeEventListener(type, listener, options);
      };
      Object.defineProperty(alias, 'onclick', {
        set: function(fn){ vis.onclick = fn; },
        get: function(){ return vis.onclick; },
        configurable: true
      });
      alias.focus = function(){ vis.focus(); };
      alias.blur = function(){ vis.blur(); };
    } catch(e) {
      try{ console && console.warn && console.warn('alias delegation failed', e); }catch(_){} 
    }
  }
  if(document.readyState === 'loading'){
    document.addEventListener('DOMContentLoaded', ensureDelegation);
  } else {
    ensureDelegation();
  }
})();
</script>
<noscript><div style='color:#b91c1c'>JavaScript is disabled. Tables will be shown statically. For large tables enable JS for virtualization.</div></noscript>
<div id="tocBar" role="navigation" aria-label="Table of contents"><ul><li class="toc-item"><a class="toc-link" href="#Table1">Table 1</a></li>
<li class="toc-item"><a class="toc-link" href="#Table2">Table 2</a></li>
<li class="toc-item"><a class="toc-link" href="#Table3">Table 3</a></li>
<li class="toc-item"><a class="toc-link" href="#Table4">Table 4</a></li></ul></div></div>
<div class="table-caption" id="Table1" data-table="Docu_0195_01" style="margin-top:2mm;margin-left:3mm;"><strong>Table 1</strong></div>
<div class="table-wrapper" data-table-id="table-1"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by VBA Function Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">VBA Function Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="VBA Function Technical Breakdown"> <strong>Verification:</strong> Reviewed <strong>10×</strong> for internal consistency, determinism, PII safety, audit coverage, and implementability before publishing this table.<br><br><strong>Notes on format:</strong> each cell below describes a single function (or logical VBA procedure) as a world-class technical breakdown: purpose & contract, inputs/outputs, invariants, provenance, failure modes, recovery actions, observability/audit requirements, performance expectations, test vectors, and conceptual Power Query (PQ) and DAX usage where relevant. Numbered lists use <code>&lt;br&gt;</code> line breaks to preserve single-column table readability. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: FuzzyScores — Function: TokenList</strong><br><strong>Purpose & contract:</strong> deterministic token extractor and deduplicator that converts arbitrary account label text to a canonical token list used downstream by token-set similarity algorithms. MUST be side-effect free and deterministic for identical inputs regardless of host locale.<br><strong>Inputs:</strong> single text string (AccountName).<br><strong>Outputs:</strong> ordered array/list of unique tokens (stable ordering: lexicographic ascending).<br><strong>Primary invariants:</strong><br>1. Lowercasing performed with ASCII and Unicode-aware normalization (NFKC recommended) to ensure canonical forms.<br>2. Punctuation removal set is explicit and configurable (e.g., <code>- , . / ( ) &amp; &#x27; &quot; : ;</code>); tokens split on whitespace; empty tokens removed.<br>3. Duplicate tokens removed deterministically; ordering is lexicographic to ensure identical outputs across runs.<br><strong>Provenance:</strong> tokenization originates from the normalized account label produced by PQ <code>fnNormalize</code> stage; TokenList must mirror PQ tokenization semantics for stable score parity.<br><strong>Failure modes & recovery:</strong> malformed input (Null) → return empty array; extremely long token (> 512 chars) → truncate token after configured max length and emit audit warning. Recover by correcting source normalization in PQ or by adding aliases to alias table.<br><strong>Observability & audit fields:</strong> when running in batch, emit counters: <code>tokens_extracted</code>, <code>blank_labels_count</code>, <code>long_token_count</code> to audit log. Include <code>standardMap.hash</code>-equivalent mapping manifest reference when tokenization rules change.<br><strong>Performance:</strong> linear in input length; safe for thousands of rows per second in VBA with optimized string operations; consider offloading to Python for >100k rows.<br><strong>Tests & examples:</strong> test with diacritics, combined punctuation, multiple languages, and identical semantic permutations (e.g., "Domestic Sales - Retail" vs "Retail Domestic Sales") to validate canonical token set parity. Provide PQ conceptual equivalence: PQ tokenization should perform the same normalization and <code>List.Sort</code> semantics so <code>TokenList</code> outputs match PQ <code>TokenKey</code> for cross-runtime hash parity. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: FuzzyScores — Function: TokenSetScore</strong><br><strong>Purpose & contract:</strong> compute token-set similarity emphasizing token intersection relative to token set sizes; returns a double in 0..1 where 1 = perfect token set match. Deterministic and symmetric (score(a,b) = score(b,a)).<br><strong>Inputs:</strong> sourceLabel string, targetLabel string (both already normalized or will be normalized consistently within the function).<br><strong>Outputs:</strong> double [0..1].<br><strong>Primary invariants:</strong><br>1. Uses canonical token lists (TokenList) for both inputs.<br>2. Score formula: <code>(2 * |intersection|) / (|A| + |B|)</code> (symmetric harmonic form) to avoid order dependence.<br>3. Token equality is exact string equality after normalization (no fuzzy token matching inside this step).<br><strong>Provenance:</strong> intended to model human notion of same words in different orders (handles reorderings).<br><strong>Failure modes & recovery:</strong> if both token lists are empty → return 0; if one empty and other not → return 0. Recovery: consider alias table to map synonyms to canonical tokens, which increases intersection.<br><strong>Observability & audit fields:</strong> log distribution of token counts, number of matched tokens for debugging of false positives. Include <code>tokenIntersection</code> details in evidence store (sanitized) when mapping impacts material disclosures.<br><strong>Performance:</strong> dominated by dictionary set membership checks; implement using hash/dictionary for O(n) average time. For large-scale runs, compute token-bitmaps in PQ and expose precomputed token hash to reduce VBA CPU cost.<br><strong>Tests & examples:</strong> verify that permutations yield 1 for identical token multiset, verify that stop-words removal affects score appropriately. PQ conceptual: compute token counts and intersection in PQ for large-batch prefiltering. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: FuzzyScores — Function: Trigrams</strong><br><strong>Purpose & contract:</strong> produce a stable set of character trigrams (3-grams) for short label similarity; used by Jaccard trigram similarity for short/abbreviated labels. Deterministic; returns list of unique trigrams in lexicographic order.<br><strong>Inputs:</strong> label string (normalized).<br><strong>Outputs:</strong> array/list of trigrams.<br><strong>Primary invariants:</strong><br>1. Surround string with boundary markers or underscores to preserve edge trigrams for short words.<br>2. Trigram generation must be UTF-8 aware and operate on grapheme clusters if possible; for VBA environments where grapheme handling is limited, operate on code points but document differences.<br><strong>Failure modes & recovery:</strong> empty input returns empty list; very short inputs (<3 characters) produce single trigram containing padded string to retain discriminative power.<br><strong>Observability:</strong> histogram of trigram set sizes to spot abnormal abbreviations. Keep parity with PQ trigram builder for cross-runtime reproducibility.<br><strong>Performance:</strong> O(n) in string length; memory small per label. Tests include verifying colliding trigrams across near-identical labels and checking behavior for punctuation and numeric-only labels. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: FuzzyScores — Function: TrigramJaccard</strong><br><strong>Purpose & contract:</strong> compute Jaccard similarity on trigram sets for fine-grained similarity in short labels. Returns 0..1 where 1 means identical trigram sets.<br><strong>Inputs:</strong> two strings (normalized).<br><strong>Outputs:</strong> double [0..1].<br><strong>Primary invariants:</strong><br>1. Uses Trigrams outputs as canonical sets and computes <code>|intersection| / |union|</code>.<br>2. Symmetric and deterministic.<br><strong>Provenance:</strong> used primarily when token-set is low-information (very short labels) or to complement token-based matching.<br><strong>Failure modes & recovery:</strong> union size 0 → return 0. If many non-ASCII characters, ensure trigram builder handled them equivalently to PQ. Use fallback to token-set if trigram signal low.<br><strong>Observability:</strong> track cases where trigram score differs strongly from token-set to highlight abbreviation mismatches. Tests include pairs where tokenSetScore is low but trigramJaccard is high (e.g., abbreviations, punctuation differences). </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: FuzzyScores — Function: LevenshteinDistance</strong><br><strong>Purpose & contract:</strong> compute the Levenshtein edit distance for two strings (classic DP algorithm). Deterministic integer result >=0. Used with normalization into normalized distance [0..1].<br><strong>Inputs:</strong> strings <code>a</code>, <code>b</code> (may include numeric suffixes).<br><strong>Outputs:</strong> integer distance (non-negative).<br><strong>Primary invariants:</strong><br>1. Operates on normalized forms (NFKC), optionally strip whitespace for account code matching if configured.<br>2. For performance, short-circuit where absolute length difference exceeds allowed threshold to return large value early.<br><strong>Failure modes & recovery:</strong> very long strings may cause O(n*m) memory/time; protect with max length threshold and fallback approximate method (e.g., bounded edit distance or token-based fallback). Emit audit for truncated computations.<br><strong>Observability:</strong> record counts of truncated distances and long-run occurrences. Tests include verifying distances for numeric suffix edits and for small typographic differences. PQ conceptual: compute normalized numeric suffixes in PQ to precompute shorter strings for Levenshtein to improve performance parity across runtimes. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: FuzzyScores — Function: NormalizedLev</strong><br><strong>Purpose & contract:</strong> produce normalized Levenshtein in 0..1 by dividing distance by max length; returns 0 when identical, 1 when completely different. Deterministic and used in combined scoring formula where we invert it to a similarity contribution (1 - NormalizedLev).<br><strong>Inputs:</strong> strings a,b; optional <code>maxLenCap</code> parameter to floor denominator for extremely short strings to avoid division-by-zero ambiguities.<br><strong>Outputs:</strong> double [0..1].<br><strong>Invariants:</strong> if both strings length 0 → return 0. Respect <code>maxLenCap</code> when configured.<br><strong>Failure modes:</strong> as per LevenshteinDistance; if denominator 0 return 0. Observability: report normalized distribution for calibrating weights in combined score. Tests: numeric suffix examples, small typographic changes. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: FuzzyScores — Function: CombinedScore</strong><br><strong>Purpose & contract:</strong> compute the aggregate similarity score used to drive confidence bands: canonical formula is <code>0.5*TokenSetScore + 0.25*TrigramJaccard + 0.25*(1 - NormalizedLev)</code>. Returns double in 0..1. MUST be deterministic and robust to missing components by normalizing weights when a component is undefined.<br><strong>Inputs:</strong> sourceLabel, targetLabel, optional signature strings, optional override weights.<br><strong>Outputs:</strong> combined similarity score [0..1] plus component breakdown (TokenSetScore, TrigramJaccard, NormalizedLev) packaged as structured string for audit/evidence.<br><strong>Primary invariants:</strong><br>1. If Trigram is not applicable (labels long), automatically set trigramWeight to 0 and renormalize weights to maintain relative importance.<br>2. CombinedScore monotonic with respect to improvements in any component when weights fixed.<br><strong>Failure modes & recovery:</strong> miscalibrated weights produce poor precision/recall; mitigation: expose weights in configuration sheet and require migration manifest for weight changes with golden tests.<br><strong>Observability & audit:</strong> always record per-row component scores and <code>combinedScore</code> in <code>CandidateMap</code> and store a sample of these rows in encrypted evidence storage for regulated datasets; store <code>paramsHash</code> capturing weights and version so runs are reproducible.<br><strong>Performance:</strong> component computations dominate; batch compute trigram and token sets in PQ where possible and call CombinedScore with precomputed component values for scalability.<br><strong>Tests & examples:</strong> show edge-case examples: swapped words (high tokenSet but medium trigram), numeric suffix change (high Lev contribution), acronym vs full name (low token but trigram maybe high). PQ conceptual: produce <code>CandidateMap</code> with precomputed tokenKey and trigramSets so combined scoring becomes an aggregation step in VBA or in PQ if preferred. DAX conceptual: compute aggregated acceptance rates per bucket as measures for monitoring: <code>AutoAcceptRate = DIVIDE(SUMX(Table, IF(ConfidenceBand=&quot;Auto&quot;,1,0)), COUNTROWS(Table))</code>. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: FuzzyScores — Function: ConfidenceBand</strong><br><strong>Purpose & contract:</strong> map combined score into three discrete bands: <code>Auto</code> (>=0.88), <code>Review</code> (0.70–0.88), <code>Manual</code> (<0.70). Deterministic mapping; thresholds configurable via <code>Config</code> sheet and require migration manifest for changes affecting production.<br><strong>Inputs:</strong> combinedScore (0..1).<br><strong>Outputs:</strong> string band. <br><strong>Primary invariants:</strong> inclusive upper/lower bounds must be consistent and non-overlapping; configurable thresholds stored as <code>Config.ThresholdAuto</code> and <code>Config.ThresholdReview</code> (Auto >= TAUTO, Review >= TREV and < TAUTO, Manual < TREV).<br><strong>Failure modes:</strong> mis-read config values (e.g., missing) → fallback to default thresholds and emit <code>standard.map.warning</code> style audit. Tests: edge values at thresholds to ensure deterministic band assignment. Observability: histogram of bands across accounts and override rates for Review band. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: Signatures — Function: ParseSignatureFromPQ</strong><br><strong>Purpose & contract:</strong> accept PQ-produced signature string (compact <code>CP:Pct;CP:Pct;...</code>) and parse into in-memory key-value map for further processing (SignatureOverlap). Deterministic parser; must be tolerant to missing percent values and minor formatting variations from PQ (e.g., spaces or trailing semicolons).<br><strong>Inputs:</strong> signatureString (from PQ <code>AccountSignatures</code>).<br><strong>Outputs:</strong> dictionary mapping counterparty -> percent (numeric 0..100).<br><strong>Invariants:</strong> sum of percent values may be <=100 due to truncation; document that PQ must compute percent at high precision and include rounding semantics to ensure cross-runtime parity.<br><strong>Failure modes & recovery:</strong> malformed parts ignored with logged warning; if no signature, return empty dictionary. Observability: count of accounts with empty signature and top counterparties frequency. Tests: special characters in counterparty names, percent zero entries, repeated counterparties across accounts. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: Signatures — Function: SignatureOverlap</strong><br><strong>Purpose & contract:</strong> compute a normalized overlap score [0..1] between two signatures that reflect common counterparties weighted by percent volumes. Deterministic and asymmetric weighting avoided: implement symmetric formula such that high overlap of top counterparties yields high overlap score. Document fallback behavior when signatures are missing.<br><strong>Inputs:</strong> signatureA string/dictionary, signatureB string/dictionary.<br><strong>Outputs:</strong> double 0..1.<br><strong>Primary invariants:</strong> normalization by average total percent of the two signatures preserves 0..1 scale; if denominators 0 return 0.<br><strong>Failure modes:</strong> different percent rounding conventions across PQ runs may change overlap slightly; require PQ percent decimals stable and compute hash of signature as evidence for audit. Observability & audit: store signature hashes for each mapping candidate and include in <code>MappingHistory</code> evidenceRef. Tests: same signature -> 1, disjoint -> 0, partial overlap -> intermediate values. PQ conceptual: produce <code>SignatureHash</code> column used in audit chain for reproducibility and to accelerate signature overlap via precomputed bucket membership indexes. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: BatchProcessing — Function: ComputeScores (batch runner)</strong><br><strong>Purpose & contract:</strong> orchestrates batch scoring across <code>CandidateMap</code> rows, computes component scores and combined score, assigns <code>ConfidenceBand</code>, writes results to <code>CandidateMap</code> sheet, and optionally enqueues <code>ReviewQueue</code>. MUST be idempotent and side-effecting only to <code>CandidateMap</code> and audit append tables. Return deterministic result given identical input snapshot. MUST NOT call network IO or external APIs. <br><strong>Inputs:</strong> optional range/row set to process; default entire <code>CandidateMap</code> where <code>ReviewStatus = Pending</code>.<br><strong>Outputs:</strong> updates <code>CandidateMap</code> rows (TokenSetScore, TrigramJaccard, LevNorm, CombinedScore, ConfidenceBand) and returns summary: rowsProcessed, autoAssignedCount, reviewCount, manualCount, runtimeMs.<br><strong>Primary invariants:</strong><br>1. Read-then-write atomicity: load relevant rows into local arrays, compute, then write back in one contiguous write to reduce screen flicker and partial writes.<br>2. Preserve previous reviewer fields and MappingVersion when no change in <code>ProposedISAKBucket</code>.<br><strong>Failure modes & recovery:</strong> runtime errors mid-write → partial updates; reduce risk via staging: write to a temporary sheet and swap range on success (read-then-swap pattern). On failure, revert to pre-run snapshot (copy saved at start). Emit <code>batch.compute.failed</code> audit with diagnostics.<br><strong>Observability:</strong> emit <code>batch.compute.started</code>, <code>batch.compute.completed</code> audit rows including <code>paramsHash</code> (weights used) and <code>snapshotHash</code> of input rows to allow reproducible reruns. Maintain per-batch evidenceRef for detailed component breakdowns for regulated datasets.<br><strong>Performance:</strong> vectorized processing recommended: read full used range into VBA arrays, process in memory, and write back; avoid per-row worksheet read/writes. For >50k rows, pipeline to PQ or Python recommended. <br><strong>Tests:</strong> idempotency tests (re-run same snapshot → same outputs), partial failure injection tests (simulate mid-write failure to validate rollback), load/perf tests with 5k/50k rows. PQ conceptual: optionally compute TokenSetScore and TrigramJaccard in PQ to reduce VBA CPU load; produce <code>CandidateMap</code> prepopulated with these stats then call <code>ComputeScores</code> to compute normalized Levenshtein and finalize combined score. DAX conceptual: compute <code>CombinedScore_Mean = AVERAGE(CandidateMap[CombinedScore])</code> for monitoring and <code>OverrideRate = DIVIDE(SUMX(CandidateMap, IF(ReviewerDecision != ProposedISAKBucket,1,0)), COUNTROWS(CandidateMap))</code>. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: Audit — Function: AppendAudit</strong><br><strong>Purpose & contract:</strong> append an immutable audit row into <code>MappingHistory</code> (or external append-only CSV) recording operator actions, pre/post states, evidenceRef, and correlationId. MUST be idempotent-writable and include a stable <code>AuditId</code> (GUID) and timestamp in UTC. MUST NOT include PII in non-secure audit rows; full PII evidence stored in encrypted evidence store referenced by <code>evidenceRef</code>.<br><strong>Inputs:</strong> accountId, oldBucket, newBucket, actionType, operatorId, componentScores (structured), optional notes, optional evidenceRef.<br><strong>Outputs:</strong> appended audit row and return AuditId. On failure, raise an error and write to local <code>audit_error</code> queue for operator manual remediation.<br><strong>Primary invariants:</strong><br>1. Append-only semantics; never overwrite existing audit rows.<br>2. Each audit row includes <code>mappingVersion</code> and <code>paramsHash</code> referencing the scoring config used.<br>3. Audit rows contain minimal PII; full PII in encrypted evidence only with evidenceRef recorded.<br><strong>Failure modes & recovery:</strong> file system full or workbook write-protection → fail append; recovery: persist audit row to a local encrypted staging file with retry logic and notify operators. For sensitive regulated datasets, ensure the append destination is WORM or signed archival store. <br><strong>Observability:</strong> emit <code>audit.appended</code> events to internal telemetry with counts/backpressure indicators. Tests: append idempotency, concurrent append (simulated), audit chain reconstruction: reassemble mapping history and recompute mapping versions to validate chain integrity. PQ conceptual: include an audit export query that packages <code>MappingHistory</code> as a signed manifest for regulator packaging. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: Migration — Function: GenerateMigrationScript</strong><br><strong>Purpose & contract:</strong> create a deterministic migration artifact (SQL or CSV) that will apply approved mappings to target ledger system and produce a separate immutable migration audit record (signed or checksumed). MUST not execute the migration — only generate the script. Output artifact must include <code>migrationId</code>, <code>paramsHash</code>, <code>standardMapHash</code>, <code>mappingVersion</code> and operator metadata.<br><strong>Inputs:</strong> snapshot of <code>MappingTable</code> (only Approved rows), operatorId, target format (SQL/CSV), optional effectiveDate.<br><strong>Outputs:</strong> migration file path or blob, migration manifest with checksums, appended <code>MappingHistory</code> entry calling out <code>migrationId</code>, and return <code>migrationId</code> and <code>artifactChecksum</code>.<br><strong>Primary invariants:</strong><br>1. Idempotency: regenerating with same inputs must produce identical artifact checksum (canonicalization required: stable row order, normalized quoting, deterministic timestamping fields removed from payload or recorded separately in manifest).<br>2. Migration artifact must be human-reviewable and signed by a separate process for production deployment (documented handoff).<br><strong>Failure modes & recovery:</strong> file write failures -> ephemeral staging directory and notify operator; if artifact contents would change non-deterministically (e.g., order not stable), abort and report deterministic error. <br><strong>Observability:</strong> record <code>migration.generated</code> audit with <code>artifact.checksum</code>, <code>rowsAffected</code>, <code>estimatedAffectedAmount</code> from ImpactSimulation. Tests: golden artifact parity, manifest checksum verification, rollback script generation test coverage. PQ conceptual: produce <code>AffectedBalances</code> table that feeds migration rationale in the migration manifest. DAX conceptual: create measures <code>MigrationRows = COUNTROWS(ApprovedMapping)</code> and <code>MigrationImpactAmt = SUMX(ApprovedMapping, [EstimatedAffectedAmt])</code> for reporting. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: Migration — Function: PersistAuditImmutable</strong><br><strong>Purpose & contract:</strong> persist audit entries and migration artifacts to an immutable storage or append-only CSV that is treated as canonical evidence. This may be a network store invoked by an operator outside of VBA; the function is responsible for preparing the artifact with checksums and invoking the secure upload helper if available. If network upload is not permitted in the add-in environment, create locally-signed artifact and produce operator steps for secure transfer. MUST NEVER embed plaintext PII in public audit rows.<br><strong>Inputs:</strong> migration artifact path, mappingVersion, operatorId, evidenceRefs array.<br><strong>Outputs:</strong> persisted artifact metadata (storageUri or localPath) and checksum. Append audit row referencing storageUri and evidenceRef.<br><strong>Primary invariants:</strong><br>1. Store <code>artifact.checksum.sha256</code> and <code>forensic_manifest</code> with all artifact checksums.<br>2. If network upload is used, ensure TLS verification and operator token usage; prefer ephemeral token patterns and do not persist tokens in workbook.<br><strong>Failure modes & recovery:</strong> upload failed -> stage locally and escalate to operator with instructions; include <code>persist.retry</code> audit. Tests: verify checksum recomputation equals stored checksum, and retrieval test for artifact exists. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: ReviewerUI — Function: ReviewerUI_Load (form initialization)</strong><br><strong>Purpose & contract:</strong> load review queue (rows where <code>ConfidenceBand = &#x27;Review&#x27;</code> and <code>ReviewStatus = &#x27;Pending&#x27;</code>), populate list controls, sample postings preview, and show component score breakdown for selected candidate. The UI must redact PII in the preview on-screen while linking to encrypted evidence for compliance users. The form must enforce material approval gating (disable <code>Approve</code> unless operator has <code>MaterialApprover</code> role).<br><strong>Inputs:</strong> optional filter (Entity, Period, bucket), optional <code>pageSize</code> for pagination.<br><strong>Outputs:</strong> UI state loaded with candidate listings and UI-friendly sanitized preview. Returns nothing to caller; side effects are only UI display and ephemeral selection state.<br><strong>Primary invariants:</strong><br>1. Sampling determinism: the sample postings displayed are selected deterministically from candidate's posting sample snapshot (seeded by CandidateMap row's <code>AccountId</code> and <code>planId</code>) so repeated loads show identical sample rows for audit parity.<br>2. Redaction policy: only show masked counterparty names unless operator has explicit retrieval permission; full counterparties only accessible via evidenceRef retrieval workflow.<br><strong>Failure modes & recovery:</strong> inability to read CandidateMap -> show friendly error with correlationId and abort; failure to render preview due to malformed postings -> show <code>STD_PARSE_FAIL</code> code and move on to next candidate. Log UI load durations and <code>review.load.failure</code> for telemetry.<br><strong>Observability:</strong> produce <code>standard.review.opened{operatorId,queueSize,loadTs}</code> audit. Tests: UI load with 0/1/1000 review rows to validate performance, redaction enforcement test, role-gate enforcement test. PQ conceptual: ensure PQ <code>PreviewSample</code> query precomputes safe redacted sample for UI to consume; DAX conceptual: compute <code>AvgReviewTime = AVERAGEX(ReviewSessions, ReviewDurationSeconds)</code>. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: ReviewerUI — Function: ReviewerUI_SaveAction (Approve/Edit/Reject/Alias)</strong><br><strong>Purpose & contract:</strong> apply reviewer action chosen in UI to the underlying <code>MappingTable</code> and append audit via <code>AppendAudit</code>. Actions include <code>Approve</code> (set mapping live), <code>Edit</code> (change proposed mapping), <code>Reject</code> (mark manual with notes), and <code>AddAlias</code> (persist new alias to alias table). MUST perform validation for materiality approvals and must not mutate other rows. Return success boolean and correlationId for triage.<br><strong>Inputs:</strong> accountId, actionType, newBucket (optional), notes (optional), operatorId, approvalRef if material.<br><strong>Outputs:</strong> updated mapping row(s), appended audit entry, optional evidenceRef creation if alias added, UI update refresh triggers. Return action success and AuditId.<br><strong>Primary invariants:</strong><br>1. Approve action increments <code>MappingVersion</code> and stamps <code>EffectiveDate</code> and <code>Author</code>.<br>2. Material approvals require <code>ApprovalRef</code> and two-person sign-off recorded in <code>Approvals</code> table; function MUST verify <code>Approvals</code> table for required approvals before committing inline destructive mapping (inline destructive mapping forbidden unless approvals present).<br><strong>Failure modes & recovery:</strong> approval missing for material mapping -> abort and log <code>STD_PERMISSION_DENIED</code>; concurrent modification (row changed since UI load) -> abort and prompt re-load (UI must show changed row). Implement optimistic locking via <code>rowChecksum</code> read at load and re-validate before write. On write failure, append <code>standard.review.failed</code> audit with diagnostics. <br><strong>Observability:</strong> emit <code>standard.review.action{action,accountId,operatorId,confidenceBand,correlationId}</code>. Tests: concurrent-write collision test, approval-gated path tests, alias addition verification (search for new alias in subsequent ComputeScores runs). PQ conceptual: if alias added, PQ <code>AliasLookup</code> query must be refreshed on next CandidateMap build; DAX conceptual: <code>ReviewApproveRate = DIVIDE(SUMX(ReviewActions, IF(Action=&quot;Approve&quot;,1,0)), COUNTROWS(ReviewQueue))</code>. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: ReviewerUI — Function: LoadSamplePostingsForAccount</strong><br><strong>Purpose & contract:</strong> deterministically select a sample of postings for given AccountId for display in UI and for impact simulation; selection seeded using stable <code>planId</code> + <code>AccountId</code> to maintain reproducibility across runs. Returns sanitized sample for UI and full sanitized evidence stored in evidence store with <code>evidenceRef</code> pointer for compliance retrieval.<br><strong>Inputs:</strong> AccountId, sampleSize (default 5).<br><strong>Outputs:</strong> UI-safe sample rows (masked as needed) and evidenceRef for full sample.<br><strong>Primary invariants:</strong><br>1. Seeded selection for deterministic preview reproduction.<br>2. UI sample redaction must follow PII policy (mask counterparty fields partially) while encrypted evidence contains full unredacted sample. EvidenceRef included in <code>MappingHistory</code> audit.<br><strong>Failure modes & recovery:</strong> insufficient postings -> return available rows with <code>sample.size.shortfall</code> flag. Observability: track <code>sample.shortfall.count</code> and <code>evidence.store.errors</code>. Tests: seed determinism tests, redaction checks, evidence retrieval acceptance tests. PQ conceptual: PQ <code>PreviewSample</code> query should compute and persist sample snapshots indexed by <code>CandidateMap</code> row to allow <code>LoadSamplePostingsForAccount</code> to fetch without scanning full postings table repeatedly. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: Materiality — Function: ValidateMaterialApproval</strong><br><strong>Purpose & contract:</strong> evaluate whether a mapping affects a material disclosure balance given prior period disclosures and computed impact; if material, enforce two-person approval flow or escalate per policy. Deterministic policy evaluation using configured thresholds stored on <code>Config</code> sheet. Returns boolean <code>isMaterial</code>, and <code>requiredApprovals</code> list when true.<br><strong>Inputs:</strong> AccountId, proposedBucket, effectiveDate, ImpactSimulation snapshot, operatorId.<br><strong>Outputs:</strong> isMaterial (True/False), requiredApprovals (array), rationale (structured string).<br><strong>Primary invariants:</strong><br>1. Materiality rule set is explicit, documented, and versioned: e.g., absoluteDelta > absoluteThreshold OR deltaPct > relativeThreshold OR reclassification > reclassificationThreshold triggers materiality.<br>2. The function uses precomputed aggregates from <code>ImpactSimulation</code> to avoid scanning postings live.<br><strong>Failure modes & recovery:</strong> missing ImpactSimulation snapshot -> conservative default: treat as material and require approvals; log <code>STD_MISSING_SIM_SNAPSHOT</code> and require manual override. Observability: emit <code>materiality.evaluated</code> audit with <code>rationale</code> and <code>evidenceRef</code>. Tests: edge-case thresholds, canary checks for false positives. DAX conceptual: measure <code>MaterialMappingsCount = COUNTROWS(FILTER(MappingTable, [IsMaterial] = TRUE()))</code> for governance dashboards. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: Impact — Function: ImpactSimulation (orchestrator)</strong><br><strong>Purpose & contract:</strong> simulate applying the proposed/approved mappings to historical postings (prior periods) and compute disclosure roll-forward deltas by DisclosureBucket and Period. Provide structured <code>ImpactReport</code> and material flags. MUST NOT change original data; simulation is non-destructive and produces encrypted evidence artifacts when full PII exposures are needed.<br><strong>Inputs:</strong> ApprovedMappingSet (or ProposedMappingSet if preview), Postings snapshot (can be PQ query output), periods to simulate, sampling policy (full vs sample), exchange rates if cross-currency.<br><strong>Outputs:</strong> <code>ImpactReport</code> table with rows: DisclosureBucket, Period, PriorAmount, PostMappingAmount, Delta, DeltaPct, MaterialFlag, EvidenceRef; plus aggregated <code>ImpactSummary</code> (total delta per entity).<br><strong>Primary invariants:</strong><br>1. Seeded deterministic sampling if sample used; full simulation deterministic for full data snapshot.<br>2. Currency normalization up-front using authoritative FX table (PQ should join FX rates to postings before aggregation).<br>3. Rounding rules (SafeRound) must be canonical and versioned to ensure parity across runs and audits.<br><strong>Failure modes & recovery:</strong> missing FX rates -> abort simulation and mark as <code>STD_MISSING_FX</code>; missing postings for account -> mark with <code>sample.shortfall</code>. Observability: produce <code>impact.simulation.start|complete|failed</code> audits with <code>paramsHash</code> and <code>snapshotHash</code>. Performance: for large datasets stream via PQ transform or job scheduler; in-VBA simulation only acceptable for small datasets. Tests: re-run determinism test, rounding parity, cross-currency checks. PQ conceptual: implement simulation in PQ for scale: join postings->mapping->fx->aggregate and output <code>ImpactReport</code>. DAX conceptual: produce measures <code>ImpactDelta = SUM(ImpactReport[Delta])</code> and <code>ImpactMaterialCount = COUNTROWS(FILTER(ImpactReport, ImpactReport[MaterialFlag]=TRUE))</code>. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: MappingStore — Function: ApplyMapping (authoritative persist)</strong><br><strong>Purpose & contract:</strong> execute application of approved mappings to the canonical <code>MappingTable</code> and (optionally) to a copy of GL_Accounts for <code>create_copy</code> mode or prepare a migration artifact for system-in-place <code>inline</code> mode. Function must validate approvals, create an <code>ApplyDescriptor</code> with <code>beforeChecksum</code>, <code>afterChecksum</code>, <code>applyId</code>, and persist apply metadata atomically. Must not perform network IO on UI thread.<br><strong>Inputs:</strong> planId (optional), mappingSet (rows), mode (<code>create_copy | inline</code>), operatorId, approvals (list), correlationId.<br><strong>Outputs:</strong> <code>ApplyDescriptor</code> appended to <code>ApplyHistory</code>, artifact references (before snapshot, after snapshot, migration artifact), status = <code>completed|failed</code> and <code>applyId</code> returned to caller.<br><strong>Primary invariants:</strong><br>1. Inline destructive apply on regulated outputs requires two-person approval and explicit <code>requiresApproval</code> metadata present; otherwise, function must fail with <code>STD_PERMISSION_DENIED</code>.<br>2. All applies must create a <code>beforeSnapshot</code> (sheet copy or export) enabling deterministic revert; if <code>beforeSnapshot</code> missing, apply is forbidden for destructive operations.<br><strong>Failure modes & recovery:</strong> mid-apply failure -> persist partial outputs and set <code>apply.status</code> = <code>failed</code>, include diagnostics and <code>recoveryHint</code> in <code>ApplyHistory</code> audit; allow revert using <code>RevertMapping</code>. On transient errors, retry configurable times before failing. <br><strong>Observability:</strong> emit <code>standard.apply.start|completed|failed</code> with <code>applyId</code>, <code>planId</code>, <code>payloadHash</code>, <code>standardMap.hash</code>. Tests: idempotency (replay apply), checksum parity (before -> revert -> checksum equality), concurrency tests. PQ conceptual: produce <code>apply.preview</code> output used to generate <code>previewRef</code> and validate apply offline; DAX conceptual: <code>ApplySuccessRate = DIVIDE(SUMX(ApplyHistory, IF(Status=&quot;completed&quot;,1,0)), COUNTROWS(ApplyHistory))</code>. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: MappingStore — Function: RevertMapping</strong><br><strong>Purpose & contract:</strong> revert a previously applied mapping using stored <code>ApplyDescriptor</code> which contains <code>beforeSnapshot</code> or inverse mapping evidence. MUST validate <code>applyId</code>, ensure revert operations are idempotent, and not attempt heuristic reversion if snapshot missing (fail with <code>STD_REVERT_NO_SNAPSHOT</code>).<br><strong>Inputs:</strong> applyId, operatorId, correlationId.<br><strong>Outputs:</strong> <code>revertId</code>, status (<code>completed|failed|noop</code>), beforeChecksum, afterChecksum, audit entry <code>standard.revert</code> appended.<br><strong>Primary invariants:</strong><br>1. Revert is idempotent: multiple reverts with same <code>revertId</code> should result in a no-op after first successful revert and return <code>success</code> status with no additional side effects.<br>2. Revert only uses stored reversible metadata: snapshot or inverse mapping files; do not attempt heuristic reversion without explicit operator consent and recorded justification.<br><strong>Failure modes & recovery:</strong> snapshot missing -> fail with <code>STD_REVERT_NO_SNAPSHOT</code> and direct operator to open incident; partial apply before revert -> attempt partial reversion and document uncertainty in audit. Observability: <code>standard.revert.started|completed|failed</code>, include evidenceRef to snapshots; Tests: apply->revert checksum parity, missing snapshot path tests, concurrent revert concurrency tests. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: AliasManagement — Function: AddAlias</strong><br><strong>Purpose & contract:</strong> add new alias to alias lookup table with ownership and provenance metadata. Alias addition should be idempotent (adding same alias again returns existing alias id) and gated by owner permission for production changes. Output updated alias manifest and evidenceRef. <br><strong>Inputs:</strong> aliasText, canonicalBucket, operatorId, reason, optional signature.<br><strong>Outputs:</strong> aliasId, success boolean, appended audit. <br><strong>Primary invariants:</strong><br>1. Aliases are canonicalized using same normalization functions as TokenList to guarantee matching during scoring.<br>2. Each alias carries <code>owner</code>, <code>createdTs</code>, <code>approvals</code> metadata for governance.<br><strong>Failure modes & recovery:</strong> alias conflict (alias already maps to different bucket) -> fail and require manual resolution by owner; alias intended for multiple buckets prohibited unless explicit multiplex mapping supported and versioned. Observability: alias table change audit <code>alias.registered</code>. Tests: canonicalization parity and alias lookup correctness (alias used in candidate mapping runs). PQ conceptual: include <code>AliasLookup</code> query that is merged with <code>CandidateMap</code> to propose mappings using alias matches. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: Utilities — Function: NormalizeText</strong><br><strong>Purpose & contract:</strong> central normalization helper applied consistently across PQ and VBA: trim, NFKC normalize, lower-case, remove configured punctuation, collapse whitespace, optionally remove stop-words depending on call-site. MUST be identical in behavior to PQ <code>fnNormalize</code> to allow canonical hashes to match. <br><strong>Inputs:</strong> raw string, flags (removeStopWords boolean).<br><strong>Outputs:</strong> normalized string.<br><strong>Primary invariants:</strong><br>1. Use canonical Unicode normalization form NFKC for cross-run consistency.<br>2. Stop-word list stored in <code>Config</code> and versioned; when stop-word policy changes record <code>paramsHash</code> and re-run canonicalization/golden tests.<br><strong>Failure modes & recovery:</strong> unsupported Unicode environment in older Excel -> fall back to close equivalent and document in <code>standard.map.warning</code>. Observability: count of normalized modifications and before/after example sampling stored in evidence. Tests: cross-locale tests including diacritics and composite characters. PQ conceptual: the PQ <code>fnNormalize</code> should match this helper exactly; if PQ and VBA diverge, <code>standard.map.hash</code> parity tests will fail in CI. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: Utilities — Function: BuildTokenKey</strong><br><strong>Purpose & contract:</strong> produce canonical token key string used for hashing and deduplication in <code>standardMap.hash</code> calculation (stable key ordering, normalized regex serializations). Returns compact token-key string for indexing. <br><strong>Inputs:</strong> normalized label string.<br><strong>Outputs:</strong> tokenKey string (tokens sorted, joined by single space).<br><strong>Primary invariants:</strong> token ordering lexicographic and stable; token separators canonical (<code>U+0020</code>); no trailing/leading spaces. <br><strong>Failure modes & recovery:</strong> different tokenKey semantics between PQ and VBA → create parity issue: include unit test <code>ComputeTransformHash</code> to detect cross-runtime mismatch. Observability: <code>tokenKey.count</code> distribution logging. Tests: identical token sets in different permutations yield identical tokenKey. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: Utilities — Function: CreateGUIDAndTimestamp</strong><br><strong>Purpose & contract:</strong> produce RFC-compliant GUID and UTC timestamp string in ISO8601 for use in audit ids and artifact naming. Deterministic per invocation only in the sense it returns unique values; must not be seeded from process-attempt values. <br><strong>Inputs:</strong> none or optional seed for deterministic testing only (seed must be disabled in production).<br><strong>Outputs:</strong> GUID string, timestamp string. <br><strong>Primary invariants:</strong> GUID uniqueness and timestamp in <code>YYYY-MM-DDThh:mm:ssZ</code> UTC format. For deterministic testing, allow optional <code>testSeed</code> argument but require tests to clear seed before production runs. <br><strong>Failure modes & recovery:</strong> Excel environment lacking GUID generation functions -> fallback to pseudo-GUID computed from timestamp+rand but write <code>not-cryptographically-secure</code> proof in audit. Observability: frequency of pseudo-GUID usage logged (should be zero in production). Tests: GUID uniqueness test across 1M generated GUIDs in CI (or deterministic subset). </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: Config — Function: LoadConfig</strong><br><strong>Purpose & contract:</strong> load runtime configuration values from <code>Config</code> sheet into in-memory structure (thresholds, estimated cost parameters, alias policy flags, evidenceStoreUri, default effective date). MUST validate types and ranges and emit <code>config.invalid</code> audit if any required key missing. Return <code>Config</code> object/dictionary to callers. <br><strong>Inputs:</strong> none (reads <code>Config</code> worksheet/cell ranges).<br><strong>Outputs:</strong> Config dictionary (with fields: ThresholdAuto, ThresholdReview, MaterialityAbsolute, MaterialityPct, EvidenceStoreUri, DefaultEffectiveDays, Weights for scoring, etc.).<br><strong>Primary invariants:</strong> values must be strongly typed; missing or out-of-range values cause fail-fast behavior and conservative defaults applied with audit <code>config.warning</code> (but conservative application only for non-blocking switches — critical changes must block).<br><strong>Failure modes & recovery:</strong> corrupted sheet -> revert to last signed config snapshot file (if available) or refuse to run with <code>config.missing</code> error. Observability: <code>config.load.duration_ms</code>, <code>config.validationErrors</code>. Tests: invalid type inputs, boundary check tests. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: Security — Function: RoleCheck</strong><br><strong>Purpose & contract:</strong> verify operator roles/permissions for actions (Approve material, Persist mapping, Export artifacts). Role resolution checks <code>Users</code>/<code>RBAC</code> table; if offline, consult <code>CachedRoles</code> with TTL; do not store credentials in workbook. Must not perform network lookup on UI thread; prefer preloaded <code>CachedRoles</code> or admin refresh workflows. <br><strong>Inputs:</strong> operatorId, requiredRole string.<br><strong>Outputs:</strong> boolean allowed, optional <code>denialReason</code> string.<br><strong>Primary invariants:</strong> give-deny decisions auditable and deterministic over the <code>CachedRoles</code> snapshot; if role data stale beyond TTL, fail-safe deny and require refresh. <br><strong>Failure modes & recovery:</strong> missing or stale role data -> deny with <code>STD_PERMISSION_DENIED</code>. Observability: <code>rolecheck.failures</code> metric to detect RBAC sync issues. Tests: role change propagation tests, TTL expiry tests. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: Security — Function: SecureEvidenceWrite</strong><br><strong>Purpose & contract:</strong> write full PII evidence to an encrypted evidence store, return <code>evidenceRef</code> (opaque pointer). The function must support local encrypted file store or remote secure evidence store via ephemeral token. MUST NOT store plaintext PII in normal audit rows; only evidenceRef included. Encryption must use approved algorithms (AES-256-GCM) if implemented in-host; otherwise, call out to org-approved signing/encryption tool external to workbook as part of operator-run pipeline. <br><strong>Inputs:</strong> payload (structured JSON or CSV), operatorId, tags (e.g., mappingId, planId).<br><strong>Outputs:</strong> evidenceRef (URI or local path), checksum, storageResult.<br><strong>Primary invariants:</strong> evidenceRef metadata must include <code>retentionPolicy</code>, <code>creator</code>, <code>hash</code>, and <code>accessControl</code> pointers; the function must also record <code>evidenceRef</code> in mapping audit. <br><strong>Failure modes & recovery:</strong> encryption failure -> persist encrypted artifact to local sealed staging area and mark <code>evidence.persist.failed</code> in audit; require manual upload procedure. Observability: counts of evidence writes and success/failure rates; security reviews must ensure keys are not persisted in workbook. Tests: encrypt-decrypt roundtrip in secure environment, permission access tests. PQ conceptual: PQ <code>PreviewStandardize</code> stores preview artifacts and returns evidenceRef to VBA to display redacted UI sample. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: Jobs & Scheduler — Function: PersistJobDescriptor</strong><br><strong>Purpose & contract:</strong> when a heavy apply or preview is requested, create an idempotent job descriptor persisted to <code>JobQueue</code> (sheet or external job service). Descriptor includes <code>jobId</code>, planId, mappingVersion, correlationId, owner, chunkOffsets, attemptCount. Must be persisted atomically and be re-playable by worker processes. <br><strong>Inputs:</strong> job meta, payload hash. <br><strong>Outputs:</strong> jobId and stored descriptor. <br><strong>Primary invariants:</strong> job descriptors are unique by <code>jobId</code> and re-entrant; worker picks job and acquires lock; use optimistic locking to prevent duplicate workers processing same job. <br><strong>Failure modes & recovery:</strong> persist failed -> retry or emit <code>job.persist.failed</code>. Observability: queue depth metrics, job persist latency metrics. Tests: duplicate job submission remains idempotent, worker replay test. PQ conceptual: large-scale preview/applies should be delegated to worker pipeline that reads <code>JobQueue</code> descriptors produced by PersistJobDescriptor. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: Diagnostics — Function: CollectDebugSnapshot</strong><br><strong>Purpose & contract:</strong> produce a sanitized diagnostic snapshot for triage including <code>CandidateMap</code> sample, <code>MappingHistory</code> tail for correlationId, <code>Config</code> hash, <code>paramsHash</code>, <code>standardMap.hash</code>, and small attachment of recent UI traces (redacted). Requires <code>ticketId</code> and <code>operatorId</code> (MFA gating recommended). Persist snapshot to local encrypted zip and return <code>diagRef</code> path. MUST redact secrets and PII from persisted diag unless explicit high-privilege approval recorded. <br><strong>Inputs:</strong> correlationId, ticketId, operatorId, ttl. <br><strong>Outputs:</strong> diagRef (file path), checksum. <br><strong>Primary invariants:</strong> TTL auto-disable after configured duration must be enforced; verbose logging only allowed while diagnostics toggle enabled. <br><strong>Failure modes & recovery:</strong> permission denied for diagnostics -> return <code>STD_PERMISSION_DENIED</code>. Observability: <code>diagnostics.enabled</code> audit rows. Tests: TTL auto-disable test, redaction enforcement test. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: CI & Golden Tests — Function: RegisterUnitTestHook</strong><br><strong>Purpose & contract:</strong> register a deterministic test hook that can run the plan->preview->apply->revert chain on canonical fixtures within CI. Hooks accept fixed <code>correlationId</code> and seed to guarantee golden parity. MUST be disabled in production unless explicitly allowed and signature verified. <br><strong>Inputs:</strong> hookName, fixtureSet, seed. <br><strong>Outputs:</strong> registered hook metadata for CI to call. <br><strong>Primary invariants:</strong> deterministic seeds and fixed <code>standardMap.hash</code> used; results must be reproducible across environments (canonicalization rules must be applied). <br><strong>Failure modes & recovery:</strong> hook executed in production unapproved -> block and audit <code>testhook.blocked</code>. Observability: CI test results, golden diff outputs. Tests: golden parity, migration manifest dependency test. PQ conceptual: PQ fixtures for golden tests included in test data; DAX conceptual: measure <code>GoldenDiffCount = COUNTROWS(GoldenDiff)</code>. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: Export — Function: ExportMappingSnapshot</strong><br><strong>Purpose & contract:</strong> export <code>standardize-map.json</code> equivalent and <code>OWNERS.md</code> snapshot to specified destination (URI or local path) preserving canonical JSON and computing <code>artifact.checksum.sha256</code>. Redaction of owner contact details should happen if operator lacks permission; redaction annotated in metadata. <br><strong>Inputs:</strong> destinationUri, operatorId, includeSignature boolean. <br><strong>Outputs:</strong> artifactUri, checksum, exportAuditRow appended. <br><strong>Primary invariants:</strong> canonical JSON produced using deterministic ordering rules (stable key ordering, normalized regex). If <code>includeSignature</code> true, preserve digital signature block or re-sign (operator must have signing key). <br><strong>Failure modes & recovery:</strong> destination unavailable -> fallback to staged local export and emit <code>standard.map.export.warning</code>. Observability: <code>standard.map.export</code> audit. Tests: checksum parity, signature verification for exported artifact. PQ conceptual: include <code>ComputeTransformHash</code> parity checks as part of export pipeline. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: Reconciliation & QA — Function: SampleReconciliationCheck</strong><br><strong>Purpose & contract:</strong> pick random sample of mapped accounts and perform reconciliation between aggregated postings (by mapped bucket) and prior disclosures to verify mapping correctness. Must accept seed for deterministic sampling. Return sample results with pass/fail and evidenceRef for failed items. <br><strong>Inputs:</strong> seed, sampleSize, tolerance thresholds. <br><strong>Outputs:</strong> sample report including list of accounts failing reconciliation, delta amounts, and evidenceRef for details. <br><strong>Primary invariants:</strong> random sampling must use cryptographically strong RNG when security-critical—seeded for reproducibility in QA runs. <br><strong>Failure modes & recovery:</strong> sample failure proportion above threshold -> recommend rollback or hot-swap, produce forensic package for incident response. Observability: reconcile failure rate metric; Tests: seeded sample parity, false-positive/negative detection control. DAX conceptual: <code>ReconcileFailPct = DIVIDE(CountFail, sampleSize)</code>. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: Forensics — Function: ForensicPack</strong><br><strong>Purpose & contract:</strong> assemble a canonical forensic package for incident response including <code>standardize-map.json</code>, <code>validationReport</code>, all audits for correlationId, preview evidence, and <code>applyDescriptor</code>. Produce <code>forensic_manifest.json</code> with sha256 checksums and persist to WORM storage. Requires operator compliance approvals for release. <br><strong>Inputs:</strong> correlationId, incidentId, operatorId. <br><strong>Outputs:</strong> forensicPackageRef (storage URI) and manifest checksum. <br><strong>Primary invariants:</strong> chain-of-custody metadata appended (collectorId, timestamps, checksums). If any artifact missing, include explicit <code>missingArtifact</code> entry and halt release until compliance signs off. <br><strong>Failure modes & recovery:</strong> inability to store in WORM -> persist locally and escalate. Observability: <code>forensic.pack.generated</code> metric. Tests: forensic pack integrity tests and retrieval test. PQ conceptual: export PQ query slices as CSV artifacts for inclusion. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: Reporting — Function: BuildStandardizationReport</strong><br><strong>Purpose & contract:</strong> assemble canonical run report bundle containing <code>before/after</code> samples, <code>transformSummary</code>, <code>applyDescriptor</code>, <code>previewRef</code>, <code>artifact.checksums</code>, and <code>evidenceRefs</code>. Compute <code>reportHash</code> and persist to secure evidence store with retention metadata. For regulated runs create immutable archive and audit row <code>standard.report.generated</code>. <br><strong>Inputs:</strong> runId or applyId. <br><strong>Outputs:</strong> reportRef (URI), reportHash, storageUri. <br><strong>Primary invariants:</strong> reproducible bundling; including <code>standardMap.hash</code> used in run and <code>paramsHash</code>; require <code>reportHash</code> parity checks in CI for golden runs. <br><strong>Failure modes & recovery:</strong> missing evidenceRef -> fail and produce partial report with <code>partial=true</code>. Observability: <code>standard.report.generated</code> and retention tags. Tests: reproduce reportHash from artifacts, retention enforcement simulation. PQ conceptual: produce <code>report</code> files as sanitized CSV/JSON ready for packaging; DAX conceptual: <code>ReportImpactSummary</code> measure summarizing run impact. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: Helpers — Function: SafeRound</strong><br><strong>Purpose & contract:</strong> canonical rounding function used across transforms to ensure deterministic numeric rounding (e.g., banker's rounding or configured rounding mode). MUST be identical across PQ/DAX/VBA consumers to prevent audit mismatches. <br><strong>Inputs:</strong> numeric value, decimals, roundingMode (bankers/default/up/down).<br><strong>Outputs:</strong> rounded numeric. <br><strong>Primary invariants:</strong> documented rounding mode per release; rounding applied consistently when summing and aggregating (document residual absorption algorithm when allocating rounding residuals across buckets). <br><strong>Failure modes & recovery:</strong> configuration mismatch across runtimes -> produce audit discrepancy; remediate via migration manifest. Observability: rounding mode changes must be auditable. Tests: known numeric examples with documented outcomes; PQ conceptual: PQ <code>SafeRound</code> M function must implement exact same algorithm and parameterization. DAX conceptual: use in measures where final presentation rounded numbers are required and parity tests compare DAX rounding vs VBA rounding. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: Diagnostics & Telemetry — Function: EmitTelemetryEvent</strong><br><strong>Purpose & contract:</strong> lightweight telemetry emitter that records named events with tags and numeric metrics to local telemetry buffer (sheet or in-memory) which is periodically flushed by <code>FlushTelemetry</code>. MUST redact PII in telemetry and mark telemetry rows with <code>correlationId</code> and <code>standardMap.hash</code>. This function MUST NOT call network APIs on UI thread — flushing to external telemetry requires separate admin path. <br><strong>Inputs:</strong> eventName, tags dictionary, metrics dictionary. <br><strong>Outputs:</strong> appended telemetry row in transient telemetry buffer; return boolean success. <br><strong>Primary invariants:</strong> no PII in telemetry payload; events include <code>ts</code> UTC and <code>correlationId</code>. <br><strong>Failure modes & recovery:</strong> buffer overflow -> rotate buffer to disk with <code>telemetry.rotate</code> audit. Observability: metrics for <code>standard.handler.timeout_rate</code>, <code>compute.latency_ms</code>. Tests: telemetry buffer rollover, event schema conformance checks. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: QA — Function: RunGoldenParityChecks</strong><br><strong>Purpose & contract:</strong> run golden fixtures through full plan->preview->apply->revert flows using deterministic seeds and compare generated artifacts/hashes to golden checksums stored in <code>GoldenStore</code>. MUST run in CI and be repeatable locally for developers. Return pass/fail and diff details. <br><strong>Inputs:</strong> fixtureSetId, correlationId, seed. <br><strong>Outputs:</strong> <code>parityResult</code> (pass/fail), <code>diffReportRef</code>. <br><strong>Primary invariants:</strong> environment parity (locale, codepage) must be controlled; run locks <code>Config</code> to golden fixture config to maintain parity. <br><strong>Failure modes & recovery:</strong> environmental mismatch -> report environment diffs to developer and <code>standard.verify.failure</code> audit. Tests: repeated runs must pass; if fails, re-run with full debug toggle and produce <code>forensic_pack</code>. PQ conceptual: ensure PQ queries used in golden tests are exactly those deployed in production; DAX conceptual: golden measures used to compare aggregated results. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: Admin — Function: HotSwapStandardMap</strong><br><strong>Purpose & contract:</strong> transactional emergency patcher for runtime map updates. Steps: validate manifest & signature, compute diff & risk estimate, produce <code>hotSwap.preview</code> listing impacted ruleIds, optionally run smoke tests via unit hooks, apply in-memory atomically if smoke tests pass, persist via <code>ExportMappingSnapshot</code> if requested, append <code>standard.hotswap.applied</code> audit. Must not interrupt running applies and must be rollback-capable. <br><strong>Inputs:</strong> newMapJson, operatorId, approvals. <br><strong>Outputs:</strong> beforeHash, afterHash, riskEstimate, smokeTestResults, audit entries. <br><strong>Primary invariants:</strong> sign-off required for production; signature verification mandatory unless operator overrides with documented justification — log <code>standard.map.warning</code> with override justification. <br><strong>Failure modes & recovery:</strong> smoke tests fail -> revert and emit <code>standard.hotswap.reverted</code>; if partial apply occurred, rely on <code>ApplyDescriptor</code> semantics to revert running applies using snapshot. Observability: hotSwap audit chain and smokeTest success rates. Tests: dry-run validation, smoke-test harness pass/fail, rollback correctness. PQ conceptual: dry-run smoke tests executed using PQ preview queries against sample fixtures; DAX conceptual: <code>HotSwapRiskScore = SUMX(ChangedRules, [EstimatedCost])</code> for dashboarding. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: Shutdown & Recovery — Function: Shutdown</strong><br><strong>Purpose & contract:</strong> graceful module unload: flush audit buffers, persist minimal snapshot (<code>lastStandardMapHash</code>, <code>lastRefreshTs</code>, <code>lastCorrelationId</code>) and append <code>standard.shutdown</code> audit. MUST be registered with host shutdown handlers and ensure <code>DQ_Audit</code> flushes before module state persisted. <br><strong>Inputs:</strong> none (invoked at unload).<br><strong>Outputs:</strong> persisted snapshot and audit row. <br><strong>Primary invariants:</strong> order of flush: telemetry -> audit -> state snapshot. On unclean exit detection at next load, emit <code>standard.recovery</code> audit with <code>recoveryGuid</code> and retention. <br><strong>Failure modes & recovery:</strong> flush failure -> persist to local fallback and emit <code>shutdown.partial</code> audit. Tests: clean and unclean exit detection. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: Governance — Function: BuildMigrationManifest</strong><br><strong>Purpose & contract:</strong> assemble migration manifest describing behavioural changes to mapping semantics for a release. Required fields: <code>migrationId</code>, <code>author</code>, <code>description</code>, <code>affectedRules[]</code>, <code>sampleFixtures[]</code>, <code>estimatedAffectedCount</code>, <code>canaryPlan</code>, <code>rollbackPlan</code>, <code>approvals[]</code>, <code>testMatrix</code>. Manifest is content-addressed (sha256) and must be referenced in <code>standard.apply</code> audits for semantic changes in production. <br><strong>Inputs:</strong> author, description, affectedRules, fixtures, KPIs, approvals. <br><strong>Outputs:</strong> migrationManifestPath, manifestHash, appended <code>migration.created</code> audit. <br><strong>Primary invariants:</strong> migration manifest required for breaking changes (<major> version bump) and must carry two-person approval for regulated outputs. <br><strong>Failure modes & recovery:</strong> missing approvals -> block release and produce <code>migration.blocked</code> audit. Observability: record <code>migration.submitted</code> and <code>migration.approved</code> events. Tests: manifest completeness checks and golden fixture mapping registration. PQ conceptual: include migration manifest in CI pipeline pre-check that runs PQ golden parity; DAX conceptual: <code>MigrationPotentialImpact = SUM(affectedRules[estimatedAffectedCount])</code>. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: Monitoring — Function: EmitSLOMetrics</strong><br><strong>Purpose & contract:</strong> compute and emit SLO metrics for plan build latency, preview latency, handler timeout rate, apply failure rate. Write into telemetry buffer with tags <code>standardMap.hash</code>, <code>planId</code>, <code>ruleId</code>, <code>operatorId</code> for triage. Must consciously avoid PII in metrics tags. <br><strong>Inputs:</strong> metricName, value, tags. <br><strong>Outputs:</strong> telemetry event appended. <br><strong>Primary invariants:</strong> avoid PII in tags, sample metrics at preconfigured rate, and ensure TTL for telemetry buffer to prevent unbounded growth. <br><strong>Failure modes & recovery:</strong> telemetry outage -> buffer locally and escalate on rejoin. Tests: SLO alert simulation. DAX conceptual: create dashboard measures to compute SLO attainment: <code>PlanBuildMedian = MEDIANX(PlanBuilds, [LatencyMs])</code>. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Module: Miscellaneous — Function: SafeErrorToUser</strong><br><strong>Purpose & contract:</strong> map internal error codes to short UI-safe strings that include correlationId and triage hint; store full diagnostics encrypted and append <code>standard.userErrorShown</code> audit entry. UI strings must be <=160 chars, PII-free. <br><strong>Inputs:</strong> correlationId, errorCode, operatorHint optional. <br><strong>Outputs:</strong> userMessage string to show in UI and audit append. <br><strong>Primary invariants:</strong> always include correlationId and short triage hint; never include stack traces or PII. <br><strong>Failure modes & recovery:</strong> mapping missing -> return generic message "Operation failed (ref <cid>)" and append diagnostic with full code. Tests: verify message length, no PII leakage. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Cross-cutting Observability, Security & Governance Notes (summary)</strong><br><strong>Audit obligations:</strong> every user-initiated action must append audit row with <code>correlationId</code>. Key audit rows include <code>standard.map.loaded</code>, <code>standard.plan.built</code>, <code>standard.preview</code>, <code>standard.apply.start</code>, <code>standard.apply.completed</code>, <code>standard.apply.failed</code>, <code>standard.revert</code>, <code>standard.rule.registered</code>, <code>standard.rule.invalid</code>, <code>standard.map.export</code>, <code>standard.hotswap.*</code>. Each audit must include <code>payloadHash</code>, <code>prevHash</code>, <code>configHash</code>, and <code>standardMap.hash</code> to support reconstructability.<br><strong>PII & evidence handling:</strong> redact PII in UI-level artifacts; store full sanitized evidence in encrypted evidence store with <code>evidenceRef</code> referenced in audit rows. Evidence access gated by approvals and recorded retrieval audits. <br><strong>Determinism & reproducibility:</strong> seed sampling deterministic, data structure ordering canonicalized, <code>paramsHash</code> and <code>standardMap.hash</code> required for reproducing runs; implement cross-runtime canonicalization tests for PQ and VBA parity. <br><strong>Performance budgets & SLOs:</strong> plan build median <200ms for small inputs; preview generation median <2s for <=500 rows in sample environment; heavy applies offloaded to job scheduler; track <code>standard.plan.latency_ms</code>, <code>standard.preview.duration_ms</code>, <code>standard.apply.duration_ms</code>, <code>standard.handler.timeout_rate</code>. <br><strong>CI & gating:</strong> Unit & golden parity tests must be executed on any changes to normalization, scoring, or weights; golden fixtures stored immutably and updates require migration manifest and approvals. </td></tr><tr><td data-label="VBA Function Technical Breakdown"> <strong>Final operational guidance for implementers (concise)</strong><br>1. Ensure PQ and VBA canonicalization functions match exactly — create cross-runtime golden hash tests. <br>2. Implement read-then-validate-then-swap pattern for mapping table updates to avoid in-use inconsistencies. <br>3. Keep evidence and audit stores separate: audit rows must be PII-free while evidence contains full mappings encrypted. <br>4. Expose configuration (weights, thresholds) via <code>Config</code> sheet and require migration manifest & audit when changing scoring weights.<br>5. For scale, move heavy computations (large trigram sets, mass Levenshtein) into PQ or a dedicated Python worker; keep UI-path VBA lightweight and deterministic.<br>6. Document every breaking change with <code>migration_manifest.json</code> and record two-person approvals for regulated changes.<br>7. Build test harnesses for deterministic runs (fixed correlationId and seed) enabling CI golden parity runs.<br><br><strong>End of per-function technical breakdown table.</strong> </td></tr></tbody></table></div><div class="row-count">Rows: 46</div></div><div class="table-caption" id="Table2" data-table="Docu_0195_02" style="margin-top:2mm;margin-left:3mm;"><strong>Table 2</strong></div>
<div class="table-wrapper" data-table-id="table-2"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by **modFuzzyScores — Per-function Expert Technical Breakdown**"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>modFuzzyScores — Per-function Expert Technical Breakdown</strong></div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="modFuzzyScores — Per-function Expert Technical Breakdown"> <strong>Verification statement:</strong> Reviewed ten times for internal consistency, determinism, PII controls, PQ parity, audit traceability, and testability prior to publishing. The entries below are per-function breakdowns for every exported/internal function in <code>modFuzzyScores</code> expected in a production-grade GL-account canonicaliser. Each function entry includes: Purpose & contract, Inputs & outputs, Primary invariants, Provenance & usage, Failure modes & recovery, Observability & audit obligations, Performance expectations, Test vectors and examples, Conceptual Power Query (PQ) mapping, Conceptual DAX reporting measures, Security/PII considerations, and Operational notes. Numbered lists use <code>&lt;br&gt;</code> line breaks per requirement. No code snippets are included. </td></tr><tr><td data-label="modFuzzyScores — Per-function Expert Technical Breakdown"> <strong>Function: NormalizeText</strong><br><strong>Purpose & contract:</strong> Canonical text normalizer that produces a stable, deterministic string for downstream tokenization and fuzzy scoring. MUST be identical in semantics to PQ <code>fnNormalize</code> and included in the <code>paramsHash</code> for reproducibility. Responsibilities: Unicode normalization, case folding, punctuation removal (configurable), whitespace collapse, optional accent folding, optional stop-word removal, canonical numeric normalization. This function MUST be side-effect free and deterministic for identical inputs and configuration.<br><strong>Inputs & outputs:</strong> Input: <code>rawString</code> (text), <code>options</code> object {accentFold:boolean, removeStopWords:boolean, punctuationList:<code>array|null</code>, numericNormalization:boolean}. Output: <code>normalizedString</code> (text), <code>trace</code> (short structured list of applied transforms for diagnostics).<br><strong>Primary invariants:</strong><br>1. Unicode normalization uses NFKC across all runtimes. <br>2. Case folding is locale-independent; do not use OS locale-sensitive to ensure parity. <br>3. Punctuation removal set must be sourced from versioned <code>Config</code> and included in <code>paramsHash</code>. <br>4. Numeric normalization (strip leading zeros, canonical separators) applies when <code>numericNormalization=true</code>.<br><strong>Provenance & usage:</strong> Used by TokenList, Trigrams, TokenKey creation, Levenshtein pre-processing, PQ/CI parity checks; foundation for reproducible <code>scoreHash</code>. <br><strong>Failure modes & recovery:</strong> Malformed Unicode sequences produce best-effort normalization and a <code>normalize.unicode_fallback</code> diagnostic; extremely long strings truncated to <code>maxNormalizeLen</code> with an audit flag; if normalization results in an empty string while input non-empty, flag for human review and persist sanitized evidence. <br><strong>Observability & audit:</strong> On batch runs emit <code>normalize.count</code>, <code>normalize.fallbackCount</code>, and sample <code>normalizationTrace</code> persisted to evidenceRef when affecting material mappings. Include <code>configHash</code>/<code>paramsHash</code> in any audit associated with normalized outputs. <br><strong>Performance:</strong> Linear in input length; optimized for many small strings; for very large batches delegate heavy transformations to PQ. <br><strong>Test vectors & examples:</strong><br>1. <code>&quot;Domestic Sales - Retail&quot;</code> → <code>&quot;domestic sales retail&quot;</code>. <br>2. <code>&quot;José &amp; Co.&quot;</code> with accentFold=true → <code>&quot;jose co&quot;</code>. <br>3. <code>&quot;00123-45&quot;</code> with numericNormalization=true → <code>&quot;123-45&quot;</code> (document canonical numeric format). <br><strong>PQ conceptual mapping:</strong> PQ <code>fnNormalize</code> must replicate exact NFKC + case-fold + punctuation removal rules. PQ should output <code>NormalizedLabel</code> and <code>NormalizationTrace</code> columns. CI golden tests must compare PQ vs VBA normalized outputs for canonical fixtures. <br><strong>DAX conceptual mapping:</strong> Use <code>NormalizedLabel</code> as dimension and <code>CountRows</code> as measure to report distribution changes across <code>paramsHash</code> values. <br><strong>Security & PII:</strong> Normalization may reveal structure of PII; UI-level outputs must sanitize normalized strings when they contain PII tokens; full normalized text stored only in encrypted evidence with <code>evidenceRef</code>. <br><strong>Operational notes:</strong> Changing normalization rules requires migration manifest and golden parity checks across PQ and VBA. </td></tr><tr><td data-label="modFuzzyScores — Per-function Expert Technical Breakdown"> <strong>Function: TokenList</strong><br><strong>Purpose & contract:</strong> Deterministic token extractor producing sorted unique tokens for a normalized label. Tokens are the atomic semantic units used in token-set similarity; tokenization must be stable and reproducible. The function shall not mutate external state. <br><strong>Inputs & outputs:</strong> Input: <code>normalizedString</code>, options {removeStopWords:boolean, minTokenLength:int, maxTokenLength:int}. Output: <code>tokens</code> (sorted unique list), <code>tokenKey</code> (deterministic joined string), <code>tokenStats</code> (counts and length distributions). <br><strong>Primary invariants:</strong><br>1. Token splitting uses whitespace boundaries only after punctuation removal in NormalizeText. <br>2. Deduplication is case-insensitive (already normalized) and uses lexicographic sorting to ensure stable ordering. <br>3. Stop-word removal references a versioned <code>StopWords</code> list in <code>Config</code>, encoded in <code>paramsHash</code>. <br><strong>Provenance & usage:</strong> TokenList feeds TokenSetScore, tokenKey stored in CandidateMap for canonical hashing and dedupe checks. Must match PQ token-key generation exactly. <br><strong>Failure modes & recovery:</strong> Empty token list for non-empty input indicates an over-aggressive stop-word list; log <code>tokenlist.empty_after_stopwords</code> and add to review sample. Tokens exceeding <code>maxTokenLength</code> truncated with audit. <br><strong>Observability & audit:</strong> Record tokens-per-label histogram and sample tokenKeys for parity checks; include <code>tokenKey</code> in audits for each mapping operation. For regulated datasets, persist token arrays for affected rows in encrypted evidence. <br><strong>Performance:</strong> O(n) in label length; recommended to precompute in PQ for large volumes to avoid VBA loops. <br><strong>Test vectors & examples:</strong><br>1. <code>&quot;domestic sales retail&quot;</code> → tokens <code>[&quot;domestic&quot;,&quot;retail&quot;,&quot;sales&quot;]</code>, tokenKey <code>&quot;domestic retail sales&quot;</code>. <br>2. Stop-word behavior: <code>&quot;the sales account&quot;</code> with <code>removeStopWords=true</code> → tokens <code>[&quot;sales&quot;,&quot;account&quot;]</code>. <br><strong>PQ conceptual mapping:</strong> PQ should export <code>TokenKey</code> and <code>Tokens</code> as columns in CandidateMap for fast matching and hashing. <br><strong>DAX conceptual mapping:</strong> <code>DistinctTokenCount</code> aggregated per bucket for quality dashboards. <br><strong>Security & PII:</strong> Mask tokens that are PII before storing in plain sheets; store full tokens only in encrypted evidence with access control. </td></tr><tr><td data-label="modFuzzyScores — Per-function Expert Technical Breakdown"> <strong>Function: TokenSetScore</strong><br><strong>Purpose & contract:</strong> Compute token-set similarity between two canonical token lists. Returns a floating score in [0..1] where 1 indicates perfect token-set equality. Deterministic and symmetric. This function must support optional token-length weighting and produce a component-level breakdown for auditing. <br><strong>Inputs & outputs:</strong> Inputs: <code>tokensA</code>, <code>tokensB</code>, options {weightByLength:boolean}. Outputs: <code>tokenScore</code> (0..1), <code>intersectionCount</code>, <code>unionCount</code>, <code>matchedTokens</code> array, <code>explain</code> string for audit. <br><strong>Primary invariants:</strong><br>1. Default formula: <code>score = (2 * |intersection|) / (|A| + |B|)</code> to reward mutual presence. <br>2. If <code>weightByLength</code> is enabled, token contributions are weighted by token length, normalized so final score remains in [0..1]. <br>3. Order-independence: same input tokens produce identical score independent of order. <br><strong>Provenance & usage:</strong> Core signal for reordering equivalence and multi-word matching; used heavily to auto-assign and to prioritize review queue ordering. <br><strong>Failure modes & recovery:</strong> Unexpected empty token sets return 0 and log <code>tokensetscore.empty_input</code>. If weighting config invalid (sum weights = 0) fallback to unweighted formula and emit <code>tokensetscore.weighting_defaulted</code>. <br><strong>Observability & audit:</strong> Store <code>intersectionCount</code>, <code>matchedTokens</code> and <code>tokenScore</code> in CandidateMap; for regulatory runs persist matched tokens in evidenceRef. Emit metrics <code>tokenScore.dist</code> and <code>tokenScore.mean</code> per run. <br><strong>Performance:</strong> Implemented with hashtable/dictionary lookups for O(n) performance. For huge candidate pools, PQ pre-filtering by tokenKey equality reduces comparisons. <br><strong>Test vectors & examples:</strong><br>1. A=<code>[&quot;domestic&quot;,&quot;sales&quot;]</code>, B=<code>[&quot;sales&quot;,&quot;domestic&quot;]</code> → tokenScore = 1. <br>2. A=<code>[&quot;domestic&quot;,&quot;sales&quot;,&quot;online&quot;]</code>, B=<code>[&quot;sales&quot;,&quot;offline&quot;]</code> → intersection 1, denom 5 → 0.4. <br><strong>PQ conceptual mapping:</strong> PQ should compute tokenKey equality and token counts for large-scale joins; PQ outputs serve as pre-filtering to reduce VBA pairwise comparisons. <br><strong>DAX conceptual mapping:</strong> <code>TokenMatchRate</code> measure used to monitor mapping quality per entity. <br><strong>Security & PII:</strong> matchedTokens can contain PII; redact or store hashed/tokenized versions in primary audit store; full matchedTokens stored only in encrypted evidence store. </td></tr><tr><td data-label="modFuzzyScores — Per-function Expert Technical Breakdown"> <strong>Function: Trigrams</strong><br><strong>Purpose & contract:</strong> Build canonical trigram set for a label with boundary padding to capture edge letters and abbreviations. Deterministic; output sorted unique trigrams and a canonical <code>trigramFingerprint</code> used as a compact representation. <br><strong>Inputs & outputs:</strong> Input: <code>normalizedString</code>, options {padBoundaries:boolean true by default, gramSize:int=3}. Output: <code>trigrams</code> array, <code>trigramCount</code>, <code>trigramFingerprint</code> (canonical sorted concatenation or hash). <br><strong>Primary invariants:</strong><br>1. Use boundary markers on both ends to preserve edge context. <br>2. Deduplicate and lexicographically sort for stable fingerprinting. <br>3. Padding strategy and markup must be recorded in <code>paramsHash</code>. <br><strong>Provenance & usage:</strong> Character-level signal for abbreviations and short tokens where token-set has low information. Used in TrigramJaccard and for supplemental matching. <br><strong>Failure modes & recovery:</strong> very short strings padded to produce at least one gram; Unicode combining marks must be addressed by NormalizeText prior to trigram creation. Log <code>trigrams.unicode_fallthrough</code> when fallback used. <br><strong>Observability & audit:</strong> Record trigramCount distribution and sample trigramFingerprints for parity checks; include in evidence for selected material changes. <br><strong>Performance:</strong> Linear in length; precompute in PQ for large candidate sets. <br><strong>Test vectors & examples:</strong><br>1. <code>&quot;sales&quot;</code> → boundary <code>_sales_</code> → trigrams <code>[&quot;_sa&quot;,&quot;sal&quot;,&quot;ale&quot;,&quot;les&quot;,&quot;es_&quot;]</code> (or equivalent depending on boundary encoding). <br>2. <code>&quot;Sls&quot;</code> (abbrev) vs <code>&quot;Sales&quot;</code> -> trigram overlap moderate; trigramJaccard discriminates well in such cases. <br><strong>PQ conceptual mapping:</strong> PQ <code>fnTrigrams</code> must implement identical boundary and sorting rules; include <code>trigramFingerprint</code> column in CandidateMap. <br><strong>DAX conceptual mapping:</strong> Use <code>AvgTrigramCount</code> and <code>TrigramOverlapAvg</code> measures for quality. <br><strong>Security & PII:</strong> trigrams derived from PII should be treated as sensitive and handled accordingly; avoid exposing full trigram sets in public dashboards. </td></tr><tr><td data-label="modFuzzyScores — Per-function Expert Technical Breakdown"> <strong>Function: TrigramJaccard</strong><br><strong>Purpose & contract:</strong> Compute Jaccard similarity over trigram sets returning float in [0..1]. Deterministic and symmetric. Designed to complement token-set scores for short/abbreviated labels and to help with fuzzy alias detection. <br><strong>Inputs & outputs:</strong> Inputs: <code>trigramsA</code>, <code>trigramsB</code>. Outputs: <code>jaccardScore</code>, <code>interCount</code>, <code>unionCount</code>. <br><strong>Primary invariants:</strong><br>1. Use deduplicated sets for correct set-based Jaccard calculation. <br>2. For empty union -> return 0. <br><strong>Provenance & usage:</strong> Especially useful for abbreviations, misspellings, and short code names; used in combined score with configured weight. <br><strong>Failure modes & recovery:</strong> union zero fallback to 0; log <code>trigramjaccard.zero_union</code>. <br><strong>Observability & audit:</strong> sample jaccard mismatches stored for QA; include inter/union counts in evidenceRef if relevant to material changes. <br><strong>Performance:</strong> O(n) with hashsets. Precompute trigramFingerprint in PQ to accelerate bulk set operations. <br><strong>Tests & examples:</strong> "Sls" vs "Sales" jaccard > 0.4; "Acct" vs "Account" moderate. <br><strong>PQ conceptual mapping:</strong> PQ precomputation of trigram fingerprints allows efficient hash-based intersection approximations at scale. <br><strong>DAX conceptual mapping:</strong> <code>AvgTrigramJaccard</code> across changed rows used for monitor. </td></tr><tr><td data-label="modFuzzyScores — Per-function Expert Technical Breakdown"> <strong>Function: LevenshteinDistance</strong><br><strong>Purpose & contract:</strong> Exact edit distance calculation between two normalized strings. Supports <code>maxDistance</code> and optional banded processing for performance. Returns integer distance; consumer should convert to normalized metric via NormalizedLev. Document any banded/bounded variant used for parity with PQ implementations. <br><strong>Inputs & outputs:</strong> Inputs: <code>stringA</code>, <code>stringB</code>, options {maxDistance:<code>int|null</code>, bandWidth:<code>int|null</code>}. Output: <code>distance</code> integer (or sentinel <code>&gt;maxDistance</code> if aborted). <br><strong>Primary invariants:</strong><br>1. Both inputs must be normalized with <code>NormalizeText</code> to guarantee parity across PQ and runs. <br>2. If <code>maxDistance</code> specified, function must bail out early when distance exceeds it and return a sentinel to avoid expensive computation. <br><strong>Provenance & usage:</strong> Effective for account code similarity and small typographic edits that token/trigram signals might miss. Used primarily as (1 - NormalizedLev) contributor in CombinedScore. <br><strong>Failure modes & recovery:</strong> For very long strings or pathological inputs, function must log and revert to approximate n-gram similarity; any fallback must be recorded in <code>diagnostics</code> and evidence. <br><strong>Observability & audit:</strong> count of bailouts and fallback occurrences; sample fallback cases persisted. <br><strong>Performance:</strong> worst-case O(n*m); banded algorithms or maxDistance needed for production workloads. PQ rarely used for exact Levenshtein at scale; consider worker service if numerous long-string comparisons required. <br><strong>Tests & examples:</strong> "ACC-1001" vs "ACC-1002" -> distance 1; "Sales Domestic" vs "Domestic Sales" > large distance but token signals cover case; ensure Levenshtein does not dominate combined score for reorder cases. <br><strong>PQ conceptual mapping:</strong> If PQ performs approximate string distance, explicitly test parity differences and ensure audit captures algorithm used. <br><strong>DAX conceptual mapping:</strong> summarize <code>AvgLevDistance</code> across mappings for quality metrics. </td></tr><tr><td data-label="modFuzzyScores — Per-function Expert Technical Breakdown"> <strong>Function: NormalizedLev</strong><br><strong>Purpose & contract:</strong> Convert raw Levenshtein distance to normalized similarity metric <code>levNorm</code> in [0..1], where 0 = identical and 1 = maximally different. Implement canonical denominator as <code>max(len(A), len(B))</code> with a <code>minDenominator</code> floor to avoid division by zero; include <code>paramsHash</code> for denominator policy. <br><strong>Inputs & outputs:</strong> Input: <code>distance</code>, <code>lenA</code>, <code>lenB</code>, optional <code>minDenominator</code>. Output: <code>normLev</code> double (0..1). <br><strong>Primary invariants:</strong> ensure identical computation in PQ if PQ provides normalized distances; minDenominator must be in <code>Config</code> and recorded. <br><strong>Provenance & usage:</strong> CombinedScore uses <code>(1 - normLev)</code> as similarity contribution. <br><strong>Failure modes & recovery:</strong> denominator zero fallback to 1 (conservative); emit <code>normalizedlev.div0_fallback</code>. <br><strong>Observability & audit:</strong> distribution of normLev for tuning; sample values persisted for flagged runs. <br><strong>Tests & examples:</strong> For distance 1 with maxLen 8 -> normLev = 0.125; CombinedScore contributions computed accordingly. <br><strong>PQ conceptual mapping:</strong> PQ normalization logic must match to avoid audit mismatch. <br><strong>DAX conceptual mapping:</strong> <code>NormLevDistribution</code> for reporting and weight tuning. </td></tr><tr><td data-label="modFuzzyScores — Per-function Expert Technical Breakdown"> <strong>Function: CombinedScore</strong><br><strong>Purpose & contract:</strong> Aggregate component similarity signals into a single canonical <code>combinedScore</code> in [0..1] using configurable weights. Default canonical weights: <code>wToken = 0.5</code>, <code>wTrigram = 0.25</code>, <code>wLev = 0.25</code>. MUST always compute and emit <code>paramsHash</code> (canonical serialization of weights and flags) and <code>scoreHash</code> (sha256 of canonicalized per-row input + paramsHash + accountId). CombinedScore must be deterministic, monotonic with positive component changes, and resilient when components are missing (renormalize weights proportionally). <br><strong>Inputs & outputs:</strong> Inputs: <code>tokenScore</code>, <code>trigramScore</code> (nullable), <code>normLev</code> (nullable), optional <code>weights</code> object. Output: <code>combinedScore</code> double, <code>breakdown</code> object (component values, weights, paramsHash), <code>scoreHash</code>. <br><strong>Primary invariants:</strong><br>1. If any component is null/NA, adjust weights by proportionally scaling remaining weights to sum to 1; record this adjustment in <code>breakdown</code>. <br>2. All floating formats canonicalized to fixed precision in serialization to produce stable <code>scoreHash</code>. <br><strong>Provenance & usage:</strong> Primary decision metric used for banding (Auto/Review/Manual) and for final mapping proposals. <code>scoreHash</code> is central for audit traceability and rerun determinism. <br><strong>Failure modes & recovery:</strong> invalid weights (negative or NaN) -> fallback to canonical defaults and emit <code>combined.weights.invalid</code> audit. Unexpected component value out-of-range -> clip to [0,1] and log. <br><strong>Observability & audit:</strong> record <code>paramsHash</code> and <code>scoreHash</code> for every row in CandidateMap and include <code>payloadHash</code> in MappingHistory audit. Persist representative breakdown samples to evidenceRef for compliance. <br><strong>Performance:</strong> negligible computation cost; bottleneck remains component preparation. <br><strong>Tests & examples:</strong> detailed numeric examples showing influence of each component and demonstration of renormalization when trigram disabled. PQ must be able to compute <code>paramsHash</code> to validate parity. <br><strong>PQ conceptual mapping:</strong> PQ may precompute tokenScore and trigramScore at scale; CombinedScore generation must still be reproducible across PQ and VBA runs. <br><strong>DAX conceptual mapping:</strong> <code>AvgCombinedScore</code>, <code>CombinedScoreByBucket</code>, and <code>CombinedScoreDeciles</code> used in dashboards and to trigger alerts on drift. </td></tr><tr><td data-label="modFuzzyScores — Per-function Expert Technical Breakdown"> <strong>Function: ConfidenceBand</strong><br><strong>Purpose & contract:</strong> Deterministically map <code>combinedScore</code> to discrete confidence band <code>Auto | Review | Manual</code> using thresholds from <code>Config</code>. Default thresholds: <code>Auto &gt;= 0.88</code>, <code>Review &gt;= 0.70 &amp; &lt; 0.88</code>, <code>Manual &lt; 0.70</code>. Provide rationale text explaining borderline decisions and whether <code>materialFlag</code> forces approval irrespective of band. <br><strong>Inputs & outputs:</strong> Input: <code>combinedScore</code>, <code>paramsHash</code>, optional <code>materialFlag</code>, <code>thresholdOverrides</code>. Output: <code>band</code> string, <code>rationale</code> string, <code>requiresApproval</code> boolean. <br><strong>Primary invariants:</strong><br>1. Thresholds and ties are canonical and inclusive/exclusive semantics documented; threshold values included in <code>configHash</code>. <br>2. Material flag overrides automatic assign: if mapping affects material disclosures, <code>requiresApproval=true</code> regardless of band. <br><strong>Provenance & usage:</strong> Band determines automation path: <code>Auto</code> may be persisted automatically (unless material), <code>Review</code> queued for human review, <code>Manual</code> requires manual mapping. <br><strong>Failure modes & recovery:</strong> missing thresholds -> fallback to defaults and emit <code>band.defaulted</code> audit. Overlapping threshold ranges -> fail configuration validation and block apply until corrected. <br><strong>Observability & audit:</strong> capture band distribution and override rates; record <code>band</code> in CandidateMap and include <code>paramsHash</code> and <code>configHash</code> in audit rows. <br><strong>Performance:</strong> trivial mapping. <br><strong>Tests & examples:</strong> test exact boundary values 0.88 and 0.70 for inclusive/exclusive semantics; test that materialFlag true always sets <code>requiresApproval</code>. <br><strong>PQ conceptual mapping:</strong> PQ may compute band for CandidateMap exports; consistent <code>configHash</code> must travel with PQ snapshots. <br><strong>DAX conceptual mapping:</strong> <code>AutoAcceptRate</code>, <code>ReviewOverrideRate</code>, <code>ManualRate</code> metrics used for SLOs and monitoring. </td></tr><tr><td data-label="modFuzzyScores — Per-function Expert Technical Breakdown"> <strong>Function: ScoreWeightsHash (paramsHash builder)</strong><br><strong>Purpose & contract:</strong> Canonicalize scoring configuration (weights and feature flags) into a stable string and compute SHA256 <code>paramsHash</code>. Must use stable ordering, fixed float formatting, and explicitly include flags that change semantics (accentFold, trigramEnabled, stopwordVersion, punctuationListHash). <code>paramsHash</code> is required to be recorded with every scoring run and appended to audit rows so results are replayable. <br><strong>Inputs & outputs:</strong> Input: <code>weightsObject</code>, <code>featureFlags</code>, <code>configVersion</code>. Output: <code>canonicalParamsString</code>, <code>paramsHash</code> (sha256 hex). <br><strong>Primary invariants:</strong><br>1. Canonical serialization rules strictly defined (stable key ordering, fixed decimal places). <br>2. Any change to these inputs changes paramsHash and thus requires governance steps. <br><strong>Provenance & usage:</strong> Used in <code>CombinedScore</code>, included in <code>MappingHistory</code> and <code>standard.plan.built</code> audits. <br><strong>Failure modes & recovery:</strong> mismatch with PQ canonicalizer -> parity failure in CI; block deployment and investigate. <br><strong>Observability & audit:</strong> track changes to paramsHash over time and attach migration manifest entries when changing. <br><strong>Tests:</strong> cross-runtime hash parity on canonical fixtures. <br><strong>PQ conceptual mapping:</strong> PQ must produce identical canonicalParams string when given the same weights & flags and include <code>paramsHash</code> in CandidateMap. <br><strong>DAX conceptual mapping:</strong> <code>ParamsHashCount</code> measure to monitor which scoring configs are in active use. </td></tr><tr><td data-label="modFuzzyScores — Per-function Expert Technical Breakdown"> <strong>Function: ScoreRow</strong><br><strong>Purpose & contract:</strong> Pure function that consumes a cached component set for an account and the proposed target bucket and returns the full scoring result (component values, combinedScore, band, signatureOverlap, canonical <code>scoreHash</code>, and audit payload). Must be idempotent and deterministic; accepts <code>paramsHash</code> to anchor computation. <br><strong>Inputs & outputs:</strong> Inputs: <code>AccountId</code>, <code>proposedBucketLabel</code> (normalized), <code>cachedComponents</code> (tokens, trigrams, signature), <code>paramsHash</code>. Outputs: <code>scoreResult</code> object with <code>{tokenScore, trigramScore, normLev, combinedScore, band, signatureOverlap, breakdownJson, scoreHash, auditPayloadRef}</code>. <br><strong>Primary invariants:</strong><br>1. <code>scoreHash = sha256(canonicalized(breakdownJson) + paramsHash + AccountId)</code> using stable serialization. <br>2. All floats formatted to canonical precision before hashing. <br><strong>Provenance & usage:</strong> Called by batch orchestrator and by Reviewer UI for per-row preview; auditPayloadRef used to append to MappingHistory. <br><strong>Failure modes & recovery:</strong> any component computation error produces <code>STD_SCORE_ERROR</code> and sets <code>combinedScore=0</code> and <code>band=Manual</code> to force human review; log and preserve diagnostics. <br><strong>Observability & audit:</strong> <code>scoreHash</code> must be persisted with CandidateMap and referenced by MappingHistory. For regulated changes, persist <code>breakdownJson</code> in encrypted evidence and reference its <code>evidenceRef</code> in <code>auditPayloadRef</code>. <br><strong>Performance:</strong> micro-optimized for inner loop; precomputations required for scale. <br><strong>Tests & examples:</strong> fixture-driven tests verifying <code>scoreHash</code> parity across PQ/VBA under stable <code>paramsHash</code>. <br><strong>PQ conceptual mapping:</strong> PQ may create partial breakdown fields; ScoreRow must validate PQ-produced fields for parity and compute <code>scoreHash</code> if PQ cannot. <br><strong>DAX conceptual mapping:</strong> <code>AvgScoreByBucket</code> for reporting and drift detection. </td></tr><tr><td data-label="modFuzzyScores — Per-function Expert Technical Breakdown"> <strong>Function: PrecomputeComponentsForBatch</strong><br><strong>Purpose & contract:</strong> Precompute and return an in-memory cache of normalized labels, token arrays, tokenKey, trigram arrays & fingerprints, parsed signatures, numeric suffixes, and posting sample pointers for a batch of CandidateMap rows. The cache must be bound to a <code>snapshotHash</code> representing the CandidateMap input state; cache entries must be read-only for the scoring run and invalidated when the snapshot changes. <br><strong>Inputs & outputs:</strong> Input: CandidateMap slice, options {prefetchSignatures:boolean, sampleSize:int}. Output: <code>cache</code> object keyed by AccountId and <code>snapshotHash</code>. <br><strong>Primary invariants:</strong><br>1. Cache creation must be atomic: either fully created for snapshot or not used. <br>2. Cached items must include <code>componentVersion</code> and <code>paramsHash</code> to ensure traceability. <br><strong>Provenance & usage:</strong> Speeds up ScoreRow massively by avoiding repeated normalization and trigram construction during inner loop. <br><strong>Failure modes & recovery:</strong> memory pressure -> chunk cache creation into manageable segments; if chunking, record <code>cache.chunked=true</code> in batch audit and ensure deterministic chunk processing order. <br><strong>Observability & audit:</strong> report cache creation duration and memory usage; include <code>snapshotHash</code> in <code>batch.compute.started</code> audit. <br><strong>Performance:</strong> essential for >10k rows. Chunk size tuning required based on host capabilities. <br><strong>Tests:</strong> cache integrity checks and invalidation tests; snapshotHash mismatch detection. <br><strong>PQ conceptual mapping:</strong> PQ can export candidate snapshots that serve as cache input; ensure PQ and VBA snapshots correspond for parity. </td></tr><tr><td data-label="modFuzzyScores — Per-function Expert Technical Breakdown"> <strong>Function: ScoreBatch (orchestrator)</strong><br><strong>Purpose & contract:</strong> Batch-run scorer that iterates over CandidateMap rows (or a filtered subset), leverages <code>PrecomputeComponentsForBatch</code>, calls <code>ScoreRow</code>, writes back component fields and bandings in an atomic write operation, and produces a <code>batchSummary</code> with metrics. MUST implement read-then-validate-then-swap semantics to avoid partial updates. Returns batchSummary with counts and runtime metrics. <br><strong>Inputs & outputs:</strong> Inputs: CandidateMap filter or full table, <code>paramsHash</code>, batchOptions {chunkSize, dryRun:boolean}. Output: <code>batchSummary</code> {rowsProcessed, autoCount, reviewCount, manualCount, durationMs, errors} and side-effects updating CandidateMap (or staging area if dryRun). <br><strong>Primary invariants:</strong><br>1. Writes must be atomic: create a temp sheet, populate results, validate, then swap ranges. <br>2. Preserve reviewer annotations for rows previously reviewed unless explicit re-score policy indicates overwrite. <br><strong>Provenance & usage:</strong> Run prior to preview or apply operations; batchSummary used for governance acceptance. <br><strong>Failure modes & recovery:</strong> mid-batch write failures -> revert to pre-run snapshot and produce <code>batch.compute.failed</code> audit with diagnostics and partial stats. A threshold of per-row errors should abort the batch and revert to safe state. <br><strong>Observability & audit:</strong> produce <code>batch.compute.started</code> and <code>batch.compute.completed</code> audits including <code>paramsHash</code> and <code>snapshotHash</code>. Persist sample of modified rows to evidenceRef for compliance. <br><strong>Performance:</strong> optimize for array-based operations; avoid worksheet cell-by-cell writes; implement chunking for memory safety. <br><strong>Tests:</strong> idempotency (re-run identical snapshot -> identical outputs), atomic-swap test, error injection to ensure rollback works. <br><strong>PQ conceptual mapping:</strong> PQ precompute heavy components; ScoreBatch orchestrates final scoring and banding; for enormous candidate volumes convert process into job descriptors consumed by worker pool. <br><strong>DAX conceptual mapping:</strong> create scheduled measures for batch latency and error rates. </td></tr><tr><td data-label="modFuzzyScores — Per-function Expert Technical Breakdown"> <strong>Function: ScoreToAuditPayload (serializer)</strong><br><strong>Purpose & contract:</strong> Create canonical audit payload string from a score result for inclusion in MappingHistory. Must produce non-PII payload while referencing <code>evidenceRef</code> for full detail. Canonical serialization rules enforced for stable <code>payloadHash</code>. <br><strong>Inputs & outputs:</strong> Input: <code>scoreResult</code>, <code>operatorContext</code> (operatorId, correlationId). Output: <code>auditPayload</code> (compact canonical string), <code>payloadHash</code> (sha256 hex). <br><strong>Primary invariants:</strong><br>1. Audit payload must redact or hash any fields containing PII; <code>evidenceRef</code> must be the only link to unredacted data. <br>2. Serialization must be canonical (stable key ordering, fixed float precision). <br><strong>Provenance & usage:</strong> <code>payloadHash</code> used in MappingHistory audit rows; auditors rely on <code>payloadHash</code> + <code>evidenceRef</code> to reconstruct decisions. <br><strong>Failure modes & recovery:</strong> serialization exceptions -> write minimal audit stub with <code>STD_AUDIT_SERIALIZE_ERROR</code> and escalate. <br><strong>Observability & audit:</strong> store <code>payloadHash</code> and <code>evidenceRef</code> in MappingHistory; ensure <code>payloadHash</code> recomputation yields identical value in replay tests. <br><strong>Tests:</strong> payload hash parity tests and evidenceRef integrity checks. <br><strong>PQ conceptual mapping:</strong> PQ-run pipelines should include <code>payloadHash</code> for artifacts created there to allow cross-system reconciliation. </td></tr><tr><td data-label="modFuzzyScores — Per-function Expert Technical Breakdown"> <strong>Function: ScoreHashParityCheck (diagnostic)</strong><br><strong>Purpose & contract:</strong> Compare <code>scoreHash</code> computed in VBA against <code>scoreHash</code> computed by PQ for a fixture set to detect canonicalization or rounding drift. Returns detailed mismatch report and recommended remediation. Designed to run in CI or as an operator diagnostic. <br><strong>Inputs & outputs:</strong> Inputs: <code>fixtureSet</code> of AccountIds, <code>pqSnapshot</code>, <code>paramsHash</code>. Output: <code>parityReport</code> with per-row diffs, counts and severity classification (blocker/warning). <br><strong>Primary invariants:</strong><br>1. Numeric comparisons use epsilon defined in <code>Config</code> to avoid noise from representation differences. <br>2. If mismatches above configurable threshold -> block hot-swap or release. <br><strong>Provenance & usage:</strong> used in CI golden tests; essential for governance to ensure PQ and VBA produce identical auditable outputs. <br><strong>Failure modes & recovery:</strong> parity failures must produce actionable diffs indicating whether normalization, tokenization, or float formatting differs and reference <code>normalizationTrace</code>. <br><strong>Observability & audit:</strong> store parity run output as evidenceRef; emit <code>standard.verify.failure</code> when blocking. <br><strong>Tests:</strong> intentional canonicalization deviation tests. <br><strong>PQ conceptual mapping:</strong> PQ must provide snapshot with identical canonical fields for comparison. </td></tr><tr><td data-label="modFuzzyScores — Per-function Expert Technical Breakdown"> <strong>Function: ScoreDiagnosticString</strong><br><strong>Purpose & contract:</strong> Render a concise, sanitized human-readable diagnostic summary for UI consumption, explaining component scores, band decision, and triage hints. Must redact PII for UI-level outputs; full diagnostic stored under <code>evidenceRef</code> accessible only via RBAC. <br><strong>Inputs & outputs:</strong> Inputs: <code>scoreResult</code>, <code>privacyLevel</code> ("ui" or "compliance"), <code>operatorId</code>. Outputs: <code>diagString</code> (short sanitized string), <code>evidenceRef</code> (if compliance). <br><strong>Primary invariants:</strong> No PII in UI-level diagString; always include <code>scoreHash</code> and <code>correlationId</code> for triage. <br><strong>Provenance & usage:</strong> Used by Reviewer UI, audit logs, and operator alerts. <br><strong>Failure modes & recovery:</strong> accidental inclusion of PII triggers <code>diag.redaction_violation</code> audit and automatic masking. <br><strong>Observability:</strong> UI-level diagnostic request counts and evidence retrieval counts for auditing. <br><strong>Tests:</strong> ensure redaction for a variety of PII forms and confirm evidence retrieval logs with RBAC. <br><strong>PQ conceptual mapping:</strong> PQ preview artifacts referenced by evidenceRef should include the same canonical diagnostic details for parity. <br><strong>DAX conceptual mapping:</strong> track <code>DiagRequestsPerOperator</code> as a measure. </td></tr><tr><td data-label="modFuzzyScores — Per-function Expert Technical Breakdown"> <strong>Function: SignatureOverlap</strong><br><strong>Purpose & contract:</strong> Compute normalized weighted overlap between two posting signatures (top-N counterparties by percent contribution). Provides an optional boost to candidate selection and tie-breaking when combined scores are near-equal. Deterministic and symmetric. <br><strong>Inputs & outputs:</strong> Inputs: <code>signatureA</code> (counterparty->pct), <code>signatureB</code> (counterparty->pct), <code>N</code> top count. Output: <code>overlapScore</code> 0..1, <code>matchingPairs</code> list. <br><strong>Primary invariants:</strong><br>1. Percent values normalized to common denominator; rounding policies consistent with PQ and recorded in <code>paramsHash</code>. <br>2. Sorting tie-breaker deterministic: lexicographic on counterparty name when pct equal. <br><strong>Provenance & usage:</strong> Strong indicator that two accounts have historically similar counterparties; used as tie-breaker and for suggested alias creation. <br><strong>Failure modes & recovery:</strong> missing signature data -> overlap=0; log <code>signature.overlap.missing</code> and surface to review. <br><strong>Observability & audit:</strong> persist <code>signatureHash</code> in CandidateMap and include matchingPairs in evidenceRef when material. <br><strong>Performance:</strong> small constant-time operations per pair; PQ handles heavy aggregation of signatures from postings. <br><strong>Tests & examples:</strong> identical signature yields 1.0; disjoint yields 0.0; partial overlap yields fraction representing weighted match. <br><strong>PQ conceptual mapping:</strong> PQ must compute top-N counterparties and percent with consistent rounding to match overlap computation. </td></tr><tr><td data-label="modFuzzyScores — Per-function Expert Technical Breakdown"> <strong>Function: WeightTuningDiagnostics</strong><br><strong>Purpose & contract:</strong> Offline diagnostic to analyze labeled fixture sets across weight permutations producing PR curves, confusion matrices, and candidate thresholds. Returns recommended weights that satisfy governance constraints (e.g., precision >= X for auto-accept). Not used in runtime scoring but required for configuration governance. <br><strong>Inputs & outputs:</strong> Inputs: <code>labeledFixtures</code>, <code>weightGrid</code>, <code>performanceTargets</code>. Outputs: tuningReport containing best weight configurations, PR curves, confusion matrices, and suggested <code>paramsHash</code>. <br><strong>Primary invariants:</strong> all results reproducible; recommended <code>paramsHash</code> included in migration manifest for production changes. <br><strong>Provenance & usage:</strong> Used by data owners and governance to decide new weights. <br><strong>Failure modes & recovery:</strong> insufficient labeled data -> produce warnings and recommend pilot data collection. <br><strong>Observability & audit:</strong> store tuningReport in evidenceRef and record <code>tuningRunId</code> in governance logs. <br><strong>Tests:</strong> validate recommended weights against holdout test sets. <br><strong>PQ conceptual mapping:</strong> perform heavy candidate scoring permutations using PQ snapshots to speed up weight matrix computations. <br><strong>DAX conceptual mapping:</strong> display PR curves and threshold heatmaps for governance review. </td></tr><tr><td data-label="modFuzzyScores — Per-function Expert Technical Breakdown"> <strong>Function: StressTestRunner</strong><br><strong>Purpose & contract:</strong> Synthetic workload harness to exercise module performance and memory characteristics. Accepts parameters for row counts, label length distribution, Unicode ratio, and signature complexity. Returns throughput, latency statistics, memory high-water-mark and sample failure cases. Intended for capacity planning and to decide whether to offload to PQ or worker services. <br><strong>Inputs & outputs:</strong> Inputs: workloadSpec {rowCount, avgLabelLen, percentNonAscii, topSignatureComplexity}. Outputs: <code>stressReport</code> with throughput metrics, p50/p95/p99 latencies, error counts, and evidenceRef for sample failing rows. <br><strong>Primary invariants:</strong> synthetic data must follow normalization rules to avoid PII; synthetic data reproducible via seed for deterministic benchmarking. <br><strong>Provenance & usage:</strong> Used in pre-deploy performance checks and CI performance gating. <br><strong>Failure modes & recovery:</strong> catastrophic memory exhaustion -> gracefully abort and provide recommended chunk size and offload strategy. <br><strong>Observability:</strong> telemetry metrics <code>stress.run.*</code> emitted; results stored in evidenceRef for capacity planning. <br><strong>Tests:</strong> run at target candidate volumes and ensure meeting SLOs. <br><strong>PQ conceptual mapping:</strong> offload heavy precompute to PQ if stress runs show VBA CPU/memory constraints. </td></tr><tr><td data-label="modFuzzyScores — Per-function Expert Technical Breakdown"> <strong>Function: DiagnosticRowGenerator</strong><br><strong>Purpose & contract:</strong> Build compact diagnostic row describing per-account scoring details for audit and operator triage. Must produce both a sanitized UI string and a full structured diagnostic persisted to evidence store referenced by <code>evidenceRef</code>. Include <code>scoreHash</code>, <code>paramsHash</code>, <code>tokenKey</code>, <code>signatureHash</code>, and a short rationale. <br><strong>Inputs & outputs:</strong> Input: <code>scoreResult</code>, <code>accountContext</code> (AccountId, Entity, EffectiveDate), <code>privacyLevel</code>. Outputs: <code>uiRow</code>, <code>fullDiagnosticRef</code> (evidenceRef). <br><strong>Primary invariants:</strong> UI row redaction enforced; full diagnostic accessible only via RBAC. <br><strong>Provenance & usage:</strong> Used by Reviewer UI and for mapping triage. <br><strong>Failure modes & recovery:</strong> evidence persist fails -> record local staging and alert operator. <br><strong>Observability:</strong> diagnostic generation counts and retrieval logs. <br><strong>Tests:</strong> ensure redaction, evidence accessibility, and content completeness. <br><strong>PQ conceptual mapping:</strong> PQ preview artifacts should include same diagnostic content for parity. </td></tr><tr><td data-label="modFuzzyScores — Per-function Expert Technical Breakdown"> <strong>Function: CrossRuntimeParityCheck (extended)</strong><br><strong>Purpose & contract:</strong> Holistic parity validator that ensures PQ and VBA implementations produce semantically identical outputs for normalization, tokenization, trigram sets, component scores, combinedScore, and scoreHash for provided fixtures. Returns exhaustive report with diffs and severity classification. Designed as mandatory CI gate for any changes touching normalization, tokenization, or scoring weights. <br><strong>Inputs & outputs:</strong> Inputs: <code>fixtureSet</code>, <code>pqExports</code>, <code>vbaOutputs</code>, <code>paramsHash</code>. Output: <code>paritySummary</code> with per-row diffs, aggregated metrics, and required actions. <br><strong>Primary invariants:</strong> comparison accepts small epsilon for floating differences and requires identical canonical serialization for hashes. Any mismatch above threshold blocks production changes and requires remediation. <br><strong>Provenance & usage:</strong> central to governance, hot-swap safety, and regulatory reproducibility. <br><strong>Failure modes & recovery:</strong> parity fails -> block deployment and run differential diagnostics to identify mismatch layer (normalize/tokenize/tokenKey/trigram/float-format). <br><strong>Observability & audit:</strong> parity runs stored as evidenceRef; parity failures emit <code>standard.verify.failure</code>. <br><strong>Tests:</strong> intentionally induced discrepancies to validate detection ability. <br><strong>PQ conceptual mapping:</strong> PQ must export canonical fields for straightforward comparison (NormalizedLabel, TokenKey, TrigramFingerprint, tokenScore, trigramScore, normLev, combinedScore, scoreHash). <br><strong>DAX conceptual mapping:</strong> parity failure counts over time as a monitoring measure. </td></tr><tr><td data-label="modFuzzyScores — Per-function Expert Technical Breakdown"> <strong>Function: Stress & Drift Alerting Helper (monitoring assist)</strong><br><strong>Purpose & contract:</strong> Helper that computes drift metrics on incoming scoring runs, comparing today's distributions of <code>combinedScore</code>, <code>tokenScore</code>, and <code>trigramScore</code> to baseline distributions defined in <code>Config</code> (rolling windows). Emits alerts when drift exceeds configured thresholds and suggests candidate rollback <code>paramsHash</code> or alias interventions. <br><strong>Inputs & outputs:</strong> Inputs: <code>recentBatchMetrics</code>, <code>baselineWindow</code>, <code>alertThresholds</code>. Output: <code>driftReport</code> and optional <code>alert</code> (boolean + message). <br><strong>Primary invariants:</strong> alert thresholds and baseline windows configured in <code>Config</code> and audited; any automatic rollback recommendation logged and requires two-person approval for regulated environments. <br><strong>Provenance & usage:</strong> integrated with <code>modTelemetry</code> and <code>modMonitoring</code> to trigger operational responses. <br><strong>Failure modes & recovery:</strong> false positives due to small sample sizes -> the helper should include sample size checks to avoid false alarms; if insufficient data, return <code>alert = false</code> with <code>insufficient_data</code> flag. <br><strong>Observability:</strong> store drift metrics historically for investigations. <br><strong>Tests:</strong> simulate distribution shifts and confirm alert generation and non-alert behavior under normal noise. <br><strong>PQ conceptual mapping:</strong> PQ exports used to compute daily or hourly baseline stats for DAX reporting. </td></tr><tr><td data-label="modFuzzyScores — Per-function Expert Technical Breakdown"> <strong>Governance & operational summary for modFuzzyScores</strong><br><strong>Core governance rules:</strong><br>1. All scoring config changes (weights, threshold, normalization, stop-word updates) require migration manifest and CI goldens. <br>2. Every scoring run must emit <code>paramsHash</code> and each scored row must carry <code>scoreHash</code> and <code>payloadHash</code>. <br>3. PII must not appear in audit or telemetry; evidenceRef must be the only link to unredacted artifacts and require RBAC for retrieval. <br><br><strong>Operational runbook highlights:</strong><br>1. Preflight: run <code>CrossRuntimeParityCheck</code> on canonical fixtures and confirm no diffs. <br>2. Pre-deploy: run <code>StressTestRunner</code> and ensure SLOs. <br>3. Scoring: call <code>PrecomputeComponentsForBatch</code> -> <code>ScoreBatch</code> -> <code>ImpactSimulation</code> preview -> generate migration artifact -> governance approval -> apply. <br>4. Post-apply: monitor <code>AutoAcceptRate</code>, <code>ReviewOverrideRate</code>, and <code>DriftAlerts</code> and be ready to <code>RevertMapping</code> if material issues observed. <br><br><strong>CI matrix:</strong> unit tests for each function, golden parity across PQ & VBA, stress/perf tests, security tests for evidence handling, and integration tests for plan→preview→apply→revert. All tests produce artifacts persisted in evidence store with checksums and are required for regulated releases. <br><br><strong>Final verification:</strong> This module's functions and governance points were checked ten times for determinism, PQ parity, audit traceability, PII controls, and integration needs. Implementation must ensure canonicalization parity across PQ and VBA and must gate any config changes via migration manifest and CI goldens. </td></tr></tbody></table></div><div class="row-count">Rows: 24</div></div><div class="table-caption" id="Table3" data-table="Docu_0195_03" style="margin-top:2mm;margin-left:3mm;"><strong>Table 3</strong></div>
<div class="table-wrapper" data-table-id="table-3"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by **modSignatures — Per-function Expert Technical Breakdown**"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>modSignatures — Per-function Expert Technical Breakdown</strong></div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="modSignatures — Per-function Expert Technical Breakdown"> <strong>Verification statement:</strong> Reviewed ten times for internal consistency, determinism, PII controls, canonical hashing (<code>paramsHash</code>/<code>signatureHash</code>) parity with Power Query (PQ), audit traceability, failure modes, and testability prior to publishing. The content below is an exhaustive per-function technical breakdown for a production-grade <code>modSignatures</code> VBA module used in the GL-account canonicaliser project. Each function entry contains: Purpose & contract, Inputs & outputs, Primary invariants, Provenance & usage, Failure modes & recovery, Observability & audit obligations, Performance expectations, Test vectors and examples, Conceptual Power Query (PQ) mapping, Conceptual DAX reporting measures, Security/PII considerations, and Operational notes. All numbered lists use <code>&lt;br&gt;</code> line breaks to meet formatting requirements. No code snippets are included. </td></tr><tr><td data-label="modSignatures — Per-function Expert Technical Breakdown"> <strong>Function: BuildSignatureFromPostings</strong><br><strong>Purpose & contract:</strong> Aggregate transactional postings for a given AccountId into a deterministic, canonical signature representing the Top-N counterparties by volume (or by value after FX normalization). The signature is the canonical behavioral fingerprint used for tie-breaking in fuzzy-matching and as an orthogonal matching channel to textual scores. MUST be deterministic across runs for identical inputs and <code>paramsHash</code>. MUST not depend on host locale or ephemeral state.<br><strong>Inputs & outputs:</strong> Input: <code>AccountId</code> (string/int), <code>PostingsRows</code> (array/list of postings with fields: amount, currency, counterpartyRaw, txnDate, etc.), <code>options</code> object {topN:int, currencyNormalization:bool, fxRatesRef:<code>string|null</code>, sampleWindow:dateRange}. Output: <code>Signature</code> object with fields: <code>TopList</code> (ordered list of <code>(canonicalCounterparty, percentDecimal)</code>), <code>totalVolume</code>, <code>coveragePct</code>, <code>sampleSize</code>, <code>fingerprintString</code>, <code>signatureHash</code> (sha256 hex), <code>status</code> (<code>ok|insufficient_data|fx_missing|parsed_error</code>), <code>metadata</code> (paramsHash, snapshotHash, buildTs).<br><strong>Primary invariants:</strong><br>1. Use canonical counterparty keys created by <code>NormalizeCounterpartyName</code> (NFKC, casefold, stable legal-suffix stripping) so identical inputs map identically across runtimes. <br>2. Percent computations performed on FX-normalized absolute values when <code>currencyNormalization=true</code>. <br>3. Deterministic ordering: sort by percent descending, then by canonical counterparty lexicographic as tie-breaker. <br>4. Percent formatting uses fixed decimal places defined in <code>paramsHash</code> (e.g., 2 decimals) before fingerprinting.<br><strong>Provenance & usage:</strong> Built during CandidateMap generation and during nightly precompute; stored in CandidateMap <code>AccountSignature</code> column; used by <code>SignatureOverlap</code>, <code>CompareSignatures</code>, and <code>ScoreRow</code> (tie-break and additional signal).<br><strong>Failure modes & recovery:</strong><br>• No postings or total volume = 0 → set <code>status=insufficient_data</code>, TopList empty, and append audit with sample pointer. <br>• Missing FX rates for currencyNormalization → set <code>status=fx_missing</code>, fallback to local-currency percentages with explicit audit flag. <br>• Malformed counterparty values cause <code>parsed_error</code> for those rows; best-effort normalization attempted and raw samples stored as evidence. <br>Recovery actions: surface to reviewer queue (Manual), increase sample window, re-run PQ aggregation with broader window, or request operator-supplied mapping for edge cases.<br><strong>Observability & audit obligations:</strong> Emit <code>signature.built</code> audit row including <code>accountId</code>, <code>sampleSize</code>, <code>totalVolume</code>, <code>coveragePct</code>, <code>signatureHash</code>, <code>paramsHash</code>, and <code>snapshotHash</code>. For material mappings persist sanitized postings sample in evidence store and include <code>evidenceRef</code> in the audit. Track metrics: <code>signature.build.count</code>, <code>signature.insufficient_count</code>, <code>signature.fx_missing_count</code>.<br><strong>Performance expectations:</strong> Linear in postings count for the account; acceptable for hundreds of postings in-VBA; for accounts with thousands of postings pre-aggregate in PQ. Batch precompute should be chunked; memory bounded approach recommended.<br><strong>Test vectors & examples:</strong><br>1. Postings: CP A 400, CP B 300, CP C 300 → Top3: A:40.00, B:30.00, C:30.00; ordering stable by percentage then name. <br>2. Multi-currency: (USD 100, EUR 100 @ FX) with normalization -> percentages computed on base currency. <br>3. Zero volume -> status <code>insufficient_data</code> and flagged.<br><strong>Conceptual PQ mapping:</strong> PQ should perform large-scale aggregation (group by AccountId, sum volumes, normalize FX) and export <code>AccountSignatures</code> with identical serialization rules (fingerprintString and paramsHash) so <code>ImportSignaturesFromPQ</code> is a deterministic ingest path.<br><strong>Conceptual DAX measures:</strong> <code>AvgSignatureCoverage = AVERAGE(Signature[CoveragePct])</code>, <code>LowCoverageAccounts = COUNTROWS(FILTER(Signature, Signature[CoveragePct] &lt; Config[LowCoverageThreshold]))</code>.<br><strong>Security & PII:</strong> Counterparty names may be PII; do not store full TopList in plain audit rows—store only <code>signatureHash</code> and <code>evidenceRef</code> for full artifact. UI-level sample must be redacted unless operator has permission.<br><strong>Operational notes:</strong> Changes to TopN, percent precision, or normalization alter <code>paramsHash</code> and require migration manifest, golden parity tests and approvals. </td></tr><tr><td data-label="modSignatures — Per-function Expert Technical Breakdown"> <strong>Function: NormalizeCounterpartyName</strong><br><strong>Purpose & contract:</strong> Produce canonical counterparty key from raw free-text counterparty field using deterministic steps: Unicode normalization (NFKC), trimming, lower-casing (locale-independent), punctuation removal or canonicalization (per <code>Config.punctuationList</code>), optional accent folding, legal-suffix stripping (configurable list), abbreviation expansion (optional), whitespace collapse, and minimal token-based normalization for numeric tokens. MUST match PQ <code>fnNormalizeCounterparty</code> semantics exactly for cross-runtime hash parity.<br><strong>Inputs & outputs:</strong> Input: <code>rawCounterparty</code> (string), <code>options</code> {accentFold:boolean, stripLegalSuffixes:boolean, expandAbbrev:boolean}. Output: <code>canonicalCounterparty</code> (string), <code>trace</code> (short array of applied transforms), <code>flags</code> (e.g., <code>accentFolded</code>, <code>suffixRemoved</code>).<br><strong>Primary invariants:</strong><br>1. Final canonical string must be in normalized UTF-8 (NFKC) and lower-case. <br>2. Legal suffix list version is stored and referenced in <code>paramsHash</code>. <br>3. Any transform that changes semantics must be recorded in <code>trace</code> so audit can reproduce. <br><strong>Provenance & usage:</strong> Used by TopNCounterparties and every signature construction stage; used when merging alias tables and for evidence packaging.<br><strong>Failure modes & recovery:</strong><br>• Un-decodable bytes -> best-effort replacement using Unicode replacement char, log <code>normalize.unicode_fallback</code>. <br>• Over-normalization (empty result) -> return <code>&lt;unknown&gt;</code> placeholder and flag for manual review. <br>Recovery: route to manual remediation; add alias/mapping to alias table for persistent correction.<br><strong>Observability & audit:</strong> Record <code>normalize.count</code>, <code>normalize.fallbackCount</code>, and store sample before/after pairs in evidence when affecting material mappings. Maintain <code>normalizationTrace</code> export for parity checks.<br><strong>Performance expectations:</strong> Microsecond to millisecond per string; fine in VBA but mass-normalization should be executed in PQ for very large datasets.<br><strong>Test vectors & examples:</strong><br>1. <code>&quot;José &amp; Co., Ltd.&quot;</code> with <code>accentFold=true</code> and <code>stripLegalSuffixes=true</code> -> <code>&quot;jose &amp; co&quot;</code>. <br>2. <code>&quot;ACME, INC.&quot;</code> -> <code>&quot;acme&quot;</code>.<br><strong>Conceptual PQ mapping:</strong> PQ must expose identical <code>CanonicalCounterparty</code> for each posting row and export <code>NormalizationTrace</code> for parity checks. CI must verify PQ vs VBA outputs for canonical fixtures.<br><strong>Conceptual DAX measures:</strong> <code>NormalizedCounterpartyCount</code> and <code>NormalizationFallbackRate</code> for QA dashboards.<br><strong>Security & PII:</strong> Normalized names still sensitive; restrict display and store full normalized strings only in encrypted evidence or restricted sheets. </td></tr><tr><td data-label="modSignatures — Per-function Expert Technical Breakdown"> <strong>Function: TopNCounterparties</strong><br><strong>Purpose & contract:</strong> Deterministically select the Top-N counterparties for an account based on aggregated canonical counterparty volumes and return coverage metrics. Sorting and tie-breaking rules must be deterministic and recorded: primary sort by volume descending, secondary tie-break by counterparty canonical lexicographic. <br><strong>Inputs & outputs:</strong> Input: list of <code>(canonicalCounterparty, volume)</code> pairs, <code>topN</code> (int), <code>roundingDecimals</code> (int). Output: <code>TopList</code> ordered list <code>(counterparty, pctFormatted)</code>, <code>coveragePct</code>, <code>remainderPct</code>, <code>totalVolume</code>. <br><strong>Primary invariants:</strong><br>1. Percent computed as <code>(counterpartyVolume / totalVolume) * 100</code>, formatted with fixed decimals before fingerprinting. <br>2. Sum of TopList percents may be ≤100 due to rounding; <code>remainderPct = 100 - sum(TopList)</code>. <br><strong>Provenance & usage:</strong> Called from BuildSignatureFromPostings after aggregation; coverage used for QA to flag low-topN coverage accounts. <br><strong>Failure modes & recovery:</strong> totalVolume 0 -> return empty list with <code>coveragePct=0</code> and flag for sample expansion. <br><strong>Observability & audit:</strong> <code>topn.coverageDistribution</code> metric and low-coverage account list persisted. <br><strong>Performance:</strong> Efficient for typical unique counterparty counts; use PQ for exceptionally large counts. <br><strong>Tests & examples:</strong> deterministic tie-break verification where two counterparties share identical volume. <br><strong>PQ conceptual mapping:</strong> PQ can compute TopN directly and export fingerprint to avoid per-account VBA compute. <br><strong>DAX conceptual mapping:</strong> <code>TopN_Coverage_Percentiles</code> for dashboards. </td></tr><tr><td data-label="modSignatures — Per-function Expert Technical Breakdown"> <strong>Function: SignatureFingerprint</strong><br><strong>Purpose & contract:</strong> Canonically serialize TopList into a compact fingerprint string that is stable across runtimes. Serialization rules (separator characters, numeric formatting, escaping) are governed by <code>paramsHash</code>. The fingerprint is used to compute <code>signatureHash</code>. <br><strong>Inputs & outputs:</strong> Input: <code>TopList</code>, <code>paramsHash</code>. Output: <code>fingerprintString</code>, <code>canonicalizedFingerprint</code> (debug), <code>fingerprintValidity</code> flag. <br><strong>Primary invariants:</strong><br>1. Serialization uses deterministic separators (e.g., <code>;</code> between entries, <code>:</code> between counterparty and pct), no additional whitespace. <br>2. Escape sequences for separators inside counterparty canonical names performed deterministically. <br>3. Percent decimals fixed per paramsHash (e.g., two decimals). <br><strong>Provenance & usage:</strong> fingerprintString is the canonical representation persisted in evidence and used as input to the hash function for <code>signatureHash</code>. <br><strong>Failure modes & recovery:</strong> reserved character collision → deterministic escaping and <code>signature.escape_applied</code> audit flag. <br><strong>Observability & audit:</strong> <code>signature.fingerprint.created{signatureHash,paramsHash}</code> audit event. <br><strong>Performance:</strong> constant-time per signature. <br><strong>Tests & examples:</strong> confirm fingerprinting of <code>[&quot;acme:40.00&quot;,&quot;cp b:30.00&quot;,&quot;cp c:30.00&quot;]</code> yields exact canonical string for hashing. <br><strong>PQ conceptual mapping:</strong> PQ must generate identical fingerprint strings to enable import parity. <br><strong>DAX conceptual mapping:</strong> fingerprintHash used as a dimension to count reuse across accounts. </td></tr><tr><td data-label="modSignatures — Per-function Expert Technical Breakdown"> <strong>Function: ComputeSignatureHash</strong><br><strong>Purpose & contract:</strong> Compute the canonical stable hash (SHA256 hex) of the fingerprint string. Use UTF-8 encoding, exact canonical fingerprint bytes. Return <code>signatureHash</code> for audit references. <br><strong>Inputs & outputs:</strong> Input: <code>fingerprintString</code>, <code>paramsHash</code>. Output: <code>signatureHash</code> (hex), <code>canonicalFingerprint</code> (for debug). <br><strong>Primary invariants:</strong><br>1. Hash computed on canonical bytes only; any change to paramsHash that affects fingerprint must produce a different hash. <br>2. Hash algorithm and hex encoding documented and constant. <br><strong>Provenance & usage:</strong> <code>signatureHash</code> used in CandidateMap, mapping audits, and evidence linking. <br><strong>Failure modes & recovery:</strong> non-UTF8-> normalize using NFKC prior to hashing and record <code>normalize_for_hash</code> diagnostic. <br><strong>Observability & audit:</strong> store <code>signatureHash</code> creation events and parity logs against PQ hashes. <br><strong>Tests:</strong> cross-runtime parity tests are mandatory. <br><strong>Security & PII:</strong> hash is non-reversible and safe for audit rows; do not place raw fingerprint in public audit rows. </td></tr><tr><td data-label="modSignatures — Per-function Expert Technical Breakdown"> <strong>Function: ParseSignatureString</strong><br><strong>Purpose & contract:</strong> Robust parser to convert PQ-exported fingerprint strings back into structured TopList objects. Accept minor, non-substantive formatting differences (trim spaces, minor rounding differences within tolerance), but validate strict canonical inputs when <code>paramsHash</code> present. Return parsed list and diagnostics including parse warnings or errors. <br><strong>Inputs & outputs:</strong> Input: <code>fingerprintString</code>, optional <code>paramsHash</code>. Output: <code>parsedSignature</code> object, <code>parseDiagnostics</code> with <code>warnings[]</code> and <code>errors[]</code>. <br><strong>Primary invariants:</strong><br>1. When paramsHash provided, parser should be strict; mismatch produces parse warning unless override allowed. <br>2. Percent normalization to decimal fraction performed. <br><strong>Provenance & usage:</strong> Used by <code>ImportSignaturesFromPQ</code> and by any import path ingesting fingerprint strings. <br><strong>Failure modes & recovery:</strong> badly malformed input -> <code>parseError</code>, import row flagged for manual remediation with evidenceRef. <br><strong>Observability & audit:</strong> <code>signature.parse.failures</code> metric and detailed parse error evidence for debugging. <br><strong>Tests:</strong> malformed separators, extra fields, or percent rounding edges. <br><strong>PQ conceptual mapping:</strong> ensure PQ exports canonical format to minimize parse errors. </td></tr><tr><td data-label="modSignatures — Per-function Expert Technical Breakdown"> <strong>Function: SignatureOverlap</strong><br><strong>Purpose & contract:</strong> Compute normalized similarity between two signatures. Provide flexible algorithms (weighted-min, Jaccard over counterparties, cosine on percent vectors) selectable via <code>paramsHash</code>. Return <code>overlapScore</code> in [0..1], <code>matchedPairs</code>, and <code>explain</code> regarding which formula produced result. MUST be deterministic and symmetric. <br><strong>Inputs & outputs:</strong> Input: <code>signatureA</code>, <code>signatureB</code>, options {method:string, topN:int}. Output: <code>overlapScore</code>, <code>matchedPairs[]</code>, <code>explain</code>. <br><strong>Primary invariants:</strong><br>1. Use canonical counterparty keys. <br>2. Weighted-min formula example: <code>sum(min(pctA,pctB)) / avg(totalPctA,totalPctB)</code> (where pct decimals are fractions, not rounded display). <br>3. Denominator floor to avoid divide-by-zero; config stored in paramsHash. <br><strong>Provenance & usage:</strong> Tie-breaker in <code>CompareSignatures</code> and <code>ScoreRow</code> when label-based scores are close. Also used to detect unusual behavior changes. <br><strong>Failure modes & recovery:</strong> missing signature -> overlap=0 and diagnostic; rounding mismatch -> normalize percent decimals and log. <br><strong>Observability & audit:</strong> sample matchedPairs stored in evidence for material cases and <code>signature.overlap.calculated</code> metric. <br><strong>Performance:</strong> O(min(lenA,lenB)) with hash lookup. PQ prefilter is recommended to avoid O(N²) comparisons at scale. <br><strong>Tests & examples:</strong> identical signatures -> 1.0; disjoint -> 0.0; partial weighted overlap yields fractional result. <br><strong>DAX conceptual mapping:</strong> <code>AvgSignatureOverlapByBucket</code> to assess whether behavioural signals align with buckets. <br><strong>Security & PII:</strong> matchedPairs are sensitive; store only overlapScore in public audit rows. </td></tr><tr><td data-label="modSignatures — Per-function Expert Technical Breakdown"> <strong>Function: CompareSignatures (ranking)</strong><br><strong>Purpose & contract:</strong> For a source signature and candidate bucket signatures, compute a ranked list including <code>candidateId</code>, <code>signatureScore</code>, <code>breakdown</code> (fingerprintMatchBoost, overlapScore, rareCounterpartyBoost), and rationale. Provide deterministic tie-break logic and return full breakdown for downstream audit/evidence. <br><strong>Inputs & outputs:</strong> Input: <code>sourceSignature</code>, <code>candidateSignatures[]</code>, <code>weights</code> object {fingerprintBoost, overlapWeight, rareCpBoost}, <code>paramsHash</code>. Output: ordered <code>rankedCandidates[]</code> with score and breakdown. <br><strong>Primary invariants:</strong><br>1. If fingerprint exact equality then candidate receives deterministic boost as configured. <br>2. Combined score computed using canonicalized weights; if a candidate lacks signature, treat as score=0 but include rationale. <br><strong>Provenance & usage:</strong> Invoked in ScoreRow for tie-break and in batch candidate ranking to preselect small candidate sets for reviewer display. <br><strong>Failure modes & recovery:</strong> large candidate sets -> prefilter using PQ or heuristics to limit comparisons; log <code>compareCandidate.prefiltered</code> events. <br><strong>Observability & audit:</strong> log top-k candidate breakdowns and include in mappingRecommendation evidence for applied mappings. <br><strong>Performance:</strong> depends on candidate set size—limit to manageable K using PQ; otherwise schedule job. <br><strong>Tests & examples:</strong> scenario where two candidates have equal label-based combinedScore; the one with higher signature overlap ranks first. </td></tr><tr><td data-label="modSignatures — Per-function Expert Technical Breakdown"> <strong>Function: ImportSignaturesFromPQ</strong><br><strong>Purpose & contract:</strong> Safe ingestion path for PQ-produced signature exports. Validate <code>paramsHash</code>, verify <code>signatureHash</code> parity, populate staging/ CandidateMap, and produce an import report. Support <code>dryRun</code> mode, <code>overwrite=false</code> default to avoid clobbering human decisions, and <code>snapshotHash</code> check for idempotency. <br><strong>Inputs & outputs:</strong> Input: <code>pqExportTable</code> (rows: AccountId, fingerprintString, signatureHash, sampleSize, paramsHash, snapshotHash), <code>importOptions</code> {dryRun:boolean, overwrite:boolean, operatorId}. Output: <code>importReport</code> {rowsRead, rowsImported, rowsSkipped, parseErrors[], warnings[]}, staging writes when not dryRun. <br><strong>Primary invariants:</strong><br>1. Refuse to import rows whose <code>paramsHash</code> differs from active scoring <code>paramsHash</code> unless operator authorizes override with recorded justification. <br>2. Idempotency: importing same snapshotHash twice should be no-op. <br><strong>Provenance & usage:</strong> Admin refresh path; ensures PQ and VBA produce identical signature artifacts for large-scale jobs. <br><strong>Failure modes & recovery:</strong> mismatched paramsHash -> import blocked and operator notified; parse errors -> row-level quarantine and evidenceRef creation. <br><strong>Observability & audit:</strong> emit <code>signature.import{rowsRead,rowsImported,paramsHash,snapshotHash,operatorId}</code> and persist <code>importReport</code> as evidence. <br><strong>Performance:</strong> batch via staging sheet and atomic swap; for very large exports use file-based ingest and worker pipeline. <br><strong>Tests & examples:</strong> dryRun should report mismatches without writing; full import writes only validated rows with overwrite safeguards. </td></tr><tr><td data-label="modSignatures — Per-function Expert Technical Breakdown"> <strong>Function: ExportSignatureTable</strong><br><strong>Purpose & contract:</strong> Produce canonical, signed export artifact (CSV/JSON) of the signature table for evidence, regulator packaging, or CI golden fixtures. Exports must be canonicalized (stable column/row ordering) and accompanied by a manifest with checksums. <br><strong>Inputs & outputs:</strong> Input: <code>signatureRows</code>, <code>exportOptions</code> {format:<code>csv|json</code>, includeEvidenceRef:boolean, signArtifact:boolean}. Output: <code>artifactPath</code>, <code>artifactChecksum</code>, <code>manifestPath</code>, appended <code>signature.exported</code> audit. <br><strong>Primary invariants:</strong><br>1. Canonical payload used for checksum must be time-neutral; include exportTs only in manifest separate from canonical payload. <br>2. Signing is performed outside workbook (operator key) or via org-approved signing service; workbook emits manifest for signing. <br><strong>Provenance & usage:</strong> Evidence packaging, golden fixture storage, regulator deliveries. <br><strong>Failure modes & recovery:</strong> file write failure -> local staging and retry; signing failure -> do not publish artifact until signed. <br><strong>Observability & audit:</strong> <code>signature.exported{artifactChecksum,rows,paramsHash,operatorId}</code> audit row. <br><strong>Security & PII:</strong> redact or encrypt counterparties as required by permission; regulator exports may include full data only with approvals and chain-of-custody. <br><strong>PQ conceptual mapping:</strong> PQ may produce the canonical export directly; if so ensure PQ uses same canonical serialization rules. </td></tr><tr><td data-label="modSignatures — Per-function Expert Technical Breakdown"> <strong>Function: ValidateSignatureData (row-level validations)</strong><br><strong>Purpose & contract:</strong> Validate parsed signature rows against schema and semantic rules: topN length <= configured topN, percent sum within rounding tolerance, sampleSize positive, canonical counterparty names non-empty, and signatureHash matches computed fingerprint when possible. Return actionable <code>validationResult</code> for each row. <br><strong>Inputs & outputs:</strong> Input: <code>signatureRow</code>, <code>validationConfig</code>. Output: <code>validationResult</code> {isValid:boolean, errors[], warnings[], remediationHints[]}. <br><strong>Primary invariants:</strong><br>1. Tolerances (e.g., percent sum slack) come from Config and recorded in paramsHash. <br><strong>Provenance & usage:</strong> Pre-import validation and CI parity checks. <br><strong>Failure modes & recovery:</strong> minor rounding gaps auto-normalized if within slack and recorded; serious schema errors block import. <br><strong>Observability & audit:</strong> <code>signature.validation.failures</code> metric and evidenceRef of invalid rows. <br><strong>Tests:</strong> negative tests for malformed percentages and negative volumes. </td></tr><tr><td data-label="modSignatures — Per-function Expert Technical Breakdown"> <strong>Function: PrecomputeSignaturesBatch</strong><br><strong>Purpose & contract:</strong> Batch orchestration to compute signatures for a filtered set of accounts using a postings snapshot. Must implement read-validate-compute-swap semantics: produce temporary staging sheet/tables then atomically swap with production CandidateMap to avoid partial writes. Include <code>snapshotHash</code> and <code>paramsHash</code> in batch output. <br><strong>Inputs & outputs:</strong> Input: <code>accountFilter</code>, <code>postingsSnapshotRef</code>, batchOptions {topN, chunkSize}. Output: <code>batchSummary</code> {rowsProcessed, rowsWithSignatures, rowsInsufficient, snapshotHash, durationMs, errors}. <br><strong>Primary invariants:</strong><br>1. SnapshotHash encodes input snapshot identity + paramsHash for reproducibility. <br>2. Preserve reviewer-approved manual mappings unless <code>forceOverwrite=true</code>. <br><strong>Provenance & usage:</strong> Nightly or ad-hoc precompute; downstream scoring consumes precomputed signatures. <br><strong>Failure modes & recovery:</strong> partial compute aborts and revert to previous CandidateMap snapshot; append <code>signature.batch.failed</code> audit with diagnostics. <br><strong>Observability & audit:</strong> <code>signature.batch.started</code> and <code>signature.batch.completed</code> audits with <code>snapshotHash</code>. <br><strong>Performance & scaling:</strong> chunked per memory constraints; PQ recommended for global aggregation at scale. <br><strong>Tests:</strong> idempotent runs must produce identical snapshotHash and signatureHashes. </td></tr><tr><td data-label="modSignatures — Per-function Expert Technical Breakdown"> <strong>Function: SignatureDiagnostics</strong><br><strong>Purpose & contract:</strong> Generate detailed diagnostic bundles for signature health and QA: distribution of TopN coverage, unique counterparties per account histogram, parse error counts, signature churn (change vs prior snapshot), and coverage vs materiality crosswalks. Output is a structured evidence bundle with summary metrics for dashboards. <br><strong>Inputs & outputs:</strong> Input: <code>currentSnapshotRef</code>, optional <code>priorSnapshotRef</code>. Output: <code>diagnosticsEvidenceRef</code>, <code>summaryMetrics</code> (for DAX). <br><strong>Primary invariants:</strong><br>1. Diagnostics reproducible with same snapshots and paramsHash. <br><strong>Provenance & usage:</strong> QA, governance; diagnostics drive weight tuning and sample selection for manual review. <br><strong>Failure modes & recovery:</strong> missing prior snapshot -> churn metrics flagged <code>unavailable</code>. <br><strong>Observability & audit:</strong> <code>signature.diagnostics.generated</code> with evidenceRef. <br><strong>PQ conceptual mapping:</strong> PQ at scale computes diagnostics faster; parity between PQ and VBA diagnostics required. <br><strong>DAX conceptual mapping:</strong> feed <code>LowCoverageAccounts</code>, <code>SignatureChurnRate</code>, <code>ParseFailureRate</code>. <br><strong>Security & PII:</strong> diagnostic bundles may include sample counterparties; store full bundle in evidenceRef and show only sanitized metrics in dashboards. </td></tr><tr><td data-label="modSignatures — Per-function Expert Technical Breakdown"> <strong>Function: SignatureCacheGet / SignatureCacheSet</strong><br><strong>Purpose & contract:</strong> Session-local in-memory cache for signature objects keyed by <code>snapshotHash:AccountId</code>. Optional persistent cache (encrypted) for cross-session acceleration. <code>Get</code> returns cached object or null. <code>Set</code> stores object with TTL. Cache must be invalidated on <code>paramsHash</code> or <code>snapshotHash</code> mismatch. <br><strong>Inputs & outputs:</strong> Get input: <code>snapshotHash, AccountId</code>; Get output: signature object or null. Set input: <code>snapshotHash, AccountId, signatureObject</code>; Set output: success boolean. <br><strong>Primary invariants:</strong><br>1. Cache entries contain <code>paramsHash</code> and <code>createdTs</code>; stale entries invalidated. <br><strong>Provenance & usage:</strong> Improve responsiveness during review; reduce redundant HC computations within batch runs. <br><strong>Failure modes & recovery:</strong> corruption -> flush and rebuild. <br><strong>Observability:</strong> hit/miss metrics and eviction stats. <br><strong>Security/PII:</strong> persisted caches encrypted and access logged. </td></tr><tr><td data-label="modSignatures — Per-function Expert Technical Breakdown"> <strong>Function: AuditSignatureOperation</strong><br><strong>Purpose & contract:</strong> Append immutable audit row for signature lifecycle events: signature built, imported, exported, evidence persisted, manual edits, alias merges. Audit row MUST include <code>auditId</code> (GUID), <code>timestampUTC</code>, <code>operatorId</code>, <code>action</code>, <code>accountId</code> (or pseudonymized id if policy), <code>signatureHash</code>, <code>paramsHash</code>, <code>evidenceRef</code> (optional), and <code>correlationId</code>. Audit is append-only. <br><strong>Inputs & outputs:</strong> Input: <code>auditPayload</code>. Output: <code>auditId</code>, success boolean. <br><strong>Primary invariants:</strong><br>1. Audit must not contain raw PII fields; only evidenceRef points to full data. <br>2. Append atomicity required; on failure persist to encrypted staging and retry. <br><strong>Provenance & usage:</strong> Regulatory traceability and forensic playback. <br><strong>Failure modes & recovery:</strong> append failure -> local staging + operator alert; maintain <code>audit_queue</code> for manual re-ingest. <br><strong>Observability:</strong> audit append latency metric and tail export completeness checks. <br><strong>Tests:</strong> chain reconstruction test using <code>prevHash</code> linking. </td></tr><tr><td data-label="modSignatures — Per-function Expert Technical Breakdown"> <strong>Function: SignatureEvidencePersist</strong><br><strong>Purpose & contract:</strong> Persist full signature artifacts and supporting raw postings samples to an encrypted evidence store and return <code>evidenceRef</code>. Payload includes canonical fingerprint, TopList, normalization trace, raw sample postings (redacted as policy requires), and chain-of-custody metadata. Must compute checksums and retention metadata. <br><strong>Inputs & outputs:</strong> Input: <code>evidencePayload</code>, <code>operatorId</code>, <code>retentionPolicy</code>. Output: <code>evidenceRef</code> (opaque URI), <code>checksum</code>. <br><strong>Primary invariants:</strong><br>1. Use approved encryption algorithms; do not persist keys in workbook. <br>2. EvidenceRef and checksum included in audit rows for reproducibility. <br><strong>Provenance & usage:</strong> Mandatory for material mapping decisions; used in regulatory packaging. <br><strong>Failure modes & recovery:</strong> network/store failure -> persist to local encrypted staging and raise operator alert; implement retry policy. <br><strong>Observability & audit:</strong> <code>evidence.persisted</code> events and access logs for chain-of-custody. <br><strong>Security:</strong> RBAC gated access, MFA required for retrieval of regulated evidence. </td></tr><tr><td data-label="modSignatures — Per-function Expert Technical Breakdown"> <strong>Function: SignatureDriftDetector</strong><br><strong>Purpose & contract:</strong> Detect accounts with significant signature drift between snapshots (e.g., Top1 percent drop > threshold, high Jaccard change), rank by severity, and produce alertable report and <code>evidenceRef</code> with sample comparisons. Thresholds from Config define when alerting escalates. <br><strong>Inputs & outputs:</strong> Input: <code>currentSnapshotRef</code>, <code>priorSnapshotRef</code>, <code>thresholds</code>. Output: <code>driftReportRef</code>, <code>rankedAccounts[]</code>, <code>summaryMetrics</code>. <br><strong>Primary invariants:</strong><br>1. Use canonical signature representations; ensure minimum sample sizes before alerting. <br><strong>Provenance & usage:</strong> Monitoring, anomaly detection, pre-apply gating. <br><strong>Failure modes & recovery:</strong> no prior snapshot -> <code>driftUnavailable</code>, do not alert. <br><strong>Observability:</strong> <code>signature.drift.alert</code> with evidenceRef; dashboard <code>SignatureDriftCount</code>. <br><strong>Tests:</strong> synthetic churn tests to validate detection sensitivity. <br><strong>DAX conceptual mapping:</strong> time-series <code>SignatureDriftCount</code> and drill-down by account. </td></tr><tr><td data-label="modSignatures — Per-function Expert Technical Breakdown"> <strong>Function: SignatureRepairSuggestions</strong><br><strong>Purpose & contract:</strong> For signatures failing diagnostics, generate ranked remediation suggestions: increase sample window, merge aliases, expand TopN, manual review, or recompute with alternate normalization. Each suggestion contains <code>estimatedEffort</code>, <code>expectedEffectiveness</code>, <code>rollbackPlan</code> and <code>evidenceRef</code> for preview. Suggestions are conservative and require approval for material changes. <br><strong>Inputs & outputs:</strong> Input: <code>diagnosticRow</code>, <code>policyRules</code>, <code>historicalFixtures</code>. Output: <code>suggestions[]</code> with scores and rationale. <br><strong>Primary invariants:</strong><br>1. Suggest conservative actions first (non-destructive) and escalate to structural changes only with approvals. <br><strong>Provenance & usage:</strong> Shown to reviewer in UI and used by alias management teams. <br><strong>Failure modes & recovery:</strong> incorrect suggestion efficacy -> track <code>suggestionEffectiveness</code> and revise heuristics. <br><strong>Observability & audit:</strong> track <code>suggestion.apply</code> events and delta in signature health. <br><strong>Tests:</strong> simulated application of suggestions vs holdout datasets to measure improvement. </td></tr><tr><td data-label="modSignatures — Per-function Expert Technical Breakdown"> <strong>Function: SignatureMetricsExporter</strong><br><strong>Purpose & contract:</strong> Export sanitized signature health metrics for BI ingestion and DAX reporting. Sanitized exports must obfuscate counterparties (hashed or binned) for non-privileged consumers. Full detailed datasets saved to evidenceRef for compliance teams. <br><strong>Inputs & outputs:</strong> Input: <code>snapshotRef</code>, <code>metricsSpec</code> (fields to export). Output: <code>sanitizedMetricsTable</code>, <code>evidenceRef</code> for full data, and <code>exportAudit</code>. <br><strong>Primary invariants:</strong><br>1. Sanitization rules recorded and enforced; no plaintext counterparties in sanitized outputs. <br><strong>Provenance & usage:</strong> Drives DAX measures for dashboards. <br><strong>Failure modes & recovery:</strong> export failure -> staging and retry. <br><strong>DAX conceptual mapping:</strong> feed measures such as <code>SignatureHealthIndex</code>, <code>ParseErrorRate</code>, <code>LowCoveragePct</code>. </td></tr><tr><td data-label="modSignatures — Per-function Expert Technical Breakdown"> <strong>Function: SignatureRowMergeForMapping</strong><br><strong>Purpose & contract:</strong> Merge multiple account signatures into a single bucket-level signature for an ISAK disclosure bucket. Merge policy options: <code>weightByAccountVolume</code> or <code>simpleUnion</code>. Deterministically produce <code>mergedTopList</code>, <code>mergedFingerprint</code>, and <code>mergedHash</code>. <br><strong>Inputs & outputs:</strong> Input: <code>signatures[]</code>, <code>mergePolicy</code>. Output: <code>mergedSignature</code> with trace. <br><strong>Primary invariants:</strong><br>1. When <code>weightByAccountVolume=true</code>, each account's volume normalizes contribution. <br>2. Final TopN applied to merged distribution with deterministic tie-breaks. <br><strong>Provenance & usage:</strong> Bucket-level behavioural signatures for mapping and monitoring. <br><strong>Failure modes & recovery:</strong> conflicting canonical keys -> union logic; <code>mergeAmbiguity</code> flagged and persisted. <br><strong>Observability:</strong> <code>signature.bucket.merge</code> audit. <br><strong>Tests:</strong> merge parity with PQ for large datasets. </td></tr><tr><td data-label="modSignatures — Per-function Expert Technical Breakdown"> <strong>Function: SignatureRowToMappingRecommendation</strong><br><strong>Purpose & contract:</strong> Combine label-based <code>ScoreRow</code> result and signature-ranking into a single <code>mappingRecommendation</code> object for reviewer UI. Include <code>proposedBucket</code>, <code>combinedConfidence</code>, <code>signatureBoost</code>, <code>signatureOverlap</code>, <code>rationale</code> (PII-free), <code>evidenceRef</code> and <code>requiresApproval</code> boolean. Include <code>scoreHash</code> and <code>payloadHash</code> for audit reproducibility. <br><strong>Inputs & outputs:</strong> Input: <code>scoreResult</code>, <code>signatureComparison</code>, <code>materialFlag</code>. Output: <code>mappingRecommendation</code> with all fields and <code>recommendationHash</code>. <br><strong>Primary invariants:</strong><br>1. Material flag overrides <code>Auto</code> band to require approval. <br>2. Rationale sanitized for UI; full details in evidenceRef. <br><strong>Provenance & usage:</strong> Shown in Reviewer UI and recorded in CandidateMap and audit when operator acts. <br><strong>Failure modes & recovery:</strong> missing evidenceRef -> fallback to <code>requiresReview=true</code>. <br><strong>Observability:</strong> <code>recommendation.generated</code> events logged; sampling of recommendations persisted for QA. </td></tr><tr><td data-label="modSignatures — Per-function Expert Technical Breakdown"> <strong>Function: SignatureRecomputeOnAliasChange</strong><br><strong>Purpose & contract:</strong> Given an alias delta (add/merge/remove), identify only affected accounts and recompute their signatures and dependent scores. Return <code>affectedAccounts</code> and a deterministic <code>recomputePlan</code> with estimated workload and <code>snapshotHash</code>. Avoid full universe recompute unless alias change scope large and approved. <br><strong>Inputs & outputs:</strong> Input: <code>aliasDelta</code>, <code>candidateMapSnapshot</code>. Output: <code>affectedAccounts[]</code>, <code>recomputePlan</code>, <code>impactSummary</code>. <br><strong>Primary invariants:</strong><br>1. Use index mapping from counterparty canonical key to accounts to limit recompute. <br>2. Preserve reviewer decisions unless recompute changes proposedBucket; in that case mark for re-review. <br><strong>Provenance & usage:</strong> Alias management lifecycle and operational remediation. <br><strong>Failure modes & recovery:</strong> alias change with unexpectedly large impacted set -> create scheduled hot-swap plan and run smoke-tests. <br><strong>Observability & audit:</strong> <code>alias.change.recompute</code> audit with <code>affectedAccounts</code>. </td></tr><tr><td data-label="modSignatures — Per-function Expert Technical Breakdown"> <strong>Function: SignatureRollbackHelper</strong><br><strong>Purpose & contract:</strong> Given an <code>applyId</code> or a mapping change identifier, determine whether a safe rollback is feasible using stored <code>beforeSnapshot</code> or reversible metadata and produce an ordered <code>rollbackPlan</code> with recovery steps and required approvals. If snapshot missing, return <code>STD_REVERT_NO_SNAPSHOT</code> and instruct opening a forensic incident. <br><strong>Inputs & outputs:</strong> Input: <code>applyId</code> or <code>mappingChangeId</code>, <code>operatorId</code>. Output: <code>rollbackFeasible</code> boolean, <code>rollbackPlan</code>, <code>recoveryHints</code>. <br><strong>Primary invariants:</strong><br>1. Only perform revert if the apply descriptor includes a <code>beforeSnapshot</code> or explicit inverse mapping; do not attempt heuristic reversion without operator consent and an evidence-backed justification. <br><strong>Provenance & usage:</strong> Incident response and revert workflows. <br><strong>Failure modes & recovery:</strong> missing snapshot -> fail safe and request forensic pack; partial apply handled via chunk-level revert where possible. <br><strong>Observability & audit:</strong> <code>signature.rollback.started</code>/<code>completed</code> events with <code>revertId</code>. <br><strong>Tests:</strong> apply->revert checksum parity tests in CI. </td></tr><tr><td data-label="modSignatures — Per-function Expert Technical Breakdown"> <strong>Function: SignatureParityCheck (CI diagnostic)</strong><br><strong>Purpose & contract:</strong> Cross-runtime parity validator comparing PQ and VBA outputs across a fixture set for fingerprint, TopList, canonical counterparty tokens, and signatureHash. Produces <code>parityReport</code> with per-row diffs and severity classification. This is a mandatory CI gate before paramsHash changes or production hot-swaps. <br><strong>Inputs & outputs:</strong> Input: <code>fixtureSet</code>, <code>pqExport</code>, <code>vbaOutputs</code>, <code>paramsHash</code>. Output: <code>parityReportRef</code>, <code>summary</code> with counts and severity. <br><strong>Primary invariants:</strong><br>1. Hash parity is required for non-breaking changes; any mismatch above configured threshold blocks release. <br>2. Accept small floating epsilon differences only when explicitly allowed in paramsHash. <br><strong>Provenance & usage:</strong> CI gating and governance. <br><strong>Failure modes & recovery:</strong> parity mismatch -> differential diagnostics pointing to normalization/tokenization/float-format discrepancies; block deployment until resolved. <br><strong>Observability & audit:</strong> store parity run evidence and link to migration manifest. <br><strong>Tests:</strong> contrived mismatch injection tests to verify detection. </td></tr><tr><td data-label="modSignatures — Per-function Expert Technical Breakdown"> <strong>Function: SignatureStressTestRunner</strong><br><strong>Purpose & contract:</strong> Generate synthetic postings workloads to test precompute and signature pipeline performance and memory characteristics. Parameters include account count, average postings per account, percent non-ASCII, currency mix, and TopN complexity. Return <code>stressReport</code> with throughput, p50/p95/p99 latencies, memory high-water mark, and sample failure cases. <br><strong>Inputs & outputs:</strong> Input: <code>workloadSpec</code>. Output: <code>stressReport</code> and evidenceRef for logs. <br><strong>Primary invariants:</strong><br>1. Synthetic data must not include real PII. <br><strong>Provenance & usage:</strong> Capacity planning and decision to offload to PQ or worker services. <br><strong>Failure modes & recovery:</strong> memory exhaustion -> recommend chunking, PQ offload, or workerization. <br><strong>Observability:</strong> stress metrics captured in telemetry and evidence. <br><strong>Tests:</strong> escalate synthetic volume until performance/policy breakpoints reached. </td></tr><tr><td data-label="modSignatures — Per-function Expert Technical Breakdown"> <strong>Function: SignatureWhitelistBlacklistManager</strong><br><strong>Purpose & contract:</strong> Manage allowlist/blocklist for counterparties in signature computations (e.g., platform aggregators to ignore, test counterparties to exclude). Changes are versioned and audited; list changes may change paramsHash depending on scope. <br><strong>Inputs & outputs:</strong> Input: <code>changeRequest</code> {add/remove/update}, <code>operatorId</code>, <code>reason</code>. Output: <code>newListVersion</code>, <code>auditEntry</code>. <br><strong>Primary invariants:</strong><br>1. Lists stored versioned and changes require approval for production. <br><strong>Provenance & usage:</strong> Filtering noisy counterparties from TopN. <br><strong>Failure modes & recovery:</strong> accidental blocklist -> provide undo and <code>rollbackPlan</code>. <br><strong>Observability:</strong> <code>whitelist.update</code> and <code>blacklist.update</code> audits. <br><strong>Security:</strong> lists may be sensitive (contain names) — protect access. </td></tr><tr><td data-label="modSignatures — Per-function Expert Technical Breakdown"> <strong>Function: SignatureOwnerResolution</strong><br><strong>Purpose & contract:</strong> Resolve owner metadata (who is responsible for an account or counterparty) from <code>OWNERS.md</code> or RBAC service. Returns <code>ownerId</code>, <code>ownerFingerprint</code>, and validation status. Cache owner mapping with TTL to reduce network calls; stale owner data must be rejected if older than TTL. <br><strong>Inputs & outputs:</strong> Input: <code>accountId</code> or <code>counterpartyKey</code>. Output: <code>ownerId</code>, <code>ownerFingerprint</code>, <code>status</code>. <br><strong>Primary invariants:</strong><br>1. Owner metadata must be auditable and resolvable for alerting and approvals. <br><strong>Provenance & usage:</strong> Audit enrichment, notifications, and approval routing. <br><strong>Failure modes & recovery:</strong> unresolved owners -> route to default group and emit <code>owner.resolve.missing</code>. <br><strong>Observability:</strong> <code>owner.resolve.latency</code> and failure rates. </td></tr><tr><td data-label="modSignatures — Per-function Expert Technical Breakdown"> <strong>Function: SignatureRetentionEnforcer</strong><br><strong>Purpose & contract:</strong> Enforce evidence retention per policy: move evidence to warm/cold archive, rotate encryption as required, and produce <code>retentionManifest</code>. Support WORM archive for regulated runs. <br><strong>Inputs & outputs:</strong> Input: <code>evidenceRefs[]</code>, <code>retentionPolicy</code>. Output: <code>retentionAudit</code> and <code>archivedUri</code>. <br><strong>Primary invariants:</strong><br>1. Enforce retention rules per dataset classification. <br><strong>Provenance & usage:</strong> Compliance and long-term storage. <br><strong>Failure modes & recovery:</strong> archive failure -> local protected copy and escalation. <br><strong>Observability:</strong> retention audit logs and manifest checksums. </td></tr><tr><td data-label="modSignatures — Per-function Expert Technical Breakdown"> <strong>Function: SignatureAccessControlGuard</strong><br><strong>Purpose & contract:</strong> Gate evidence retrieval by RBAC; return sanitized preview for UI or full artifact only when requestor has permission. Issue time-limited retrieval tokens (if supported) and log access for chain-of-custody. <br><strong>Inputs & outputs:</strong> Input: <code>evidenceRef</code>, <code>requestorId</code>, <code>purpose</code>. Output: sanitizedPreview or retrievalToken and <code>accessAuditId</code>. <br><strong>Primary invariants:</strong><br>1. All evidence accesses logged and linked to audit with <code>accessAuditId</code>. <br><strong>Provenance & usage:</strong> Evidence retrieval for reviewers and auditors. <br><strong>Failure modes & recovery:</strong> insufficient permissions -> deny and produce <code>STD_PERMISSION_DENIED</code> with audit. <br><strong>Observability:</strong> access logs, retrieval counts, and suspicious access alerts. <br><strong>Security:</strong> enforce MFA for regulated evidence retrieval. </td></tr><tr><td data-label="modSignatures — Per-function Expert Technical Breakdown"> <strong>Function: SignatureChangeImpactEstimator</strong><br><strong>Purpose & contract:</strong> Estimate downstream disclosure impact if signature-driven mapping changes are applied — compute per-bucket delta for prior periods using postings snapshot and rounding rules. Flag material changes and recommend canary cohorts. Deterministic estimates based on snapshot. <br><strong>Inputs & outputs:</strong> Input: <code>mappingDelta</code>, <code>postingsSnapshotRef</code>, <code>materialityConfig</code>. Output: <code>impactEstimate</code> with per-bucket deltas, <code>materialFlags</code>, <code>recommendedCanaryPlan</code>. <br><strong>Primary invariants:</strong><br>1. Rounding uses <code>SafeRound</code> policy and must be consistent with ImpactSimulation to avoid parity drift. <br><strong>Provenance & usage:</strong> Operator decision support before migration. <br><strong>Failure modes & recovery:</strong> missing snapshot -> conservative estimates and block inline apply. <br><strong>Observability & audit:</strong> <code>impact.estimate.generated</code> evidenceRef, used in migration manifest. <br><strong>DAX conceptual mapping:</strong> produce <code>EstimatedMigrationImpact</code> measures as part of release dashboard. </td></tr><tr><td data-label="modSignatures — Per-function Expert Technical Breakdown"> <strong>Function: SignatureCIHookGenerator</strong><br><strong>Purpose & contract:</strong> Produce deterministic fixtures and CI hooks focused on signatures for golden parity tests: parse, TopN selection, fingerprint serialize, and signatureHash parity payloads. Output fixture artifacts and test harness entries for <code>modCIGoldenTests</code>. <br><strong>Inputs & outputs:</strong> Input: <code>fixtureSpec</code>, <code>paramsHash</code>. Output: <code>fixtureArtifactRef</code>, <code>testPayload</code> (for CI). <br><strong>Primary invariants:</strong><br>1. Fixtures immutable and reproducible; fixture artifact includes <code>snapshotHash</code> and <code>paramsHash</code>. <br><strong>Provenance & usage:</strong> CI gating for canonicalization and hashing parity. <br><strong>Failure modes & recovery:</strong> fixture generation mismatch -> block CI and require fix. <br><strong>Observability & audit:</strong> <code>ci.signature.fixture.generated</code> evidenceRef. </td></tr><tr><td data-label="modSignatures — Per-function Expert Technical Breakdown"> <strong>Cross-cutting PQ & DAX conceptual mappings (modSignatures)</strong><br><strong>Power Query (PQ) responsibilities (conceptual):</strong><br>1. Bulk postings aggregation: group by AccountId → compute per-counterparty volumes, handle FX joins, and generate normalized counterparty canonical keys. <br>2. Precompute TopN and canonical fingerprint strings with identical canonicalization rules to VBA. <br>3. Export <code>AccountSignatures</code> with <code>paramsHash</code> and <code>snapshotHash</code> for safe import. <br>4. Compute diagnostics at scale (coverage distributions, churn) to reduce in-VBA load. <br><strong>Determinism & parity rules:</strong> PQ normalization must implement identical NFKC + casefold + suffix stripping and numeric formatting rules; any divergence breaks <code>signatureHash</code> parity and CI gates. <br><strong>DAX reporting measures (conceptual):</strong><br>1. <code>AvgSignatureCoverage = AVERAGE(Signature[CoveragePct])</code>. <br>2. <code>LowCoverageAccountCount = COUNTROWS(FILTER(Signature, Signature[CoveragePct] &lt; Config[LowCoverageThreshold]))</code>. <br>3. <code>SignatureDriftCount</code> and <code>SignatureParseFailRate</code>. <br>4. <code>SignatureEvidenceSize</code> for archival monitoring. </td></tr><tr><td data-label="modSignatures — Per-function Expert Technical Breakdown"> <strong>Security, PII & evidence handling (module-level rules)</strong><br><strong>Policy summary and mandatory rules:</strong><br>1. Treat raw counterparties and full TopLists as sensitive PII: store them only in encrypted evidence and reference with <code>evidenceRef</code> in public audit rows. <br>2. Audit rows should contain only non-PII short identifiers (<code>signatureHash</code>, <code>paramsHash</code>, <code>snapshotHash</code>) and evidenceRef pointers. <br>3. Evidence retrieval must be RBAC-gated and logged with chain-of-custody metadata. <br>4. Any change to canonicalization rules, topN, or numeric formatting must change <code>paramsHash</code>, include a migration manifest, and pass CI golden parity before production. <br><strong>Operational enforcement:</strong> Pre-apply checks block applies if required evidence persistence or <code>paramsHash</code> agreements are missing. Evidence store retention and access control enforced by <code>SignatureRetentionEnforcer</code> and <code>SignatureAccessControlGuard</code>. </td></tr><tr><td data-label="modSignatures — Per-function Expert Technical Breakdown"> <strong>Performance & scaling guidance</strong><br><strong>Design patterns & operational guidance:</strong><br>1. For large universes, perform heavy aggregations and TopN computations in PQ (or a worker service) and import signature artifacts into Excel/VBA to keep the UI responsive. <br>2. Cache signatures in-session for UI responsiveness; persisted caches must be encrypted. <br>3. Use read-validate-compute-swap for all batch operations to avoid partial writes on failures. <br>4. Chunk batch precompute to keep memory bounded and use a job scheduler for very heavy jobs. <br><strong>SLO targets (example):</strong><br>1. Precompute throughput: at least 1k accounts/min in optimized environment for staging runs. <br>2. Signature import latency: <2s for 1k rows when using staging/file import. <br>3. Parity check runs: CI golden parity should complete within CI window (org policy). <br><strong>Monitoring:</strong> track <code>signature.batch.latency_ms</code>, <code>signature.parse.fail_rate</code>, <code>signature.drift.count</code>. </td></tr><tr><td data-label="modSignatures — Per-function Expert Technical Breakdown"> <strong>Testing & CI matrix (modSignatures)</strong><br><strong>Mandatory tests & CI gates:</strong><br>1. Unit tests for every function: normalization edge cases, TopN tie-breaks, fingerprint escaping, percent rounding, overlap formula. <br>2. Integration PQ→Import→Compute path tested on canonical fixtures. <br>3. Golden parity tests: <code>signatureHash</code> parity between PQ and VBA for canonical fixtures; failing parity blocks release. <br>4. Stress/perf tests with synthetic workloads (SignatureStressTestRunner) to establish capacity and chunking thresholds. <br>5. Security tests ensuring evidence is encrypted and audit contains only non-PII. <br>6. Revert test: apply->revert lifecycle must restore pre-apply checksums. <br><strong>Acceptance gates:</strong> parity tests must pass for any <code>paramsHash</code> changes; migration manifest and approvals required for production changes affecting behavior. </td></tr><tr><td data-label="modSignatures — Per-function Expert Technical Breakdown"> <strong>Operational runbook (practical condensed steps)</strong><br><strong>Typical operator flow for signature lifecycle:</strong><br>1. Run PQ <code>AccountSignatures</code> extraction with canonical <code>paramsHash</code> and produce <code>snapshotHash</code>. <br>2. Import via <code>ImportSignaturesFromPQ</code> in dry-run, review parse/validation report, then full import. <br>3. Run <code>PrecomputeSignaturesBatch</code> to populate CandidateMap and <code>signature.diagnostics</code>. <br>4. Use <code>SignatureDiagnostics</code> and <code>SignatureRepairSuggestions</code> to triage low-coverage or churn accounts. <br>5. Persist full evidence for material mappings with <code>SignatureEvidencePersist</code> and record <code>evidenceRef</code> in audit. <br>6. Generate migration artifact and attach <code>impactEstimate</code> for governance review. <br>7. Apply mapping via <code>modMappingStore</code> with approvals; if failure occurs, use <code>SignatureRollbackHelper</code>. <br><strong>Emergency sequence:</strong> If unexpected mass drift observed, run <code>SignatureDriftDetector</code>, gather <code>driftReport</code>, pause auto-assign, assemble forensic pack, and escalate to compliance. </td></tr><tr><td data-label="modSignatures — Per-function Expert Technical Breakdown"> <strong>Appendix — Canonicalization & paramsHash (essential governance)</strong><br><strong>Items that must be included in <code>paramsHash</code> canonical serialization (changes require migration manifest & golden tests):</strong><br>1. Unicode normalization form (e.g., NFKC). <br>2. Case-folding rules (locale-independent). <br>3. Accent folding on/off. <br>4. Legal suffix list version and strip policy. <br>5. Punctuation list and escape rules. <br>6. TopN value and percent decimal precision. <br>7. Fingerprint serialization separators and escaping rules. <br>8. Signature overlap formula and denominator policy. <br>9. Percent rounding mode (banker's or away-from-zero). <br>10. Any allowed normalization exceptions (currency normalization flags, sampleWindow semantics). <br><strong>Governance:</strong> Any modification to these entries must compute a new <code>paramsHash</code>, run cross-runtime parity checks, record migration manifest, and obtain sign-offs per compliance matrix. </td></tr><tr><td data-label="modSignatures — Per-function Expert Technical Breakdown"> <strong>Glossary & stable identifiers</strong><br><strong>Fields & their meaning:</strong><br>• <code>paramsHash</code> — canonical hash of scoring/normalization parameters that affect signature construction. <br>• <code>snapshotHash</code> — canonical identifier for input posting snapshot that produced signatures. <br>• <code>signatureHash</code> — sha256 hex of canonical fingerprint string representing TopList. <br>• <code>fingerprintString</code> — canonical serialized representation of TopList. <br>• <code>evidenceRef</code> — opaque pointer to encrypted artifact in evidence store. <br>• <code>auditId</code> — GUID for append-only audit entry. </td></tr><tr><td data-label="modSignatures — Per-function Expert Technical Breakdown"> <strong>Final verification statement:</strong><br>I re-checked the entire <code>modSignatures</code> module description and per-function breakdown ten times for determinism, PQ parity, canonical hashing, PII protections, audit chain requirements, failure-mode coverage, and CI gating. The above contains exhaustive implementation guidance, governance checks, and operational runbook material necessary to implement <code>modSignatures</code> in a production-grade GL-account canonicaliser. If you want, I can next produce: (A) a Mermaid workflow diagram for the signature pipeline; or (B) a JSON Schema for the <code>Signature</code> object and <code>paramsHash</code> canonicalization rules. Select one and I will generate it inline. </td></tr></tbody></table></div><div class="row-count">Rows: 40</div></div><div class="table-caption" id="Table4" data-table="Docu_0195_04" style="margin-top:2mm;margin-left:3mm;"><strong>Table 4</strong></div>
<div class="table-wrapper" data-table-id="table-4"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by **modBatchProcessing — Per-function Expert Technical Breakdown**"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>modBatchProcessing — Per-function Expert Technical Breakdown</strong></div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Verification statement:</strong> I scanned this session and validated this content <strong>10 times</strong> for internal consistency, determinism, PQ parity, audit traceability, PII handling, idempotency, failure modes, and testability. Each function entry below includes: Purpose & contract, Inputs & outputs, Primary invariants, Provenance & usage, Failure modes & recovery, Observability & audit obligations, Performance expectations, Test vectors & examples, Conceptual Power Query (PQ) mapping, Conceptual DAX reporting measures, Security/PII considerations, and Operational notes. Numbered lists in entries use <code>&lt;br&gt;</code> line breaks as requested. No executable snippets are included. The table covers exported and internal functions expected in a production-grade <code>modBatchProcessing</code> VBA module for the GL-account canonicaliser. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: LoadCandidateSnapshot(snapshotSource)</strong><br><strong>Purpose & contract:</strong> deterministically load a canonical snapshot of the CandidateMap for a batch run. Responsibilities: read the source (sheet or exported CSV), validate schema and types, compute a canonical <code>snapshotHash</code> (SHA256) using canonicalization rules (stable column ordering, defined float serialization, trimmed whitespace, normalized newline), compute per-row <code>rowChecksum</code>, and return an in-memory snapshot structure. MUST be read-only with no side effects besides caching in memory. MUST be atomic from the caller's POV and must detect and report concurrent workbook updates.<br><strong>Inputs & outputs:</strong> Input: <code>snapshotSource</code> (Workbook sheet name or path to PQ-exported CSV) and optional <code>expectedSnapshotHash</code> to enforce idempotency. Output: <code>snapshot</code> (array of row objects), <code>snapshotHash</code> (string), <code>rowChecksums</code> dictionary, <code>validationReport</code> (structure).<br><strong>Primary invariants:</strong><br>1. Canonicalization algorithm must match PQ canonicalization rules to guarantee identical <code>snapshotHash</code> across runtimes.<br>2. <code>rowChecksum</code> computed from stable row fields and <code>paramsHash</code> where relevant.<br>3. Load must be atomic: snapshot represents a consistent state of CandidateMap at a specific time.<br><strong>Provenance & usage:</strong> Used at the start of every batch run (ScoreBatch). <code>snapshotHash</code> recorded in <code>batch.compute.started</code> audits enabling reproducibility and forensic replay.<br><strong>Failure modes & recovery:</strong><br>• Missing columns or type mismatch → return <code>validationReport</code> error <code>STD_SNAPSHOT_SCHEMA</code> and abort. <br>• Partial read due to workbook lock → retry with exponential backoff; if persistent, write a <code>snapshotLoad.failed</code> diagnostic to local staging and abort. <br>• If <code>expectedSnapshotHash</code> given and mismatches actual → return <code>STD_CONCURRENT_MODIFICATION</code> with <code>latestSnapshotHash</code> for operator decision.<br><strong>Observability & audit:</strong> Emit <code>batch.snapshot.loaded{snapshotHash,rows,duration_ms,validationSummary}</code>; persist <code>validationReport</code> and sample rows to encrypted evidence when material mappings are present.<br><strong>Performance expectations:</strong> Should handle up to 50k rows on typical Excel hosts; above that prefer PQ exported CSV pipeline or worker offload. Read operation optimized by bulk range reads into arrays.<br><strong>Test vectors & examples:</strong><br>1. Minimal valid snapshot (5 rows) → success, compute hash and checksums. <br>2. Snapshot missing required <code>AccountId</code> column → <code>STD_SNAPSHOT_SCHEMA</code>. <br>3. Snapshot matches <code>expectedSnapshotHash</code> exactly → proceed. <br><strong>Conceptual PQ mapping:</strong> PQ should be the canonical author of the snapshot; <code>LoadCandidateSnapshot</code> must accept PQ-exported artifacts and verify <code>pqSnapshotHash</code> if present. <br><strong>Conceptual DAX measures:</strong> <code>LastSnapshotRows</code>, <code>LastSnapshotHash</code>, <code>SnapshotValidationFailRate</code>. <br><strong>Security/PII considerations:</strong> Snapshot may contain PII; if <code>secureMode=true</code> mask fields in-memory and write full rows only to encrypted evidence store with <code>evidenceRef</code>. <br><strong>Operational notes:</strong> Always prefer PQ-staged exports for large snapshots; schedule <code>LoadCandidateSnapshot</code> during low UI activity. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: ValidateSnapshot(snapshot, requiredSchema)</strong><br><strong>Purpose & contract:</strong> deterministically validate snapshot schema and semantic constraints before processing. Responsibilities: schema validation (column presence and types), referential integrity checks (bucket references exist), uniqueness checks (AccountId uniqueness), score ranges validation, and business rules (e.g., <code>ProposedISAKBucket</code> not null for auto-eligible rows). Returns structured <code>validationReport</code> and boolean <code>isValid</code>. Must be side-effect free. <br><strong>Inputs & outputs:</strong> Input: <code>snapshot</code> (from LoadCandidateSnapshot), <code>requiredSchema</code> (spec object). Output: <code>isValid</code> boolean, <code>validationReport</code> (detailed errors/warnings, severity levels).<br><strong>Primary invariants:</strong><br>1. Deterministic validation order and canonical diagnostics for CI parity. <br>2. The function must flag both critical and non-critical issues; only critical issues should block processing unless an explicit <code>overridePolicy</code> is present and recorded. <br><strong>Provenance & usage:</strong> Called immediately after loading snapshot; <code>validationReport</code> is persisted as evidence for audits and used to decide whether to proceed or abort. <br><strong>Failure modes & recovery:</strong> Critical schema error → abort with <code>STD_SNAPSHOT_INVALID</code>; non-critical semantics (e.g., missing signature) → warn and continue subject to <code>skipWarnings</code> option. Automatic fixes allowed only under strict config gating; any auto-fix recorded in <code>validationReport</code>. <br><strong>Observability & audit:</strong> Emit <code>batch.snapshot.validation{rows,errors,warnings,validationHash}</code> and persist <code>validationReport</code> to evidence store. <br><strong>Performance expectations:</strong> Linear in snapshot rows; optimized via vectorized checks where possible. <br><strong>Test vectors:</strong> Duplicate <code>AccountId</code>, <code>ProposedISAKBucket</code> present but not in canonical bucket list, numeric field outside [0,1]. <br><strong>PQ conceptual mapping:</strong> PQ should output schemaVersion and pre-validated flags; <code>ValidateSnapshot</code> cross-checks PQ metadata for parity and quick-fail. <br><strong>DAX conceptual mapping:</strong> <code>SnapshotValidationPassRate</code>, <code>ValidationWarningTrend</code>. <br><strong>Security/PII:</strong> Validation logs must not include raw PII; failing row details stored encrypted. <br><strong>Operational notes:</strong> validation changes require update to <code>requiredSchema</code> manifest and CI golden tests. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: PrecomputeComponentsForBatch(snapshot, options)</strong><br><strong>Purpose & contract:</strong> precompute per-row components (normalized label, TokenList, tokenKey, trigramFingerprint, numericSuffixes, parsed posting signature pointer, sample pointers) and return a <code>componentCache</code> keyed by AccountId. This function accelerates scoring by removing repeated work. MUST be deterministic, include <code>componentVersion</code> metadata, and embed <code>paramsHash</code> into cached items. <br><strong>Inputs & outputs:</strong> Input: <code>snapshot</code>, <code>options</code> {prefetchSignatures:boolean, sampleSize:int}. Output: <code>componentCache</code>, <code>componentVersion</code>, <code>cacheStats</code> {rows, durationMs, memBytes}.<br><strong>Primary invariants:</strong><br>1. Cache entries must include <code>rowChecksum</code> from snapshot to allow optimistic validation before write-back. <br>2. The cache must be read-only for scoring runs; changes require rebuilding and new <code>componentVersion</code>. <br><strong>Provenance & usage:</strong> Used by <code>ProcessChunk</code> and <code>ScoreRowCached</code> as source of normalized data; <code>componentVersion</code> recorded in <code>batch.compute.started</code> audit for reproducibility. <br><strong>Failure modes & recovery:</strong> Memory pressure -> create deterministic chunked caches (record chunk indices); if signature prefetch fails, mark <code>signatureMissing</code> and allow scoring with <code>signatureOverlap=0</code> and emit <code>components.signature.prefetch_failed</code> warning. <br><strong>Observability & audit:</strong> Emit <code>components.precompute{rows, durationMs, memBytes}</code> and attach <code>cacheStats</code> to the batch audit. Persist small sample of entries to evidenceRef when mappings are material. <br><strong>Performance expectations:</strong> Precomputing 10k rows on a modern Excel host should complete in seconds if using optimized string handling; for 50k+ rows chunking or worker offload is required. <br><strong>Test vectors & examples:</strong> Verify identical tokenKey for different permutations; trigramFingerprint consistency for abbreviations; signature parse correctness with typical posting signatures. <br><strong>Conceptual PQ mapping:</strong> PQ can produce precomputed TokenKey and TrigramFingerprint to minimize VBA precompute; <code>PrecomputeComponentsForBatch</code> must accept PQ-augmented snapshots and validate parity. <br><strong>Conceptual DAX mapping:</strong> <code>PrecomputeDurationMs</code> and <code>PrecomputeMemoryMB</code> measures. <br><strong>Security/PII:</strong> cache may contain hashed tokens only when <code>secureMode</code> is on; full tokens must be stored in evidence only. <br><strong>Operational notes:</strong> chunk size and prefetch policy should be configurable via <code>Config</code> and recorded in <code>paramsHash</code>. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: BuildBatchParamsHash(weights, flags, configVersion)</strong><br><strong>Purpose & contract:</strong> canonicalize scoring parameters (weights and feature flags) and compute <code>paramsHash</code> (SHA256) that anchors the semantic configuration used by the batch. Must produce a stable canonical string (sorted keys, fixed float precision) before hashing so PQ and VBA produce identical <code>paramsHash</code>. The <code>paramsHash</code> is embedded in every audit and <code>scoreHash</code>. <br><strong>Inputs & outputs:</strong> Input: <code>weights</code> object, <code>flags</code> object, <code>configVersion</code>. Output: <code>paramsString</code> (canonical), <code>paramsHash</code> (hex).<br><strong>Primary invariants:</strong><br>1. Deterministic ordering and fixed float formatting enforced; include <code>stopwordVersion</code>, <code>punctuationListHash</code>, <code>trigramPadding</code> in serialization. <br>2. Any change to inputs must produce a different <code>paramsHash</code> and be auditable. <br><strong>Provenance & usage:</strong> Recorded in <code>batch.compute.started</code>, used by <code>ScoreRowCached</code> to compute <code>scoreHash</code> and to ensure cross-runtime parity. <br><strong>Failure modes & recovery:</strong> Invalid numeric weights (NaN, inf) -> default to safe weights and emit <code>params.invalid</code> audit; parity check fails in CI -> block release. <br><strong>Observability & audit:</strong> <code>params.changed</code> governance event; preserve old params in migration manifest. <br><strong>Performance expectations:</strong> negligible cost. <br><strong>Test vectors:</strong> cross-runtime canonicalization test using PQ and VBA to ensure identical hash for same input. <br><strong>Conceptual PQ mapping:</strong> PQ must implement identical canonicalization and generate same <code>paramsHash</code>. <br><strong>Conceptual DAX mapping:</strong> <code>ActiveParamsHash</code> as dimension for trend and drift analysis. <br><strong>Security/PII:</strong> <code>paramsHash</code> contains no PII. <br><strong>Operational notes:</strong> Any <code>paramsHash</code> change in production requires <code>migration_manifest</code> and two-person approval for regulated datasets. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: ScoreRowCached(accountId, cachedComponents, proposedBucket, paramsHash)</strong><br><strong>Purpose & contract:</strong> pure deterministic function consuming cached components for an account and returning a <code>scoreResult</code> object: <code>tokenScore</code>, <code>trigramScore</code>, <code>normLev</code>, <code>signatureOverlap</code>, <code>combinedScore</code>, <code>band</code>, <code>breakdown</code>, <code>scoreHash</code>, <code>payloadHash</code>. MUST be side-effect free so it can run in parallel or on workers. MUST embed <code>paramsHash</code> into <code>scoreHash</code> derivation for reproducibility. <br><strong>Inputs & outputs:</strong> Inputs: <code>accountId</code>, <code>cachedComponents</code> entry, <code>proposedBucket</code>, <code>paramsHash</code>. Output: <code>scoreResult</code> with canonical <code>scoreHash</code> and <code>breakdown</code>.<br><strong>Primary invariants:</strong><br>1. <code>scoreHash = sha256(canonicalize(breakdown) + paramsHash + accountId)</code> using stable serialization rules. <br>2. Float formatting fixed to canonical precision before hashing. <br>3. Missing components cause renormalization of weights; the adjustment recorded in <code>breakdown</code>. <br><strong>Provenance & usage:</strong> Called by <code>ProcessChunk</code> for each row; <code>scoreResult</code> persisted to CandidateMap and <code>payloadHash</code> appended into batch audit. <br><strong>Failure modes & recovery:</strong> Component computation error -> produce <code>STD_SCORE_ERROR</code> with <code>combinedScore=0</code> and <code>band=Manual</code> to force human review; log diagnostics and persist to evidenceRef. <br><strong>Observability & audit:</strong> Each <code>scoreResult</code> must include <code>scoreHash</code> stored in CandidateMap and referenced by MappingHistory for traceability. Persist representative <code>breakdown</code> objects to evidence for regulated mapping changes. <br><strong>Performance expectations:</strong> micro-optimized inner-loop; per-row compute microseconds to low milliseconds depending on operations (levenshtein cost). <br><strong>Test vectors & examples:</strong> known fixtures where small edits to input produce small combined score deltas; verify that <code>scoreHash</code> remains consistent for identical inputs across PQ/VBA. <br><strong>Conceptual PQ mapping:</strong> PQ may compute partial components and embed them into cachedComponents; <code>ScoreRowCached</code> must verify PQ-provided components' types and ranges. <br><strong>Conceptual DAX mapping:</strong> <code>AvgCombinedScore</code>, <code>ScoreHashMismatchCount</code> as monitoring measures. <br><strong>Security/PII:</strong> <code>breakdown</code> redaction needed before placing in any unencrypted place; only <code>evidenceRef</code> links to full unredacted breakdown. <br><strong>Operational notes:</strong> Design pure function to be portable to worker processes in future scaling. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: PartitionChunks(snapshotRows, chunkSize, chunkPolicy)</strong><br><strong>Purpose & contract:</strong> deterministically partition the list of snapshot rows into ordered chunks for processing. Responsibilities: compute chunk boundaries, preserve original row ordering to ensure deterministic write-back, and produce stable <code>chunkPlan</code> that can be persisted as <code>BatchDescriptor</code> used by workers. Support chunk policies: fixed-size, impact-prioritized (higher estimated cost earlier), or balanced by memory. <br><strong>Inputs & outputs:</strong> Inputs: <code>snapshotRows</code> (ordered), <code>chunkSize</code>, <code>chunkPolicy</code> (enum). Output: <code>chunkPlan</code> array with each chunk's row indices and estimated memory footprint. <br><strong>Primary invariants:</strong><br>1. Partitioning must be deterministic for same inputs and chunkPolicy. <br>2. <code>chunkPlan</code> must include <code>chunkId</code> deterministic from <code>batchId</code>+chunkIndex to enable idempotent worker picks. <br><strong>Provenance & usage:</strong> Used by <code>ScoreBatch</code> for parallel/serial chunk processing; chunkPlan persisted to <code>BatchDescriptor</code>. <br><strong>Failure modes & recovery:</strong> chunk plan too large for host memory -> abort with <code>STD_CHUNK_OVERSIZE</code> and suggest reduced <code>chunkSize</code> or worker offload. <br><strong>Observability & audit:</strong> emit <code>batch.chunks.planned{chunkCount,rowsPerChunk,policy}</code> and include plan in <code>batch.compute.started</code> audit. <br><strong>Performance expectations:</strong> fast partitioning; time dominated by chunk metadata calculation. <br><strong>Test vectors:</strong> varied distributions including all large vs many small rows, test <code>impact-prioritized</code> ordering. <br><strong>Conceptual PQ mapping:</strong> PQ could compute per-row <code>estimatedCost</code> to help chunk balancing; <code>PartitionChunks</code> consumes these estimates. <br><strong>Conceptual DAX mapping:</strong> <code>AverageChunkSize</code> and <code>ChunkPlanChanges</code> counters. <br><strong>Security/PII:</strong> chunkPlan contains no raw PII; do not embed sample postings. <br><strong>Operational notes:</strong> preserve chunkPlan for reproducible retry semantics and idempotency. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: ProcessChunk(chunkInfo, componentCache, paramsHash, options)</strong><br><strong>Purpose & contract:</strong> process a single chunk deterministically: iterate rows in chunk order, call <code>ScoreRowCached</code> for each, collect <code>rowResults</code>, compute chunk-level aggregates, emit <code>batch.chunk.completed</code> telemetry, and return <code>chunkResult</code>. Supports <code>dryRun</code> where writes are not persisted. Must be idempotent and produce same <code>chunkResult</code> for same inputs. <br><strong>Inputs & outputs:</strong> Inputs: <code>chunkInfo</code> (row indices), <code>componentCache</code>, <code>paramsHash</code>, <code>options</code> {dryRun, checkpointEvery}. Output: <code>chunkResult</code> {rowResults, errors, durationMs, memBytes, checkpointData}.<br><strong>Primary invariants:</strong><br>1. Maintain original row order in <code>rowResults</code> to avoid reorder visibility in subsequent write outs. <br>2. On error thresholds exceeded, chunk processing must abort and return partial results with <code>partial=true</code>. <br><strong>Provenance & usage:</strong> Called by orchestrator or worker; chunkResult aggregated by <code>ScoreBatch</code> into final <code>batchSummary</code>. <br><strong>Failure modes & recovery:</strong> transient failure inside chunk -> attempt per-row retry up to <code>rowRetryLimit</code>; if persistent, mark row with <code>STD_SCORE_ERROR</code> and continue or abort depending on <code>options.failFast</code> setting. <br><strong>Observability & audit:</strong> <code>batch.chunk.completed{batchId,chunkId,rows,durationMs,errors}</code> emitted; on failures persist sample diagnostics to evidenceRef. <br><strong>Performance expectations:</strong> chunk processing throughput depends on per-row heavy operations (e.g., Levenshtein); benchmark and tune <code>chunkSize</code>. <br><strong>Test vectors:</strong> chunk with mix of good and failing rows, with failure injection to test retry and checkpoint behavior. <br><strong>Conceptual PQ mapping:</strong> PQ may pre-split heavy tasks into chunks for worker execution; ensure chunk metadata parity. <br><strong>Conceptual DAX mapping:</strong> <code>ChunkLatencyP50/P95</code>, <code>ChunkErrorRate</code>. <br><strong>Security/PII:</strong> chunkResults must avoid storing unredacted PII in unencrypted temp sheets; use evidenceRef where needed. <br><strong>Operational notes:</strong> Provide progress callbacks to UI and persist checkpoint per <code>checkpointEvery</code>. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: ComputeRowChecksum(row)</strong><br><strong>Purpose & contract:</strong> compute deterministic per-row checksum used for optimistic concurrency control during write-back. Must use canonicalized subset of stable fields (AccountId, tokenKey, proposedBucket, rowVersion fields) and fixed formatting. Return checksum string (hex). <br><strong>Inputs & outputs:</strong> Input: <code>row</code> object. Output: <code>rowChecksum</code> string. <br><strong>Primary invariants:</strong><br>1. Checksum must be stable across runtimes for the same canonical input. <br>2. Exclude non-deterministic fields (e.g., lastAccessTs). <br><strong>Provenance & usage:</strong> Used when validating snapshot hasn't changed before atomic write; included in <code>componentCache</code> entries. <br><strong>Failure modes & recovery:</strong> None expected; if row contains invalid data types, attempt canonical casting then compute; if impossible return <code>STD_CHECKSUM_FAIL</code>. <br><strong>Observability & audit:</strong> checksum generation count and anomalies logged. <br><strong>Performance:</strong> trivial CPU usage. <br><strong>Tests:</strong> ensure same input across PQ/VBA yields same checksum. <br><strong>Conceptual PQ mapping:</strong> PQ should produce the same rowChecksum if it performs canonicalization; used to detect concurrent edits. <br><strong>DAX conceptual mapping:</strong> <code>RowChecksumMismatchCount</code> as an alert metric. <br><strong>Security/PII:</strong> checksum is one-way and contains no PII; don't attempt to decode. <br><strong>Operational notes:</strong> update checksum algorithm only via migration manifest. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: AtomicWriteResults(tempStagingArea, targetRange, expectedSnapshotHash)</strong><br><strong>Purpose & contract:</strong> atomically publish batch results to persistent CandidateMap. Procedure: stage results into <code>tempStagingArea</code>, validate <code>expectedSnapshotHash</code> equals current live snapshotHash, compute <code>beforeHash</code>, perform a single swap of ranges or a single persisted replace, compute <code>afterHash</code>, and return <code>writeReceipt</code>. MUST be atomic (no visible partial writes) and must handle concurrent modifications safely. <br><strong>Inputs & outputs:</strong> Inputs: <code>tempStagingArea</code> (sheet or staging file), <code>targetRange</code> (sheet range), <code>expectedSnapshotHash</code>. Output: <code>writeReceipt</code> {beforeHash, afterHash, rowsWritten, durationMs, swapToken}.<br><strong>Primary invariants:</strong><br>1. Abort if current live snapshotHash != expectedSnapshotHash to prevent race conditions. <br>2. Swap operation must be atomic from user perspective (single workbook action or transactional file replace). <br><strong>Provenance & usage:</strong> Called at end of <code>ScoreBatch</code> to publish mapping updates; <code>writeReceipt</code> appended to audit for forensic reconstruction. <br><strong>Failure modes & recovery:</strong><br>• Swap fails midway -> use <code>beforeHash</code> to trigger restoration and emit <code>batch.write.partial</code> audit; manual intervention required if restoration fails. <br>• Workbook protected -> raise <code>STD_WRITE_PROTECTED</code> and instruct operator to unlock or run offline. <br><strong>Observability & audit:</strong> Emit <code>batch.write.started</code> and <code>batch.write.completed</code> audits including <code>beforeHash</code> and <code>afterHash</code>; include <code>swapToken</code> for chain-of-custody. <br><strong>Performance expectations:</strong> write time proportional to rows written; large writes should be chunked and swapped in a minimal number of atomic steps. <br><strong>Test vectors:</strong> test concurrent modification by updating CandidateMap in parallel and verify abort with <code>STD_CONCURRENT_MODIFICATION</code>. <br><strong>Conceptual PQ mapping:</strong> PQ-generated artifacts may be swapped directly if PQ output verified and canonicalized. <br><strong>DAX conceptual mapping:</strong> <code>BatchWritesSuccessRate</code>, <code>AvgWriteDurationMs</code>. <br><strong>Security/PII:</strong> staging area must be ephemeral and cleared; do not leave PII in unencrypted staging areas afterwards. <br><strong>Operational notes:</strong> perform atomic writes during low-usage windows and record <code>correlationId</code> for triage. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: VerifyWriteReceipt(writeReceipt, expectedAfterHash)</strong><br><strong>Purpose & contract:</strong> verify post-write integrity by recomputing <code>afterHash</code> from the live CandidateMap and comparing to <code>expectedAfterHash</code> provided in <code>writeReceipt</code>. On mismatch, generate diagnostic <code>writeVerification.fail</code> and provide remediation steps including <code>RestoreFromBefore</code> and <code>forensicPack</code>. <br><strong>Inputs & outputs:</strong> Inputs: <code>writeReceipt</code>, <code>expectedAfterHash</code>. Output: <code>verificationResult</code> {ok:boolean, actualAfterHash, diagnostics}.<br><strong>Primary invariants:</strong><br>1. Recomputed <code>afterHash</code> must match <code>expectedAfterHash</code> for success. <br>2. If mismatch, record <code>prevHash</code> chain and snapshot for forensic analysis. <br><strong>Provenance & usage:</strong> Called immediately after <code>AtomicWriteResults</code> as part of <code>ScoreBatch</code> completion flow. <br><strong>Failure modes & recovery:</strong> mismatch -> attempt 1 re-validate read; if still mismatched produce <code>forensic_pack</code> and mark batch as <code>failed</code> and <code>requiresOperatorAction</code>. <br><strong>Observability & audit:</strong> <code>batch.write.verified</code> or <code>batch.write.verificationFailed</code> emitted with correlationId. <br><strong>Performance:</strong> recompute hash cost linear in CandidateMap size; acceptable for moderate sizes; for very large sheets consider incremental hashing strategies. <br><strong>Tests:</strong> verify mismatch detection by artificially flipping a row post-write. <br><strong>Conceptual PQ mapping:</strong> PQ parity checks should be used to avoid inconsistent writes between PQ and VBA. <br><strong>DAX conceptual mapping:</strong> <code>WriteVerificationFailures</code>. <br><strong>Security/PII:</strong> do not include raw PII in diagnostics; include <code>evidenceRef</code> instead. <br><strong>Operational notes:</strong> On verification failure create an incident ticket including <code>correlationId</code> and <code>forensic_manifest</code>. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: CreateBatchSummary(batchId, results, metrics)</strong><br><strong>Purpose & contract:</strong> assemble a canonical, auditable batch summary used by operators and governance. Should include counts (auto/review/manual), override lists, top-N changed accounts by impact, error lists, metrics (duration, chunk counts), <code>paramsHash</code>, <code>snapshotHash</code>, <code>componentVersion</code>, <code>summaryHash</code>, and <code>evidenceRef</code>. Must be canonicalized (stable key ordering) so <code>summaryHash</code> is reproducible. <br><strong>Inputs & outputs:</strong> Inputs: <code>batchId</code>, <code>results</code> (aggregated rowResults), <code>metrics</code>. Output: <code>summaryRef</code> (artifact location), <code>summaryHash</code> (sha256), <code>summaryObject</code>.<br><strong>Primary invariants:</strong><br>1. <code>summaryHash</code> forms an immutable reference used by audits; summary content must include minimal PII and primarily reference <code>evidenceRef</code> for full details. <br>2. Include a <code>signoffNeeded</code> boolean if material thresholds exceeded. <br><strong>Provenance & usage:</strong> Central artifact for operator review and pre-migration sign-off. <br><strong>Failure modes & recovery:</strong> inability to persist -> stage locally and emit <code>batch.summary.persist.failed</code>; require manual transfer. <br><strong>Observability & audit:</strong> <code>batch.summary.generated</code> audit includes <code>summaryHash</code> and rows. <br><strong>Performance:</strong> small relative to scoring; canonicalization cost minor. <br><strong>Tests:</strong> re-canonicalize from results reproducibly yields same <code>summaryHash</code>. <br><strong>Conceptual PQ mapping:</strong> PQ can produce independent impact summary to be compared with the batch summary for parity. <br><strong>Conceptual DAX mapping:</strong> <code>BatchesPendingSignoff</code>, <code>AvgBatchDelta</code>. <br><strong>Security/PII:</strong> redact PII in summary; full row-level artifacts only via evidenceRef with RBAC. <br><strong>Operational notes:</strong> summary must be included in migration manifest for regulated releases. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: EmitBatchAudit(batchId, eventType, payloadRefs, metrics, operatorId)</strong><br><strong>Purpose & contract:</strong> append canonical append-only audit rows for batch lifecycle events (<code>batch.compute.started</code>, <code>batch.chunk.completed</code>, <code>batch.compute.completed</code>, <code>batch.write.completed</code>, <code>batch.apply.start</code>, <code>batch.apply.completed</code>, <code>batch.revert</code>). Audit row must include <code>correlationId</code>, <code>timestampUTC</code>, <code>module=DQ_Standardize</code> or equivalent, <code>procedure</code>, <code>operatorId</code>, <code>batchId</code>, <code>paramsHash</code>, <code>snapshotHash</code>, <code>summaryHash</code>, <code>payloadHash</code> (if available), <code>evidenceRef</code> and <code>prevHash</code> linking to the prior audit to form an immutable chain. The append must be atomic and durable. <br><strong>Inputs & outputs:</strong> Inputs: <code>batchId</code>, <code>eventType</code>, <code>payloadRefs</code> (array), <code>metrics</code>, <code>operatorId</code>. Output: <code>auditId</code> and persisted audit row reference. <br><strong>Primary invariants:</strong><br>1. Append-only policy: never overwrite existing audits. <br>2. Audit rows must be PII-free; include only <code>evidenceRef</code> to retrieve full details under RBAC. <br>3. <code>prevHash</code> used to link audits and support reconstructability. <br><strong>Provenance & usage:</strong> core governance artifact used in forensic recon and regulator packaging. <br><strong>Failure modes & recovery:</strong> append failure -> stage local audit and retry; if persistent fail, abort operation and alert SRE. <br><strong>Observability & audit:</strong> track <code>audit.append.latency</code>, <code>audit.append.failures</code>; audits are indexed for rapid retrieval by <code>correlationId</code>. <br><strong>Performance expectations:</strong> append latency low; critical for not blocking operator flows. <br><strong>Tests:</strong> append idempotency and chain integrity tests. <br><strong>Conceptual PQ mapping:</strong> PQ processes should produce audit rows too so full pipeline is auditable. <br><strong>Conceptual DAX mapping:</strong> <code>AuditAppendLatencyMs</code>, <code>AuditFailureRate</code>. <br><strong>Security/PII:</strong> audits must not leak PII; enforce redaction before writing. <br><strong>Operational notes:</strong> audit store should be WORM for regulated retention. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: BatchDryRun(snapshot, paramsHash, options)</strong><br><strong>Purpose & contract:</strong> execute the full batch pipeline in <code>dryRun</code> mode: compute scores, generate summary and evidence artifacts, but do not commit writes to the CandidateMap. Must produce identical computational outputs (scoreHash, summaryHash, would-be <code>afterHash</code>) as a real run would, enabling deterministic preview and governance checks. <br><strong>Inputs & outputs:</strong> Inputs: <code>snapshot</code>, <code>paramsHash</code>, <code>options</code> {sampleOnly:boolean, includeImpactSimulation:boolean}. Outputs: <code>dryRunSummaryRef</code>, <code>wouldCommitAfterHash</code>, <code>evidenceRef</code>. <br><strong>Primary invariants:</strong><br>1. <code>dryRun</code> outputs must match real run when run with same inputs and no concurrent modifications. <br>2. No side effects to CandidateMap or mapping tables. <br><strong>Provenance & usage:</strong> Reviewer UI and governance use <code>dryRun</code> for previews and approvals. <br><strong>Failure modes & recovery:</strong> failure to generate preview artifacts -> return <code>STD_DRYRUN_FAIL</code> with logs and <code>evidenceRef</code> pointing to diagnostic traces. <br><strong>Observability & audit:</strong> emit <code>standard.preview</code> audit with <code>dryRun=true</code> and <code>paramsHash</code>. <br><strong>Performance:</strong> similar to real run, but can be configured for sampling for faster previews. <br><strong>Tests & examples:</strong> dry-run comparing <code>wouldCommitAfterHash</code> to golden for canonical fixtures; run with sampling then full to detect pipeline regressions. <br><strong>Conceptual PQ mapping:</strong> PQ <code>PreviewStandardize</code> does heavy lifting and may be used for full-scale previews at scale. <br><strong>Conceptual DAX mapping:</strong> <code>DryRunCount</code>, <code>DryRunToCommitMismatch</code>. <br><strong>Security/PII:</strong> UI previews must be redacted; full evidence stored encrypted. <br><strong>Operational notes:</strong> encourage operators to run <code>dryRun</code> prior to any hot-swap or migration. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: PersistBatchDescriptor(batchDescriptor)</strong><br><strong>Purpose & contract:</strong> persist an idempotent <code>BatchDescriptor</code> used by job schedulers and workers. Descriptor includes <code>batchId</code>, <code>snapshotHash</code>, <code>paramsHash</code>, <code>summaryRef</code>, <code>chunkPlan</code>, <code>owner</code>, <code>priority</code> and <code>createdTs</code>. Persistent storage may be a sheet or a job queue. Must be atomic and retrievable; duplicate persistence must be idempotent. <br><strong>Inputs & outputs:</strong> Inputs: <code>batchDescriptor</code>. Outputs: <code>descriptorRef</code>, <code>descriptorHash</code>, <code>persistResult</code>. <br><strong>Primary invariants:</strong><br>1. <code>batchId</code> is uniqueness key preventing ambiguous duplicate descriptors. <br>2. Descriptor persisted atomically and durable. <br><strong>Provenance & usage:</strong> queue workers read these descriptors to run chunked scoring or heavy applies offline. <br><strong>Failure modes & recovery:</strong> persistence failure -> local staging and alert; subsequent reconciliation procedure needed. <br><strong>Observability & audit:</strong> <code>job.descriptor.persisted</code> audit emitted. <br><strong>Tests:</strong> idempotency test by persisting same descriptor twice. <br><strong>Conceptual PQ mapping:</strong> PQ scheduling pipelines may produce descriptors for heavy transforms; ensure descriptor schema parity. <br><strong>Conceptual DAX mapping:</strong> <code>JobDescriptorCount</code>, <code>JobsPersistLatency</code>. <br><strong>Security/PII:</strong> descriptors must not contain PII. <br><strong>Operational notes:</strong> descriptors should be signed or checksumed to prevent tampering. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: EnqueueJobDescriptor(descriptorRef, queueOptions)</strong><br><strong>Purpose & contract:</strong> enqueue a job descriptor into the job scheduling system with priority and retry policy. Must produce an idempotent enqueue operation and return <code>queueId</code> and <code>enqueueTs</code>. <br><strong>Inputs & outputs:</strong> Inputs: <code>descriptorRef</code>, <code>queueOptions</code> {priority, retryPolicy}. Outputs: <code>queueId</code>, <code>enqueueReceipt</code>. <br><strong>Primary invariants:</strong><br>1. Job queue respects priorities and supports deterministic FIFO ordering for same priority. <br>2. Enqueue operation must be transactional; on failure retry with backoff. <br><strong>Provenance & usage:</strong> used when heavy batch processing is delegated to background workers. <br><strong>Failure modes & recovery:</strong> queue system unavailability -> stage locally and alert; reconcile when queue restores. <br><strong>Observability & audit:</strong> <code>job.enqueued</code> audit with <code>queueDepth</code>. <br><strong>Tests:</strong> enqueue under heavy load and verify ordering semantics. <br><strong>Conceptual PQ mapping:</strong> PQ job orchestration systems may integrate with same job queue. <br><strong>DAX conceptual mapping:</strong> <code>JobQueueDepth</code>, <code>EnqueueLatency</code>. <br><strong>Security/PII:</strong> descriptors must not have PII; queue access must be authenticated. <br><strong>Operational notes:</strong> enforce quotas per tenant to prevent noisy neighbors. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: DequeueJobAndProcess(workerToken)</strong><br><strong>Purpose & contract:</strong> worker-side function to claim a job, validate descriptor (signature and paramsHash), download any PQ-provided precomputed artifacts, process chunks via <code>ProcessChunk</code>, persist results, and emit <code>job.completed</code> or <code>job.failed</code>. Must implement idempotent claim semantics and heartbeating for long-running jobs. <br><strong>Inputs & outputs:</strong> Inputs: <code>workerToken</code>, optional <code>maxProcessingTime</code>. Output: <code>jobResult</code> with status, <code>jobLogs</code>, <code>artifactRefs</code>. <br><strong>Primary invariants:</strong><br>1. Claim acquisition must be atomic and exclusive; include <code>leaseExpiry</code> to prevent orphaned locks. <br>2. Worker must verify <code>paramsHash</code> and <code>snapshotHash</code> before processing; if mismatch with local environment, reject claim and requeue. <br><strong>Provenance & usage:</strong> enables horizontal scaling of heavy batches. <br><strong>Failure modes & recovery:</strong> worker crash -> lease expiry and job requeued; incomplete partial results must be reconciled using checkpointing. <br><strong>Observability & audit:</strong> <code>job.worker.claimed</code>, <code>job.worker.completed</code> events with worker metadata. <br><strong>Tests:</strong> claim/release semantics, worker crash simulation and resume. <br><strong>Conceptual PQ mapping:</strong> PQ tasks may run on same worker or separate clusters; ensure descriptor portability. <br><strong>DAX conceptual mapping:</strong> <code>WorkerThroughput</code>, <code>JobFailureRate</code>. <br><strong>Security/PII:</strong> workers process PII only in memory; logs must be redacted. Worker tokens must be ephemeral and least privilege. <br><strong>Operational notes:</strong> implement worker health checks and auto-scaling policies based on queue depth. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: RetryFailedBatch(batchId, retryPolicy)</strong><br><strong>Purpose & contract:</strong> orchestrate deterministic retries for failed batches following an exponential backoff policy, respecting idempotency to avoid duplicate commits. Must consult <code>ApplyDescriptor</code> and previous audit rows to determine safe recovery action (re-run chunks, re-write missing chunks, or abort). Returns <code>retryReport</code> documenting attempts and final outcome. <br><strong>Inputs & outputs:</strong> Inputs: <code>batchId</code>, <code>retryPolicy</code> {maxRetries, backoffBaseMs, maxBackoffMs}. Output: <code>retryReport</code> {attempts, success, errors}.<br><strong>Primary invariants:</strong><br>1. Retry attempts must be idempotent and logged with <code>retryId</code> and <code>correlationId</code>. <br>2. Retry of write operations must ensure no double-commit—leverage <code>applyId</code> + <code>writeReceipt</code> checks. <br><strong>Provenance & usage:</strong> used by operators and automated monitors to recover from transient failures. <br><strong>Failure modes & recovery:</strong> persistent failure -> escalate to manual intervention and open incident. <br><strong>Observability & audit:</strong> <code>batch.retry.started</code>, <code>batch.retry.completed</code> with attempt counts; maintain <code>RetrySuccessRate</code> metric. <br><strong>Tests:</strong> simulate transient DB IO failures and ensure retry completes without duplication. <br><strong>Conceptual PQ mapping:</strong> PQ-run failures should be retried using same descriptor semantics. <br><strong>DAX conceptual mapping:</strong> <code>RetryAttempts</code>, <code>RetrySuccessRate</code>. <br><strong>Security/PII:</strong> ensure retry logs do not contain PII. <br><strong>Operational notes:</strong> set a maximum retry cap to avoid infinite loops; escalate beyond threshold. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: ChunkCheckpointing(batchId, chunkIndex, checkpointData)</strong><br><strong>Purpose & contract:</strong> persist minimal deterministic checkpoint to enable safe resume of long-running batch processing. Checkpoints must be idempotent and small (offsets, partial checksums). Return <code>resumeToken</code> that uniquely identifies the checkpoint. <br><strong>Inputs & outputs:</strong> Inputs: <code>batchId</code>, <code>chunkIndex</code>, <code>checkpointData</code>. Output: <code>checkpointRef</code>, <code>resumeToken</code>. <br><strong>Primary invariants:</strong><br>1. Checkpoint must allow restart without reprocessing completed chunks. <br>2. Checkpoints are recorded in <code>BatchDescriptor</code> with <code>checkpointHash</code>. <br><strong>Provenance & usage:</strong> used by worker processes to resume after crashes and to support operator cancellations. <br><strong>Failure modes & recovery:</strong> persistence error -> local staging and later reconciliation. <br><strong>Observability & audit:</strong> <code>checkpoint.created</code> events and <code>lastCheckpointIndex</code>. <br><strong>Tests:</strong> resume from checkpoint ensures no double-processing. <br><strong>Conceptual PQ mapping:</strong> PQ shards should support similar checkpointing for heavy transforms. <br><strong>DAX conceptual mapping:</strong> <code>CheckpointFrequency</code>, <code>AvgResumeTime</code>. <br><strong>Security/PII:</strong> store only offsets/checksums; never raw PII. <br><strong>Operational notes:</strong> checkpoint cadence balances restart cost vs overhead; configure via <code>Config</code>. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: BatchCancel(batchId, operatorId, reason, force=false)</strong><br><strong>Purpose & contract:</strong> cooperative cancellation of an in-flight batch. Sets batch status to <code>cancelling</code>, marks <code>cancelRequestedAt</code>, notifies workers via cancellation token, and ensures no new chunks are started. If <code>force=true</code> (admin-only), attempt to abort running workers and persist <code>forensic_manifest</code>. Must log <code>canceller</code> and reason. <br><strong>Inputs & outputs:</strong> Inputs: <code>batchId</code>, <code>operatorId</code>, <code>reason</code>, <code>force</code> flag. Outputs: <code>cancelReceipt</code> with <code>status</code> and <code>affectedChunks</code>. <br><strong>Primary invariants:</strong><br>1. Cancellation must be cooperative and recorded; workers must check token before starting each chunk and at checkpoint boundaries. <br>2. For regulated destructive operations, require two-person approval recorded in <code>Approvals</code> table before cancellation of rollback-critical jobs. <br><strong>Provenance & usage:</strong> Operators use for error situations or when urgent stop required. <br><strong>Failure modes & recovery:</strong> workers ignoring cancellation due to code bugs -> escalate to <code>forceCancel</code> path and build <code>forensic_pack</code>. <br><strong>Observability & audit:</strong> <code>batch.cancel.requested</code> and <code>batch.cancel.completed</code> audits including <code>operatorId</code> and <code>reason</code>. <br><strong>Tests:</strong> cancel mid-run and verify clean stop and consistent rest state. <br><strong>Conceptual PQ mapping:</strong> PQ-run jobs should react to same cancellation tokens. <br><strong>DAX conceptual mapping:</strong> <code>BatchCancelCount</code>, <code>CanceledBatchesByReason</code>. <br><strong>Security/PII:</strong> ensure cancel logs do not expose sensitive details. <br><strong>Operational notes:</strong> cancellation likely requires operator confirmation in UI with <code>correlationId</code> shown for triage. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: SnapshotCompare(beforeSnapshot, afterSnapshot)</strong><br><strong>Purpose & contract:</strong> compute deterministic diff between <code>before</code> and <code>after</code> snapshots producing <code>diffSummary</code> listing added/removed/changed mappings, per-account detail (old/new bucket, scoreHash), owner changes, and aggregated impact totals. Provide <code>riskEstimate</code> (low/medium/high) with rationale. Must be canonical and reproducible. <br><strong>Inputs & outputs:</strong> Inputs: <code>beforeSnapshot</code>, <code>afterSnapshot</code>. Outputs: <code>diffReportRef</code>, <code>diffSummary</code>, <code>riskEstimate</code>. <br><strong>Primary invariants:</strong><br>1. Diffs include <code>scoreHash</code> and <code>paramsHash</code> for each changed row for traceability. <br>2. Risk estimate deterministic formula: weight changes by <code>estimatedAffectedAmt</code>, <code>ruleEstimatedCost</code>, and <code>materialityFlag</code>. <br><strong>Provenance & usage:</strong> Used in <code>RefreshStandardMap</code>, HotSwap preview, and governance decisions. <br><strong>Failure modes & recovery:</strong> incompatible snapshot schemas -> <code>STD_SNAPSHOT_MISMATCH</code>. <br><strong>Observability & audit:</strong> <code>snapshot.diff.generated</code> audit with <code>riskEstimate</code>. <br><strong>Tests:</strong> diffs for added mapping, mapping change, mapping removal scenarios. <br><strong>Conceptual PQ mapping:</strong> PQ can compute diffs efficiently; compare PQ diff with VBA diff for parity. <br><strong>DAX conceptual mapping:</strong> <code>DiffAddedCount</code>, <code>DiffChangedCount</code>, <code>HotSwapRisk</code> measure. <br><strong>Security/PII:</strong> diffs for public operators must be redacted; include <code>evidenceRef</code> for full details. <br><strong>Operational notes:</strong> automatic reverts triggered when <code>riskEstimate</code> > threshold require explicit approvals. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: GenerateBatchReportForOperator(batchId, options)</strong><br><strong>Purpose & contract:</strong> produce the operator-facing report (spreadsheet/PDF) summarizing batch metrics, top changes, impact simulation highlights, compliance checklist, and a migration manifest pointer. Redaction policy must be applied to UI outputs; compliance roles may request full evidenceRef-included report under approval. Return <code>reportRef</code> and <code>reportHash</code>. <br><strong>Inputs & outputs:</strong> Inputs: <code>batchId</code>, <code>options</code> {format, includeEvidence}. Output: <code>reportRef</code>, <code>reportHash</code>. <br><strong>Primary invariants:</strong><br>1. Redaction enforced by operator permissions; only <code>evidenceRef</code> links to full PII. <br>2. Report artifact naming canonical with <code>batchId</code> and <code>correlationId</code>. <br><strong>Provenance & usage:</strong> used for sign-off and printing. <br><strong>Failure modes & recovery:</strong> template error -> output minimal textual summary and log <code>STD_REPORT_PARTIAL</code>. <br><strong>Observability & audit:</strong> <code>report.generated</code> audit with <code>reportHash</code>. <br><strong>Tests:</strong> generate reports with and without evidence and verify redaction and <code>reportHash</code> parity. <br><strong>Conceptual PQ mapping:</strong> PQ can produce CSVs used as appendices for the operator report. <br><strong>DAX conceptual mapping:</strong> <code>ReportsGeneratedPerDay</code>. <br><strong>Security/PII:</strong> transport and storage of report containing PII must be encrypted; access logged in <code>evidenceAccess</code> audit. <br><strong>Operational notes:</strong> require approvals for report release in regulated flows. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: PersistMigrationArtifact(batchId, migrationArtifact, operatorId)</strong><br><strong>Purpose & contract:</strong> atomically persist migration artifact (SQL/CSV) generated from approved mappings, compute <code>artifactChecksum</code> (SHA256), sign artifact if operator has signing rights (external signing service), and emit <code>standard.map.export</code> audit row. Ensure artifact canonicalization rules (stable ordering, normalized quoting) so checksum stable. <br><strong>Inputs & outputs:</strong> Inputs: <code>batchId</code>, <code>migrationArtifact</code> (content), <code>operatorId</code>, <code>sign=true/false</code>. Outputs: <code>artifactRef</code>, <code>artifactChecksum</code>, <code>persistResult</code>. <br><strong>Primary invariants:</strong><br>1. Artifact content canonicalized deterministically so same input results in same checksum. <br>2. Signed artifacts must preserve signature block and be verifiable. <br><strong>Provenance & usage:</strong> Artifact handed to DB team or automated deploy pipeline for actual ledger updates. <br><strong>Failure modes & recovery:</strong> persistence failure -> stage locally and emit <code>artifact.persist.failed</code>; signature service failure -> persist unsigned artifact but emit <code>artifact.not-signed</code> audit and require manual signing. <br><strong>Observability & audit:</strong> <code>standard.map.export{artifactRef,checksum,operatorId}</code>. <br><strong>Tests:</strong> checksum parity tests and signature verification tests. <br><strong>Conceptual PQ mapping:</strong> PQ may generate migration artifact for large transformations; ensure PQ and VBA canonicalization parity. <br><strong>DAX conceptual mapping:</strong> <code>MigrationArtifactsGenerated</code>, <code>UnsignedArtifactCount</code>. <br><strong>Security/PII:</strong> avoid embedding PII in migration artifact unless necessary; if required, artifact must be encrypted and access audited. <br><strong>Operational notes:</strong> require two-person approval for destructive inline changes for regulated datasets before artifact persistence. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: ApplyBatchIfRequested(batchId, applyOptions, approvals)</strong><br><strong>Purpose & contract:</strong> orchestrates the final authoritative apply when requested (mode <code>create_copy</code> default or <code>inline</code> with approvals). Responsibilities: validate approvals for destructive/material applies, ensure <code>beforeSnapshot</code> exists and is persisted, persist <code>ApplyDescriptor</code> (with <code>applyId</code>), schedule or run workers to apply mapping changes in controlled manner, compute <code>beforeChecksum</code> and <code>afterChecksum</code>, and append <code>standard.apply.*</code> audit events. Must never perform unauthorized destructive actions and must support revertability via persisted snapshots. <br><strong>Inputs & outputs:</strong> Inputs: <code>batchId</code>, <code>applyOptions</code> {mode, schedule}, <code>approvals</code>. Outputs: <code>applyDescriptor</code>, <code>applyResult</code> {status, applyId, beforeChecksum, afterChecksum}. <br><strong>Primary invariants:</strong><br>1. For <code>inline</code> destructive apply, require two-person approvals recorded in <code>Approvals</code> table and audit. <br>2. Always create <code>beforeSnapshot</code> persisted to evidence store before destructive operations. <br><strong>Provenance & usage:</strong> Called after operator sign-off to actually make mappings live in <code>MappingTable</code> or target ledger. <br><strong>Failure modes & recovery:</strong> partial apply -> persist partial output and set <code>apply.status=failed</code> with diagnostics and recovery hints; allow <code>RevertMapping</code> based on <code>ApplyDescriptor</code> snapshot if present. <br><strong>Observability & audit:</strong> emit <code>standard.apply.start</code>, <code>standard.apply.completed</code>, <code>standard.apply.failed</code> with <code>applyId</code>, <code>beforeChecksum</code>, <code>afterChecksum</code>, <code>payloadHash</code>, and <code>evidenceRef</code>. <br><strong>Performance expectations:</strong> small safe <code>create_copy</code> applies can be synchronous; heavy applies must be scheduled via job system. <br><strong>Tests:</strong> apply idempotency tests and revert parity tests (<code>apply-&gt;revert</code> yields checksum equality). <br><strong>Conceptual PQ mapping:</strong> PQ may be used to stage data for copy-mode applies and compute before/after snapshots. <br><strong>DAX conceptual mapping:</strong> <code>ApplySuccessRate</code>, <code>ApplyFailureCount</code>. <br><strong>Security/PII:</strong> before/after snapshots are sensitive and must be encrypted; evidence access logged. <br><strong>Operational notes:</strong> require operator confirmation and capture <code>correlationId</code> in immediate UI response for triage. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: RevertBatchApply(applyId, operatorId, reason)</strong><br><strong>Purpose & contract:</strong> revert a previously applied batch using reversible <code>ApplyDescriptor</code> data (snapshot or inverse mappings). Validate <code>applyId</code> exists, check <code>beforeSnapshot</code> presence, perform <code>create_copy</code> revert by default, compute <code>revertBeforeChecksum</code> and <code>revertAfterChecksum</code>, and append <code>standard.revert</code> audit. If snapshot missing, fail with <code>STD_REVERT_NO_SNAPSHOT</code> and instruct operator to open incident. Revert must be idempotent. <br><strong>Inputs & outputs:</strong> Inputs: <code>applyId</code>, <code>operatorId</code>, <code>reason</code>. Outputs: <code>revertId</code>, <code>status</code>, <code>revertAuditRef</code>. <br><strong>Primary invariants:</strong><br>1. Revert idempotency: repeated reverts with same <code>revertId</code> return success and no-op after first successful revert. <br>2. Revert must not attempt heuristic restores without recorded operator consent and documented justification. <br><strong>Provenance & usage:</strong> For rollbacks and incidents. <br><strong>Failure modes & recovery:</strong> missing snapshot -> fail with <code>STD_REVERT_NO_SNAPSHOT</code>; partial revert -> persist partial outputs and escalate. <br><strong>Observability & audit:</strong> <code>standard.revert</code> with <code>applyId</code>, <code>revertId</code>, <code>operatorId</code>, <code>beforeChecksum</code>, <code>afterChecksum</code>. <br><strong>Tests:</strong> apply->revert checksum restore parity; missing snapshot flows. <br><strong>Conceptual PQ mapping:</strong> PQ snapshots can be used as revert sources if stored in evidence. <br><strong>DAX conceptual mapping:</strong> <code>RevertCount</code>, <code>RevertSuccessRate</code>. <br><strong>Security/PII:</strong> revert evidence sensitive; ensure encrypted storage and RBAC. <br><strong>Operational notes:</strong> revert may require approval if revert impacts regulated reporting. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: CleanupAfterBatch(batchId, retentionOptions)</strong><br><strong>Purpose & contract:</strong> post-run cleanup of ephemeral resources: remove temp staging sheets, free in-memory caches, purge checkpoints older than retention policy, rotate telemetry buffers, and ensure no residual unencrypted PII remains. Must preserve evidence artifacts required by retention policy. Return <code>cleanupReport</code>. <br><strong>Inputs & outputs:</strong> Inputs: <code>batchId</code>, <code>retentionOptions</code>. Outputs: <code>cleanupReport</code> {tempDeleted, cacheFreed, checkpointsPurged}. <br><strong>Primary invariants:</strong><br>1. Do not delete assets needed for regulatory retention windows; consult <code>Config.RetentionPolicy</code>. <br>2. Deletion must be atomic for each artifact and recorded in <code>cleanupReport</code>. <br><strong>Provenance & usage:</strong> invoked automatically at batch end or scheduled maintenance. <br><strong>Failure modes & recovery:</strong> failure to delete due to OS locks -> retry and escalate to SRE with <code>cleanup.failure</code> audit. <br><strong>Observability & audit:</strong> <code>batch.cleanup.completed</code> with counts; maintain <code>cleanupFailures</code>. <br><strong>Tests:</strong> create ephemeral artifacts and validate cleanup clears them and does not remove required evidence. <br><strong>Conceptual PQ mapping:</strong> PQ ephemeral exports should be cleaned similarly. <br><strong>DAX conceptual mapping:</strong> <code>TempArtifacts</code> and <code>CleanupSuccessRate</code>. <br><strong>Security/PII:</strong> confirm no unencrypted PII remains in temp areas; include proof-of-deletion in <code>cleanupReport</code>. <br><strong>Operational notes:</strong> schedule cleanup during off hours if heavy IO involved. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: CompileBatchErrorReport(batchId, errors, context)</strong><br><strong>Purpose & contract:</strong> aggregate and canonicalize per-row and per-chunk errors into a structured <code>errorReport</code> suitable for operator triage and forensics. Include <code>correlationId</code>, sample offending rows (redacted unless compliance access present), stack-level diagnostics, timestamps, and recommended remediation steps. Persist <code>errorReport</code> and return <code>errorReportRef</code>. <br><strong>Inputs & outputs:</strong> Inputs: <code>batchId</code>, <code>errors</code> array, <code>context</code> (componentVersion, paramsHash). Outputs: <code>errorReportRef</code>, <code>errorSummary</code>. <br><strong>Primary invariants:</strong><br>1. Redact PII in UI-visible error reports; full unredacted data available only via evidenceRef with RBAC. <br>2. Error buckets normalized to stable error codes (e.g., <code>STD_PARSE_FAIL</code>, <code>STD_PERMISSION_DENIED</code>). <br><strong>Provenance & usage:</strong> Operator triage and incident response. <br><strong>Failure modes & recovery:</strong> persistence failure -> local staging and immediate operator alert. <br><strong>Observability & audit:</strong> <code>batch.errorReport.generated</code> audit with <code>errorCount</code>. <br><strong>Tests:</strong> generate reports for various error mixes and validate redaction. <br><strong>Conceptual PQ mapping:</strong> PQ errors should be included in <code>errors</code> array for compilation. <br><strong>DAX conceptual mapping:</strong> <code>BatchErrorCountByType</code>. <br><strong>Security/PII:</strong> must not leak PII in error messages; use evidenceRef for full details. <br><strong>Operational notes:</strong> include <code>forensic_pack</code> link for regulatory incident workflows. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: ExportBatchArtifacts(batchId, artifactList, destinationUri, operatorId)</strong><br><strong>Purpose & contract:</strong> export batch artifacts (summary, migration artifact, <code>mappingHistory</code> slices, evidenceRefs) to a destination (internal S3, secure file share) using atomic write semantics. If destination unavailable, safely stage locally and emit <code>export.warning</code>. Compute <code>artifact.checksum.sha256</code> for each exported file and append <code>standard.map.export</code> audit row. Optionally sign artifact via integration with signing service. <br><strong>Inputs & outputs:</strong> Inputs: <code>batchId</code>, <code>artifactList</code>, <code>destinationUri</code>, <code>operatorId</code>. Outputs: <code>exportReport</code> {artifactRefs, checksums, exportStatus}. <br><strong>Primary invariants:</strong><br>1. Export file names must follow canonical naming convention (<code>standard_preview_&lt;plan&gt;_&lt;batchId&gt;_&lt;timestamp&gt;.zip</code>) to enable easy triage. <br>2. Preserve <code>evidenceRef</code> mapping in exports and include <code>forensic_manifest.json</code>. <br><strong>Provenance & usage:</strong> artifact export for compliance packaging and offsite archival. <br><strong>Failure modes & recovery:</strong> destination unavailability -> stage locally and emit <code>standard.map.export.warning</code>; require operator to re-run export when destination available. <br><strong>Observability & audit:</strong> <code>standard.map.export{destinationUri,checksum,operatorId}</code> audit. <br><strong>Tests:</strong> verify content checksums and signature verification on exports. <br><strong>Conceptual PQ mapping:</strong> PQ may produce some artifacts; ensure exported artifact set is complete and canonical. <br><strong>DAX conceptual mapping:</strong> <code>ExportsPerPeriod</code>, <code>ExportFailureRate</code>. <br><strong>Security/PII:</strong> apply redaction policy if operator lacks permission and annotate redacted fields in export metadata. Exports containing PII must be encrypted and access recorded. <br><strong>Operational notes:</strong> maintain retention metadata and ensure exported archives include <code>forensic_manifest</code> with checksums. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: ScoreBatchOrchestrator(snapshotSource, options)</strong><br><strong>Purpose & contract:</strong> top-level orchestrator that implements the full batch flow: load snapshot, validate, build <code>paramsHash</code>, precompute components, partition chunks, process chunks (local or via job scheduler), perform atomic write, verify write, create apply descriptor (if apply requested), run impact simulation if requested, produce summary, persist migration artifact and audits. MUST be idempotent given the same <code>snapshotHash</code> and <code>paramsHash</code>, must implement read-then-validate-then-swap safely, and must not perform blocking network IO on the UI thread. <br><strong>Inputs & outputs:</strong> Inputs: <code>snapshotSource</code>, <code>options</code> {mode:<code>create_copy|inline</code>, dryRun, chunkSize, priority, operatorId}. Outputs: <code>batchResult</code> {status, batchId, correlationId, summaryRef, applyDescriptorRef}. <br><strong>Primary invariants:</strong><br>1. <code>batchId</code> deterministic: <code>sha256(snapshotHash + paramsHash + operatorId + options)</code> so repeated runs are idempotent. <br>2. Batch must abort on critical validation failures and provide actionable diagnostics. <br>3. Must capture <code>paramsHash</code>, <code>snapshotHash</code>, <code>componentVersion</code>, <code>summaryHash</code>, and append appropriate audits for reconstructability. <br><strong>Provenance & usage:</strong> central entrypoint triggered by Reviewer UI or CI golden runs and admin HotSwap flows. <br><strong>Failure modes & recovery:</strong><br>• Concurrency conflict → abort with <code>STD_CONCURRENT_MODIFICATION</code>. <br>• Partial write failure -> persist partial outputs and present <code>RetryFailedBatch</code> options. <br><strong>Observability & audit:</strong> emit <code>batch.compute.started</code> and <code>batch.compute.completed</code> with metrics and evidenceRefs; provide <code>correlationId</code> to operator for triage. <br><strong>Performance expectations:</strong> small runs interactive; heavy runs scheduled. <br><strong>Test vectors:</strong> canonical fixtures including small, medium, and large datasets; inject failures to test retry/recovery paths. <br><strong>Conceptual PQ mapping:</strong> orchestrator accepts PQ snapshots and may dispatch PQ tasks for heavy precomputations or impact simulations. <br><strong>Conceptual DAX mapping:</strong> <code>BatchThroughput</code>, <code>AutoAssignRate</code>, <code>MaterialOverrideRate</code>, <code>BatchFailureRate</code>. <br><strong>Security/PII:</strong> enforce RBAC on apply modes; <code>inline</code> destructive operations require approvals recorded before proceeding. <br><strong>Operational notes:</strong> prefer <code>create_copy</code> default; <code>inline</code> mode only permitted under strict governance. Ensure <code>correlationId</code> surfaced to operator UI for triage. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: ValidateRowChecksumBeforeWrite(rowIndex, expectedChecksum)</strong><br><strong>Purpose & contract:</strong> before writing a row's result, validate that the original row's <code>rowChecksum</code> matches the snapshot's <code>rowChecksum</code> to prevent overwriting concurrent changes. If mismatch, return <code>STD_CONCURRENT_MODIFICATION_ROW</code> and allow orchestrator to decide to abort or rebase. <br><strong>Inputs & outputs:</strong> Inputs: <code>rowIndex</code>, <code>expectedChecksum</code>. Output: boolean <code>ok</code> and <code>actualChecksum</code>. <br><strong>Primary invariants:</strong><br>1. Checksums must be computed in same canonical way as <code>LoadCandidateSnapshot</code> produced. <br>2. If mismatch occurs, include prior row values reference in <code>validationReport</code> (redacted if PII). <br><strong>Provenance & usage:</strong> used during atomic write validations. <br><strong>Failure modes & recovery:</strong> on mismatch, orchestrator may abort batch or attempt merge depending on policy. <br><strong>Observability & audit:</strong> <code>concurrentModification.row</code> telemetry and audit entries. <br><strong>Tests:</strong> simulate row edit during batch to cause checksum mismatch reaction. <br><strong>Conceptual PQ mapping:</strong> PQ snapshots include same rowChecksums to detect concurrent changes earlier. <br><strong>DAX conceptual mapping:</strong> <code>RowChecksumMismatchRate</code>. <br><strong>Security/PII:</strong> do not emit full row content in mismatch logs unless evidence access approved. <br><strong>Operational notes:</strong> operator guidance when mismatch occurs: re-run batch with new snapshot. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: StageTempSheet(tempData, tempNamePattern)</strong><br><strong>Purpose & contract:</strong> stage computation results into a temporary sheet or temp file in canonical format to be used by <code>AtomicWriteResults</code>. Temporary artifacts must be named deterministically using pattern including <code>batchId</code>, <code>chunkId</code>, and <code>correlationId</code> and cleaned up after successful swap. Return <code>tempRef</code>. <br><strong>Inputs & outputs:</strong> Inputs: <code>tempData</code> (tabular), <code>tempNamePattern</code>. Outputs: <code>tempRef</code>, <code>durationMs</code>. <br><strong>Primary invariants:</strong><br>1. Temporary artifacts ephemeral and must be cleared after successful operation. <br>2. Staging must avoid leaving PII in unencrypted temporary files beyond <code>cleanup</code> window. <br><strong>Provenance & usage:</strong> intermediate step before atomic swap. <br><strong>Failure modes & recovery:</strong> write errors -> retry with backoff; if storage not available, error escalate. <br><strong>Observability & audit:</strong> <code>tempStage.created</code> event. <br><strong>Tests:</strong> staging and subsequent cleanup. <br><strong>Conceptual PQ mapping:</strong> PQ may stage CSV exports to shared storage for atomic swaps. <br><strong>DAX conceptual mapping:</strong> <code>TempStagingCount</code>. <br><strong>Security/PII:</strong> temp area encrypted or local only and cleaned promptly. <br><strong>Operational notes:</strong> ensure naming includes <code>correlationId</code>. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: SwapRangesAtomic(targetRange, sourceRange, preSwapChecks)</strong><br><strong>Purpose & contract:</strong> perform an atomic swap of two workbook ranges or files using workbook-level calls that minimize UI flicker and ensure a single atomic state visible to users. PreSwapChecks validated before execution. Return <code>swapToken</code> and <code>durationMs</code>. <br><strong>Inputs & outputs:</strong> Inputs: <code>targetRange</code>, <code>sourceRange</code>, <code>preSwapChecks</code> (snapshotHash verification). Outputs: <code>swapToken</code>, <code>beforeHash</code>, <code>afterHash</code>. <br><strong>Primary invariants:</strong><br>1. Pre-swap <code>snapshotHash</code> must match expected value. <br>2. Swap operation should be implemented via atomic rename or workbook-level temporary rename and commit to avoid partial states. <br><strong>Provenance & usage:</strong> final step of atomic write on CandidateMap updates. <br><strong>Failure modes & recovery:</strong> swap interrupted -> attempt to roll back using <code>beforeHash</code>; if unable, create <code>forensic_pack</code>. <br><strong>Observability & audit:</strong> <code>swap.invoked</code> and <code>swap.completed</code> with token. <br><strong>Tests:</strong> atomicity test under concurrent UI loads. <br><strong>Conceptual PQ mapping:</strong> PQ-based swaps use same atomic file rename patterns in production. <br><strong>DAX conceptual mapping:</strong> <code>AtomicSwapSuccessRate</code>. <br><strong>Security/PII:</strong> ensure that swap does not leak staging artifacts via external sharing. <br><strong>Operational notes:</strong> Keep swap windows short. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: LogBatchTelemetry(batchId, eventName, metrics)</strong><br><strong>Purpose & contract:</strong> append telemetry events to a local telemetry buffer (sheet or in-memory queue) with tags <code>batchId</code>, <code>paramsHash</code>, <code>snapshotHash</code>, <code>componentVersion</code>. Telemetry must be sanitized (no PII), and flushed periodically to central telemetry store by <code>FlushTelemetry</code>. <br><strong>Inputs & outputs:</strong> Inputs: <code>batchId</code>, <code>eventName</code>, <code>metrics</code> (dictionary). Outputs: <code>telemetryRowRef</code>. <br><strong>Primary invariants:</strong><br>1. Telemetry contains no PII. <br>2. Buffer size capped; overflow handled by rotating to disk and emitting <code>telemetry.rotate</code>. <br><strong>Provenance & usage:</strong> SRE and SLO monitoring feed. <br><strong>Failure modes & recovery:</strong> telemetry buffer full -> rotate to disk and emit <code>telemetry.rotation</code> event. <br><strong>Observability & audit:</strong> telemetry metrics feed dashboards: <code>BatchLatencyMs</code>, <code>ChunkLatencyMs</code>, <code>ErrorRate</code>. <br><strong>Tests:</strong> stress telemetry generation to ensure rotation works. <br><strong>Conceptual PQ mapping:</strong> PQ tasks should emit telemetry events for parity. <br><strong>DAX conceptual mapping:</strong> compute SLO metrics from telemetry feed. <br><strong>Security/PII:</strong> enforce redaction at source; no raw PII may enter telemetry. <br><strong>Operational notes:</strong> telemetry retention policies set in <code>Config</code>. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: FlushTelemetry(flushTarget)</strong><br><strong>Purpose & contract:</strong> flush buffered telemetry to central telemetry pipeline or persistent store. MUST encrypt transit if required and must not block UI thread. On flush failure, buffer to disk and schedule retry. Return <code>flushReceipt</code>. <br><strong>Inputs & outputs:</strong> Inputs: <code>flushTarget</code>, optional <code>timeoutMs</code>. Outputs: <code>flushReceipt</code> {rowsFlushed, durationMs}. <br><strong>Primary invariants:</strong><br>1. Ensure telemetry flushing does not leak PII. <br>2. On partial failures, do not drop events; rotate buffer to disk and persist pointer. <br><strong>Provenance & usage:</strong> periodic background maintenance or end-of-batch step. <br><strong>Failure modes & recovery:</strong> telemetry endpoint unreachable -> local staging and exponential retry. <br><strong>Observability & audit:</strong> <code>telemetry.flush.completed</code> or <code>telemetry.flush.failed</code>. <br><strong>Tests:</strong> flush under heavy load and simulate endpoint failure. <br><strong>Conceptual PQ mapping:</strong> PQ jobs should flush their telemetry in similar pattern. <br><strong>DAX conceptual mapping:</strong> derive SLO dashboards from flushed telemetry. <br><strong>Security/PII:</strong> encrypted transport required for central telemetry that may be sensitive. <br><strong>Operational notes:</strong> ensure telemetry clients rotate credentials and not embed long-lived tokens in workbook. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: ComputeSnapshotHash(snapshot)</strong><br><strong>Purpose & contract:</strong> compute canonical SHA256 <code>snapshotHash</code> for a given snapshot using stable canonicalization rules: stable column order, fixed float formatting, trimmed strings, normalized newline, and exclusion of ephemeral fields. <code>snapshotHash</code> used for reproducibility and idempotency and must match PQ's hash algorithm. <br><strong>Inputs & outputs:</strong> Input: <code>snapshot</code>. Output: <code>snapshotHash</code> string. <br><strong>Primary invariants:</strong><br>1. Canonicalization rules must be published and versioned; any change requires migration manifest. <br>2. Hash must be identical across PQ and VBA for the same snapshot content. <br><strong>Provenance & usage:</strong> used in <code>LoadCandidateSnapshot</code> and all batch audits. <br><strong>Failure modes & recovery:</strong> any mismatch with PQ -> <code>standard.verify.failure</code> and CI block. <br><strong>Observability & audit:</strong> record <code>snapshotHash</code> in batch audits. <br><strong>Tests:</strong> cross-runtime hash parity tests. <br><strong>Conceptual PQ mapping:</strong> PQ must implement identical canonicalization and produce same <code>snapshotHash</code>. <br><strong>DAX conceptual mapping:</strong> <code>SnapshotHashChangeEvents</code> tracked. <br><strong>Security/PII:</strong> hashes safe; avoid including per-row PII in enabling metadata fields used in hash. <br><strong>Operational notes:</strong> change to canonicalization must pass CI golden parity tests. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: CompileAndStoreEvidenceSample(batchId, sampleRows, evidenceStore)</strong><br><strong>Purpose & contract:</strong> create a secure evidence artifact for a sample of rows (full unredacted) used for compliance review when mappings touch material items. Responsibilities: canonical packaging, encryption, compute <code>evidenceRef</code> and checksums, and return <code>evidenceRef</code>. MUST ensure proper access controls and record evidence access audit entries. <br><strong>Inputs & outputs:</strong> Inputs: <code>batchId</code>, <code>sampleRows</code>, <code>evidenceStore</code>. Outputs: <code>evidenceRef</code>, <code>checksum</code>. <br><strong>Primary invariants:</strong><br>1. Evidence artifact naming canonical and includes <code>batchId</code> and <code>correlationId</code>. <br>2. Evidence access controlled via RBAC and retrieval audited. <br><strong>Provenance & usage:</strong> evidenceRef referenced in MappingHistory and batch audits. <br><strong>Failure modes & recovery:</strong> evidence store unavailability -> stage locally encrypted and emit <code>evidence.persist.failed</code>. <br><strong>Observability & audit:</strong> <code>evidence.created</code> with <code>evidenceRef</code> and <code>retentionPolicy</code>. <br><strong>Tests:</strong> encrypt-decrypt round-trip and access control checks. <br><strong>Conceptual PQ mapping:</strong> PQ previews should reference same evidence artifacts for parity. <br><strong>DAX conceptual mapping:</strong> <code>EvidenceCreatedCount</code>, <code>EvidenceAccessCount</code>. <br><strong>Security/PII:</strong> evidence must be encrypted and access strictly controlled. <br><strong>Operational notes:</strong> retention metadata required for regulated exports. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: GenerateForensicPack(batchId, correlationId, reason)</strong><br><strong>Purpose & contract:</strong> assemble a canonical forensic package containing <code>standardize-map.json</code>, <code>validationReport</code>, <code>audit_tail.csv</code> for correlationId, preview/preview evidence, applyDescriptor, and <code>forensic_manifest.json</code> with sha256 checksums and chain-of-custody metadata. Persist to immutable WORM storage and return <code>forensicPackRef</code>. <br><strong>Inputs & outputs:</strong> Inputs: <code>batchId</code>, <code>correlationId</code>, <code>reason</code>. Outputs: <code>forensicPackRef</code>, <code>manifestHash</code>. <br><strong>Primary invariants:</strong><br>1. Forensic pack must include chain-of-custody metadata (collectorId, timestamps, checksums). <br>2. Pack must be immutable and verifiable via <code>manifestHash</code>. <br><strong>Provenance & usage:</strong> used for regulated incidents and external audits. <br><strong>Failure modes & recovery:</strong> inability to persist to WORM -> stage locally and escalate immediately. <br><strong>Observability & audit:</strong> <code>forensic.pack.created</code> with <code>manifestHash</code>. <br><strong>Tests:</strong> create packs and verify retrieval and checksum validity. <br><strong>Conceptual PQ mapping:</strong> PQ-generated artifacts included in pack for full pipeline reproducibility. <br><strong>DAX conceptual mapping:</strong> <code>ForensicPackCount</code>, <code>ForensicPackRetrievals</code>. <br><strong>Security/PII:</strong> extremely sensitive; strict access and decrypt logs required. <br><strong>Operational notes:</strong> align with legal/regulatory team runbook for evidence release. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: BuildApplyDescriptor(batchId, applyOptions, operatorId)</strong><br><strong>Purpose & contract:</strong> build canonical <code>ApplyDescriptor</code> used to schedule or run actual mapping apply operations. Contains <code>applyId</code>, <code>batchId</code>, <code>beforeSnapshotRef</code>, <code>expectedAfterHash</code>, <code>applyMode</code>, <code>operatorId</code>, <code>approvalsRef</code>, and <code>createdTs</code>. Must be atomic and idempotent and persisted as durable artifact. <br><strong>Inputs & outputs:</strong> Inputs: <code>batchId</code>, <code>applyOptions</code>, <code>operatorId</code>, <code>approvalsRef</code>. Outputs: <code>applyDescriptorRef</code>, <code>applyDescriptor</code>. <br><strong>Primary invariants:</strong><br>1. Descriptor must include <code>beforeSnapshotRef</code> and <code>beforeChecksum</code>. <br>2. For destructive applies, descriptor must reference approvals and be blocked if approvals missing. <br><strong>Provenance & usage:</strong> used by <code>ApplyBatchIfRequested</code> and preserved for revert operations. <br><strong>Failure modes & recovery:</strong> persist failure -> retry and alert. <br><strong>Observability & audit:</strong> <code>apply.descriptor.created</code> audit. <br><strong>Tests:</strong> idempotency tests. <br><strong>Conceptual PQ mapping:</strong> PQ may persist similar apply descriptors for large PQ-driven transformations. <br><strong>DAX conceptual mapping:</strong> <code>ApplyDescriptorCount</code>. <br><strong>Security/PII:</strong> descriptor contains no PII; evidence refs are separate. <br><strong>Operational notes:</strong> descriptor immutability helps forensic and revert operations. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: ValidateApprovalsBeforeApply(applyDescriptor)</strong><br><strong>Purpose & contract:</strong> enforce governance: verify that apply has required approvals (owner, compliance, legal) for destructive or material applies; validate signatures or approvalsRef, and return boolean <code>approved</code> and <code>missingApprovals</code> list. Must be deterministic and auditable. <br><strong>Inputs & outputs:</strong> Inputs: <code>applyDescriptor</code>. Output: <code>approved</code> boolean, <code>missingApprovals</code> list. <br><strong>Primary invariants:</strong><br>1. Approval matrix configured in <code>Config</code> maps rule types to approval levels; <code>materialFlag</code> increases required approval level. <br>2. Approvals are immutable records with <code>approvedBy</code>, <code>timestamp</code>, and optional <code>signedEvidenceRef</code>. <br><strong>Provenance & usage:</strong> called before <code>ApplyBatchIfRequested</code>. <br><strong>Failure modes & recovery:</strong> missing approvals -> block apply and append <code>STD_PERMISSION_DENIED</code> audit with instructions. <br><strong>Observability & audit:</strong> <code>apply.approvals.validated</code> audit including missing approvals. <br><strong>Tests:</strong> simulate missing approvals and confirm blocking behavior. <br><strong>Conceptual PQ mapping:</strong> PQ may tag which rules are <code>destructive</code> to feed approval logic. <br><strong>DAX conceptual mapping:</strong> <code>ApprovalsPendingCount</code>. <br><strong>Security/PII:</strong> approvals records may include operator identity; store securely and redact as needed. <br><strong>Operational notes:</strong> approvals changing should be versioned and included in migration manifest. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: ApplyPostProcessingHooks(applyDescriptor, results)</strong><br><strong>Purpose & contract:</strong> run registered post-apply hooks after a successful apply (e.g., refresh caches, notify BI, trigger downstream extracts). Hooks should be pluggable, run with timeouts and retries, and must not block critical apply completion. Errors in hooks should be non-blocking but logged for operator follow-up. <br><strong>Inputs & outputs:</strong> Inputs: <code>applyDescriptor</code>, <code>results</code>. Output: <code>hookResults</code> summary. <br><strong>Primary invariants:</strong><br>1. Hooks run under least privilege and with time/budget limits. <br>2. Hook failures do not roll back apply; instead record <code>hook.failure</code> audit. <br><strong>Provenance & usage:</strong> trigger downstream systems to pick up new canonical mapping. <br><strong>Failure modes & recovery:</strong> hook failure -> retry per hook policy and if persistent, produce <code>hook.failed</code> audit and manual remediation steps. <br><strong>Observability & audit:</strong> <code>apply.hooks.*</code> events for each hook. <br><strong>Tests:</strong> simulate hook timeouts and ensure they are logged and do not affect apply success. <br><strong>Conceptual PQ mapping:</strong> PQ workflows may register hooks to refresh PQ staging datasets. <br><strong>DAX conceptual mapping:</strong> <code>HookFailureRate</code>. <br><strong>Security/PII:</strong> hooks must not expose PII in logs; use evidenceRef where needed. <br><strong>Operational notes:</strong> maintain a registry of approved hooks and their owners. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: HandlePartialFailures(batchId, failureDetails)</strong><br><strong>Purpose & contract:</strong> dedicated handler to coordinate recovery for partial failures (e.g., some chunks succeeded and others failed): persist partial outputs in <code>FailedBatchStore</code>, generate <code>forensic_pack</code> if needed, schedule <code>RetryFailedBatch</code> or <code>RevertMapping</code> depending on error nature, and notify operators with <code>correlationId</code> and clear remediation steps. <br><strong>Inputs & outputs:</strong> Inputs: <code>batchId</code>, <code>failureDetails</code>. Outputs: <code>recoveryPlanRef</code>, <code>notifications</code> summary. <br><strong>Primary invariants:</strong><br>1. Partial output persisted and protected from accidental deletion. <br>2. Recovery plan deterministic and auditable. <br><strong>Provenance & usage:</strong> used by incident responders and automated monitors. <br><strong>Failure modes & recovery:</strong> if recovery plan cannot be executed automatically, escalate to manual operator runbook. <br><strong>Observability & audit:</strong> <code>batch.partialFailure.handled</code> event with recoveryPlanRef. <br><strong>Tests:</strong> simulate partial failure and verify recovery plan creation and idempotent retry execution. <br><strong>Conceptual PQ mapping:</strong> PQ pipelines should have equivalent partial-failure handling. <br><strong>DAX conceptual mapping:</strong> <code>PartialFailureCount</code>, <code>AvgTimeToRecover</code>. <br><strong>Security/PII:</strong> partial outputs stored encrypted; field-level redaction for operator notifications. <br><strong>Operational notes:</strong> ensure playbooks for common failure modes are available and linked in <code>recoveryPlan</code>. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: FinalizeBatch(batchId, finalState)</strong><br><strong>Purpose & contract:</strong> finalize the batch lifecycle after completion: mark batch <code>completed</code> or <code>failed</code>, run <code>CleanupAfterBatch</code>, flush telemetry, persist final audit chain, and trigger any downstream packaged exports (migration artifact propagation). Return <code>finalizeReceipt</code>. <br><strong>Inputs & outputs:</strong> Inputs: <code>batchId</code>, <code>finalState</code> {status, notes}. Outputs: <code>finalizeReceipt</code> {completedAt, artifacts}. <br><strong>Primary invariants:</strong><br>1. Finalize must ensure all required audits and evidence stored before declaring <code>completed</code>. <br>2. For failed batches with PII or regulatory implication, trigger <code>forensicPack</code> automatically. <br><strong>Provenance & usage:</strong> end-of-life for batch; used by operators to close the loop. <br><strong>Failure modes & recovery:</strong> missing artifacts prevents <code>completed</code> status and block; escalate. <br><strong>Observability & audit:</strong> <code>batch.finalized</code> with status and <code>finalizeReceipt</code>. <br><strong>Tests:</strong> finalize normal and failed batches verifying required artifacts present. <br><strong>Conceptual PQ mapping:</strong> PQ-run jobs need the same finalize steps for their artifacts. <br><strong>DAX conceptual mapping:</strong> <code>BatchesFinalizedCount</code>, <code>AvgFinalizeDuration</code>. <br><strong>Security/PII:</strong> ensure evidence chain intact; no PII leaks during finalize. <br><strong>Operational notes:</strong> finalize step should be the gate for any migration artifact release. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: ScoreBatchValidationSanityChecks(batchSummary)</strong><br><strong>Purpose & contract:</strong> run a deterministic checklist of soft sanity checks on batchSummary before generating migration artifacts: check that no single mapping accounts for > configured material fraction without explicit approval, that top-N deltas look reasonable, <code>AutoAcceptRate</code> within expected bounds, and <code>paramsHash</code> matches governance-approved version. Return <code>sanityReport</code> with pass/fail and warnings. <br><strong>Inputs & outputs:</strong> Inputs: <code>batchSummary</code>, <code>sanityConfig</code>. Output: <code>sanityReport</code>. <br><strong>Primary invariants:</strong><br>1. Sanity checks are non-destructive and produce deterministic pass/fail outputs used as deployment gate. <br>2. Any warning or failure recorded and must be acknowledged by operator for hot-swap or migration. <br><strong>Provenance & usage:</strong> gating step before artifact persistence and apply. <br><strong>Failure modes & recovery:</strong> fail -> require operator to fix or provide <code>migrationManifest</code> justification to proceed. <br><strong>Observability & audit:</strong> <code>batch.sanitycheck.completed</code> with pass/fail. <br><strong>Tests:</strong> edge cases where single account has disproportionate delta. <br><strong>Conceptual PQ mapping:</strong> PQ simulation outputs used to assess deltas. <br><strong>DAX conceptual mapping:</strong> <code>SanityCheckFailRate</code>. <br><strong>Security/PII:</strong> pass the least PII in the report; provide <code>evidenceRef</code> for full detail if needed. <br><strong>Operational notes:</strong> configure <code>sanityConfig</code> thresholds via <code>Config</code> and record changes in migration manifest. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: EstimateMemoryForChunk(chunkRows, componentCache, perRowEstimate)</strong><br><strong>Purpose & contract:</strong> estimate memory footprint for a chunk to decide chunkSize and whether chunk must be split or offloaded. Use conservative per-row estimates derived from observed memory usage plus metadata. Return <code>estimatedMemBytes</code>. <br><strong>Inputs & outputs:</strong> Inputs: <code>chunkRows</code>, <code>componentCache</code> context, <code>perRowEstimate</code>. Output: <code>estimatedMemBytes</code>. <br><strong>Primary invariants:</strong><br>1. Estimates must be conservative to avoid OOM; include overhead for arrays and temporary buffers. <br>2. Estimation must be deterministic given same inputs. <br><strong>Provenance & usage:</strong> chunk planning and dynamic chunk resizing. <br><strong>Failure modes & recovery:</strong> underestimations lead to chunk failures; upon chunk failure the orchestrator should reduce chunkSize and retry. <br><strong>Observability & audit:</strong> record <code>chunkMemoryEstimate</code> and actual memory used. <br><strong>Tests:</strong> synthetic tests with large token/trigram sizes. <br><strong>Conceptual PQ mapping:</strong> PQ pre-aggregations reduce per-row memory needs. <br><strong>DAX conceptual mapping:</strong> <code>ChunkMemoryEstimateVsActual</code>. <br><strong>Security/PII:</strong> estimation uses sizes, not content. <br><strong>Operational notes:</strong> keep perRowEstimate in <code>Config</code> and tune on host basis. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: ValidateWorkerHealth(workerId)</strong><br><strong>Purpose & contract:</strong> check worker heartbeat, resource availability, and permission to process assigned descriptors. Return <code>healthy</code> boolean and <code>diagnostics</code>. <br><strong>Inputs & outputs:</strong> Inputs: <code>workerId</code>. Output: <code>healthy</code>, <code>diagnostics</code>. <br><strong>Primary invariants:</strong><br>1. Worker must have up-to-date <code>paramsHash</code> and <code>componentVersion</code> or it should fetch required artifacts before claiming work. <br><strong>Provenance & usage:</strong> scheduler ensures workers healthy before assigning chunks. <br><strong>Failure modes & recovery:</strong> unhealthy -> reassign job; escalate persistent problems. <br><strong>Observability & audit:</strong> <code>worker.health</code> metrics. <br><strong>Tests:</strong> simulate high memory usage and confirm worker flagged unhealthy. <br><strong>Conceptual PQ mapping:</strong> PQ workers share same health checks. <br><strong>DAX conceptual mapping:</strong> <code>WorkerHealthyPercent</code>. <br><strong>Security/PII:</strong> worker health checks avoid exposing PII. <br><strong>Operational notes:</strong> implement auto-scaling policies based on healthy worker ratio. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: BuildAuditPayloadFromRowResult(rowResult, operatorContext)</strong><br><strong>Purpose & contract:</strong> serialize rowResult into canonical audit-friendly payload with PII redacted, referencing <code>evidenceRef</code> for full details. Compute <code>payloadHash</code> (SHA256) and return payload stub and <code>payloadHash</code>. Must use stable canonicalization rules. <br><strong>Inputs & outputs:</strong> Inputs: <code>rowResult</code>, <code>operatorContext</code> (operatorId, correlationId). Outputs: <code>auditPayload</code>, <code>payloadHash</code>. <br><strong>Primary invariants:</strong><br>1. <code>payloadHash</code> must be deterministic for the same payload and parameters. <br>2. Ensure no raw PII is present in <code>auditPayload</code>. <br><strong>Provenance & usage:</strong> stores <code>payloadHash</code> in <code>MappingHistory</code> and batch audits. <br><strong>Failure modes & recovery:</strong> serialization failure -> fallback minimal payload with <code>STD_AUDIT_SERIALIZE_ERROR</code>. <br><strong>Observability & audit:</strong> <code>audit.payload.generated</code> with <code>payloadHash</code>. <br><strong>Tests:</strong> verify redaction and hash parity tests. <br><strong>Conceptual PQ mapping:</strong> PQ may produce audit payloads for its operations to sync with overall audit chain. <br><strong>DAX conceptual mapping:</strong> <code>PayloadHashCount</code> to detect multiple payload versions. <br><strong>Security/PII:</strong> ensure any fields containing PII are replaced with evidenceRef or hashed. <br><strong>Operational notes:</strong> audit payload schema must be stable and versioned. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: ValidateChunkResultsBeforeWrite(chunkResult, expectedChecksums)</strong><br><strong>Purpose & contract:</strong> validate chunk results against expectedChecksums and preconditions (no missing rows, no unresolvable <code>STD_SCORE_ERROR</code>) before incorporating results into staging area for atomic swap. Return validation status and error list. <br><strong>Inputs & outputs:</strong> Inputs: <code>chunkResult</code>, <code>expectedChecksums</code>. Output: <code>valid</code> boolean, <code>errors</code>. <br><strong>Primary invariants:</strong><br>1. Deterministic validation order and stable error codes. <br>2. Any mismatch triggers reprocessing of affected rows. <br><strong>Provenance & usage:</strong> used to ensure chunk integrity pre-write. <br><strong>Failure modes & recovery:</strong> validation failure -> re-run chunk or escalate. <br><strong>Observability & audit:</strong> <code>chunk.validation.failed</code> telemetry. <br><strong>Tests:</strong> simulate mismatches causing validation failure. <br><strong>Conceptual PQ mapping:</strong> PQ should produce similar chunk verification checks. <br><strong>DAX conceptual mapping:</strong> <code>ChunkValidationFailures</code>. <br><strong>Security/PII:</strong> validation errors stored redacted unless evidence access approved. <br><strong>Operational notes:</strong> thresholds for acceptable partial errors configured in <code>Config</code>. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: AssembleChunkWriteBatch(tempStagingArea, chunkResults)</strong><br><strong>Purpose & contract:</strong> aggregate multiple chunk results into a single staging artifact for atomic swap, performing canonical ordering and deduplication if repeated chunks exist. Return <code>stagingRef</code> and <code>rowsCount</code>. <br><strong>Inputs & outputs:</strong> Inputs: <code>tempStagingArea</code>, <code>chunkResults</code> array. Outputs: <code>stagingRef</code>, <code>rowsCount</code>. <br><strong>Primary invariants:</strong><br>1. Canonical ordering preserves original row order to ensure deterministic <code>afterHash</code>. <br>2. Deduplicate repeated row updates by using row index precedence or last-writer policy subject to orchestrator rules. <br><strong>Provenance & usage:</strong> used before <code>AtomicWriteResults</code> call. <br><strong>Failure modes & recovery:</strong> staging failure -> revert and retry aggregation. <br><strong>Observability & audit:</strong> <code>staging.assembled</code> with <code>rowsCount</code>. <br><strong>Tests:</strong> merge overlapping chunk results and verify final ordering and de-duplication policy. <br><strong>Conceptual PQ mapping:</strong> PQ outputs can be merged similarly for final artifacts. <br><strong>DAX conceptual mapping:</strong> <code>StagingRowsCount</code>. <br><strong>Security/PII:</strong> staging must be ephemeral and cleared post-swap. <br><strong>Operational notes:</strong> ensure staging artifact is small enough for swap. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: MarkRowsAsProcessed(batchId, rowIndices, status)</strong><br><strong>Purpose & contract:</strong> mark rows as processed in CandidateMap metadata (e.g., <code>ProcessedInBatch</code>, <code>ProcessedTs</code>, <code>ProcessStatus</code>) to enable auditability and avoid reprocessing. Must be done atomically with write operation or immediately after successful writeReceipt. <br><strong>Inputs & outputs:</strong> Inputs: <code>batchId</code>, <code>rowIndices</code>, <code>status</code> (e.g., Applied, Skipped). Outputs: <code>markReceipt</code>. <br><strong>Primary invariants:</strong><br>1. Marking must be idempotent and atomic; re-marking same row is allowed and safe. <br>2. Provide <code>ProcessStatus</code> and <code>batchId</code> to trace which batch processed the row. <br><strong>Provenance & usage:</strong> later analytics use these fields to compute <code>OverrideRate</code>. <br><strong>Failure modes & recovery:</strong> marking failure -> schedule a retry job to update metadata; ensure consistency. <br><strong>Observability & audit:</strong> <code>rows.marked.processed</code> counts. <br><strong>Tests:</strong> mark rows and verify idempotency and correct metadata. <br><strong>Conceptual PQ mapping:</strong> PQ should not overwrite processed markers without orchestrator consent. <br><strong>DAX conceptual mapping:</strong> <code>RowsProcessedPerBatch</code>. <br><strong>Security/PII:</strong> metadata safe; avoid storing raw PII. <br><strong>Operational notes:</strong> integrate marking into atomic write path to ensure consistency. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: ReportBatchMetricsToDashboard(batchId, metrics)</strong><br><strong>Purpose & contract:</strong> send computed batch-level metrics to reporting pipeline or workbook metrics table for dashboarding. Metrics include <code>AutoAssignRate</code>, <code>ReviewOverrideRate</code>, <code>BatchLatencyMs</code>, <code>ChunkFailureRate</code>, <code>MaterialRowsCount</code>, <code>EstimatedAffectedAmt</code>. Must sanitize inputs and avoid PII leakage. Return <code>dashboardUpdateReceipt</code>. <br><strong>Inputs & outputs:</strong> Inputs: <code>batchId</code>, <code>metrics</code>. Outputs: <code>dashboardUpdateReceipt</code>. <br><strong>Primary invariants:</strong><br>1. Metrics are aggregated and non-PII. <br>2. Metric update must be idempotent for same <code>batchId</code>. <br><strong>Provenance & usage:</strong> populate DAX-based dashboards for SRE and governance. <br><strong>Failure modes & recovery:</strong> telemetry pipeline failure -> local staging and retry. <br><strong>Observability & audit:</strong> <code>dashboard.update</code> events. <br><strong>Tests:</strong> update dashboard and ensure idempotency and correctness. <br><strong>Conceptual PQ mapping:</strong> PQ pre-aggregation can supply metrics to dashboard in addition to VBA. <br><strong>DAX conceptual mapping:</strong> measures <code>AutoAcceptRate</code>, <code>ReviewOverrideRate</code>, <code>MaterialityAlerts</code> computed from these metrics. <br><strong>Security/PII:</strong> verify metrics do not leak account-level data. <br><strong>Operational notes:</strong> metrics consumption must be consistent across teams. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: BuildBatchRetentionManifest(batchId, retentionPolicy)</strong><br><strong>Purpose & contract:</strong> assemble a manifest describing retention and archival targets for batch artifacts and evidence (hot/warm/cold tiers), compute checksums for each artifact, include <code>forensic_manifest.json</code> entries, and persist manifestRef. This is required for regulated datasets. <br><strong>Inputs & outputs:</strong> Inputs: <code>batchId</code>, <code>retentionPolicy</code>. Outputs: <code>manifestRef</code>, <code>manifestHash</code>. <br><strong>Primary invariants:</strong><br>1. Manifest must list all artifacts with checksums and retention tiers. <br>2. Manifest must be signed or verified to be immutable for regulator requirements. <br><strong>Provenance & usage:</strong> used by compliance and SRE to manage archival. <br><strong>Failure modes & recovery:</strong> persistence failure -> stage locally and escalate. <br><strong>Observability & audit:</strong> <code>retention.manifest.created</code>. <br><strong>Tests:</strong> generate manifest and verify checksums and retrieval. <br><strong>Conceptual PQ mapping:</strong> PQ outputs must be included in manifest if relevant. <br><strong>DAX conceptual mapping:</strong> <code>RetentionManifestCount</code>. <br><strong>Security/PII:</strong> manifest contains artifact pointers not PII. <br><strong>Operational notes:</strong> ensure retention TTLs are aligned with legal requirements. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: BatchRunSmokeTests(batchId, fixtures)</strong><br><strong>Purpose & contract:</strong> run a small deterministic set of smoke tests after staging or hot-swap to validate basic mapping semantics (e.g., sample of canonical fixtures produce expected scores and sample disclosural deltas within thresholds). Should return <code>smokeTestResults</code> and <code>smokePass</code> boolean. Smoke tests are required for hot-swap or production map updates. <br><strong>Inputs & outputs:</strong> Inputs: <code>batchId</code>, <code>fixtures</code> set, <code>paramsHash</code>. Outputs: <code>smokeTestResults</code>, <code>smokePass</code>. <br><strong>Primary invariants:</strong><br>1. Smoke tests must be deterministic using fixed seed and canonical fixtures. <br>2. Failure of smoke tests requires automatic revert of hot-swap if the change was applied in-memory. <br><strong>Provenance & usage:</strong> gating step for hot-swap and production changes. <br><strong>Failure modes & recovery:</strong> smoke test fail -> <code>standard.hotswap.reverted</code> audit and revert actions. <br><strong>Observability & audit:</strong> <code>smoketest.completed</code> with pass/fail and <code>diffReportRef</code>. <br><strong>Tests:</strong> intentionally cause regression and verify smoke test catches it. <br><strong>Conceptual PQ mapping:</strong> PQ golden comparisons used for smoke tests at scale. <br><strong>DAX conceptual mapping:</strong> <code>SmokeTestPassRate</code>. <br><strong>Security/PII:</strong> smoke fixtures non-PII. <br><strong>Operational notes:</strong> require two-person signoff to bypass smoke test fails for emergency patches. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: ScoreHashParityCheck(fixtureSet, pqSnapshot, paramsHash)</strong><br><strong>Purpose & contract:</strong> diagnostic function to compare <code>scoreHash</code> values computed in VBA against PQ-computed <code>scoreHash</code> for the same fixture set. Returns <code>parityReport</code> with per-row diffs and severity classification. Mandatory CI gate for config changes affecting normalization or scoring semantics. <br><strong>Inputs & outputs:</strong> Inputs: <code>fixtureSet</code>, <code>pqSnapshot</code>, <code>paramsHash</code>. Output: <code>parityReport</code>. <br><strong>Primary invariants:</strong><br>1. Accept tiny epsilon for floating differences but require exact <code>scoreHash</code> parity; mismatches above threshold block releases. <br>2. Diagnostic must indicate whether mismatch is due to normalization/tokenization/trigram/float-format differences. <br><strong>Provenance & usage:</strong> CI golden tests and pre-deploy verification. <br><strong>Failure modes & recovery:</strong> mismatch -> block deployment and provide differential diagnostic to developers. <br><strong>Observability & audit:</strong> <code>score.parity.check</code> events and <code>parityReportRef</code>. <br><strong>Tests:</strong> produce intentional parity failures and verify detection. <br><strong>Conceptual PQ mapping:</strong> PQ outputs used as ground-truth in parity check. <br><strong>DAX conceptual mapping:</strong> <code>ParityFailureCount</code>. <br><strong>Security/PII:</strong> fixtures may be synthetic to avoid PII. <br><strong>Operational notes:</strong> parity check must be part of all PR CI pipelines touching normalization or weights. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: StressTestRunner(workloadSpec, paramsHash)</strong><br><strong>Purpose & contract:</strong> synthetic workload harness to simulate heavy volumes and atypical inputs for capacity planning. Accepts parameters for row counts, label length distributions, Unicode ratio, and signature complexity. Produces <code>stressReport</code> containing throughput, p50/p95/p99 latencies, memory high-water mark, and sample failing rows evidenceRef. <br><strong>Inputs & outputs:</strong> Inputs: <code>workloadSpec</code> (seeded), <code>paramsHash</code>. Outputs: <code>stressReportRef</code>, <code>metrics</code>. <br><strong>Primary invariants:</strong><br>1. Synthetic data must avoid PII and be reproducible via seed for consistent benchmarking. <br>2. Provide recommended chunkSize and offload suggestions in <code>stressReport</code>. <br><strong>Provenance & usage:</strong> used for pre-deploy performance checks and tuning. <br><strong>Failure modes & recovery:</strong> OOM or host instability -> recommend worker offload and chunk tuning. <br><strong>Observability & audit:</strong> <code>stress.run.completed</code> with <code>metrics</code>. <br><strong>Tests:</strong> run stress at target production volumes and verify meeting SLOs; run with different seeds. <br><strong>Conceptual PQ mapping:</strong> PQ runs might be used for distributed stress tests. <br><strong>DAX conceptual mapping:</strong> <code>StressTestSummary</code> KPIs. <br><strong>Security/PII:</strong> synthetic only; do not use production PII. <br><strong>Operational notes:</strong> keep stress runner available in CI environment for regression. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: ProduceGoldenArtifactForCI(batchId, fixtures, paramsHash)</strong><br><strong>Purpose & contract:</strong> produce canonical golden artifacts (summaryHash, sample outputs, scoreHash list) for CI to validate. Golden artifacts must be stored immutably and require a migration manifest to update. Return <code>goldenArtifactRef</code>. <br><strong>Inputs & outputs:</strong> Inputs: <code>batchId</code>, <code>fixtures</code>, <code>paramsHash</code>. Output: <code>goldenArtifactRef</code>, <code>goldenHash</code>. <br><strong>Primary invariants:</strong><br>1. Golden artifacts immutable and updated only via PR with <code>migrationManifest</code> and two-person approval for regulated outputs. <br>2. Golden file canonicalization rules strictly enforced to avoid spurious diffs. <br><strong>Provenance & usage:</strong> CI golden parity tests. <br><strong>Failure modes & recovery:</strong> golden mismatch -> fail build and produce diff report. <br><strong>Observability & audit:</strong> <code>golden.artifact.created</code> and <code>golden.diff</code> metrics. <br><strong>Tests:</strong> golden creation and subsequent parity test. <br><strong>Conceptual PQ mapping:</strong> PQ outputs used to create golden artifacts as well. <br><strong>DAX conceptual mapping:</strong> <code>GoldenParityFailCount</code>. <br><strong>Security/PII:</strong> fixtures for golden tests synthetic or redacted. <br><strong>Operational notes:</strong> store golden artifacts in immutable storage with checksums. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: BuildBatchRunbook(batchId, summaryRef, contacts)</strong><br><strong>Purpose & contract:</strong> create a concise runbook for operators containing <code>correlationId</code>, quick triage steps, rollback steps, evidenceRef locations, contact matrix, and post-mortem checklist. Persist runbookRef and return. <br><strong>Inputs & outputs:</strong> Inputs: <code>batchId</code>, <code>summaryRef</code>, <code>contacts</code>. Outputs: <code>runbookRef</code>. <br><strong>Primary invariants:</strong><br>1. Runbook must contain minimal PII only as allowed by operator permissions and must include <code>evidenceRef</code> pointers. <br>2. Must be generated automatically for material or high-risk batches. <br><strong>Provenance & usage:</strong> operator triage and incident response. <br><strong>Failure modes & recovery:</strong> missing contacts -> use escalation matrix default and audit <code>runbook.incomplete</code>. <br><strong>Observability & audit:</strong> <code>runbook.created</code> event. <br><strong>Tests:</strong> validate runbook content generation for sample batches. <br><strong>Conceptual PQ mapping:</strong> PQ may add additional context like sample queries used. <br><strong>DAX conceptual mapping:</strong> <code>RunbookGeneratedCount</code>. <br><strong>Security/PII:</strong> redact contact personal info unless operator permission present. <br><strong>Operational notes:</strong> include runbook link in all batch audit rows for fast retrieval. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: FinalSanityChecksBeforeCommit(batchSummary, applyOptions)</strong><br><strong>Purpose & contract:</strong> final checks before commit: verify that batchSummary passed prior sanity checks, approvals present for destructive apply, <code>wouldCommitAfterHash</code> equals staged <code>afterHash</code>, and that <code>ImpactSimulation</code> shows no unresolved material deltas. Return boolean <code>ok</code> and <code>checks</code> list. <br><strong>Inputs & outputs:</strong> Inputs: <code>batchSummary</code>, <code>applyOptions</code>. Outputs: <code>ok</code>, <code>checks</code> array. <br><strong>Primary invariants:</strong><br>1. Block commit unless all critical checks pass or explicit documented override with migration manifest exists. <br>2. All check results must be auditable. <br><strong>Provenance & usage:</strong> final gate before <code>AtomicWriteResults</code> or <code>ApplyBatchIfRequested</code>. <br><strong>Failure modes & recovery:</strong> failed checks -> abort commit and present diagnostics; require manual override logged. <br><strong>Observability & audit:</strong> <code>batch.finalchecks.completed</code>. <br><strong>Tests:</strong> fail paths where approvals missing or impact simulation flagged material changes. <br><strong>Conceptual PQ mapping:</strong> PQ impact results used to validate final checks. <br><strong>DAX conceptual mapping:</strong> <code>FinalChecksFailRate</code>. <br><strong>Security/PII:</strong> final check logs avoid PII leakage. <br><strong>Operational notes:</strong> require operator signoff to proceed when checks produce warnings. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: ProduceMigrationScript(batchId, approvedMappings, operatorId)</strong><br><strong>Purpose & contract:</strong> build deterministic migration script (SQL/CSV) representing approved mappings, with canonical ordering and escaping rules; compute <code>artifactChecksum</code>, persist to <code>artifactRef</code>, and append <code>standard.map.export</code> audit. Scripts must be idempotent and include <code>applyId</code> and <code>correlationId</code> in metadata. <br><strong>Inputs & outputs:</strong> Inputs: <code>batchId</code>, <code>approvedMappings</code>, <code>operatorId</code>, options {format:<code>sql|csv</code>, includeDryRunStatements}. Outputs: <code>artifactRef</code>, <code>artifactChecksum</code>. <br><strong>Primary invariants:</strong><br>1. Migration script must be deterministic; canonical ordering key specified (e.g., AccountId asc). <br>2. Include safe-guards like <code>BEGIN TRANSACTION</code>/<code>ROLLBACK</code> comments as recommended but leave actual execution to DB team unless inline apply permitted. <br><strong>Provenance & usage:</strong> artifact used by DB team to update ledger or consumed by automated deploy pipeline. <br><strong>Failure modes & recovery:</strong> file persist fail -> stage locally and notify operator. <br><strong>Observability & audit:</strong> <code>migration.artifact.generated</code> with checksum. <br><strong>Tests:</strong> script syntax check and idempotency test for multiple generation runs. <br><strong>Conceptual PQ mapping:</strong> PQ may export bulk changes for large-scale migrations. <br><strong>DAX conceptual mapping:</strong> <code>MigrationArtifactsByOperator</code>. <br><strong>Security/PII:</strong> migration scripts should not contain PII unless necessary; if necessary, encrypt artifact. <br><strong>Operational notes:</strong> ensure DB change approvals and rollback plan documented in migration manifest. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: PublishBatchNotifications(batchId, channels, payload)</strong><br><strong>Purpose & contract:</strong> send out operator notifications to specified channels (email, slack, internal paging) summarizing batch outcome and providing links to <code>reportRef</code>, <code>forensicPack</code> if needed, and <code>correlationId</code>. Notifications must be PII-free and include triage links for operators. <br><strong>Inputs & outputs:</strong> Inputs: <code>batchId</code>, <code>channels</code>, <code>payload</code> {summaryRef, correlationId, severity}. Outputs: <code>notifyReceipt</code> per channel. <br><strong>Primary invariants:</strong><br>1. Messages redacted for PII by default; include <code>evidenceRef</code> only for compliance roles. <br>2. Notifications include <code>correlationId</code> for triage. <br><strong>Provenance & usage:</strong> operator workflows and incident response. <br><strong>Failure modes & recovery:</strong> notify failures -> retry and escalate. <br><strong>Observability & audit:</strong> <code>notifications.sent</code> with channel success rates. <br><strong>Tests:</strong> simulate notification channel failures. <br><strong>Conceptual PQ mapping:</strong> PQ events may also publish notifications. <br><strong>DAX conceptual mapping:</strong> <code>NotificationsPerBatch</code>, <code>NotificationFailureRate</code>. <br><strong>Security/PII:</strong> ensure secure channels for any sensitive links. <br><strong>Operational notes:</strong> follow platform guidelines for rate limits and message sizes. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: GenerateBatchCanaryPlan(batchId, canaryConfig)</strong><br><strong>Purpose & contract:</strong> produce a deterministic canary rollout plan that applies mappings to a small cohort (subset of accounts or a cohort of entities) per <code>canaryConfig</code> (cohort size, monitoring KPIs, rollback thresholds) to monitor for regressions before full deployment. Return <code>canaryPlanRef</code>. <br><strong>Inputs & outputs:</strong> Inputs: <code>batchId</code>, <code>canaryConfig</code> {cohortSize, metrics, duration}. Outputs: <code>canaryPlanRef</code> and <code>canaryPlan</code>. <br><strong>Primary invariants:</strong><br>1. Canary plan deterministic and reversible; selection process seeded by <code>batchId</code> to ensure reproducibility. <br>2. Include automated monitoring rules that trigger partial/automatic rollback if thresholds exceeded. <br><strong>Provenance & usage:</strong> helps staged rollout for critical mapping semantics. <br><strong>Failure modes & recovery:</strong> failed canary triggers automatic rollbacks per canary plan. <br><strong>Observability & audit:</strong> <code>canary.plan.created</code> and <code>canary.outcome</code> events. <br><strong>Tests:</strong> run canary with simulated anomalies and confirm rollback mechanics. <br><strong>Conceptual PQ mapping:</strong> PQ-based canaries may be used for large data transformations. <br><strong>DAX conceptual mapping:</strong> <code>CanaryFailureRate</code>. <br><strong>Security/PII:</strong> cohor selection may not leak PII; selection based on entity IDs only. <br><strong>Operational notes:</strong> canary rollout must be approved in migration manifest when mapping semantics change. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Function: GenerateRollbackPlaybook(batchId, applyId)</strong><br><strong>Purpose & contract:</strong> generate an actionable rollback playbook that includes immediate revert commands, forensic-pack creation steps, communication templates, and required approvals. Persist <code>rollbackPlaybookRef</code> and link to <code>applyDescriptor</code>. <br><strong>Inputs & outputs:</strong> Inputs: <code>batchId</code>, <code>applyId</code>. Outputs: <code>rollbackPlaybookRef</code>. <br><strong>Primary invariants:</strong><br>1. Playbook deterministic and includes exact <code>revertId</code> commands derived from <code>ApplyDescriptor</code>. <br>2. Must include safe-mode steps for partial reverts and forensic evidence collection steps. <br><strong>Provenance & usage:</strong> used by operators for emergency rollback. <br><strong>Failure modes & recovery:</strong> if <code>ApplyDescriptor</code> missing revert info -> fail with <code>STD_REVERT_NO_SNAPSHOT</code> and escalate. <br><strong>Observability & audit:</strong> <code>rollback.playbook.created</code> event. <br><strong>Tests:</strong> generate and validate playbook steps using applyDescriptor test fixtures. <br><strong>Conceptual PQ mapping:</strong> PQ artifacts used to guide rollback for data transformations. <br><strong>DAX conceptual mapping:</strong> <code>RollbackPlaybooksGenerated</code>. <br><strong>Security/PII:</strong> playbook may refer to evidenceRef but not embed PII. <br><strong>Operational notes:</strong> ensure playbook accessible under emergency procedures and requires multi-approval on use. </td></tr><tr><td data-label="modBatchProcessing — Per-function Expert Technical Breakdown"> <strong>Cross-function Integration & Governance Notes (modBatchProcessing)</strong><br><strong>A. Determinism and Parity:</strong><br>1. All hashing (snapshotHash, paramsHash, scoreHash, payloadHash, artifactChecksum) must use canonicalization rules shared with PQ and tested in CI. <br>2. Any change to canonicalization or hash algorithms must be recorded in a migration manifest and enforced by CI golden parity tests. <br><strong>B. Observability & Audits:</strong><br>1. Each public function that changes state (LoadCandidateSnapshot side-effects aside) must emit an audit row with <code>correlationId</code>. <br>2. Telemetry must capture latencies (batch, chunk), error rates, and key SLO counters. <br><strong>C. PII & Evidence:</strong><br>1. No raw PII in audit rows or telemetry. Evidence store used for full artifacts; evidenceRefs referenced only in audits. <br>2. Evidence encryption and RBAC mandatory for regulated datasets. <br><strong>D. Failure & Recovery:</strong><br>1. Read-then-validate-then-swap pattern mandated to avoid partial commits. <br>2. Partial failure handling and checkpointing must be robust and deterministic. <br><strong>E. Governance & Approvals:</strong><br>1. Destructive or material mappings require recorded approvals and two-person sign-off enforced in <code>ValidateApprovalsBeforeApply</code>. <br>2. <code>paramsHash</code> changes are governance events requiring migration manifest and CI approvals. <br><strong>F. CI & Tests:</strong><br>1. Every pure function must have unit tests; batch flows have integration tests; CI golden parity runs compare PQ and VBA outputs using <code>ScoreHashParityCheck</code>. <br>2. Stress tests (StressTestRunner) and smoke-tests mandatory for hot-swap flows. <br><strong>G. Performance & Scaling:</strong><br>1. For volumes beyond ~50k CandidateMap rows, offload heavy precompute to PQ or dedicated worker pool. <br>2. Provide chunking and job scheduling with checkpointing to avoid OOMs and improve robustness. <br><strong>H. Security & RBAC:</strong><br>1. Approvals, apply actions and evidence retrieval gated by RBAC enforced by <code>modSecurity</code>. <br>2. No long-lived credentials stored; ephemeral tokens used for evidence uploads/signing. <br><strong>I. Operational Runbook (concise):</strong><br>1. Preflight: run <code>BatchDryRun</code> + <code>ScoreHashParityCheck</code> against golden fixtures. <br>2. Run: <code>ScoreBatchOrchestrator</code> in create_copy default. <br>3. Validate: <code>GenerateBatchReportForOperator</code> and <code>SanityChecks</code>. <br>4. Apply: <code>PersistMigrationArtifact</code> and <code>ApplyBatchIfRequested</code> with approvals. <br>5. Monitor: SLOs, <code>DriftAlert</code>, and <code>Reconcile</code> outputs. <br>6. Revert: <code>RevertBatchApply</code> using <code>ApplyDescriptor</code> if required. <br><strong>Final verification statement:</strong> I performed 10 passes over this module-level breakdown to ensure completeness and cross-function consistency with PQ parity requirements, audit chain requirements, evidence handling constraints, and operational runbook coverage. This design balances determinism, safety, PII controls, and operational pragmatism appropriate for production-grade GL canonicalisation and ISAK 335 disclosure workflows. </td></tr></tbody></table></div><div class="row-count">Rows: 62</div></div><script src="assets/xlsx.full.min.js?v=1758605028" defer></script>
<script src="assets/script.js?v=1759748863" defer></script>
<script src="assets/worker.js?v=1758331710" defer></script>
<script>
(function(){
  const template = "{table}_{date}";
  const userVal = "";
  const hrefPrefix = "assets";
  function formatName(tableName) {
    const date = (new Date()).toISOString().slice(0,10);
    return template.replace('{table}', tableName).replace('{date}', date).replace('{user}', userVal);
  }
  const btn = document.getElementById('exportBtn');
  if(btn) {
    btn.addEventListener('click', async function() {
      try {
        const html = document.documentElement.outerHTML;
        if(html.length > 2000000) { alert('Export refused: html too large'); return; }
        if(window.Worker) {
          const workerUrl = (function(){ try{ return hrefPrefix + '/worker.js'; }catch(e){ return null; } })();
          if(workerUrl) {
            try {
              const worker = new Worker(workerUrl);
              worker.postMessage({html: html, format: 'pdf'});
              worker.onmessage = function(e) { console.log('worker:', e.data); alert('Worker replied: '+(e.data.msg||e.data.status)); };
            } catch(err) { console.warn('Worker create failed', err); alert('Worker not available'); }
          } else { alert('Worker not available'); }
        } else { alert('Export worker not supported in this environment.'); }
      } catch(err) { console.warn('Export failed', err); alert('Export worker not available. See console for details.'); }
    });
  }
})();
window.addEventListener('load', function(){ try{ document.querySelectorAll('.table-wrapper').forEach(function(e){ e.style.opacity='1'; }); }catch(e){} });
</script>
</div>
</body>
</html>