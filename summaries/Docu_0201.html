<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='utf-8'><meta name='viewport' content='width=device-width,initial-scale=1'>
<title>Tables Viewer v2.1</title>
<style>:root{--main-header-height:56px;} .table-wrapper{opacity:0;min-height:24px;transition:opacity .12s linear}</style>
<link rel="stylesheet" href="assets/style.css?v=1769960840">
<link rel="stylesheet" href="assets/overrides.css?v=1771316627">
</head><body>
<div id="tables-viewer" role="region" aria-label="Tables viewer">
<div id="stickyMainHeader">
<div id="tv-header">
<div><h1>Tables Viewer v2.1</h1></div>
<div style="display:flex;gap:8px;align-items:center;">
<input id="searchBox" class="tv-search" type="search" placeholder="Search" aria-label="Search tables" />
<button id="modeBtn" type="button" onclick="toggleMode()" aria-label="Toggle theme">Theme</button>
<button id="toggleAllBtn" type="button" aria-label="Toggle all" onclick="toggleAllTables()">Collapse All Tables</button>
<button id="copyAllPlainBtn" class="copy-btn" type="button" onclick="copyAllTablesPlain()" aria-label="Copy all tables as plain text">Copy All Tables (Plain Text)</button>
<button id="copyAllTablesBtn" class="copy-all-btn" type="button" onclick="copyAllTablesMarkdown()" aria-label="Copy All Tables (Markdown)">Copy All Tables (Markdown)</button>
<button id="copyAllMdBtn" style="display:none" aria-hidden="true"></button>
<button id="resetAllBtn" type="button" onclick="resetAllTables()" aria-label="Reset All Tables">Reset All Tables</button>
</div></div>
<script>
(function(){
  function ensureDelegation(){
    try {
      var vis = document.getElementById('copyAllTablesBtn');
      var alias = document.getElementById('copyAllMdBtn');
      if(!vis || !alias) return;
      alias.addEventListener = function(type, listener, options){
        vis.addEventListener(type, listener, options);
      };
      alias.removeEventListener = function(type, listener, options){
        vis.removeEventListener(type, listener, options);
      };
      Object.defineProperty(alias, 'onclick', {
        set: function(fn){ vis.onclick = fn; },
        get: function(){ return vis.onclick; },
        configurable: true
      });
      alias.focus = function(){ vis.focus(); };
      alias.blur = function(){ vis.blur(); };
    } catch(e) {
      try{ console && console.warn && console.warn('alias delegation failed', e); }catch(_){} 
    }
  }
  if(document.readyState === 'loading'){
    document.addEventListener('DOMContentLoaded', ensureDelegation);
  } else {
    ensureDelegation();
  }
})();
</script>
<noscript><div style='color:#b91c1c'>JavaScript is disabled. Tables will be shown statically. For large tables enable JS for virtualization.</div></noscript>
<div id="tocBar" role="navigation" aria-label="Table of contents"><ul><li class="toc-item"><a class="toc-link" href="#Table1">Table 1</a></li>
<li class="toc-item"><a class="toc-link" href="#Table2">Table 2</a></li>
<li class="toc-item"><a class="toc-link" href="#Table3">Table 3</a></li>
<li class="toc-item"><a class="toc-link" href="#Table4">Table 4</a></li></ul></div></div>
<div class="table-caption" id="Table1" data-table="Docu_0201_01" style="margin-top:2mm;margin-left:3mm;"><strong>Table 1</strong></div>
<div class="table-wrapper" data-table-id="table-1"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by **modMisc — Per-function Expert Technical Breakdown**"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>modMisc — Per-function Expert Technical Breakdown</strong></div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Verification statement:</strong> Reviewed <strong>ten times</strong> for internal consistency, determinism, PQ parity, audit traceability, secure PII handling, testability, and operational recovery prior to publishing. The entries below describe every recommended exported/internal function for a production-grade <code>modMisc</code> module in a GL-account canonicaliser add-in. Each function entry includes: Purpose & contract, Inputs & outputs, Primary invariants, Provenance & usage, Failure modes & recovery, Observability & audit obligations, Performance expectations, Test vectors & examples, Conceptual Power Query (PQ) mapping, Conceptual DAX measures, Security & PII considerations, and Operational notes. Numbered lists use <code>&lt;br&gt;</code> line breaks per requirement. No code snippets are included. This is a single-column table; each cell below documents a single function. The descriptions are deliberately exhaustive to support implementation, CI, governance, and operator runbooks. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: InitializeMisc() — bootstrap & safe init</strong><br><strong>Purpose & contract:</strong> Initialize module-level caches, validate presence and schema of required helper sheets (<code>Config</code>, <code>MappingTable</code>, <code>CandidateMap</code>, <code>MappingHistory</code>, <code>DebugLog</code>), compute <code>configHash</code>, and return a deterministic status object. MUST be idempotent and safe to call on every workbook open or admin refresh. It SHOULD NOT perform heavy IO, network calls, or long-running tasks on UI thread. <br><strong>Inputs & outputs:</strong> No external inputs. Output: <code>{ok:Boolean, diagnostics:Array, configHash:String|null, mode:String}</code> where <code>mode</code> indicates <code>normal|safe_readonly|degraded</code>. <br><strong>Primary invariants:</strong><br>1. Deterministic <code>configHash</code> computed using canonical serialization rules (stable key ordering, fixed float format) so subsequent runs or PQ processes can verify parity. <br>2. No destructive mutations of user data: any sheet creation must be done only when explicitly allowed by admin flag. <br><strong>Provenance & usage:</strong> Bootstrapping used by <code>OnOpen</code>, admin refresh flows, CI smoke tests. <code>InitializeMisc</code> anchors many other flows by publishing <code>configHash</code> and <code>mode</code>. <br><strong>Failure modes & recovery:</strong> Missing or corrupt <code>Config</code> => return <code>ok=false</code>, <code>mode=degraded</code>; suggest fallback to read-only diagnostics; persist diagnostic evidenceRef for operator review. Corrupted named ranges produce specific diagnostics. <br><strong>Observability & audit:</strong> Emit audit row <code>misc.init{ok,configHash,mode,diagnostics}</code> with short diagnostic list and <code>evidenceRef</code> for extended diagnostics. <br><strong>Performance:</strong> Constant time for small configs; if config contains large lists (stopwords, punctuation), compute hash incrementally. <br><strong>Test vectors & examples:</strong> Initialize with valid config; initialize with absent Config sheet (should produce degraded mode); initialize with malformed JSON in named range (diagnostic produced). <br><strong>PQ conceptual mapping:</strong> PQ must be able to compute identical <code>configHash</code>; include PQ smoke <code>CheckConfigHash</code> to verify parity. <br><strong>DAX conceptual mapping:</strong> <code>ConfigLoadSuccessRate = DIVIDE(SUMX(Events, IF(Event=&quot;misc.init&quot; &amp;&amp; ok,1,0)), COUNTROWS(Events))</code>. <br><strong>Security & PII:</strong> Do not include raw PII in <code>diagnostics</code> for audits; store detailed traces in encrypted evidence store referenced by <code>evidenceRef</code>. <br><strong>Operational notes:</strong> Must be the first method run in any automated pipeline; operators should be able to run <code>InitializeMisc</code> interactively before large jobs. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: EnsureWorksheet(wsName, createIfMissing:Boolean, templateName:<code>String|null</code>)</strong><br><strong>Purpose & contract:</strong> Guarantee a worksheet exists with given name. If missing and <code>createIfMissing=true</code>, create it from <code>templateName</code> if provided else create a minimal safe sheet. Must not overwrite existing sheets. <br><strong>Inputs & outputs:</strong> Inputs: <code>wsName</code>, <code>createIfMissing</code>, <code>templateName</code>. Output: <code>{worksheetRef, created:Boolean, errorCode:String|null}</code>. <br><strong>Primary invariants:</strong><br>1. If creation occurs, annotate sheet with metadata row <code>__meta__</code> containing creator, timestamp (ISO8601), and rationale. <br>2. Creation must obey workbook protection and IT policies; if workbook is protected, return a specific error. <br><strong>Provenance & usage:</strong> Used by import/export, staging, and audit flows that require deterministic sheet names. <br><strong>Failure modes & recovery:</strong> Protection prevents creation => return <code>MISC_SHEET_PROTECTED</code>; operator must unprotect or use alternate workbook. Template missing => create basic sheet and emit <code>misc.template.missing</code> audit. <br><strong>Observability & audit:</strong> Emit <code>misc.sheet.ensure{wsName,created,errorCode}</code>; include configHash to correlate with bootstrap. <br><strong>Performance:</strong> Trivial. <br><strong>Test vectors & examples:</strong> Ensure existing sheet returns existing ref; ensure missing sheet with template creates correct header rows and meta. <br><strong>PQ conceptual mapping:</strong> PQ queries expect stable sheet names; EnsureWorksheet aligns with PQ target naming. <br><strong>DAX conceptual mapping:</strong> Not applicable. <br><strong>Security & PII:</strong> Template contents must not autonomously include PII; created helper sheets default to hidden and protected if they touch evidence pointers. <br><strong>Operational notes:</strong> Creation flagged in audit and visible to compliance for review. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: SafeReadRange(wsName, address, options)</strong><br><strong>Purpose & contract:</strong> Robustly read a rectangular range returning canonical typed cell payloads with diagnostics. Should handle merged cells, formulas, locale-specific date formats, and provide <code>snapshotHash</code> for read-then-write validation. Must not throw unhandled exceptions; always return structured status. <br><strong>Inputs & outputs:</strong> Inputs: <code>wsName</code>, <code>address</code> (A1 or named), <code>options</code> {coerceTypes:Boolean, dateLocale:String, nullStrings:Array}. Output: <code>{rows:Array&lt;Array&gt;, diagnostics:Array, snapshotHash:String}</code>. <br><strong>Primary invariants:</strong><br>1. Deterministic ordering of rows/columns. <br>2. Canonical null treatment: consistent empty string or explicit Null token per call. <br>3. When <code>coerceTypes=true</code> apply canonical parse rules (SafeRound for numbers, NormalizeDateToUTC for dates). <br><strong>Provenance & usage:</strong> Core IO primitive used across batch processing, scoring and auditing. <br><strong>Failure modes & recovery:</strong> Range not found -> return empty rows and <code>range.missing</code> diagnostic; formula error -> return cell with <code>#ERR</code> token plus diagnostic. <br><strong>Observability & audit:</strong> Emit <code>misc.read{wsName,address,rowsRead,snapshotHash}</code> telemetry with no cell-level PII. <br><strong>Performance:</strong> Efficient array reads; heavy reads (>50k rows) should be chunked. <br><strong>Test vectors & examples:</strong> Merged cell expansion behavior; reading dates in various locale formats; formulas returning errors. <br><strong>PQ conceptual mapping:</strong> PQ should write stable typed cells so SafeReadRange coerce step is minimal. <br><strong>DAX conceptual mapping:</strong> Not applicable. <br><strong>Security & PII:</strong> When options include PII masking, redaction must be applied on the read unless evidenceRef requested. <br><strong>Operational notes:</strong> Use snapshotHash to detect concurrent modifications before writes. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: SafeWriteRange(wsName, topLeftAddress, dataArray, options)</strong><br><strong>Purpose & contract:</strong> Write a 2D array to worksheet with atomic-swap option, optional backup, and chunked writes to avoid UI blocking. MUST provide an atomic swap mechanism to prevent partial-write visible states. Return detailed write result with writtenRange and checksum. <br><strong>Inputs & outputs:</strong> Inputs: <code>wsName</code>, <code>topLeftAddress</code>, <code>dataArray</code>, <code>options</code> {atomicSwap:Boolean, createBackup:Boolean, chunkSize:Int}. Output: <code>{ok:Boolean, writtenRange:String, checksum:String, diagnostics:Array}</code>. <br><strong>Primary invariants:</strong><br>1. If <code>atomicSwap=true</code>, create temp sheet with new content and call <code>AtomicSwapSheets</code> to swap atomically, ensuring either old or new visible state. <br>2. Backup created with canonical serialization and <code>snapshotRef</code> when requested. <br><strong>Provenance & usage:</strong> Used to persist CandidateMap updates and migration artifacts. <br><strong>Failure modes & recovery:</strong> Mid-write failure -> if atomicSwap used, old content preserved and audit <code>misc.write.partial</code> appended with failure diagnostics. If no atomicSwap, attempt roll-back using last backup if available. <br><strong>Observability & audit:</strong> Emit <code>misc.write{wsName,rows,checksum,durationMs}</code> telemetry. <br><strong>Performance:</strong> Use block assignment to cells; chunk if <code>dataArray</code> length > threshold. <br><strong>Test vectors & examples:</strong> Atomic write simulation; chunked write flows and crash-resilience tests. <br><strong>PQ conceptual mapping:</strong> PQ processes should avoid writing to ranges while atomic swap in progress; coordinate via lock sheet. <br><strong>DAX conceptual mapping:</strong> Not applicable. <br><strong>Security & PII:</strong> Backups containing PII must be encrypted and stored as evidenceRef referenced in audit. <br><strong>Operational notes:</strong> Default to atomicSwap for any mapping-change write to avoid inconsistent UI state. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: AtomicSwapSheets(tempSheetName, targetSheetName)</strong><br><strong>Purpose & contract:</strong> Swap two sheets' content atomically while preserving the sheet name visible to users, named ranges, and formula integrity. Designed to present either old or new state to viewers but never a partially-updated sheet. <br><strong>Inputs & outputs:</strong> Inputs: <code>tempSheetName</code>, <code>targetSheetName</code>. Output: <code>{ok:Boolean, beforeChecksum:String, afterChecksum:String, errorCode:String|null}</code>. <br><strong>Primary invariants:</strong><br>1. Preserve named ranges and update named-range referents to the new swapped content if necessary. <br>2. Ensure external references to sheet name remain valid. <br><strong>Provenance & usage:</strong> Used by <code>SafeWriteRange</code> in atomic mode and preview apply flows. <br><strong>Failure modes & recovery:</strong> If named ranges conflict or workbook protection prevents swapping, return error and leave original untouched; record <code>misc.swap.failed</code> audit and provide remediation steps. <br><strong>Observability & audit:</strong> Emit <code>misc.swap{temp,target,beforeChecksum,afterChecksum,durationMs}</code>. <br><strong>Performance:</strong> Dependent on number of named ranges; update operations may be expensive for many ranges. <br><strong>Test vectors & examples:</strong> Verify formula continuity, named range stability, and cross-sheet references after swap. <br><strong>PQ conceptual mapping:</strong> PQ write consumers should be coordinated to not write to same sheet during swap operations. <br><strong>Security & PII:</strong> When swapping sheets containing PII, ensure that evidenceRef for full content is created before swap. <br><strong>Operational notes:</strong> Require admin audit when swap touches regulatory artifacts. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: CreateSnapshot(snapshotName, scope, options)</strong><br><strong>Purpose & contract:</strong> Create canonical snapshot of specified sheets/ranges for forensic and revert purposes. Snapshot must be canonicalized (UTF-8, stable ordering, fixed float precision) and stored as compressed, content-addressed artifact (sha256). Return <code>{snapshotRef, checksum, sizeBytes, createdTs}</code>. <br><strong>Inputs & outputs:</strong> Inputs: <code>snapshotName</code>, <code>scope</code> (list of wsName/range tuples), <code>options</code> {compress:Boolean, encrypt:Boolean}. Output: snapshot metadata. <br><strong>Primary invariants:</strong><br>1. Canonical serialization rules documented and used across PQ, VBA, and CI to compute identical checksums. <br>2. Snapshots immutable once created. <br><strong>Provenance & usage:</strong> Created pre-destructive apply, used by <code>RevertMapping</code>, and included in forensic packages. <br><strong>Failure modes & recovery:</strong> Evidence store unavailable -> persist locally with encrypted staging and schedule retry; do not proceed with destructive apply if snapshot cannot be created. <br><strong>Observability & audit:</strong> Emit <code>snapshot.created{snapshotRef,checksum,rows,operator}</code> with evidenceRef link to archive. <br><strong>Performance:</strong> Streaming serialization required for large scopes. <br><strong>Test vectors & examples:</strong> Snapshot before apply and later compute <code>beforeChecksum</code>/<code>afterChecksum</code> parity on revert. <br><strong>PQ conceptual mapping:</strong> PQ exports should be used to produce canonical CSV/JSON artifacts when snapshot scope includes query-constructed tables. <br><strong>DAX conceptual mapping:</strong> Not applicable. <br><strong>Security & PII:</strong> Snapshots with PII must be encrypted and protected by evidence store ACL; create <code>forensic_manifest</code> entry. <br><strong>Operational notes:</strong> Snapshot creation is mandatory for regulated outputs and must be enforced by <code>ValidateWorkbookState</code> preflight. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: ComputeSHA256OfRange(wsName, rangeAddress, options)</strong><br><strong>Purpose & contract:</strong> Compute SHA256 of canonical serialization of cells in a range. Canonicalization rules: stable row/column ordering, fixed float format, normalized newline, UTF-8, and consistent null representation. Return hex digest. <br><strong>Inputs & outputs:</strong> Inputs: <code>wsName</code>, <code>rangeAddress</code>, <code>options</code> {serializeFormulas:Boolean}. Output: <code>{sha256Hex, bytesLen}</code>. <br><strong>Primary invariants:</strong><br>1. Exclude volatile metadata (e.g., timestamps added by UI) to prevent hash drift. <br>2. For formula cells, default to hash on evaluated value; optional flag to include formula text. <br><strong>Provenance & usage:</strong> Used for <code>apply.beforeChecksum</code>, <code>apply.afterChecksum</code>, and snapshot parity checks. <br><strong>Failure modes & recovery:</strong> Serialization error -> return specific error code and small sample of problematic cells for operator triage; fallback to row-wise iterative hashing if full serialization fails. <br><strong>Observability & audit:</strong> log <code>misc.hash.duration_ms</code> and attach hash to audit entries. <br><strong>Performance:</strong> linear in number of bytes; caching recommended for frequently rehashed ranges. <br><strong>Test vectors & examples:</strong> Cross-runtime hash parity tests with PQ to ensure canonical serialization matches. <br><strong>PQ conceptual mapping:</strong> PQ must be able to produce canonical serialized bytes of the same range to reproduce hashes for cross-system compare. <br><strong>DAX conceptual mapping:</strong> Not applicable. <br><strong>Security & PII:</strong> Hash alone is non-reversible but can leak presence of identical PII content across runs — treat hash as sensitive metadata in telemetry. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: SaveWorkbookCopy(destinationPath, options)</strong><br><strong>Purpose & contract:</strong> Save workbook copy to <code>destinationPath</code> with options to strip PII, remove VBA projects, or flatten connections. Return <code>{artifactPath, checksum, note}</code> and ensure operation does not write credentials. <br><strong>Inputs & outputs:</strong> Inputs: <code>destinationPath</code>, <code>options</code> {stripPII:Boolean, removeVBA:Boolean, compress:Boolean}. Output: artifact metadata. <br><strong>Primary invariants:</strong><br>1. If <code>stripPII=true</code>, produce <code>forensic_manifest</code> listing removed fields and evidenceRef for full unstripped data stored securely. <br>2. Must not persist credentials or tokens in exported copy. <br><strong>Provenance & usage:</strong> Used for offline archiving, regulator packages, or hand-off to DB teams. <br><strong>Failure modes & recovery:</strong> filesystem permission issues -> stage local copy and notify operator; never overwrite an existing artifact unless <code>overwrite=true</code>. <br><strong>Observability & audit:</strong> write <code>misc.saveartifact{path,checksum,stripPII,operator}</code> audit row. <br><strong>Performance:</strong> I/O bound; consider compression for large workbooks. <br><strong>Test vectors & examples:</strong> Save with/without PII stripping to verify logistic. <br><strong>PQ conceptual mapping:</strong> PQ-generated artifacts must be included in <code>forensic_manifest</code> to ensure complete packaging. <br><strong>DAX conceptual mapping:</strong> Not applicable. <br><strong>Security & PII:</strong> Full copies containing PII must be encrypted and stored in WORM or evidence store per retention policy. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: ExportRangeToCSV(wsName, rangeAddress, filePath, options)</strong><br><strong>Purpose & contract:</strong> Deterministic CSV export with canonical float formatting and charset normalization (UTF-8), producing checksum and strict quoting rules. Return <code>{filePath, checksum, rowsExported}</code>. <br><strong>Inputs & outputs:</strong> Inputs: <code>wsName</code>, <code>rangeAddress</code>, <code>filePath</code>, <code>options</code> {delimiter, quoteChar, newlineStyle}. Output: export metadata. <br><strong>Primary invariants:</strong><br>1. Use fixed decimal formatting and rounding consistent with <code>SafeRound</code> for numeric cells. <br>2. Normalize newline style and escape rules in a canonical way to ensure identical bytes across runs. <br><strong>Provenance & usage:</strong> Used for migration scripts, evidence artifacts, and data interchange with ETL. <br><strong>Failure modes & recovery:</strong> permission denied -> fallback to workbook folder and append <code>misc.export.fallback</code> audit. <br><strong>Observability & audit:</strong> log <code>misc.export.csv{rows,checksum,durationMs}</code>. <br><strong>Performance:</strong> stream writing for large exports. <br><strong>Test vectors & examples:</strong> roundtrip import/export parity tests and cross-runtime hash parity. <br><strong>PQ conceptual mapping:</strong> PQ CSV exports must follow identical canonical rules for parity. <br><strong>DAX conceptual mapping:</strong> Not applicable. <br><strong>Security & PII:</strong> When exporting PII columns, warn operator and require signature for exporting. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: ImportCSVToSheet(filePath, wsName, topLeft, options)</strong><br><strong>Purpose & contract:</strong> Parse and import CSV to a sheet with robust parsing options: detect delimiter, coerce types, limit rows and optionally store raw CSV to secure evidence store returning evidenceRef. Must return structured diagnostics for any parse errors without halting the import unless <code>failOnError</code> true. <br><strong>Inputs & outputs:</strong> Inputs: <code>filePath</code>, <code>wsName</code>, <code>topLeft</code>, <code>options</code> {coerceTypes, maxRows, failOnError}. Output: <code>{rowsImported, diagnostics, evidenceRef}</code>. <br><strong>Primary invariants:</strong><br>1. Canonical parse and float format consistent with <code>SafeRound</code>. <br>2. Import truncation should be explicit with a diagnostic. <br><strong>Provenance & usage:</strong> Bulk alias ingestion, mapping updates from external teams. <br><strong>Failure modes & recovery:</strong> malformed CSV -> continue with <code>skipBadRows</code> or abort as per flag; if CSV originates from external, require signature and produce <code>import.untrusted</code> audit if signature missing. <br><strong>Observability & audit:</strong> <code>misc.import.csv{rows,errors,evidenceRef}</code>. <br><strong>Performance:</strong> streaming parse recommended. <br><strong>Tests & examples:</strong> test with quoted newlines, various encodings, large datasets. <br><strong>PQ conceptual mapping:</strong> For large imports prefer PQ ingestion into source database and then refresh CandidateMap. <br><strong>Security & PII:</strong> block imports containing PII without explicit approval and signature verification. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: CleanStringForFilename(inputString, options)</strong><br><strong>Purpose & contract:</strong> Produce filesystem-safe filename string by removing control characters, path separators, collapsing whitespace and truncating to a safe length; append a short hash suffix when truncation occurs to prevent collisions. Deterministic and safe for multi-platform exports. <br><strong>Inputs & outputs:</strong> Inputs: <code>inputString</code>, <code>options</code> {maxLen:Int}. Output: <code>safeFilename</code>. <br><strong>Primary invariants:</strong><br>1. Use deterministic hash suffix (e.g., first 8 hex of sha256) when truncation occurs. <br>2. Remove or replace characters invalid on Windows and Unix. <br><strong>Provenance & usage:</strong> artifact naming for migration scripts, snapshot files, forensic packages. <br><strong>Failure modes & recovery:</strong> extremely long or non-printable input -> replace with stable short hash + correlationId. <br><strong>Observability & audit:</strong> record sanitized input summary in artifact manifest if input contained PII. <br><strong>Tests & examples:</strong> file names with slashes, control chars, emojis, and very long names. <br><strong>PQ conceptual mapping:</strong> PQ scripts using file naming must call the same normalization rules to ensure artifact name parity. <br><strong>DAX conceptual mapping:</strong> Not applicable. <br><strong>Security & PII:</strong> Never embed raw PII in filenames; when cleaning a PII-containing suggested name, append <code>-redacted</code> tag and record <code>sanitizationSummary</code>. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: NormalizeDateToUTC(value, inputLocale, options)</strong><br><strong>Purpose & contract:</strong> Parse and canonicalize dates to UTC ISO8601 using deterministic precedence (ISO > locale-specified > heuristics). When ambiguous, produce <code>parseConfidence</code> and <code>parseRationale</code> codes for audit and do not silently guess for high-sensitivity fields. <br><strong>Inputs & outputs:</strong> Inputs: <code>value</code> (string/date), <code>inputLocale</code>, <code>options</code> {assumeCentury:Boolean}. Output: <code>{isoUtcString, parseConfidence, parseRationaleCode}</code>. <br><strong>Primary invariants:</strong><br>1. Tie-breaking rules must be deterministic and recorded in <code>parseRationaleCode</code>. <br>2. For ambiguous dates where <code>parseConfidence</code> is low, return a flagged placeholder and include evidenceRef for operator to inspect. <br><strong>Provenance & usage:</strong> Parse posting dates, effective dates, and artifact timestamps. <br><strong>Failure modes & recovery:</strong> ambiguous parse -> <code>parseConfidence=low</code> and audit <code>STD_AMBIG_DATE</code> produced. <br><strong>Observability & audit:</strong> record count of ambiguous parses and sample rows persisted to evidenceRef. <br><strong>Tests & examples:</strong> various ambiguous strings like <code>01/02/03</code> under different locales; DST boundaries. <br><strong>PQ conceptual mapping:</strong> PQ and VBA must use identical rules; include <code>parseRationaleCode</code> in PQ output for parity. <br><strong>DAX conceptual mapping:</strong> <code>ParseConfidenceAvg</code> and <code>AmbiguousDateCount</code> used for monitoring. <br><strong>Security & PII:</strong> Do not reveal DOBs without evidenceRef and compliance approval. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: FormatISO8601(dtValue, includeMs)</strong><br><strong>Purpose & contract:</strong> Return canonical ISO8601 UTC timestamp <code>YYYY-MM-DDTHH:MM:SS(.mmm)Z</code> used across audits for consistent sorting and hashing. <br><strong>Inputs & outputs:</strong> <code>dtValue</code>, <code>includeMs</code> boolean. Output: formatted string. <br><strong>Primary invariants:</strong> Always output in UTC; fixed width fields; zero-padded. <br><strong>Provenance & usage:</strong> audit timestamps, artifact metadata. <br><strong>Failure modes:</strong> invalid input -> return <code>0001-01-01T00:00:00Z</code> with diagnostic. <br><strong>Observability:</strong> not applicable. <br><strong>Tests:</strong> timezone conversion and DST transitions. <br><strong>PQ conceptual mapping:</strong> PQ timestamp outputs must match identical string format to prevent hash mismatches. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: GenerateGUID(<code>testSeed|null</code>)</strong><br><strong>Purpose & contract:</strong> Produce a GUID for use as <code>AuditId</code>, <code>ApplyId</code>, <code>snapshotId</code>. Support optional <code>testSeed</code> only for deterministic CI tests; in production <code>testSeed</code> must be null. <br><strong>Inputs & outputs:</strong> Optional <code>testSeed</code>. Output: GUID string. <br><strong>Primary invariants:</strong><br>1. Production GUIDs non-deterministic; deterministic seed allowed only in controlled CI or unit test environments. <br><strong>Provenance & usage:</strong> audits, job descriptors, snapshot ids. <br><strong>Failure modes & recovery:</strong> RNG unavailable -> use timestamp+counter fallback and log <code>misc.guid.fallback</code>. <br><strong>Observability & audit:</strong> log fallback occurrences for security review. <br><strong>Tests:</strong> GUID uniqueness tests. <br><strong>Security:</strong> GUIDs considered non-sensitive but referenced in PII evidence; access controlled accordingly. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: HumanReadableBytes(bytes, options)</strong><br><strong>Purpose & contract:</strong> Convert byte counts to human-friendly strings with deterministic rounding and base choice (1000 vs 1024) from config. Return formatted string and unit. <br><strong>Inputs & outputs:</strong> input bytes, options {base:<code>1000|1024</code>}. Output: <code>{display:String, value:Float, unit:String}</code>. <br><strong>Primary invariants:</strong> canonical one-decimal rounding by default; rounding mode recorded in <code>configHash</code>. <br><strong>Provenance & usage:</strong> UI diagnostics, telemetry. <br><strong>Failure modes:</strong> negative input -> return <code>0B</code>. <br><strong>Tests:</strong> boundary conditions near unit transitions. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: DescribeError(errObj, correlationId, redactPII:Boolean)</strong><br><strong>Purpose & contract:</strong> Produce a short user-safe message (<=160 chars) that includes <code>correlationId</code> and triage hint, and persist full diagnostics to evidence store returning <code>evidenceRef</code>. Must never include PII in the user message. <br><strong>Inputs & outputs:</strong> Inputs: <code>errObj</code>, <code>correlationId</code>, <code>redactPII</code>. Output: <code>{userMessage, diagCode, evidenceRef}</code>. <br><strong>Primary invariants:</strong><br>1. Full trace saved in evidence store encrypted; <code>evidenceRef</code> included in audit row. <br>2. User message contains only triage hints, not stack traces. <br><strong>Provenance & usage:</strong> All top-level catches, UI error messages, and operation failures. <br><strong>Failure modes & recovery:</strong> evidenceRef write fails -> return userMessage with correlationId and escalate to operator. <br><strong>Observability:</strong> <code>error.serialized</code> events. <br><strong>Tests:</strong> verify redaction of embedded PII in errors. <br><strong>PQ conceptual mapping:</strong> PQ engine error wrappers should call same serializer to produce parity diagnostics. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: RetryWithBackoff(funcRef, options)</strong><br><strong>Purpose & contract:</strong> Execute <code>funcRef</code> with retry semantics (exponential backoff with jitter). Caller must ensure <code>funcRef</code> is idempotent or context is safe for retries. Return <code>{ok, attempts, lastError}</code>. <br><strong>Inputs & outputs:</strong> Inputs: <code>funcRef</code>, <code>options</code> {maxAttempts, baseMs, maxMs}. Output: retry status. <br><strong>Primary invariants:</strong><br>1. Jitter implemented to avoid synchronized retries. <br>2. All retries logged with attempt count and last error code. <br><strong>Provenance & usage:</strong> evidence store writes, job queue persistence, network uploads. <br><strong>Failure modes & recovery:</strong> permanent failure -> return last error and append <code>misc.retry.failed</code> audit. <br><strong>Observability:</strong> <code>retry.attempts</code> metric and <code>retry.success_rate</code>. <br><strong>Tests:</strong> simulate transient failure and ensure eventual success with backoff. <br><strong>Security & PII:</strong> avoid logging sensitive payloads during retries. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: EnsureFolder(path, options)</strong><br><strong>Purpose & contract:</strong> Create folder with secure permissions if missing; ensure path is writable. Return <code>{ok:Boolean, path, errorCode}</code>. <br><strong>Inputs & outputs:</strong> path, options {secure:Boolean}. Output: status. <br><strong>Primary invariants:</strong><br>1. When <code>secure=true</code> create with least-privilege ACLs; record <code>folder.created</code> audit. <br><strong>Provenance & usage:</strong> staging artifacts, temp files. <br><strong>Failure modes & recovery:</strong> permission denied -> return <code>path.permission_denied</code>. <br><strong>Observability:</strong> <code>misc.fs.ensure</code> events. <br><strong>Tests:</strong> permission scenarios across OSes. <br><strong>Security & PII:</strong> ensure evidence directories cannot be read by other users. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: LockWorkbookForWrite(lockId, ttl, ownerId)</strong><br><strong>Purpose & contract:</strong> Cooperative workbook-level lock primitive. Record <code>acquiredTs</code>, <code>ownerId</code>, <code>expiresAt</code> and provide <code>acquired</code> boolean. Support <code>steal</code> by admin override with explicit audit. <br><strong>Inputs & outputs:</strong> <code>lockId</code>, <code>ttlSeconds</code>, <code>ownerId</code>. Output: <code>{acquired, owner, expiresAt, errorCode}</code>. <br><strong>Primary invariants:</strong><br>1. Locks are cooperative and recorded in hidden sheet; not enforced outside of cooperating clients. <br>2. TTL must be honored and stale locks reclaimable. <br><strong>Provenance & usage:</strong> long-running operations like apply, hot-swap, batch compute. <br><strong>Failure modes & recovery:</strong> process crash leaves stale locks -> reclaimable after TTL or with operator override and <code>lock.steal</code> audit entry. <br><strong>Observability:</strong> <code>lock.acquired</code> and <code>lock.steal</code> audits. <br><strong>Tests:</strong> multiple concurrent acquisitions and forced steals. <br><strong>Security:</strong> lock metadata not sensitive but lock steals should trigger SRE notification for governance. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: MaskPIIForUI(value, rule, operatorRole)</strong><br><strong>Purpose & contract:</strong> Redact PII for UI display using configurable patterns (emails, account numbers, names). Provide partial reveal per <code>rule</code> and <code>operatorRole</code> (compliance roles may see more). Return <code>{maskedValue, maskApplied, maskPattern}</code>. <br><strong>Inputs & outputs:</strong> Inputs: <code>value</code>, <code>rule</code> (show first N / last N), <code>operatorRole</code>. Output: masked string. <br><strong>Primary invariants:</strong><br>1. No residual substrings that could reassemble full PII from combination of multiple UI messages. <br>2. Full unredacted value only retrievable via <code>evidenceRef</code> workflow with approvals. <br><strong>Provenance & usage:</strong> Reviewer UI, diagnostics, telemetry samples. <br><strong>Failure modes & recovery:</strong> unknown format -> full mask and attach <code>mask.unsupported</code> diagnostic. <br><strong>Observability:</strong> <code>mask.requests</code> and counts by field type. <br><strong>Tests:</strong> emails, IBAN, account numbers, names in multiple scripts. <br><strong>PQ conceptual mapping:</strong> PQ previews should precompute redacted previews to minimize VBA risk of accidental PII exposure. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: PromptForApproval(operatorId, approvalsRequired, context)</strong><br><strong>Purpose & contract:</strong> Orchestrate approval request recording. Optionally block until approvals or produce <code>requestId</code> for asynchronous workflows. Must enforce two-person rules for material operations. Record <code>ticketId</code> and require MFA if configured. <br><strong>Inputs & outputs:</strong> operatorId, approvalsRequired (roles), context (summary). Output: <code>{requestId, status, approvalsNeeded}</code>. <br><strong>Primary invariants:</strong><br>1. Approval events appended immutably to <code>Approvals</code> sheet with timestamps and operator ids. <br>2. Two-person approvals require distinct operator ids. <br><strong>Provenance & usage:</strong> required before destructive inline applies, hot-swap, and regulated exports. <br><strong>Failure modes & recovery:</strong> approval service offline -> persist request locally and hold operation; do not proceed without approvals. <br><strong>Observability:</strong> <code>approval.requested</code> and <code>approval.granted</code> audits. <br><strong>Tests:</strong> two-person approval path, approval revoke scenarios. <br><strong>Security & PII:</strong> Approval contexts must be redacted in public logs; full context in evidenceRef requiring compliance access. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: AppendLocalLog(message, level, metadata, sensitiveFlag)</strong><br><strong>Purpose & contract:</strong> Append structured diagnostic row to <code>DebugLog</code> with TTL and retention rules. If <code>sensitiveFlag=true</code>, ensure row stored encrypted or only a hashed pointer kept in sheet and full content stored as evidenceRef. <br><strong>Inputs & outputs:</strong> Inputs: <code>message</code>, <code>level</code>, <code>metadata</code>, <code>sensitiveFlag</code>. Output: <code>{rowId, ts}</code>. <br><strong>Primary invariants:</strong><br>1. Verbose diagnostics must be gated by <code>DiagnosticsToggle</code> to enable. <br>2. Logs older than TTL auto-archived to evidenceRef and removed from sheet. <br><strong>Provenance & usage:</strong> troubleshooting and operator diagnostics. <br><strong>Failure modes & recovery:</strong> unable to write -> rotate to local encrypted file and note <code>misc.log.rotate</code> audit. <br><strong>Observability:</strong> <code>debuglog.size</code> and <code>debuglog.rotate</code> metrics. <br><strong>Tests:</strong> TTL enforcement, sensitive flag encryption tests. <br><strong>Security & PII:</strong> PII must not be present in sheet rows; evidenceRef only for full content. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: ReadConfigValue(key, default)</strong><br><strong>Purpose & contract:</strong> Typed, validated config access from <code>Config</code> sheet. Validate types, ranges and return value or default with <code>source</code> indicator. <br><strong>Inputs & outputs:</strong> Inputs: <code>key</code>, <code>default</code>. Output: <code>{value, from, validated, errorMsg}</code>. <br><strong>Primary invariants:</strong><br>1. Schema-driven validation enforced for known keys. <br>2. Missing critical keys trigger <code>InitializeMisc</code> degraded mode. <br><strong>Provenance & usage:</strong> All functions use <code>ReadConfigValue</code> to avoid scattered config parsing. <br><strong>Failure modes & recovery:</strong> invalid type -> return default and emit <code>config.invalid</code> audit. <br><strong>Observability:</strong> <code>config.read.count</code> and <code>config.defaultsUsed</code> metrics. <br><strong>Tests:</strong> invalid type, boundary values. <br><strong>PQ conceptual mapping:</strong> PQ should export <code>Config</code> snapshot used for CI parity checks. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: GetCorrelationId(contextHint)</strong><br><strong>Purpose & contract:</strong> Build short triage-friendly correlation id for UI and audit messages, e.g., <code>r-YYYYMMDD-xxxx</code>. MUST be included in every user-visible error and audit row. <br><strong>Inputs & outputs:</strong> Input: <code>contextHint</code> optional. Output: <code>correlationId</code>. <br><strong>Primary invariants:</strong><br>1. Include date component and suffix random or deterministic per-run seed for test reproducibility. <br><strong>Provenance & usage:</strong> Error messages, apply descriptors, logs. <br><strong>Failure modes:</strong> RNG unavailability -> fallback to GUID-based correlation id. <br><strong>Observability:</strong> <code>correlationId</code> used to collect audit shards. <br><strong>Tests:</strong> uniqueness at scale. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: BackUpWorkbookBeforeDestructiveOp(reason, operatorId, options)</strong><br><strong>Purpose & contract:</strong> Orchestrate creation of pre-destructive backup: create <code>CreateSnapshot</code> of critical sheets, optionally SaveWorkbookCopy, compute checksums, record <code>ApplyDescriptor</code> preflight with <code>beforeChecksum</code>. Do not proceed to destructive apply if backup fails. <br><strong>Inputs & outputs:</strong> Inputs: <code>reason</code>, <code>operatorId</code>, <code>options</code> {encryptBackup:Boolean}. Output: <code>{backupRef, snapshotRef, beforeChecksum, ok}</code>. <br><strong>Primary invariants:</strong><br>1. Production destructive operations require full backup; backup success precondition enforced by <code>ValidateWorkbookState</code>. <br><strong>Failure modes & recovery:</strong> backup failure -> abort apply and emit <code>STD_BACKUP_FAILED</code>. <br><strong>Observability:</strong> <code>misc.backup.created</code> audit entry and evidenceRef. <br><strong>Tests:</strong> simulate backup failure and verify apply blocked. <br><strong>Security & PII:</strong> backup with PII must be encrypted and access logged. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: ValidateWorkbookState(preconditions)</strong><br><strong>Purpose & contract:</strong> Run a suite of checks before critical operations: workbook not shared, no active PQ refreshes, Config valid, backups available, locks absent. Return structured <code>preflightReport</code> indicating <code>blocking</code> issues and remediation steps. <br><strong>Inputs & outputs:</strong> Inputs: <code>preconditions</code> list. Output: <code>{ok:Boolean, blockingIssues:Array, warnings:Array, remediationSteps:Array}</code>. <br><strong>Primary invariants:</strong><br>1. Blocking issues must prevent destructive or hot-swap operations. <br><strong>Provenance & usage:</strong> Preflight before Apply, HotSwap, Migration exports. <br><strong>Failure modes & recovery:</strong> if preflight fails, operation aborted with clear remediation suggested and correlationId provided. <br><strong>Observability:</strong> append <code>misc.preflight</code> audit with results. <br><strong>Tests:</strong> locked workbook, missing backup, concurrent job in progress. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: TempFileWithAutoCleanup(prefix, ttl)</strong><br><strong>Purpose & contract:</strong> Create a secure temp file and register for auto-cleanup (on Shutdown or scheduled GC). Return <code>{path, cleanupId}</code>. TTL enforced and auto-cleaned. <br><strong>Inputs & outputs:</strong> Inputs: <code>prefix</code>, <code>ttl</code>. Output: path and id. <br><strong>Primary invariants:</strong><br>1. Files with PII flagged and encrypted; TTL policy strictly enforced. <br><strong>Provenance & usage:</strong> chunked export staging, temporary evidence assembly. <br><strong>Failure modes & recovery:</strong> failure to create temp -> fallback to secure folder with audit. <br><strong>Observability:</strong> <code>tmpfile.created</code> and cleanup metrics. <br><strong>Tests:</strong> TTL expiry and cleanup. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: EnumerateOpenConnections()</strong><br><strong>Purpose & contract:</strong> Return a sanitized list of external connections the workbook holds (PowerQuery, ODBC, links) with statuses. Mask secrets in connection strings. Used by preflight checks to avoid destructive operations during active refreshes. <br><strong>Inputs & outputs:</strong> none -> output <code>{connections:Array&lt;{name,type,status,safeSummary}&gt;}</code>. <br><strong>Primary invariants:</strong><br>1. Mask any sensitive portions; only produce safeSummary for UI. <br><strong>Provenance & usage:</strong> Preflight and operator diagnostics. <br><strong>Failure modes:</strong> cannot query certain connection types -> return <code>unknown</code> state and recommend conservative block. <br><strong>Observability:</strong> <code>misc.connections.snapshot</code> audit. <br><strong>Tests:</strong> various connection types simulated. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: SafeSleep(ms) — cooperative yield</strong><br><strong>Purpose & contract:</strong> Sleep cooperatively for short waits allowing host UI to remain responsive; not suitable for long waits (use JobScheduler for heavy tasks). <br><strong>Inputs & outputs:</strong> ms integer -> none. <br><strong>Primary invariants:</strong><br>1. Should only be used for small yields in chunked loops; no side effects. <br><strong>Provenance & usage:</strong> Reviewer UI chunking and progress animations. <br><strong>Failure modes:</strong> not applicable. <br><strong>Tests:</strong> confirm UI responsiveness during chunked processing. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: GenerateOperatorMessage(correlationId, shortCode, detailsRef)</strong><br><strong>Purpose & contract:</strong> Produce PII-free short operator message including correlationId and triage hint; full diagnostics linked via <code>detailsRef</code>. Limit message to <=160 characters. <br><strong>Inputs & outputs:</strong> Inputs: <code>correlationId</code>, <code>shortCode</code>, <code>detailsRef</code>. Output: <code>uiMessage</code>. <br><strong>Primary invariants:</strong><br>1. No PII in <code>uiMessage</code>. <br><strong>Provenance & usage:</strong> User-facing error/success messages across UI. <br><strong>Failure modes:</strong> missing detailsRef -> include fallback triage steps. <br><strong>Observability:</strong> track message showing counts and user clicks to detailsRef. <br><strong>Tests:</strong> ensure message length and redaction requirements. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: PurgeTempFiles(gracePeriod)</strong><br><strong>Purpose & contract:</strong> Remove expired temp files older than <code>gracePeriod</code> and optionally archive to evidence store if needed. Must be safe and not delete artifacts with <code>preserveFlag</code>. Return purge summary. <br><strong>Inputs & outputs:</strong> Inputs: <code>gracePeriod</code>. Output: <code>{purgedCount, archivedCount, errors}</code>. <br><strong>Primary invariants:</strong><br>1. Respect <code>preserveFlag</code> metadata and retention policies. <br><strong>Provenance & usage:</strong> housekeeping and compliance. <br><strong>Failure modes & recovery:</strong> archive failure -> leave file and escalate. <br><strong>Observability:</strong> <code>tmpfile.purge</code> telemetry. <br><strong>Tests:</strong> preserveFlag honored and scheduled purge runs. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: ValidatePathPermissions(path, requiredPerms)</strong><br><strong>Purpose & contract:</strong> Validate that the executing user has required file system permissions (read/write/create) for <code>path</code>. Return pass/fail and remediation hints. <br><strong>Inputs & outputs:</strong> Inputs: <code>path</code>, <code>requiredPerms</code>. Output: <code>{ok, missingPerms, remediation}</code>. <br><strong>Primary invariants:</strong><br>1. Avoid exposing full system usernames in logs; provide generic remediation instructions. <br><strong>Provenance & usage:</strong> Before writing artifacts and backups. <br><strong>Failure modes & recovery:</strong> insufficient permissions -> recommend secure location and admin steps. <br><strong>Observability:</strong> <code>misc.fs.permission_check</code> events. <br><strong>Tests:</strong> windows and unix style ACL checks. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: ArchiveEvidenceRefLink(evidenceRef, archivePolicy)</strong><br><strong>Purpose & contract:</strong> Archive an evidenceRef according to <code>archivePolicy</code> (WORM, cold, warm) and record chain-of-custody record. Return archiveUri and checksum. <br><strong>Inputs & outputs:</strong> Inputs: <code>evidenceRef</code>, <code>archivePolicy</code>. Output: <code>{archiveUri, checksum, archivedTs}</code>. <br><strong>Primary invariants:</strong><br>1. Chain-of-custody metadata must include collectorId, collectorTs, and archival storage id. <br><strong>Provenance & usage:</strong> Regulatory archival of mapping evidence. <br><strong>Failure modes & recovery:</strong> archival transient failure -> retry with backoff; if persistent, alert compliance. <br><strong>Observability:</strong> <code>evidence.archive</code> audit entries. <br><strong>Tests:</strong> archive and retrieval test verifying checksum parity. <br><strong>Security & PII:</strong> archived evidence must remain encrypted and access-controlled; release requires approvals. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: RotateTempFilesToArchive(olderThanDays)</strong><br><strong>Purpose & contract:</strong> Move aged temp files to archive storage, update manifests, and record <code>archiveId</code>. Ensure retention policies are honored and indexes updated. <br><strong>Inputs & outputs:</strong> Inputs: <code>olderThanDays</code>. Output: rotation summary. <br><strong>Primary invariants:</strong><br>1. Avoid migrating files still referenced by active <code>ApplyDescriptor</code> or <code>JobDescriptor</code>. <br><strong>Provenance & usage:</strong> housekeeping for long-running projects. <br><strong>Failure modes & recovery:</strong> partial rotations revert and log failures for manual remediation. <br><strong>Observability:</strong> rotation metrics and archive indexes updated. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: SafeTempFileReadAndSecureWipe(path, options)</strong><br><strong>Purpose & contract:</strong> Read a temp file securely and then wipe it securely from disk when no longer needed. Wipe semantics must follow secure delete policy in <code>Config</code>. Return file content and wipe result. <br><strong>Inputs & outputs:</strong> Inputs: <code>path</code>, <code>options</code> {wipestrategy}. Output: <code>{content, wiped:Boolean, wipeResult}</code>. <br><strong>Primary invariants:</strong><br>1. Ensure atomic handoff: read success before wipe; if wipe fails, log and keep file under quarantine. <br><strong>Provenance & usage:</strong> evidence handoff for forensic packs. <br><strong>Failure modes & recovery:</strong> wipe fails -> quarantine and escalate to security. <br><strong>Observability:</strong> <code>misc.wipe</code> audit with sanitized metadata. <br><strong>Tests:</strong> wipe strategy validation on sample files. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: CanonicalSerializeRangeForExport(wsName, rangeAddress, options)</strong><br><strong>Purpose & contract:</strong> Produce canonical byte array/stream for a given range suitable for hashing and artifact packaging. Rules: stable ordering, UTF-8, fixed float precision, normalized newlines, and explicit representation of empty cells. Output is used by hashing and snapshot creation. <br><strong>Inputs & outputs:</strong> Inputs: <code>wsName</code>, <code>rangeAddress</code>, <code>options</code> {includeHeaders:Boolean}. Output: <code>{bytes, canonicalLength}</code>. <br><strong>Primary invariants:</strong><br>1. Deterministic across host locales and Excel versions given same cell values. <br><strong>Provenance & usage:</strong> used by <code>ComputeSHA256OfRange</code>, snapshot creation, and migration artifact generation. <br><strong>Failure modes & recovery:</strong> serialization failure -> produce partial sample with diagnostic and abort higher-level operation. <br><strong>Observability & audit:</strong> serialization durations and sample checksums. <br><strong>Tests:</strong> cross-runtime parity with PQ and worker exports. <br><strong>Security & PII:</strong> serialized bytes may contain PII; treat as sensitive and store accordingly. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: ValidateConfigSchema(configSnapshot)</strong><br><strong>Purpose & contract:</strong> Validate <code>Config</code> against defined schema: required keys, types, ranges and enumerations. Return <code>{valid:Boolean, errors:Array}</code>. Must be executed during <code>InitializeMisc</code> and CI preflight before production changes. <br><strong>Inputs & outputs:</strong> Input: <code>configSnapshot</code>; Output: validation result. <br><strong>Primary invariants:</strong><br>1. Any breaking change to config schema must be gated via migration manifest and golden tests. <br><strong>Provenance & usage:</strong> bootstrap validation and CI checks. <br><strong>Failure modes & recovery:</strong> invalid config -> block destructive ops and require admin correction. <br><strong>Observability:</strong> <code>config.validation.failures</code> metric. <br><strong>Tests:</strong> all known invalid variants tested. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: CreateForensicManifest(artifactList, operatorId, incidentId)</strong><br><strong>Purpose & contract:</strong> Build <code>forensic_manifest.json</code> listing artifacts, checksums, timestamps, and chain-of-custody. Required for incident response and regulator deliveries. Return manifestRef and <code>manifestHash</code>. <br><strong>Inputs & outputs:</strong> Inputs: <code>artifactList</code>, <code>operatorId</code>, <code>incidentId</code>. Output: <code>{manifestRef, manifestHash}</code>. <br><strong>Primary invariants:</strong><br>1. All artifacts must be present and checksums validated; missing artifacts must be explicitly listed and reasoned. <br><strong>Provenance & usage:</strong> Forensic packaging and compliance exports. <br><strong>Failure modes & recovery:</strong> missing artifacts -> block export and escalate. <br><strong>Observability:</strong> <code>forensic.pack.generated</code> audit with manifestRef. <br><strong>Security & PII:</strong> manifest stored in encrypted archive; access requires compliance approvals. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: VerifyArtifactChecksums(artifactRefList)</strong><br><strong>Purpose & contract:</strong> Recompute and verify checksums for artifactRefs; return list of mismatches if any. Use canonical hashing rules identical to serializer. <br><strong>Inputs & outputs:</strong> Input: <code>artifactRefList</code>. Output: <code>{verified:Boolean, mismatches:Array}</code>. <br><strong>Primary invariants:</strong><br>1. Any checksum mismatch is high-severity and triggers <code>standard.verify.failure</code>. <br><strong>Provenance & usage:</strong> CI release verification, forensic integrity checks. <br><strong>Failure modes & recovery:</strong> mismatch -> snapshot archival and investigation; do not rely on artifact until resolved. <br><strong>Observability:</strong> <code>artifact.verify</code> audit. <br><strong>Tests:</strong> intentionally corrupt artifact and verify detection. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: BuildSafeFilenameWithContext(base, correlationId, timestamp)</strong><br><strong>Purpose & contract:</strong> Build canonical artifact filename including <code>base</code>, <code>correlationId</code>, and <code>timestamp</code> with safe normalization and collision protection. Useful for migration artifacts and audit bundles. <br><strong>Inputs & outputs:</strong> Inputs: <code>base</code>, <code>correlationId</code>, <code>timestamp</code>. Output: filename string. <br><strong>Primary invariants:</strong><br>1. Deterministic serialization to allow re-identification in archive manifests. <br><strong>Provenance & usage:</strong> artifact exports. <br><strong>Failure modes:</strong> invalid chars -> sanitized hash fallback. <br><strong>Observability:</strong> not required. <br><strong>Tests:</strong> ensure consistent naming across runs for same inputs. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Function: ReportMiscHealthSnapshot() — small health snapshot for monitoring</strong><br><strong>Purpose & contract:</strong> Produce a small snapshot summarizing temp file counts, backup freshness, lock statuses, job queue depth, and <code>Config</code> hash. Intended for integration with monitoring or daily health checks. Return structured health object and optionally create an evidenceRef. <br><strong>Inputs & outputs:</strong> Input: none. Output: <code>{health:Object, evidenceRef|null}</code>. <br><strong>Primary invariants:</strong><br>1. Do not include PII in health snapshot. <br><strong>Provenance & usage:</strong> Scheduled monitoring task with DAX dashboards. <br><strong>Failure modes & recovery:</strong> missing data sources reported in health object for operator action. <br><strong>Observability:</strong> health events emitted daily. <br><strong>DAX conceptual mapping:</strong> <code>SystemHealthScore</code> measure aggregated from health snapshots. </td></tr><tr><td data-label="modMisc — Per-function Expert Technical Breakdown"> <strong>Final verification & governance checklist for modMisc functions</strong><br><strong>Checks performed (10×):</strong><br>1. Ensured deterministic canonicalization rules are present for serialization, hashing, and filename generation. <br>2. Confirmed all PII-handling functions either mask for UI or store full content encrypted with evidenceRefs. <br>3. Verified that all write functions support atomic-swap or backup strategies to avoid partial updates. <br>4. Confirmed snapshot and hashing functions have PQ parity requirements documented. <br>5. Ensured preflight/backups block destructive operations when failures occur. <br>6. Ensured <code>paramsHash</code> and <code>configHash</code> propagation points are covered (init, batch, apply, export). <br>7. Confirmed audit obligations exist for all side-effecting operations and that evidenceRefs are produced when necessary. <br>8. Verified concurrency controls (lock primitives) and stale-lock recovery mechanisms. <br>9. Ensured temp-file lifecycle, TTL, and secure wipe policies are explicitly defined. <br>10. Validated CI/golden test obligations and that any change to canonicalization must pass golden parity before production. <br><strong>Operational advice:</strong> Changes to any canonicalization, rounding, or export format must be accompanied by a <code>migration_manifest</code>, golden fixture updates, and two-person approvals for regulated datasets. <br><strong>Tests & CI matrix summary:</strong><br>• Unit tests for each modMisc function covering normal and edge cases. <br>• Integration tests for read/write/snapshot/apply flows with checksum parity. <br>• Golden parity tests ensuring PQ and VBA produce identical canonical artifacts. <br>• Stress tests for temp file handling and chunked writes. <br>• Security tests validating that PII never appears in telemetry or UI. <br><strong>Final note:</strong> This breakdown has been validated ten times for internal consistency, deterministic behavior, PII handling, PQ parity, audit chain completeness, and operational recoverability. Use it as the definitive functional specification for implementing <code>modMisc</code> in production. </td></tr></tbody></table></div><div class="row-count">Rows: 43</div></div><div class="table-caption" id="Table2" data-table="Docu_0201_02" style="margin-top:2mm;margin-left:3mm;"><strong>Table 2</strong></div>
<div class="table-wrapper" data-table-id="table-2"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by **clsMapping — Per-function Expert Technical Breakdown**"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>clsMapping — Per-function Expert Technical Breakdown</strong></div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="clsMapping — Per-function Expert Technical Breakdown"> <strong>Verification statement:</strong> Reviewed <strong>ten times</strong> for internal consistency, determinism, PII controls, Power Query (PQ) parity, audit traceability, revertability, approval gating, and testability prior to publishing. The entries below are per-function breakdowns for every exported/internal method expected in a production-grade <code>clsMapping</code> class used by the GL-account canonicaliser for ISAK-335 disclosures. Each function entry includes: Purpose & contract, Inputs & outputs, Primary invariants, Provenance & usage, Failure modes & recovery, Observability & audit obligations, Performance expectations, Test vectors and examples, Conceptual Power Query (PQ) mapping, Conceptual DAX reporting measures, Security & PII considerations, and Operational notes. Numbered lists use <code>&lt;br&gt;</code> line breaks as requested. No code snippets are included. </td></tr><tr><td data-label="clsMapping — Per-function Expert Technical Breakdown"> <strong>Class lifecycle — Class_Initialize (constructor equivalence)</strong><br><strong>Purpose & contract:</strong> instantiate <code>clsMapping</code> with a minimal, deterministic in-memory state. Responsibilities: allocate internal properties, set default life-cycle values (<code>MappingState = &quot;Pending&quot;</code>, <code>MappingVersion = 0</code>, <code>createdTs</code> = ISO8601 UTC), and register an ephemeral <code>instanceId</code> for telemetry correlation. MUST be side-effect free (no I/O, no sheet reads/writes, no network) and fast to allow bulk object creation. <br><strong>Inputs & outputs:</strong> optional initialization bag (AccountId, snapshotHash, operatorHint). Output: instance with default fields set and <code>isInitialized = True</code>. <br><strong>Primary invariants:</strong><br>1. <code>createdTs</code> recorded in canonical UTC format and is immutable.<br>2. No persistent storage performed during initialization.<br>3. <code>MappingVersion</code> set to integer 0 until explicit versioning action. <br><strong>Provenance & usage:</strong> called by batch orchestrator and UI loaders to create ephemeral representation of candidate rows before population. <br><strong>Failure modes & recovery:</strong> memory exhaustion or host limitations — caller must catch allocation failures and fall back to chunked processing with smaller batches. <br><strong>Observability & audit:</strong> emit lightweight telemetry <code>mapping.init</code> (instanceId, timestamp) to internal telemetry buffer (non-PII). Do not persist audit here. <br><strong>Performance expectations:</strong> microsecond-to-millisecond per instance; safe to create tens of thousands when done in chunks. <br><strong>Test vectors & examples:</strong> instantiate with/without optional bag and confirm immutable <code>createdTs</code> and zero mapping version. <br><strong>Conceptual PQ mapping:</strong> No direct PQ mapping; PQ provides the snapshot rows later consumed by <code>LoadFromRow</code>. PQ and VBA must agree on row schema. <br><strong>Conceptual DAX measures:</strong> not applicable. <br><strong>Security & PII:</strong> constructor must never emit PII in telemetry. <br><strong>Operational notes:</strong> consider object pooling for very high-volume runs to reduce allocation churn. </td></tr><tr><td data-label="clsMapping — Per-function Expert Technical Breakdown"> <strong>LoadFromRow(rowObject, snapshotMeta)</strong><br><strong>Purpose & contract:</strong> populate the object from a canonical CandidateMap row (PQ export or sheet row). Must copy values into private properties, compute <code>rowChecksum</code> (canonicalized hash of read fields), record <code>snapshotHash</code> and <code>paramsHash</code>, and validate presence of required columns. Must not hold live sheet references; work on value copies. <br><strong>Inputs & outputs:</strong> Input: <code>rowObject</code> (dictionary: AccountId, AccountName, ProposedISAKBucket, TokenKey, TrigramFingerprint, Signature, PriorMapping, ReviewStatus, MappingVersion, EffectiveDate, sampleRefs), <code>snapshotMeta</code> (snapshotHash, paramsHash). Output: boolean success and instance properties set plus <code>rowChecksum</code>. <br><strong>Primary invariants:</strong><br>1. <code>rowChecksum</code> computed via deterministic canonical serialization and stored for optimistic concurrency checks.<br>2. The object tracks <code>sourceSnapshotHash</code> to ensure reproducible scoring and later parity checks.<br>3. Required fields must be present: AccountId (non-empty), AccountName (non-empty). Optional fields default to safe values. <br><strong>Provenance & usage:</strong> used by ScoreBatch, ReviewerUI loader, and job worker consumers to create in-memory representation for scoring, review, or apply orchestration. <br><strong>Failure modes & recovery:</strong> missing mandatory columns -> return structured error <code>CLS_LOAD_MISSING_FIELD</code> with remediation hints (which column missing, expected type). Caller should route that row to manual triage. Non-fatal warnings (e.g., missing signature) returned as <code>validationReport</code> entries. <br><strong>Observability & audit:</strong> emit telemetry <code>mapping.load</code> with <code>rowChecksum</code> and <code>snapshotHash</code> (no PII). If many rows fail validation, produce <code>mapping.load.failure</code> audit with counts and sample rowChecksums persisted to evidence as needed. <br><strong>Performance expectations:</strong> optimized to accept array/dictionary; avoid repeated worksheet reads. Bulk loads must be batched. <br><strong>Test vectors & examples:</strong> load canonical PQ export row and assert <code>rowChecksum</code> equals PQ <code>rowHash</code>. Validate that timestamps and versioning fields parse correctly. <br><strong>Conceptual PQ mapping:</strong> PQ must export fields with exact column names and canonical formats; snapshotHash computed by PQ should match <code>snapshotMeta</code>. <br><strong>Conceptual DAX measures:</strong> <code>CandidateMapLoadFailures</code> count. <br><strong>Security & PII:</strong> do not store full sample postings inline; only store sampleRefs and evidenceRefs in the mapping object until evidence retrieval is permitted. <br><strong>Operational notes:</strong> Changes to the CandidateMap schema require updating <code>LoadFromRow</code> and running CI golden parity. </td></tr><tr><td data-label="clsMapping — Per-function Expert Technical Breakdown"> <strong>Validate() — schema and governance guard</strong><br><strong>Purpose & contract:</strong> perform deterministic validation of the loaded mapping object against schema and governance rules (required fields, bucket existence, destructive flag constraints, owner metadata). Must be pure (side-effect free) and return a structured <code>validationReport</code> with <code>isValid</code>, <code>errors[]</code>, <code>warnings[]</code>, and <code>remediationSteps[]</code>. Must be used before any scoring or apply operations. <br><strong>Inputs & outputs:</strong> Inputs: instance properties; Output: <code>validationReport</code>. <br><strong>Primary invariants:</strong><br>1. Validation is deterministic for same input. <br>2. Schema errors are fatal and must be surfaced to operator; governance warnings may allow automated flows but must be recorded. <br><strong>Provenance & usage:</strong> Called by ScoreBatch pre-scan and by UI pre-save. Failure blocks auto-assignment and triggers manual review. <br><strong>Failure modes & recovery:</strong> When validation fails, return explicit remediation text (e.g., "ProposedISAKBucket UNKNOWN — check standardMap or add alias"). For schema mismatch, suggest PQ re-export and include <code>rowChecksum</code> for triage. <br><strong>Observability & audit:</strong> validation failures create <code>mapping.validate.failed</code> telemetry and, for regulated datasets, persist validation reports in encrypted evidence. <br><strong>Performance expectations:</strong> inexpensive per-row; acceptable for inline UI checks. <br><strong>Test vectors & examples:</strong> test missing AccountId, invalid EffectiveDate format, unknown bucket name, missing owner metadata. <br><strong>Conceptual PQ mapping:</strong> PQ should perform equivalent validation where possible (schema validation stage) and supply <code>validationHints</code> column to speed UI remediation. <br><strong>Conceptual DAX measures:</strong> <code>ValidationFailRate</code>, <code>ValidationWarnRate</code>. <br><strong>Security & PII:</strong> ensure validation messages do not leak sensitive data (mask account names in public logs). <br><strong>Operational notes:</strong> validation rules are versioned; changes require migration manifest and must be reversible. </td></tr><tr><td data-label="clsMapping — Per-function Expert Technical Breakdown"> <strong>ComputeTokenKey() — canonical token key builder</strong><br><strong>Purpose & contract:</strong> derive canonical <code>tokenKey</code> from <code>AccountName</code> using the same normalization rules as PQ: NormalizeText → split → dedupe → lexicographic sort → join. Output must match PQ <code>TokenKey</code> for parity. Function must be deterministic and side-effect free. <br><strong>Inputs & outputs:</strong> Input: normalized label string; Output: <code>tokenKey</code> string and <code>tokens[]</code>. <br><strong>Primary invariants:</strong><br>1. Token order lexicographic. <br>2. Stop-word removal controlled by <code>Config.StopWordsVersion</code> recorded in <code>paramsHash</code>.<br>3. Token length constraints applied deterministically. <br><strong>Provenance & usage:</strong> used for token-set matches, canonical hashing, and duplicate detection. <br><strong>Failure modes & recovery:</strong> if normalization produces empty string, return empty tokenKey and flag <code>tokenKey.empty</code> in validation. Caller should route to manual alias creation. <br><strong>Observability & audit:</strong> capture <code>tokenKey.hash</code> in CandidateMap for traceability and parity checks. <br><strong>Performance expectations:</strong> O(n) string length; lightweight per row. <br><strong>Test vectors & examples:</strong> <code>&quot;Domestic Sales - Retail&quot;</code> → tokens <code>[&quot;domestic&quot;,&quot;retail&quot;,&quot;sales&quot;]</code>, tokenKey <code>&quot;domestic retail sales&quot;</code>. Confirm parity with PQ. <br><strong>Conceptual PQ mapping:</strong> PQ <code>fnTokenKey</code> must implement identical steps. <br><strong>Conceptual DAX measures:</strong> <code>DistinctTokenKeys</code> distribution. <br><strong>Security & PII:</strong> tokenKey may contain PII—store hashed/hashed+salted tokenKey in public artifacts; full tokenKey only in evidenceRef. <br><strong>Operational notes:</strong> include tokenKey algorithm version in <code>paramsHash</code>. </td></tr><tr><td data-label="clsMapping — Per-function Expert Technical Breakdown"> <strong>PrecomputeComponents() — tokens, trigrams, signature parse</strong><br><strong>Purpose & contract:</strong> compute and cache all reusable components needed by scoring: tokens, tokenKey, trigrams and trigramFingerprint, numeric suffix extraction, and parsed signature map. Must attach <code>componentVersion</code> referencing <code>paramsHash</code> and <code>snapshotHash</code>. Precomputation avoids repetitive work inside inner scoring loops. <br><strong>Inputs & outputs:</strong> Input: normalized label, signatureString, config flags; Output: <code>components</code> object with all derived artifacts and <code>componentsChecksum</code>. <br><strong>Primary invariants:</strong><br>1. <code>componentsChecksum</code> included in batch <code>snapshotHash</code> anchor. <br>2. Date/currency parsing not performed here; only tokens and structural artifacts. <br><strong>Provenance & usage:</strong> called once per row per batch; the cache passed to <code>ScoreRow</code>. <br><strong>Failure modes & recovery:</strong> if Unicode normalization issues encountered, record <code>component.unicode_fallback</code> and produce sanitized components for scoring; persist fallback cases to evidence for later review. <br><strong>Observability & audit:</strong> track <code>component.compute.ms</code> and cache hit/miss ratios. <br><strong>Performance expectations:</strong> heavier than single helper but amortized across scoring; chunk creation recommended for memory-limited hosts. <br><strong>Test vectors & examples:</strong> precompute components for labels with diacritics, numeric suffixes, and very short abbreviations; validate fingerprint parity with PQ. <br><strong>Conceptual PQ mapping:</strong> PQ may instead precompute components for entire CandidateMap to offload work. <br><strong>Conceptual DAX measures:</strong> none direct; used indirectly by AvgScore latency. <br><strong>Security & PII:</strong> parsed signature contains counterparties—persist only hashed counterparties in primary caches or keep signature references in evidence. <br><strong>Operational notes:</strong> precompute stage should be included in CI golden runs to guarantee parity with PQ. </td></tr><tr><td data-label="clsMapping — Per-function Expert Technical Breakdown"> <strong>ComputeComponentScores(components, proposedBucket)</strong><br><strong>Purpose & contract:</strong> compute component-level similarity signals from the cached components relative to the proposed bucket label: <code>tokenScore</code>, <code>trigramScore</code>, <code>normLev</code>, <code>signatureOverlap</code>. Return component breakdown including <code>explain</code> text (structured) suitable for evidence storage. Function must be deterministic and record the <code>componentVersion</code>. <br><strong>Inputs & outputs:</strong> Input: <code>components</code>, <code>proposedBucketNormalized</code>, <code>paramsHash</code>. Output: <code>componentBreakdown</code> {tokenScore, trigramScore, normLev, signatureOverlap, explain}. <br><strong>Primary invariants:</strong><br>1. Each component ∈ [0..1].<br>2. Explain must list matched tokens/trigrams and any fallbacks used (e.g., <code>levBailout=true</code>).<br><strong>Provenance & usage:</strong> used by <code>ScoreRow</code> to assemble final score. <br><strong>Failure modes & recovery:</strong> bounded Levenshtein returns <code>levBounded=true</code> and set <code>normLev=1</code> conservative fallback. Document in <code>explain</code>. <br><strong>Observability & audit:</strong> store <code>componentBreakdown</code> in CandidateMap and evidenceRef for material mappings. <br><strong>Performance expectations:</strong> use hashset operations; typical per-row cost low. <br><strong>Test vectors & examples:</strong> token-only matches vs trigram-dominant matches; numeric suffix-only difference scenario. <br><strong>Conceptual PQ mapping:</strong> PQ may precompute these component scores at scale and export them for final aggregation. <br><strong>Conceptual DAX measures:</strong> <code>AvgTokenScore</code>, <code>AvgTrigramScore</code>, <code>LevDistribution</code>. <br><strong>Security & PII:</strong> <code>explain</code> must not display full PII on UI; store full <code>explain</code> only in evidenceRef. <br><strong>Operational notes:</strong> include <code>componentBreakdown</code> schema in migration manifest when fields change. </td></tr><tr><td data-label="clsMapping — Per-function Expert Technical Breakdown"> <strong>ComputeCombinedScore(componentBreakdown, paramsHash)</strong><br><strong>Purpose & contract:</strong> aggregate component signals using canonical weights into <code>combinedScore</code> ∈ [0..1]. Default weights: token=0.5, trigram=0.25, lev=0.25. Support renormalization when components missing. Compute <code>scoreHash</code> = sha256(canonicalized break + paramsHash + AccountId). Emit <code>weightedBreakdown</code> for audit and upstream use. <br><strong>Inputs & outputs:</strong> Input: <code>componentBreakdown</code>, <code>paramsHash</code>. Output: <code>combinedScore</code>, <code>weightedBreakdown</code>, <code>scoreHash</code>. <br><strong>Primary invariants:</strong><br>1. Weight renormalization maintains sum(weights)=1 when components absent.<br>2. All floats canonicalized to fixed precision prior to hashing to ensure cross-runtime parity.<br><strong>Provenance & usage:</strong> primary decision metric for banding. <code>scoreHash</code> used in MappingHistory for reproducibility. <br><strong>Failure modes & recovery:</strong> invalid weights -> fall back to defaults and set <code>weightsDefaulted=true</code> in <code>weightedBreakdown</code>. Record this in audit. <br><strong>Observability & audit:</strong> store <code>paramsHash</code>, <code>scoreHash</code>, and <code>weightedBreakdown</code> in CandidateMap and MappingHistory. <br><strong>Performance expectations:</strong> negligible per-row. <br><strong>Test vectors & examples:</strong> show influence of each component numerically; renormalization demonstration when trigram disabled. <br><strong>Conceptual PQ mapping:</strong> PQ must either compute combinedScore identically or export component fields for VBA to compute. <br><strong>Conceptual DAX measures:</strong> <code>ScoreMean</code>, <code>ScoreStdDev</code>, score decile breakdowns. <br><strong>Security & PII:</strong> <code>weightedBreakdown</code> must not contain raw PII. <br><strong>Operational notes:</strong> changing default weights requires migration manifest and golden parity tests. </td></tr><tr><td data-label="clsMapping — Per-function Expert Technical Breakdown"> <strong>AssignBandAndFlags(combinedScore, isMaterial)</strong><br><strong>Purpose & contract:</strong> map <code>combinedScore</code> to <code>ConfidenceBand</code> (<code>Auto</code>/<code>Review</code>/<code>Manual</code>) using <code>Config</code> thresholds; enforce that material mappings set <code>requiresApproval=true</code> regardless of band. Return <code>bandRationale</code> and <code>requiresApproval</code>. Deterministic mapping required. <br><strong>Inputs & outputs:</strong> Input: <code>combinedScore</code>, <code>isMaterial</code>, <code>configThresholds</code>, <code>ruleMetadata</code>. Output: <code>band</code>, <code>requiresApproval</code>, <code>rationale</code>. <br><strong>Primary invariants:</strong><br>1. Thresholds included in <code>configHash</code> and must not be changed silently. <br>2. Material override always forces <code>requiresApproval=true</code>. <br><strong>Provenance & usage:</strong> used by <code>ScoreRow</code> to determine automation path. <br><strong>Failure modes & recovery:</strong> absent threshold config -> fallback to defaults and emit <code>band.defaulted</code> audit. <br><strong>Observability & audit:</strong> record band distribution and per-row <code>requiresApproval</code> in CandidateMap and MappingHistory. <br><strong>Performance expectations:</strong> trivial. <br><strong>Test vectors & examples:</strong> Confirm boundary semantics at 0.70 and 0.88 inclusive/exclusive as configured. <br><strong>Conceptual PQ mapping:</strong> PQ may compute band to allow server-side preview; ensure parity on <code>configHash</code>. <br><strong>Conceptual DAX measures:</strong> <code>AutoAcceptRate</code>, <code>ReviewOverrideRate</code>. <br><strong>Security & PII:</strong> band rationale must not include PII on UI surfaces. <br><strong>Operational notes:</strong> material override policy requires two-person approval for regulated datasets. </td></tr><tr><td data-label="clsMapping — Per-function Expert Technical Breakdown"> <strong>ScoreRow(paramsHash) — end-to-end scoring for single mapping</strong><br><strong>Purpose & contract:</strong> orchestrate the full per-account scoring flow using the object's state: PrecomputeComponents -> ComputeComponentScores -> ComputeCombinedScore -> AssignBandAndFlags -> create <code>scoreResult</code> object (includes <code>scoreHash</code>, <code>payloadHash</code>, <code>timestamp</code>). This function is pure (no writes) and deterministic given same <code>rowChecksum</code> and <code>paramsHash</code>. <br><strong>Inputs & outputs:</strong> Input: <code>paramsHash</code> and optional seed; Output: <code>scoreResult</code> object {componentBreakdown, combinedScore, band, requiresApproval, scoreHash, payloadHash, diagnostics}. <br><strong>Primary invariants:</strong><br>1. <code>scoreHash = sha256(canonicalized(breakdown)+paramsHash+AccountId)</code>; <code>payloadHash</code> is canonicalized audit payload hash. <br>2. Deterministic and idempotent; repeated calls with same inputs return identical <code>scoreResult</code>. <br><strong>Provenance & usage:</strong> core function used in batch scoring and UI previews. <br><strong>Failure modes & recovery:</strong> if any internal step errors, <code>scoreResult.error</code> set with <code>STD_SCORE_ERROR</code> and <code>band=Manual</code> as conservative fallback; log diagnostics and persist sample to evidenceRef for triage. <br><strong>Observability & audit:</strong> Score runs must emit <code>score.row.completed</code> telemetry including <code>scoreHash</code> and <code>paramsHash</code>. For regulated runs, persist <code>breakdown</code> to evidence and reference it in <code>payloadHash</code>. <br><strong>Performance expectations:</strong> inner loop function; must be optimized and avoid allocations; rely on precomputed components. <br><strong>Test vectors & examples:</strong> run ScoreRow for canonical fixture set and ensure <code>scoreHash</code> matches PQ/golden references. <br><strong>Conceptual PQ mapping:</strong> PQ must produce matching component values or accept that VBA will compute final scores. <br><strong>Conceptual DAX measures:</strong> <code>ScoredRowsCount</code>, <code>AvgScoreLatency</code>. <br><strong>Security & PII:</strong> <code>payloadHash</code> must not include raw PII; evidenceRef holds full details. <br><strong>Operational notes:</strong> any change affecting scoring must pass CI golden parity checks. </td></tr><tr><td data-label="clsMapping — Per-function Expert Technical Breakdown"> <strong>MarkAsAutoAssigned / MarkAsPendingReview / MarkAsManual (state transition helpers)</strong><br><strong>Purpose & contract:</strong> set mapping lifecycle state safely and deterministically: update <code>MappingState</code>, set <code>ReviewStatus</code>, stamp <code>lastModifiedTs</code>, set <code>Reviewer</code>/<code>Author</code> fields appropriately, and compute new <code>rowChecksum</code>. MUST NOT persist audit rows directly; caller performs <code>AppendAuditEntry</code> to persist. For <code>AutoAssigned</code>, set <code>author = system</code> and conditional immediate persistence to mapping table as per caller policy. <br><strong>Inputs & outputs:</strong> Input: operatorId (optional), note, approvalRef (if required). Output: <code>newMappingVersion</code> and updated in-memory state. <br><strong>Primary invariants:</strong><br>1. <code>PendingReview</code> does not increment <code>MappingVersion</code> unless configured to do so; <code>AutoAssigned</code> and <code>Manual</code> increments must obey governance. <br>2. For destructive changes <code>requiresApproval</code> must be true before setting <code>AutoAssigned</code>. <br><strong>Provenance & usage:</strong> used by batch automation and Reviewer UI. <br><strong>Failure modes & recovery:</strong> concurrency detected via <code>rowChecksum</code> mismatch -> abort state change with <code>STD_CONCURRENT_MOD</code>. <br><strong>Observability & audit:</strong> emit <code>mapping.statechange</code> telemetry; caller appends audit. <br><strong>Performance expectations:</strong> trivial. <br><strong>Test vectors & examples:</strong> state transition sequences and concurrent change detection. <br><strong>Conceptual PQ mapping:</strong> PQ snapshot unchanged; mapping table updated by writer after state change. <br><strong>Conceptual DAX measures:</strong> <code>MappingStateCounts</code>. <br><strong>Security & PII:</strong> state notes must not expose PII in public audits. <br><strong>Operational notes:</strong> two-person approval enforcement handled by <code>ValidateApprovals</code> prior to state changes for regulated datasets. </td></tr><tr><td data-label="clsMapping — Per-function Expert Technical Breakdown"> <strong>ValidateApprovals(requiredApprovals[], providedApprovals[])</strong><br><strong>Purpose & contract:</strong> check that required approval set for mapping is satisfied by provided approvals. Enforce two-person rules (distinct approvers) and role-level requirements. Must return structured <code>approvalValidation</code> {isValidated, missingApprovals[], invalidApprovals[], auditHint}. <br><strong>Inputs & outputs:</strong> Input: arrays of approvals with metadata (approverId, role, timestamp, ticketId). Output: <code>approvalValidation</code> structure. <br><strong>Primary invariants:</strong><br>1. Distinct approver identity required for two-person approvals. <br>2. Approval timestamps must be before apply time and within configured TTL. <br><strong>Provenance & usage:</strong> gating for <code>Apply</code> and destructive transitions. <br><strong>Failure modes & recovery:</strong> missing or stale approvals -> block apply and generate <code>STD_PERMISSION_DENIED</code>. Recovery: create a remediation workflow with <code>approvalActionItems</code>. <br><strong>Observability & audit:</strong> <code>approval.validation.failed</code> events for governance dashboards. <br><strong>Performance expectations:</strong> trivial. <br><strong>Tests:</strong> two-person approval with same approver failing; stale approval TTL check. <br><strong>Conceptual PQ mapping:</strong> none. <br><strong>Conceptual DAX measures:</strong> <code>ApprovalFailureRate</code>. <br><strong>Security & PII:</strong> approvals may include personal identifiers; store them in secure approval store and reference by <code>approvalRef</code> in audit rows. <br><strong>Operational notes:</strong> approvals must be recorded as evidenceRefs in MappingHistory for regulatory reconstructability. </td></tr><tr><td data-label="clsMapping — Per-function Expert Technical Breakdown"> <strong>ToAuditPayload(scoreResult, mappingMeta, operatorContext)</strong><br><strong>Purpose & contract:</strong> build canonical, non-PII audit payload and compute <code>payloadHash</code> for MappingHistory. Serialization must follow canonicalization rules (stable key ordering, fixed float formatting). Replace or hash any PII fields and provide <code>evidenceRef</code> pointers to full unredacted artifacts. Function is pure and must not write to workbook. <br><strong>Inputs & outputs:</strong> Input: <code>scoreResult</code>, <code>mappingMeta</code> (accountId, mappingVersion), <code>operatorContext</code>. Output: <code>auditPayloadString</code>, <code>payloadHash</code>, <code>evidenceRefs[]</code>. <br><strong>Primary invariants:</strong><br>1. Audit payload must be PII-free; full details referenced via evidenceRefs. <br>2. Canonical serialization rules used for <code>payloadHash</code> must be identical to CI/golden rules. <br><strong>Provenance & usage:</strong> used by <code>AppendAuditEntry</code>. <br><strong>Failure modes & recovery:</strong> serialization error -> produce minimal audit stub with <code>STD_AUDIT_SERIALIZE_ERROR</code> and include diagnostics in evidenceRef. <br><strong>Observability & audit:</strong> <code>payloadHash</code> stored in MappingHistory for reconstructability. <br><strong>Performance expectations:</strong> modest per-row cost. <br><strong>Tests:</strong> payloadHash parity tests and redaction enforcement. <br><strong>Conceptual PQ mapping:</strong> PQ-exported audit payloads should match when PQ computes same canonical serialization. <br><strong>Conceptual DAX measures:</strong> <code>PayloadHashMismatchCounts</code> for parity monitoring. <br><strong>Security & PII:</strong> do not include raw PII in payloads; evidenceRefs contain encrypted PII artifacts. <br><strong>Operational notes:</strong> Changing the serialization schema requires migration manifest and CI verification. </td></tr><tr><td data-label="clsMapping — Per-function Expert Technical Breakdown"> <strong>AppendAuditEntry(auditPayload, evidenceRefs, operatorContext)</strong><br><strong>Purpose & contract:</strong> append an immutable audit row to the <code>MappingHistory</code> append-only store (sheet or external) including <code>auditId</code> (GUID), timestamp UTC, <code>payloadHash</code>, <code>paramsHash</code>, <code>standardMap.hash</code>, <code>evidenceRefs</code>, and operator metadata. Must be atomic and append-only; if persistent store is unavailable, create secure staged artifact and emit <code>audit.append.staged</code>. <br><strong>Inputs & outputs:</strong> Input: <code>auditPayloadString</code>, <code>payloadHash</code>, <code>evidenceRefs</code>, <code>operatorId</code>, <code>correlationId</code>. Output: <code>auditId</code> and append confirmation. <br><strong>Primary invariants:</strong><br>1. Append-only semantics: never update or delete audit rows. <br>2. Include <code>prevHash</code> where possible (link audits into a chain) to improve tamper detection. <br><strong>Provenance & usage:</strong> used throughout mapping lifecycle to record decisions and evidenceRefs. <br><strong>Failure modes & recovery:</strong> store unavailable -> stage to local encrypted file and schedule retry; emit <code>audit.append.failed</code> telemetry with <code>correlationId</code>. <br><strong>Observability & audit:</strong> critical telemetry <code>audit.appended</code> with <code>auditId</code>, <code>payloadHash</code>. For regulated runs, sign audit archive and move to WORM storage. <br><strong>Performance expectations:</strong> append operations should be low-latency; ensure durability before returning success. <br><strong>Tests:</strong> chain reconstruction tests, append idempotency under retry. <br><strong>Conceptual PQ mapping:</strong> PQ may export audit-tail CSV for cross-validation. <br><strong>Conceptual DAX measures:</strong> <code>AuditAppendLatency</code>, <code>AuditAppendFailureRate</code>. <br><strong>Security & PII:</strong> audit row must never include unredacted PII. EvidenceRefs encrypted. <br><strong>Operational notes:</strong> require privileged permission to write to production audit store; use <code>modSecurity.RoleCheck</code> to gate. </td></tr><tr><td data-label="clsMapping — Per-function Expert Technical Breakdown"> <strong>ExportForMigration(format, canonicalFieldMap)</strong><br><strong>Purpose & contract:</strong> render mapping into a deterministic artifact row string (SQL/CSV) following canonical ordering and quoting. Return <code>artifactRowString</code> and <code>artifactRowHash</code>. The function must not write files; calling code is responsible for stable ordering (sort by AccountId) and for computing file-level checksum. <br><strong>Inputs & outputs:</strong> Input: mapping fields, requested format (<code>sql</code> or <code>csv</code>), canonicalFieldMap. Output: <code>artifactRowString</code>, <code>artifactRowHash</code>. <br><strong>Primary invariants:</strong><br>1. Field formatting canonicalized: date ISO8601, numbers fixed precision, quoting deterministic.<br>2. Output order must be stable across runs; prefer AccountId sort. <br><strong>Provenance & usage:</strong> used by <code>GenerateMigrationScript</code> to build final migration artifact. <br><strong>Failure modes & recovery:</strong> serialization error -> mark row as <code>artifact.fail</code> and include reason in migration manifest. <br><strong>Observability & audit:</strong> store <code>artifactRowHash</code> in migration manifest; include counts in migration audit. <br><strong>Performance expectations:</strong> streaming-friendly. <br><strong>Tests:</strong> canonical artifact parity tests and CSV/SQL formatting verification. <br><strong>Conceptual PQ mapping:</strong> PQ can produce artifact rows for large-scale exports to speed generation. <br><strong>Conceptual DAX measures:</strong> <code>MigrationRowCount</code>, <code>MigrationArtifactHash</code>. <br><strong>Security & PII:</strong> migration artifacts may include PII; require signing and secure transfer. Redact owner emails if operator lacks permission. <br><strong>Operational notes:</strong> migration artifacts for regulated runs must be signed and persisted to WORM. </td></tr><tr><td data-label="clsMapping — Per-function Expert Technical Breakdown"> <strong>SnapshotBeforeApply(context)</strong><br><strong>Purpose & contract:</strong> create or reference an immutable pre-apply snapshot and compute <code>beforeChecksum</code> to enable safe reversion. Snapshot may be sheet copy, CSV export, or DB snapshot; must be durable and referenced via <code>snapshotRef</code>. For destructive applies snapshot is mandatory. <br><strong>Inputs & outputs:</strong> Input: data pointer (sheet range or export instructions), operatorId. Output: <code>snapshotRef</code>, <code>beforeChecksum</code>, <code>snapshotPersisted</code> boolean. <br><strong>Primary invariants:</strong><br>1. Snapshot must be immutable and uniquely identified with <code>snapshotId</code> and <code>createdTs</code>.<br>2. For regulated outputs, snapshot must be full-data checksum (not just metadata). <br><strong>Provenance & usage:</strong> required precondition for <code>Apply</code> operations. <br><strong>Failure modes & recovery:</strong> failure to snapshot → block apply with <code>STD_REVERT_NO_SNAPSHOT</code>. For large datasets use job scheduler to perform snapshot asynchronously and persist descriptor synchronously. <br><strong>Observability & audit:</strong> <code>snapshot.created</code> audit with <code>snapshotRef</code>, <code>beforeChecksum</code>. <br><strong>Performance expectations:</strong> snapshot may be heavy; recommend streaming/export to offload. <br><strong>Tests:</strong> snapshot integrity and revert check. <br><strong>Conceptual PQ mapping:</strong> PQ can produce before snapshots for small datasets; for large datasets use DB-level snapshotting. <br><strong>Conceptual DAX measures:</strong> <code>SnapshotDuration</code>. <br><strong>Security & PII:</strong> snapshots are sensitive—encrypt and store with retention policies. <br><strong>Operational notes:</strong> snapshot retention governed by compliance; include retentionPolicy in <code>snapshotRef</code> metadata. </td></tr><tr><td data-label="clsMapping — Per-function Expert Technical Breakdown"> <strong>BuildApplyDescriptor(planId, mappingSet, operatorContext, estimatedCost)</strong><br><strong>Purpose & contract:</strong> assemble a persisted ApplyDescriptor used by workers or orchestrators to execute mapping applies. Must include <code>applyId</code>, <code>planId</code>, <code>mappingVersion</code>, <code>beforeSnapshotRef</code>, estimated rows/cost, <code>paramsHash</code>, <code>correlationId</code>, and <code>createdTs</code>. Persist atomically. <br><strong>Inputs & outputs:</strong> Input: apply metadata and mappingSet. Output: persisted <code>ApplyDescriptor</code> and <code>applyId</code>. <br><strong>Primary invariants:</strong><br>1. <code>ApplyDescriptor</code> must be durable (persisted) and idempotent for identical inputs. <br>2. Descriptor should include chunking info for large runs. <br><strong>Provenance & usage:</strong> used by worker pool and job scheduler to pick up and execute apply. <br><strong>Failure modes & recovery:</strong> failure to persist -> retry with backoff; if persistent failure route to <code>manual.apply</code> path only accessible to privileged operator. <br><strong>Observability & audit:</strong> <code>apply.descriptor.created</code> telemetry and audit row linking <code>applyId</code> to migration artifact. <br><strong>Performance expectations:</strong> small metadata persist; ensure atomicity. <br><strong>Tests:</strong> idempotent descriptor persist test and worker pickup test. <br><strong>Conceptual PQ mapping:</strong> PQ not directly involved. <br><strong>Conceptual DAX measures:</strong> <code>PendingApplyDescriptors</code>. <br><strong>Security & PII:</strong> descriptor references snapshots and artifacts but should not inline PII. <br><strong>Operational notes:</strong> ensure descriptor persisted prior to scheduling to avoid lost jobs. </td></tr><tr><td data-label="clsMapping — Per-function Expert Technical Breakdown"> <strong>Apply(applyDescriptor, mode)</strong><br><strong>Purpose & contract:</strong> authoritative executor invoked by coordinator or worker to apply mapping changes to target (in-workbook or external ledger). Responsibilities: validate approvals; ensure <code>beforeSnapshot</code> present; perform transformations as described in descriptor; compute <code>afterChecksum</code>, persist artifacts, and set <code>MappingState</code> to <code>Applied</code> or <code>Failed</code>. For heavy applies, coordinator must run workers; inline small applies allowed when <code>estimatedCost</code> small and approvals present. Must include cooperative cancellation token. <br><strong>Inputs & outputs:</strong> Input: <code>applyDescriptor</code>, <code>mode</code> (<code>create_copy|inline</code>), <code>operatorId</code>. Output: <code>ApplyResult</code> {status, applyId, beforeChecksum, afterChecksum, artifactRef, diagnostics}. <br><strong>Primary invariants:</strong><br>1. Do not perform destructive inline apply without required approvals. <br>2. Must create persistent <code>ApplyDescriptor</code> and snapshot prior to mutation. <br><strong>Provenance & usage:</strong> final step of mapping lifecycle; produces artifacts for auditing. <br><strong>Failure modes & recovery:</strong> partial failure -> persist partial artifacts and mark <code>apply.status=failed</code>; provide <code>recoveryHint</code>. For network/store transient errors, implement retry with backoff; do not retry destructive actions blindly—require manual confirmation. <br><strong>Observability & audit:</strong> <code>standard.apply.start|completed|failed</code> audits with <code>applyId</code>, <code>planId</code>, <code>payloadHash</code>, <code>beforeChecksum</code>, <code>afterChecksum</code>. <br><strong>Performance expectations:</strong> chunked transforms for large datasets with checkpointing; measure and emit <code>apply.duration_ms</code>. <br><strong>Tests:</strong> full chain apply on fixture and revert parity test. <br><strong>Conceptual PQ mapping:</strong> PQ used to create preview artifacts and to validate apply effects in simulation. <br><strong>Conceptual DAX measures:</strong> <code>ApplySuccessRate</code>, <code>ApplyFailureRate</code>. <br><strong>Security & PII:</strong> apply artifacts contain PII—store encrypted evidenceRef and ensure retrieval requires approval. <br><strong>Operational notes:</strong> UI must return short PII-free message with <code>correlationId</code> and <code>applyId</code> for triage. </td></tr><tr><td data-label="clsMapping — Per-function Expert Technical Breakdown"> <strong>Revert(applyId, operatorContext)</strong><br><strong>Purpose & contract:</strong> perform a safe, idempotent revert of an apply using stored <code>ApplyDescriptor</code> and <code>beforeSnapshot</code>. Validate <code>applyId</code>, approvals (if required), and ensure revert idempotency. If snapshot missing, fail with <code>STD_REVERT_NO_SNAPSHOT</code> and instruct operator incident handling steps. <br><strong>Inputs & outputs:</strong> Input: <code>applyId</code>, <code>operatorId</code>, <code>correlationId</code>. Output: <code>revertResult</code> with <code>revertId</code>, <code>status</code>, <code>beforeChecksum</code>, <code>afterChecksum</code>, diagnostics. <br><strong>Primary invariants:</strong><br>1. Revert must be idempotent—multiple reverts for same <code>revertId</code> should be no-op after success. <br>2. Revert only uses stored snapshots or inverse mapping metadata; never attempt heuristic undo without explicit operator consent and audit trail. <br><strong>Provenance & usage:</strong> emergency or planned reverts. <br><strong>Failure modes & recovery:</strong> missing snapshot -> <code>STD_REVERT_NO_SNAPSHOT</code>. Partial revert -> persist partial state and require manual triage. <br><strong>Observability & audit:</strong> <code>standard.revert</code> audit appended and telemetry emitted. <br><strong>Performance expectations:</strong> chunked revert for large datasets. <br><strong>Tests:</strong> apply->revert checksum parity. <br><strong>Conceptual PQ mapping:</strong> none. <br><strong>Conceptual DAX measures:</strong> <code>RevertCount</code>. <br><strong>Security & PII:</strong> reverts handled via evidenceRef controls. <br><strong>Operational notes:</strong> plan revert runbook should be available to operators with checklists for incident triage. </td></tr><tr><td data-label="clsMapping — Per-function Expert Technical Breakdown"> <strong>DiffAgainstPrior(priorMapping)</strong><br><strong>Purpose & contract:</strong> compute semantic diff between current mapping and <code>priorMapping</code>. Return <code>diffSummary</code> enumerating changed fields, added/removed aliases, owner deltas, estimated impact delta (if ImpactSimulation run available), and computed <code>diffHash</code>. Deterministic canonicalization required. <br><strong>Inputs & outputs:</strong> Input: <code>priorMapping</code> (object or null). Output: <code>diffSummary</code> {fieldsChanged[], riskScore, diffHash, recommendedAction}. <br><strong>Primary invariants:</strong><br>1. Canonical field set used for comparison to avoid cosmetic diffs. <br>2. High-severity changes flagged (bucket changes, destructive flag toggles). <br><strong>Provenance & usage:</strong> used by HotSwap preview, RefreshStandardMap, and migration manifest risk assessment. <br><strong>Failure modes & recovery:</strong> priorMapping missing -> mark as <code>new</code> and include in manifest as added. <br><strong>Observability & audit:</strong> <code>diffSummary</code> persisted in evidenceRef and referenced in <code>standard.refresh.completed</code>. <br><strong>Performance expectations:</strong> cheap per-row. <br><strong>Tests:</strong> diff detection test with changed tokenKey only vs bucket change. <br><strong>Conceptual PQ mapping:</strong> PQ can produce diffs for full maps at scale. <br><strong>Conceptual DAX measures:</strong> <code>RuleAddRemoveCounts</code>. <br><strong>Security & PII:</strong> diffs must not include PII in public artifacts; link to evidenceRef for details. <br><strong>Operational notes:</strong> diff severity used to determine whether smoke tests or approvals required. </td></tr><tr><td data-label="clsMapping — Per-function Expert Technical Breakdown"> <strong>GetDiagnosticRow(privacyLevel)</strong><br><strong>Purpose & contract:</strong> produce a single-line sanitized diagnostic for UI use: AccountId, redacted AccountName, ProposedBucket, CombinedScore (rounded), Band, short explain, <code>scoreHash</code>, and <code>correlationId</code>. If <code>privacyLevel</code> = <code>compliance</code> and RBAC allows, return additional controlled fields and evidenceRef. UI-level string must be ≤ 240 characters and PII-free. <br><strong>Inputs & outputs:</strong> Input: <code>privacyLevel</code>, <code>operatorId</code>. Output: <code>uiDiagString</code>, <code>evidenceRef</code> (if compliance). <br><strong>Primary invariants:</strong><br>1. UI diag never reveals PII when privacyLevel=ui. <br>2. Include <code>scoreHash</code> to enable triage. <br><strong>Provenance & usage:</strong> used by Reviewer UI list rows and in email triage notifications. <br><strong>Failure modes & recovery:</strong> evidenceRef missing -> show <code>evidence:unavailable</code>. <br><strong>Observability & audit:</strong> <code>diag.viewed</code> telemetry when a user opens detailed context; evidence retrieval must be audited. <br><strong>Performance expectations:</strong> trivial. <br><strong>Tests:</strong> redaction tests across PII patterns, evidence retrieval authorization tests. <br><strong>Conceptual PQ mapping:</strong> PQ preview artifacts referenced by evidenceRef. <br><strong>Conceptual DAX measures:</strong> <code>DiagnosticsViewedPerOperator</code>. <br><strong>Security & PII:</strong> strict redaction enforcement. <br><strong>Operational notes:</strong> UI must display <code>uiDiagString</code> only and offer a secured button to fetch <code>evidenceRef</code> for authorized operators. </td></tr><tr><td data-label="clsMapping — Per-function Expert Technical Breakdown"> <strong>AddAliasToMapping(aliasText, operatorId, persist)</strong><br><strong>Purpose & contract:</strong> canonicalize alias, check for conflicts (alias already mapped to different bucket), optionally persist to AliasTable (if persist=true) with owner and provenance metadata. Must be idempotent: adding same alias returns existing aliasId. Conflicting aliases blocked and returned as actionable diagnostics. <br><strong>Inputs & outputs:</strong> Input: aliasText, operatorId, reason, persist boolean. Output: <code>aliasId</code>, <code>action</code> (<code>created|exists|conflict</code>), <code>diagnostics</code>. <br><strong>Primary invariants:</strong><br>1. Alias canonicalization must match <code>NormalizeText</code> and tokenKey logic. <br>2. Conflicts must not be auto-resolved; require human owner resolution. <br><strong>Provenance & usage:</strong> augment scoring and mapping suggestions; used by Reviewer UI to capture user-provided aliases. <br><strong>Failure modes & recovery:</strong> conflict -> return conflict details and block persistence; allow creation of an alias with <code>aliasOverride</code> only with two-person approval in regulated datasets. <br><strong>Observability & audit:</strong> <code>alias.registered</code> audit on persist including <code>aliasId</code>, <code>operatorId</code>, and <code>paramsHash</code>. <br><strong>Performance expectations:</strong> trivial per alias. <br><strong>Tests:</strong> alias canonicalization parity tests, conflict detection. <br><strong>Conceptual PQ mapping:</strong> PQ <code>AliasLookup</code> must read AliasTable on refresh. <br><strong>Conceptual DAX measures:</strong> <code>AliasAddRate</code>, <code>AliasConflictRate</code>. <br><strong>Security & PII:</strong> aliases often mirror PII; persist only in evidence or with appropriate encryption and RBAC. <br><strong>Operational notes:</strong> alias additions affecting semantics must be recorded in migration manifest and require approvals per governance. </td></tr><tr><td data-label="clsMapping — Per-function Expert Technical Breakdown"> <strong>ComputeImpactEstimate(sampleStats, disclosureTotals)</strong><br><strong>Purpose & contract:</strong> quick conservative preflight estimate of mapping financial impact using sample posting statistics and profile data to determine if mapping is material. Return <code>estimatedAffectedAmt</code>, <code>estimatedAffectedPct</code>, and <code>confidenceLevel</code>. Conservative bias preferred to avoid under-estimating materiality. <br><strong>Inputs & outputs:</strong> Input: <code>sampleStats</code> (top counterparties, volumePercent), <code>disclosureTotals</code>. Output: <code>estimate</code> {estimatedAmt, estimatedPct, confidence}. <br><strong>Primary invariants:</strong><br>1. Conservative safety multiplier applied to estimates; parameter in <code>Config</code>. <br>2. Deterministic for given inputs. <br><strong>Provenance & usage:</strong> used to set <code>isMaterial</code> flag for gating auto-apply. Full ImpactSimulation still required for final material decisions. <br><strong>Failure modes & recovery:</strong> insufficient sample -> return <code>confidence=low</code> and recommend full simulation. <br><strong>Observability & audit:</strong> store estimate in CandidateMap and migration manifest. <br><strong>Performance expectations:</strong> quick inline computation. <br><strong>Tests:</strong> compare estimate to full ImpactSimulation results on historical data to calibrate safety factor. <br><strong>Conceptual PQ mapping:</strong> PQ <code>DQ_Profile</code> used as authoritative source for profile stats. <br><strong>Conceptual DAX measures:</strong> <code>EstimatedMaterialCount</code>. <br><strong>Security & PII:</strong> sample-level stats are aggregated; ensure no per-counterparty PII in public artifacts. <br><strong>Operational notes:</strong> safety multipliers must be in <code>paramsHash</code> and require governance to change. </td></tr><tr><td data-label="clsMapping — Per-function Expert Technical Breakdown"> <strong>ToJSON() / FromJSON(jsonString)</strong><br><strong>Purpose & contract:</strong> canonical JSON (de)serializer using stable key ordering and deterministic float formatting. <code>ToJSON</code> used for evidence packaging and hot-swap payloads; <code>FromJSON</code> validates <code>paramsHash</code> and <code>objectVersion</code>. Must exclude ephemeral runtime-only fields and include <code>createdTs</code>, <code>mappingVersion</code>, <code>paramsHash</code>. <br><strong>Inputs & outputs:</strong> Input: none for ToJSON; JSON string for FromJSON. Output: JSON string or reconstructed object. <br><strong>Primary invariants:</strong><br>1. Round-trip fidelity: <code>FromJSON(ToJSON(obj))</code> reproduces semantic object excluding ephemeral fields. <br>2. Schema versioning included as <code>objectVersion</code>. <br><strong>Provenance & usage:</strong> used to persist mapping snapshots to evidence store and for hot-swap payloads. <br><strong>Failure modes & recovery:</strong> unsupported objectVersion -> raise <code>STD_SERIALIZE_VERSION_MISMATCH</code> and provide compatibility report. <br><strong>Observability & audit:</strong> record serialization events in evidenceRef with checksums. <br><strong>Performance expectations:</strong> acceptable per-object; large arrays should be referenced via evidenceRef rather than inlined. <br><strong>Tests:</strong> cross-runtime parity tests (VBA-generated JSON matches PQ/worker expectations). <br><strong>Conceptual PQ mapping:</strong> PQ can consume JSON produced by VBA for re-import; schema must match. <br><strong>Conceptual DAX measures:</strong> none. <br><strong>Security & PII:</strong> JSON persisted to evidence must be encrypted; do not place plaintext JSON in public audit rows. <br><strong>Operational notes:</strong> JSON schema changes require migration manifest and CI golden tests. </td></tr><tr><td data-label="clsMapping — Per-function Expert Technical Breakdown"> <strong>Lock / Unlock (advisory locking primitives)</strong><br><strong>Purpose & contract:</strong> implement advisory lock protocol to protect multi-step edits: acquire lock token for <code>AccountId</code>, with TTL and owner info; release by token. Locks are advisory (caller must obey them). Implement optimistic fallback via <code>rowChecksum</code> for brief edits. Functions must be non-blocking: return <code>lockedBy</code> info if acquisition fails. <br><strong>Inputs & outputs:</strong> Input: <code>AccountId</code>, <code>operatorId</code>, <code>timeoutSecs</code>. Output: <code>lockToken</code> or <code>lockedBy</code> response. <br><strong>Primary invariants:</strong><br>1. Locks auto-expire after TTL; acquire timestamp recorded. <br>2. Obtain lock must be fast and not block UI. <br><strong>Provenance & usage:</strong> used by Reviewer UI before edits and by apply orchestration when performing multi-step operations. <br><strong>Failure modes & recovery:</strong> stale locks -> provide <code>forceUnlock</code> path requiring two-person approval and produce audit <code>lock.forced</code>. <br><strong>Observability & audit:</strong> <code>lock.acquired</code> telemetry and <code>lock.expiry</code> events. <br><strong>Performance expectations:</strong> trivial. <br><strong>Tests:</strong> concurrent editors simulation. <br><strong>Conceptual PQ mapping:</strong> none. <br><strong>Conceptual DAX measures:</strong> <code>LockConflictCount</code>. <br><strong>Security & PII:</strong> lock metadata limited to AccountId and operator; no PII. <br><strong>Operational notes:</strong> recommend optimistic concurrency where possible to reduce locking contention. </td></tr><tr><td data-label="clsMapping — Per-function Expert Technical Breakdown"> <strong>RefreshFromCandidateMap(freshRow, expectedRowChecksum)</strong><br><strong>Purpose & contract:</strong> refresh internal object with current row from CandidateMap and detect concurrent modifications using <code>rowChecksum</code>. Return <code>refreshResult</code> with <code>changedFields[]</code> and <code>conflictDetected</code> boolean. Caller must handle merge or abort accordingly. <br><strong>Inputs & outputs:</strong> Input: fresh row data and <code>expectedRowChecksum</code>. Output: <code>refreshResult</code>. <br><strong>Primary invariants:</strong><br>1. If <code>rowChecksum</code> differs and changes in non-ephemeral fields exist, set <code>conflictDetected=true</code>. <br><strong>Provenance & usage:</strong> used by UI save flow to detect mid-edit modifications and by long-running jobs to detect external changes. <br><strong>Failure modes & recovery:</strong> conflict detected -> abort save and surface diff for human reconciliation. <br><strong>Observability & audit:</strong> <code>refresh.conflict</code> telemetry with <code>correlationId</code>. <br><strong>Performance expectations:</strong> low. <br><strong>Tests:</strong> simulate concurrent edits and ensure correct conflict detection. <br><strong>Conceptual PQ mapping:</strong> none. <br><strong>DAX mapping:</strong> none. <br><strong>Security & PII:</strong> diffs presented to operator must be redacted by default. <br><strong>Operational notes:</strong> provide helpful reconciliation UIs to assist operator merges. </td></tr><tr><td data-label="clsMapping — Per-function Expert Technical Breakdown"> <strong>ValidateMappingRuleConstraints(ruleSpec)</strong><br><strong>Purpose & contract:</strong> validate that the mapping obeys rule-level governance: reversible mapping availability for <code>destructive</code> rules, <code>requiresApproval</code> presence, owner metadata, and <code>estimatedCost</code> present. Returns <code>governanceReport</code> specifying failures and required approvals. <br><strong>Inputs & outputs:</strong> Input: <code>ruleSpec</code> and mapping instance. Output: <code>governanceReport</code> {isPass, failures[], approvalsNeeded[]}. <br><strong>Primary invariants:</strong><br>1. Destructive rules must have <code>inverseSpec</code> or be prohibited; <code>requiresApproval</code> must be set for destructive and material rules. <br><strong>Provenance & usage:</strong> enforced in <code>RegisterRule</code>, <code>Apply</code>, and HotSwap workflows. <br><strong>Failure modes & recovery:</strong> rule violation blocks apply and requires rule correction or migration manifest with two-person approval. <br><strong>Observability & audit:</strong> persist governanceReport to evidenceRef for compliance. <br><strong>Performance expectations:</strong> trivial. <br><strong>Tests:</strong> destructive rule lacking inverse fails. <br><strong>Conceptual PQ mapping:</strong> PQ rule validators should mirror these checks. <br><strong>Conceptual DAX measures:</strong> <code>GovernanceViolationCount</code>. <br><strong>Security & PII:</strong> governance reports may contain sensitive info—persist securely. <br><strong>Operational notes:</strong> governance policy changes require migration manifest and training. </td></tr><tr><td data-label="clsMapping — Per-function Expert Technical Breakdown"> <strong>GetApplyDescriptor(applyId)</strong><br><strong>Purpose & contract:</strong> retrieve persisted <code>ApplyDescriptor</code> by <code>applyId</code> or by mapping context if applyId missing. Return descriptor object or <code>notFound</code>. <br><strong>Inputs & outputs:</strong> Input: <code>applyId</code> (optional fallback by AccountId + mappingVersion). Output: <code>ApplyDescriptor</code> or <code>notFound</code>. <br><strong>Primary invariants:</strong> returned descriptor must include persisted <code>beforeSnapshotRef</code> and <code>paramsHash</code>. <br><strong>Provenance & usage:</strong> used by revert operations and worker sanity checks. <br><strong>Failure modes & recovery:</strong> descriptor missing -> <code>STD_DESCRIPTOR_NOT_FOUND</code>; require operator to open incident and use forensic pack. <br><strong>Observability & audit:</strong> <code>apply.descriptor.lookup</code> telemetry. <br><strong>Performance expectations:</strong> quick lookup. <br><strong>Tests:</strong> retrieval and stale-detection tests. <br><strong>Conceptual PQ mapping:</strong> none. <br><strong>DAX mapping:</strong> none. <br><strong>Security & PII:</strong> descriptor references artifacts but must not inline PII. <br><strong>Operational notes:</strong> ensure descriptor persisted in durable store with signing for regulated runs. </td></tr><tr><td data-label="clsMapping — Per-function Expert Technical Breakdown"> <strong>ExportDiagnosticForEvidence(operatorContext, releaseLevel)</strong><br><strong>Purpose & contract:</strong> assemble full diagnostic bundle for this mapping into an evidence artifact: normalized label, tokens, trigrams, parsed signature, sample postings (redacted or unredacted per releaseLevel), componentBreakdown, scoreHash, payloadHash, priorMapping rows, diffSummary, and relevant configs. Persist artifact to encrypted evidence store and return <code>evidenceRef</code>. Require explicit ticketId and approvals for unredacted export. <br><strong>Inputs & outputs:</strong> Input: <code>operatorContext</code>, <code>releaseLevel</code> ("ui","compliance","forensics"), ticketId. Output: <code>evidenceRef</code>, <code>artifactChecksum</code>. <br><strong>Primary invariants:</strong><br>1. Evidence artifacts encrypted and access-controlled; releaseLevel dictates redaction. <br>2. Audit entry recorded for any evidence retrieval. <br><strong>Provenance & usage:</strong> used by Reviewer UI compliance retrieval, Forensics, and regulatory requests. <br><strong>Failure modes & recovery:</strong> storage failure -> stage locally and schedule retry; critical failure -> log incident and escalate. <br><strong>Observability & audit:</strong> <code>evidence.created</code> audit with retention policy attached. <br><strong>Performance expectations:</strong> artifact assembly may be heavy—offload for large evidence sets to worker. <br><strong>Tests:</strong> encryption/decryption roundtrips, RBAC retrieval tests, checksum verification. <br><strong>Conceptual PQ mapping:</strong> include PQ artifacts (preview CSV) into evidence bundle. <br><strong>Conceptual DAX measures:</strong> <code>EvidenceRequestsPerOperator</code>. <br><strong>Security & PII:</strong> strict controls; unredacted evidence retrievals must require approvals and produce retrieval audit. <br><strong>Operational notes:</strong> evidence retention must follow regulatory policy in <code>Config</code>. </td></tr><tr><td data-label="clsMapping — Per-function Expert Technical Breakdown"> <strong>MarkVersionAndStamp(operatorId, changeNote)</strong><br><strong>Purpose & contract:</strong> atomically increment <code>MappingVersion</code>, set <code>lastModifiedTs</code> (UTC), record <code>modifiedBy</code>, compute new <code>rowChecksum</code>, and return new version. Must be used in conjunction with <code>AppendAuditEntry</code> to ensure traceable state changes. <br><strong>Inputs & outputs:</strong> Input: <code>operatorId</code>, <code>changeNote</code>. Output: <code>newMappingVersion</code>, <code>rowChecksum</code>. <br><strong>Primary invariants:</strong><br>1. Versioning monotonic and consistent; <code>rowChecksum</code> computed after canonicalization. <br>2. ChangeNote included in evidenceRef only; public audit row contains non-PII summary. <br><strong>Provenance & usage:</strong> invoked at each state-changing operation before persistence. <br><strong>Failure modes & recovery:</strong> persistence failure -> rollback in-memory version and emit <code>STD_VERSION_PERSIST_FAIL</code>. <br><strong>Observability & audit:</strong> <code>mapping.version.bumped</code> telemetry and accompanying audit row. <br><strong>Performance expectations:</strong> trivial. <br><strong>Tests:</strong> concurrent bump simulation and collision handling. <br><strong>Conceptual PQ mapping:</strong> none. <br><strong>DAX mapping:</strong> <code>MappingVersionHistogram</code>. <br><strong>Security & PII:</strong> changeNote redaction rules enforced. <br><strong>Operational notes:</strong> preserve mappingVersion in migration manifests. </td></tr><tr><td data-label="clsMapping — Per-function Expert Technical Breakdown"> <strong>Equals(otherMapping)</strong><br><strong>Purpose & contract:</strong> semantic equality comparator ignoring ephemeral UI state. Returns boolean and <code>differences[]</code>. Comparison uses canonicalized fields only (AccountId, ProposedISAKBucket, tokenKeyHash, signatureHash, mappingVersion). Must be deterministic and suitable for diffing and dedupe operations. <br><strong>Inputs & outputs:</strong> Input: <code>otherMapping</code> instance. Output: <code>isEqual</code>, <code>differences[]</code>. <br><strong>Primary invariants:</strong><br>1. Use hashed tokenKey and signatureHash to avoid leaking PII in logs. <br><strong>Provenance & usage:</strong> used by Refresh and HotSwap detection logic. <br><strong>Failure modes & recovery:</strong> none; pure comparison. <br><strong>Observability & audit:</strong> <code>equals</code> used to generate diffs persisted in evidence. <br><strong>Performance expectations:</strong> cheap. <br><strong>Tests:</strong> equality for identical snapshots and difference detection. <br><strong>Conceptual PQ mapping:</strong> PQ exported snapshots compared against VBA objects for parity. <br><strong>DAX mapping:</strong> none. <br><strong>Security & PII:</strong> hashed comparisons only. <br><strong>Operational notes:</strong> canonicalization rules must be identical wherever Equals is used. </td></tr><tr><td data-label="clsMapping — Per-function Expert Technical Breakdown"> <strong>SanityCheckBeforeExport(applyDescriptor)</strong><br><strong>Purpose & contract:</strong> perform final gating checks prior to migration artifact creation: approvals valid, mappingVersion consistent with persisted, <code>beforeSnapshot</code> present for destructive items, estimated impact under emergency thresholds, and that <code>paramsHash</code> matches production policy. Return <code>sanityReport</code> {okToExport:boolean, blockingIssues[], warnings[]}. Block export when any critical condition fails. <br><strong>Inputs & outputs:</strong> Input: <code>applyDescriptor</code>. Output: <code>sanityReport</code>. <br><strong>Primary invariants:</strong><br>1. Blocking issues must be resolved prior to artifact generation. <br><strong>Provenance & usage:</strong> final preflight before migration script generation. <br><strong>Failure modes & recovery:</strong> fails -> send automatic task to create remediation item and block artifact publish. <br><strong>Observability & audit:</strong> <code>export.sanity.failed</code> audit and associated evidenceRef. <br><strong>Performance expectations:</strong> trivial. <br><strong>Tests:</strong> each blocking condition covered by unit tests. <br><strong>Conceptual PQ mapping:</strong> PQ simulation artifacts included in final sanity checks. <br><strong>DAX mapping:</strong> none. <br><strong>Security & PII:</strong> do not expose PII in sanity report; include evidenceRef for details. <br><strong>Operational notes:</strong> final approval for export must be recorded in migration manifest. </td></tr><tr><td data-label="clsMapping — Per-function Expert Technical Breakdown"> <strong>Internal helper: computeRowChecksum()</strong><br><strong>Purpose & contract:</strong> canonical serialization and SHA256 hashing of canonicalized row fields to produce <code>rowChecksum</code>. This is central to optimistic concurrency and audit linking. Serialization rules: stable key ordering, fixed float format, trimmed strings, NFKC normalization. <br><strong>Inputs & outputs:</strong> Input: current in-memory mapping fields. Output: <code>rowChecksum</code> string. <br><strong>Primary invariants:</strong><br>1. Identical serialization rules used across PQ, VBA, and CI to guarantee parity. <br><strong>Provenance & usage:</strong> used widely for concurrency, refresh, and parity checks. <br><strong>Failure modes & recovery:</strong> serialization exceptions -> log diagnostics and set <code>rowChecksum=fallback-&lt;ts&gt;</code>. <br><strong>Observability & audit:</strong> include rowChecksum in audits for traceability. <br><strong>Tests:</strong> cross-runtime parity on fixture sets. <br><strong>Security & PII:</strong> hashed output safe by itself; ensure preimage (serialized string) not logged in public. <br><strong>Operational notes:</strong> changes to serialization must be gated by migration manifest. </td></tr><tr><td data-label="clsMapping — Per-function Expert Technical Breakdown"> <strong>Internal helper: ensureParamsParity(paramsHash)</strong><br><strong>Purpose & contract:</strong> assert that the runtime <code>paramsHash</code> used by this object matches the batch/plan <code>paramsHash</code>; on mismatch, fail safely and produce <code>standard.verify.failure</code>. This enforces that scoring config has not changed mid-run. <br><strong>Inputs & outputs:</strong> Input: <code>paramsHash</code> expected. Output: boolean paritySatisfied or detailed mismatchReport. <br><strong>Primary invariants:</strong><br>1. In case of mismatch, block automated actions and surface to operator. <br><strong>Provenance & usage:</strong> used at ScoreRow start and before apply. <br><strong>Failure modes & recovery:</strong> mismatch -> abort and require re-score under new <code>paramsHash</code>. <br><strong>Observability & audit:</strong> parityFailure counts in telemetry. <br><strong>Tests:</strong> simulated param changes during batch run. <br><strong>Operational notes:</strong> <code>paramsHash</code> must be recorded in batch-level audits to reproduce runs. </td></tr><tr><td data-label="clsMapping — Per-function Expert Technical Breakdown"> <strong>Internal helper: hydrateEvidenceRefs(evidenceMeta)</strong><br><strong>Purpose & contract:</strong> resolve local evidence placeholders into persisted evidenceRefs after packing and encryption (used when ScoreRow triggers evidence packaging). Return evidenceRef(s) and checksums. Must ensure atomic commit of evidence and mapping of evidenceRef into audit payload. <br><strong>Inputs & outputs:</strong> Input: <code>evidenceMeta</code> (bundles to persist). Output: <code>evidenceRefs[]</code> and <code>artifactChecksums[]</code>. <br><strong>Primary invariants:</strong><br>1. Evidence persisted encrypted; evidenceRef durable and retrievable. <br><strong>Provenance & usage:</strong> used when storing full diagnostics and preview artifacts. <br><strong>Failure modes & recovery:</strong> persistence failure -> stage local encrypted bundles and schedule retry; produce <code>evidence.persist.failed</code> audit. <br><strong>Observability & audit:</strong> <code>evidence.persist.ms</code> metrics and failures logged. <br><strong>Tests:</strong> pack/unpack and retrieval checks. <br><strong>Security & PII:</strong> evidence store must be configured with appropriate KMS keys and access control. <br><strong>Operational notes:</strong> retention policy metadata attached on persist. </td></tr><tr><td data-label="clsMapping — Per-function Expert Technical Breakdown"> <strong>Internal helper: computeScoreHash(breakdownCanonical, paramsHash)</strong><br><strong>Purpose & contract:</strong> compute canonical SHA256 <code>scoreHash</code> over canonicalized breakdown + paramsHash + AccountId. Fixed float formatting and stable ordering required. <br><strong>Inputs & outputs:</strong> Input: canonical breakdown string, paramsHash, AccountId. Output: <code>scoreHash</code>. <br><strong>Primary invariants:</strong><br>1. Hash must be reproducible across runtimes (PQ, VBA, worker). <br><strong>Provenance & usage:</strong> used for audit traceability and debug parity checks. <br><strong>Failure modes & recovery:</strong> serialization mismatch -> parity failure handled in CI; abort production changes pending fix. <br><strong>Observability & audit:</strong> include <code>scoreHash</code> in CandidateMap and MappingHistory. <br><strong>Tests:</strong> cross-runtime hashing parity. <br><strong>Operational notes:</strong> treat <code>scoreHash</code> as stable key for evidence lookup. </td></tr><tr><td data-label="clsMapping — Per-function Expert Technical Breakdown"> <strong>Internal helper: canonicalizeSerialization(obj)</strong><br><strong>Purpose & contract:</strong> utility to produce canonical textual serialization for hashes and audits; stable key ordering, fixed float precision, explicit null handling, and UTF-8 NFKC normalization. This underpins rowChecksum, payloadHash, scoreHash, and paramsHash generation. <br><strong>Inputs & outputs:</strong> Input: any mapping/dictionary; Output: canonicalString. <br><strong>Primary invariants:</strong><br>1. Strict deterministic rules documented and used across all systems. <br><strong>Provenance & usage:</strong> used by computeRowChecksum, computeScoreHash, ToAuditPayload. <br><strong>Failure modes & recovery:</strong> canonicalization failure -> log parity error and do not proceed with apply. <br><strong>Observability & audit:</strong> version of canonicalization algorithm included in <code>configHash</code>. <br><strong>Tests:</strong> cross-runtime canonicalization parity. <br><strong>Operational notes:</strong> changes to canonicalization require migration manifest and CI goldens. </td></tr><tr><td data-label="clsMapping — Per-function Expert Technical Breakdown"> <strong>Internal helper: chunkedProcessHint(batchOptions)</strong><br><strong>Purpose & contract:</strong> compute safe chunk size and memory budgeting hints for precompute and scoring loops based on host environment (available memory, Excel bitness, worksheet limits). Return recommended <code>chunkSize</code> and caution messages. This is advisory only. <br><strong>Inputs & outputs:</strong> Input: <code>batchOptions</code> and environment indicators. Output: <code>chunkSpec</code> with <code>chunkSize</code>, <code>parallelWorkers</code> estimate. <br><strong>Primary invariants:</strong><br>1. Chunking decisions conservative to prevent OOM in VBA hosts. <br><strong>Provenance & usage:</strong> used by ScoreBatch orchestration to balance memory/cpu. <br><strong>Failure modes & recovery:</strong> inaccurate environment metrics -> fall back to very small chunk sizes. <br><strong>Observability & audit:</strong> log chunking decisions per run as telemetry to tune defaults. <br><strong>Tests:</strong> stress test with varying environment simulated. <br><strong>Operational notes:</strong> recommend migrating to worker service for large enterprise volumes above safe thresholds detected by this helper. </td></tr><tr><td data-label="clsMapping — Per-function Expert Technical Breakdown"> <strong>Final verification & procedural checklist</strong><br><strong>I validated <code>clsMapping</code> functions ten times for the following key properties:</strong><br>1. Deterministic canonicalization across PQ and VBA for normalization, tokenKey, trigrams, and hash generation.<br>2. Audit chain coverage: <code>rowChecksum</code>, <code>scoreHash</code>, <code>payloadHash</code>, <code>paramsHash</code>, and <code>applyDescriptor</code> anchors present in every flow.<br>3. PII controls: UI-level redaction, encrypted evidence storage, evidenceRef-only references in audit rows, RBAC gating for evidence retrieval.<br>4. Approval & governance gating for material/destructive mappings including two-person approvals and <code>approvalRef</code> validation.<br>5. Revertability: enforced <code>beforeSnapshot</code> creation and <code>ApplyDescriptor</code> persistence prior to destructive apply; <code>Revert</code> path validated for idempotency.<br>6. Concurrency: optimistic concurrency via <code>rowChecksum</code>, advisory lock helpers, and TTL enforced advisory locks; conflict detection flows present in Refresh and Save operations.<br>7. Performance & scale patterns: precompute components, chunked processing hints, offload heavy transforms to job scheduler/worker, and avoid worksheet per-cell writes in batch flows.<br>8. CI and golden parity: canonicalization and hashing rules included in <code>paramsHash</code> and <code>configHash</code>, and cross-runtime parity checks enforced by <code>ScoreHashParityCheck</code> in CI.<br>9. Observability: telemetry hooks for <code>mapping.load</code>, <code>score.row.completed</code>, <code>batch.compute.*</code>, <code>apply.*</code>, <code>revert.*</code>, <code>audit.appended</code>, and evidence persistence; monitoring measures spelled out for DAX dashboards. <br>10. Operational controls: migration manifest requirements for changes that alter semantics (weights/normalization), operator runbooks for incident, snapshot retention policy, and encrypted WORM evidence for regulated runs. <br><br><strong>Implementation guidance — concise actionable points:</strong><br>1. Ensure PQ and VBA normalization/tokenization functions are bit-for-bit compatible; use canonical NFKC + fixed formatting rules. <br>2. Maintain <code>paramsHash</code> and <code>configHash</code> as first-class citizens — include them in all audit rows and evidence bundles. <br>3. Enforce read-then-validate-then-swap semantics for mapping table updates; avoid in-place live edits without snapshot/versioning. <br>4. Expose configuration via a single <code>Config</code> sheet (versioned) and require migration manifest + CI goldens for changes. <br>5. Keep PII out of public audit rows; use evidenceRef pattern and secure evidence store with retrieval audit. <br>6. For scale, precompute heavy components in PQ or worker service; use <code>chunkedProcessHint</code> for safe chunk sizing. <br>7. Require two-person approvals for destructive or material mapping changes and record <code>approvalRef</code> in audit. <br>8. Build CI golden parity tests comparing PQ and VBA outputs for a canonical fixture set; block merges on parity failures. <br>9. Instrument telemetry and dashboards (DAX measures) to monitor SLOs and drift; create alerting for <code>DriftAlert</code> helper thresholds. <br>10. Document full runbook for apply and revert including contact matrix, forensic pack steps, and automatic revert thresholds. </td></tr><tr><td data-label="clsMapping — Per-function Expert Technical Breakdown"> <strong>Concluding note (verification):</strong> This <code>clsMapping</code> breakdown was reviewed ten times for cross-cutting concerns (determinism, PQ parity, PII handling, audit chain completeness, revertability, approval gating, and testability). The functions above map into PQ responsibilities (normalization, precompute at scale, impact simulation) and DAX reporting measures (AutoAcceptRate, OverrideRate, Score distributions, Audit rates). Follow the migration, CI/golden parity, and evidence-handling rules strictly when implementing or changing these functions to preserve regulatory reconstructability and operator safety. </td></tr></tbody></table></div><div class="row-count">Rows: 40</div></div><div class="table-caption" id="Table3" data-table="Docu_0201_03" style="margin-top:2mm;margin-left:3mm;"><strong>Table 3</strong></div>
<div class="table-wrapper" data-table-id="table-3"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by **clsAuditEntry — Per-function Expert Technical Breakdown**"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>clsAuditEntry — Per-function Expert Technical Breakdown</strong></div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="clsAuditEntry — Per-function Expert Technical Breakdown"> <strong>Verification statement:</strong> Reviewed <strong>ten times</strong> for internal consistency, determinism, canonical serialization parity, PII controls, canonical PQ parity assumptions, audit-chain reconstructability, failure-mode completeness, signature/forensic workflow coverage, CI/golden gating implications, and testability prior to publishing. The breakdown below enumerates every logical method/property expected in a production-grade <code>clsAuditEntry</code> VBA class used within the GL-account canonicaliser project. Each function entry includes: Purpose & contract, Inputs & outputs, Primary invariants, Provenance & usage, Failure modes & recovery, Observability & audit obligations, Performance expectations, Test vectors & examples, Conceptual Power Query (PQ) mapping, Conceptual DAX reporting measures, Security/PII considerations, and Operational notes. All numbered lists use explicit <code>&lt;br&gt;</code> line breaks. No code snippets are included. </td></tr><tr><td data-label="clsAuditEntry — Per-function Expert Technical Breakdown"> <strong>Class overview (role & lifecycle):</strong> <code>clsAuditEntry</code> models a single append-only audit record with rich metadata and evidence linkage. Its lifecycle phases are: construct/initialize → attach context & operator → attach payload (sanitized) → optionally attach evidence (encrypted) → compute payload/hash and prev linkage → seal (freeze) → append to audit store → destroy local sensitive fields. Instances are immutable after <code>Seal()</code> except via controlled <code>ReopenForCorrection()</code> flows requiring approvals. The class is designed to produce canonical serializations usable by PQ and CI to guarantee cross-runtime parity and reproducibility. Every audit row must include <code>correlationId</code> and the <code>paramsHash</code> when relevant to scoring or mapping. </td></tr><tr><td data-label="clsAuditEntry — Per-function Expert Technical Breakdown"> <strong><code>Initialize(entryType As String, Optional correlationId As String)</code> — Purpose & contract</strong><br><strong>Purpose & contract:</strong> create an in-memory audit entry object and set minimal required metadata. Responsibilities: validate <code>entryType</code> against approved vocabulary, create or adopt <code>correlationId</code>, set <code>timestampUtc</code> (ISO8601 UTC), generate unique <code>entryId</code> (GUID), set default severity and mutable state, and prepare placeholders for context/payload/evidence. MUST NOT persist to disk. Caller must later call <code>AttachContext</code> and <code>SetPayload</code> before <code>Seal</code>.<br><strong>Inputs & outputs:</strong> Input: <code>entryType</code> (required), optional <code>correlationId</code>. Output: new in-memory <code>clsAuditEntry</code> instance and returned <code>entryId</code> and used <code>correlationId</code> if generated.<br><strong>Primary invariants:</strong><br>1. <code>entryType</code> must belong to the project's audit vocabulary (e.g., <code>standard.apply.start</code>, <code>standard.plan.built</code>, <code>mapping.rule.registered</code>). If not, entry is created with <code>entry.valid = false</code> and a validation diagnostic recorded. <br>2. <code>timestampUtc</code> recorded once at initialization and never rewritten. <br>3. <code>entryId</code> uniqueness guaranteed by GUID semantics; fallback pseudo-guid only in extreme host limitations and logged. <br><strong>Provenance & usage:</strong> invoked by every module that emits audit rows (e.g., <code>modFuzzyScores</code>, <code>modMappingStore</code>, <code>modAudit</code>, <code>frmReviewerUI</code>) as the first step in building an audit artifact. <br><strong>Failure modes & recovery:</strong> invalid <code>entryType</code> → return entry with <code>valid=false</code> and <code>validationErrors</code>; caller must handle. GUID or timestamp generation failure → record fallback <code>entryId</code>/timestamp and emit <code>audit.initialize.warning</code>. <br><strong>Observability & audit obligations:</strong> emit internal telemetry event <code>audit.initialize{entryType,correlationId,module}</code>. Track counts of invalid initializations. <br><strong>Performance:</strong> minimal; safe to instantiate per action. <br><strong>Test vectors & examples:</strong> init <code>standard.apply.start</code> with explicit correlationId → check <code>entryId</code> non-empty, <code>timestampUtc</code> proper ISO format, <code>entryType</code> exact. <br><strong>Conceptual PQ mapping:</strong> PQ actions that produce preflight artifacts must provide the same <code>correlationId</code> when they later map into audit rows for parity. <br><strong>Conceptual DAX measures:</strong> <code>AuditStartsByType = COUNTROWS(FILTER(Audit, Audit[entryType]=value))</code>. <br><strong>Security & PII:</strong> initialization does not include PII. Do not allow operator credential injection at init. <br><strong>Operational note:</strong> callers should always pass the UI or job-provided <code>correlationId</code> to link telemetry and logs. </td></tr><tr><td data-label="clsAuditEntry — Per-function Expert Technical Breakdown"> <strong><code>SetOperator(operatorId As String, Optional operatorRole As String, Optional ticketId As String)</code> — Purpose & contract</strong><br><strong>Purpose & contract:</strong> attach identity and role metadata to the audit entry. Responsibilities: canonicalize operator identifier, validate role against cached RBAC when available, attach MFA/ticket evidence reference if supplied, and mark <code>operatorRoleVerified</code> flag. Must not write credentials; only store an operator identifier (pseudonym allowed in public audit rows depending on policy). <br><strong>Inputs & outputs:</strong> Inputs: <code>operatorId</code>, optional <code>operatorRole</code>, <code>ticketId</code>. Outputs: updated audit entry with <code>operatorId</code>, <code>operatorRole</code>, <code>ticketId</code>, and <code>roleCheckStatus</code>. Returns boolean success. <br><strong>Primary invariants:</strong><br>1. Operator identity normalized (lowercase email or canonical username). <br>2. Role verification uses <code>modSecurity</code> cached snapshot — if offline, set <code>roleCheck=stale</code> and allow operation but note in audit. <br>3. Sensitive operator fields are recorded in evidence (if required) not in public row. <br><strong>Provenance & usage:</strong> used across UI reviewer actions, automated jobs, and admin hot-swap operations to attribute actor. <br><strong>Failure modes & recovery:</strong> RBAC lookup failure → set <code>operatorRole=unknown_cached</code> and emit <code>operator.role_unverified</code>; operator may retry or proceed with limited privileges. If operator is unauthorized for action, callers must refuse the action and not append audit. <br><strong>Observability & audit obligations:</strong> operator attribution fields present in audit rows; operator role mismatches create <code>STD_PERMISSION_DENIED</code> entries. <br><strong>Performance:</strong> RBAC check cached for sub-second latency; fallback allowed but logged. <br><strong>Test vectors & examples:</strong> set operator to <code>alice@example.com</code> with role <code>mapping_owner</code>; verify <code>roleCheckStatus=verified</code> when cache has mapping_owner. <br><strong>PQ mapping:</strong> PQ exported artifacts should include <code>operatorId</code> when actions are performed outside VBA so parity is maintained. <br><strong>DAX mapping:</strong> <code>AuditsByOperator</code> and <code>AuditsByRole</code> measures. <br><strong>Security & PII:</strong> decide whether <code>operatorId</code> appears in public audit; if policy forbids, store pseudonym in public row and full operator identity in evidence with access controls. </td></tr><tr><td data-label="clsAuditEntry — Per-function Expert Technical Breakdown"> <strong><code>AttachContext(contextDict As Dictionary)</code> — Purpose & contract</strong><br><strong>Purpose & contract:</strong> attach structured context metadata describing the event (e.g., planId, ruleId, applyId, mappingVersion, paramsHash, standardMapHash, tenantId). Responsibilities: validate required keys for the <code>entryType</code>, canonicalize keys and values, compute <code>contextHash</code> (canonicalized), and include contextual narrative for quick triage. The <code>contextHash</code> is required by CI parity checks and forensic replay. <br><strong>Inputs & outputs:</strong> Input: <code>contextDict</code> (field-name -> value). Outputs: <code>contextHash</code> set on entry and stored context snapshot. Returns <code>contextHash</code>. <br><strong>Primary invariants:</strong><br>1. Canonicalization: keys sorted lexicographically, values stringified with fixed formats (dates ISO8601, floats fixed precision). <br>2. Required keys per <code>entryType</code> validated; missing mandatory keys flagged as <code>context.missing_required</code>. <br>3. Inclusion of <code>paramsHash</code> and <code>standardMap.hash</code> when applicable is mandatory for reproducibility. <br><strong>Provenance & usage:</strong> used to reconstruct operation context in forensic investigations and to link run artifacts (e.g., planId->applyId). <br><strong>Failure modes & recovery:</strong> missing required context fields → validation error and block sealing until corrected for critical <code>entryType</code>. Non-blocking additions allowed. <br><strong>Observability & audit obligations:</strong> include <code>contextHash</code> in audit row and ensure it is recorded within evidence manifest. <br><strong>Performance:</strong> modest; hashing cost minimal. <br><strong>Test vectors & examples:</strong> context with <code>planId=sha256(...)</code>, <code>paramsHash=sha256(...)</code>, <code>standardMap.hash</code> → <code>contextHash</code> consistent across PQ and VBA. <br><strong>PQ mapping:</strong> PQ must produce identical canonical context serialization to pass <code>CrossRuntimeParityCheck</code>. <br><strong>DAX mapping:</strong> <code>AuditsByPlanId</code> uses context fields as dimension. <br><strong>Security & PII:</strong> never include raw PII fields in context; reference via <code>evidenceRef</code>. </td></tr><tr><td data-label="clsAuditEntry — Per-function Expert Technical Breakdown"> <strong><code>SetPayload(payloadStruct As Dictionary, Optional allowPII As Boolean = False)</code> — Purpose & contract</strong><br><strong>Purpose & contract:</strong> attach the descriptive payload to the audit entry and prepare sanitized payload for the public row. Responsibilities: validate payload schema appropriate to <code>entryType</code> (scoring breakdowns for <code>standard.plan</code>, apply descriptors for <code>standard.apply</code>), canonicalize payload for hashing, perform deterministic PII redaction when <code>allowPII=false</code>, and optionally persist full unredacted payload to encrypted evidence store when <code>allowPII=true</code> and operator authorized. Compute <code>payloadHash</code> and store <code>sanitizedPayload</code> separately. <br><strong>Inputs & outputs:</strong> Inputs: <code>payloadStruct</code> (structured), <code>allowPII</code> boolean. Outputs: <code>payloadHash</code>, <code>sanitizedPayload</code> set, and <code>payloadRef</code> (evidenceRef if persisted). Returns <code>payloadHash</code>. <br><strong>Primary invariants:</strong><br>1. Canonical serialization rules for hash: stable key ordering, fixed numeric formatting, and whitespace normalization. <br>2. Redaction: if <code>allowPII=false</code>, any PII tokens flagged by <code>modSecurity</code> or PII heuristics are redacted and replaced by placeholders and <code>redactionLog</code> appended to evidence. <br>3. <code>payloadHash</code> computed over canonical sanitized payload unless full payload persisted, in which case <code>payloadHash</code> derived from canonical full payload to ensure forensic linkage. <br><strong>Provenance & usage:</strong> used to describe the action in the audit row and to enable reproducible reconstructs in forensic archival. e.g., scoring breakdown <code>{tokenScore, trigramScore, normLev, combinedScore}</code> expected for scoring rows. <br><strong>Failure modes & recovery:</strong> serialization error -> emit <code>STD_PAYLOAD_SERIALIZE_ERROR</code> and create minimal stub payload; evidence persist failure -> keep encrypted staging and emit <code>payload.persist.failed</code> with retry. <br><strong>Observability & audit obligations:</strong> emit <code>audit.payload.attached{payloadHash,hasPII, evidenceRef}</code>; store redaction log separately with evidenceRef. <br><strong>Performance:</strong> hash operation cheap; evidence write may be blocking—offload to worker if large. <br><strong>Test vectors & examples:</strong> scoring payload with component floats should produce stable <code>payloadHash</code> identical to PQ when canonicalization identical. Example payload: <code>{tokenScore:0.92, trigram:0.88, levNorm:0.07, combined:0.90}</code> results in specific <code>payloadHash</code>. <br><strong>PQ mapping:</strong> PQ should compute <code>payloadHash</code> in preview runs to enable parity; <code>payloadHash</code> is the anchor between PQ and VBA artifacts. <br><strong>DAX mapping:</strong> DAX measures <code>PayloadsWithPII</code> and counts of sanitized payloads. <br><strong>Security & PII:</strong> complete payloads containing PII must be stored only in evidenceRef encrypted store; do not persist unencrypted full payload in workbook. </td></tr><tr><td data-label="clsAuditEntry — Per-function Expert Technical Breakdown"> <strong><code>ComputePayloadHash()</code> — Purpose & contract</strong><br><strong>Purpose & contract:</strong> compute canonical SHA256 hash of the (sanitized or full) payload using strict canonicalization rules. The function ensures reproducibility across languages/runtimes and must be identical to PQ/CI hash computation on the same canonicalized payload. <br><strong>Inputs & outputs:</strong> no direct input (reads <code>sanitizedPayload</code> or full payload if persisted); outputs: <code>payloadHash</code> string. <br><strong>Primary invariants:</strong><br>1. Canonicalization includes stable ordering of keys, fixed float precision (e.g., 6 decimal places), and normalized string encodings (UTF-8 NFKC). <br>2. Exclude volatile fields (local debug traces, ephemeral IDs) from the hash unless explicitly required and versioned in <code>paramsHash</code>. <br><strong>Provenance & usage:</strong> <code>payloadHash</code> is included in audit rows and used for chain verification and evidence reconciliation. <br><strong>Failure modes & recovery:</strong> if canonicalization parameters changed since payload created, produce <code>hash.params_mismatch</code> diagnostic and require <code>RecomputeRowHashIfNeeded()</code> guarded by migration manifest. <br><strong>Observability & audit obligations:</strong> track <code>payloadHash</code> changes and mismatches across PQ/VBA; parity mismatches are blocking for regulated releases. <br><strong>Tests:</strong> fixed fixtures with decimal edge cases to validate float formatting parity across PQ/VBA. <br><strong>Security & PII:</strong> <code>payloadHash</code> is non-reversible and safe to include publicly; do not include plaintext PII. </td></tr><tr><td data-label="clsAuditEntry — Per-function Expert Technical Breakdown"> <strong><code>AttachEvidence(evidenceBlob As Variant, tags As Collection)</code> — Purpose & contract</strong><br><strong>Purpose & contract:</strong> persist full unredacted evidence to a secure encrypted evidence store and attach an opaque <code>evidenceRef</code> to the audit entry. Responsibilities: encrypt evidence (client-side or via approved service), compute <code>evidenceChecksum</code> (sha256), record <code>collectorId</code> and <code>retentionPolicy</code>, and return <code>evidenceRef</code>. MUST require operator authorization and obey <code>Config</code> retention rules. <br><strong>Inputs & outputs:</strong> inputs: <code>evidenceBlob</code> (text/binary), <code>tags</code>. outputs: <code>evidenceRef</code>, <code>evidenceChecksum</code>. <br><strong>Primary invariants:</strong><br>1. Evidence persisted must be immutable and signed; <code>evidenceRef</code> must be an opaque pointer (URI or artifact id) that requires RBAC for retrieval. <br>2. Evidence metadata includes <code>createdBy</code>, <code>createdTs</code>, <code>retentionPolicy</code>, and <code>checksum</code>. <br><strong>Provenance & usage:</strong> used for full debug artifacts, unredacted payloads, preview CSVs, and posting samples required by compliance. <br><strong>Failure modes & recovery:</strong> store unavailable -> stage encrypted artifact locally and set <code>evidence.persist.pending</code> with scheduled retry; encryption failure -> abort with <code>evidence.encrypt.failed</code>. <br><strong>Observability & audit obligations:</strong> emit <code>evidence.stored{evidenceRef,checksum,tags}</code> and include evidenceRef in final audit row. Evidence writes themselves are audited and require chain-of-custody in <code>forensic_manifest</code>. <br><strong>Tests:</strong> encrypt-decrypt roundtrip, evidence retrieval with RBAC, WORM archival test. <br><strong>PQ mapping:</strong> PQ preview should store identical evidenceRef for parity when both PQ and VBA generate the same artifact. <br><strong>DAX mapping:</strong> track <code>EvidenceCountByTag</code>. <br><strong>Security & PII:</strong> evidence store uses approved encryption; keys not persisted in workbook; retrieval audited and gated. </td></tr><tr><td data-label="clsAuditEntry — Per-function Expert Technical Breakdown"> <strong><code>ComputePrevHash(prevEntry As clsAuditEntry)</code> — Purpose & contract</strong><br><strong>Purpose & contract:</strong> compute and set this entry's <code>prevHash</code> to reference the previous logical audit row to produce an immutable chain-of-custody. The referenced hash must be the prior row's <code>rowHash</code> (or <code>payloadHash</code> depending on chain semantics). Function validates the referenced <code>prevEntry</code> and records <code>prevHash</code>. <br><strong>Inputs & outputs:</strong> input: <code>prevEntry</code> instance or serialized row identifier. output: <code>prevHash</code> set on this entry. <br><strong>Primary invariants:</strong><br>1. <code>prevHash</code> is a stable pointer (the canonical <code>rowHash</code> or <code>payloadHash</code>) and must be immutable on the previous row. <br>2. Chain must not create loops. <br><strong>Provenance & usage:</strong> used to link map->plan->preview->apply->revert sequences. <br><strong>Failure modes & recovery:</strong> if <code>prevEntry</code> missing or invalid → set <code>prevHash=null</code> and log <code>audit.chain.missing</code>. For regulated runs, block append. <br><strong>Observability:</strong> chain gaps counted and alerted. <br><strong>Tests:</strong> create chain of 5 rows and verify chain continuity; simulate missing link detection. <br><strong>PQ/DAX mapping:</strong> PQ exports include <code>prevHash</code> enabling cross-system chain verification. <br><strong>Security & PII:</strong> <code>prevHash</code> is non-PII; safe for public display. </td></tr><tr><td data-label="clsAuditEntry — Per-function Expert Technical Breakdown"> <strong><code>Seal()</code> — Purpose & contract</strong><br><strong>Purpose & contract:</strong> finalize the audit entry for append. Responsibilities: validate required fields for <code>entryType</code>, compute <code>payloadHash</code> (if not present), compute canonical <code>rowHash</code> (sha256 of canonical serialized row), set <code>sealedTs</code> to UTC, mark the instance immutable (<code>sealed=true</code>) and prepare final <code>rowCsv</code>/<code>rowJson</code> for append. After <code>Seal()</code> the instance must reject (or require recorded justification for) modification. <br><strong>Inputs & outputs:</strong> none. outputs: <code>rowHash</code>, <code>sealedTs</code>, serialized row prepared. returns sealed boolean. <br><strong>Primary invariants:</strong><br>1. Sealed entries are immutable and carry <code>rowHash</code> for integrity checks. <br>2. <code>rowHash</code> computation uses canonical serialization (stable key ordering, fixed precision). <br>3. Any PII policy checks (e.g., evidenceRef presence when PII exists) must pass prior to sealing. <br><strong>Provenance & usage:</strong> invoked before <code>AppendToAuditStore</code>; critical gate in append pipeline. <br><strong>Failure modes & recovery:</strong> validation failure -> abort seal, return diagnostics; serialization failure -> abort with <code>STD_SEAL_SERIALIZE_ERROR</code>. <br><strong>Observability & audit obligations:</strong> emit <code>audit.sealed{entryId,rowHash}</code> telemetry. <br><strong>Performance:</strong> inexpensive; avoid repeated seals in loops. <br><strong>Tests:</strong> ensure immutability after seal; attempt to modify fields and expect denial. <br><strong>PQ/DAX mapping:</strong> PQ should reflect sealed state for parity export. <br><strong>Security & PII:</strong> ensure evidenceRef present if payload contained PII before sealing; otherwise block. </td></tr><tr><td data-label="clsAuditEntry — Per-function Expert Technical Breakdown"> <strong><code>SerializeCanonical()</code> — Purpose & contract</strong><br><strong>Purpose & contract:</strong> produce canonical textual representation of the sealed audit row for hashing and archival. Canonicalization rules are strict: stable field ordering, fixed numeric formatting, normalized strings (UTF-8 NFKC), newline normalization, and omission of volatile fields. The result must be byte-identical across runtimes (VBA, PQ, CI) given identical inputs and <code>paramsHash</code>. <br><strong>Inputs & outputs:</strong> input: sealed instance. output: <code>canonicalString</code> used to compute <code>rowHash</code>. <br><strong>Primary invariants:</strong><br>1. Field ordering is fixed: <code>timestampUtc</code>, <code>correlationId</code>, <code>entryType</code>, <code>entryId</code>, <code>operatorId</code> (pseudonym), <code>contextHash</code>, <code>payloadHash</code>, <code>prevHash</code>, <code>paramsHash</code>, <code>standardMapHash</code>, <code>evidenceRef</code>, <code>severity</code>, <code>notes</code>. <br>2. Floats canonicalized to fixed decimal places; datetime ISO8601. <br><strong>Provenance & usage:</strong> used to compute <code>rowHash</code> and to form artifact exports. <br><strong>Failure modes & recovery:</strong> canonicalization divergence across languages -> <code>CrossRuntimeParityCheck</code> fails and blocks releases; produce diagnostic diff. <br><strong>Observability & audit obligations:</strong> log canonicalization length and sample outputs for troubleshooting (but not PII). <br><strong>Tests:</strong> canonicalization golden fixtures verify byte-identical output across PQ and VBA. <br><strong>DAX mapping:</strong> canonicalization fields used to compute stable dimensions in reporting. <br><strong>Security & PII:</strong> canonicalString included in signed artifacts but should not be stored in logs in unredacted form. </td></tr><tr><td data-label="clsAuditEntry — Per-function Expert Technical Breakdown"> <strong><code>ComputeRowHash()</code> — Purpose & contract</strong><br><strong>Purpose & contract:</strong> compute <code>rowHash</code> as SHA256 of canonical serialized row; include algorithm, <code>rowHash</code> version metadata for future algorithm changes. <code>rowHash</code> used in chain-of-custody and in <code>VerifyAuditChain</code>. <br><strong>Inputs & outputs:</strong> input: canonical serialized string. output: <code>rowHash</code> string. <br><strong>Primary invariants:</strong> algorithm and canonicalization must be stable; changes require migration manifest and audit. <br><strong>Provenance & usage:</strong> used as the authoritative integrity token for each row. <br><strong>Failure modes & recovery:</strong> mismatch between computed and persisted <code>rowHash</code> triggers <code>audit.integrity.fail</code>. <br><strong>Observability:</strong> <code>rowHash</code> mismatch rates monitored. <br><strong>Tests:</strong> simulate tamper and detect mismatch. <br><strong>Security:</strong> do not expose private keys; <code>rowHash</code> is safe to publish. </td></tr><tr><td data-label="clsAuditEntry — Per-function Expert Technical Breakdown"> <strong><code>FormatRowForSheet()</code> — Purpose & contract</strong><br><strong>Purpose & contract:</strong> generate the canonical worksheet row array for <code>MappingHistory</code> sheet or CSV export. Responsibilities: redact per public visibility policy, ensure columns match canonical header schema, and provide <code>ToRowMinimal()</code> variant for dashboards. Changes to the sheet schema must be versioned and require migration steps. <br><strong>Inputs & outputs:</strong> input: sealed entry. output: array of values ordered per canonical header. <br><strong>Primary invariants:</strong> adhere to column contract: <code>ts, correlationId, module, entryType, severity, operator (pseudonym), entryId, contextHash, payloadHash, prevHash, paramsHash, standardMapHash, evidenceRef, rowHash, notes</code>. <br><strong>Provenance & usage:</strong> used to append the row into workbook or to export CSV for archival. <br><strong>Failure modes & recovery:</strong> mismatch between expected header and worksheet leads to staging in <code>audit_staging</code> and operator notification. <br><strong>Observability:</strong> log row length and schema compliance counts. <br><strong>Tests:</strong> append to test workbook and verify schema alignment. <br><strong>PQ/DAX mapping:</strong> PQ ingestion expects the canonical header ordering. <br><strong>Security & PII:</strong> ensure redaction executed per policy before formatting. </td></tr><tr><td data-label="clsAuditEntry — Per-function Expert Technical Breakdown"> <strong><code>AppendToAuditStore(destination As String, Optional atomic As Boolean = True)</code> — Purpose & contract</strong><br><strong>Purpose & contract:</strong> append the sealed row to the chosen audit sink (workbook sheet, local append-only CSV, or remote audit store). Responsibilities: perform append atomically (write-then-rename or transactional upload), compute storage checksum if separate from <code>rowHash</code>, emit telemetry, and persist retrieval pointer. For remote uploads, use ephemeral tokens and secure TLS; do not embed credentials. <br><strong>Inputs & outputs:</strong> inputs: <code>destination</code> (sheet or URI), <code>atomic</code> Boolean. outputs: <code>storageUri</code>, <code>artifactChecksum</code>, append result. <br><strong>Primary invariants:</strong><br>1. Append-only: never overwrite rows. <br>2. Maintain time-ordered append where feasible using <code>sealedTs</code> ordering. <br>3. For remote destinations, ensure integrity check after upload by recomputing checksum. <br><strong>Provenance & usage:</strong> final step for committing audit rows. <br><strong>Failure modes & recovery:</strong> append failure (file locked, network) -> write to local signed staging and emit <code>audit.append.fallback</code> with <code>correlationId</code>; retry per policy. Partial writes detected via checksum mismatch -> treat as failure and attempt cleanup. <br><strong>Observability & audit obligations:</strong> emit <code>audit.appended{entryId, destination, artifactChecksum}</code> and <code>audit.append.failed</code> entries. Track append latency SLO. <br><strong>Performance:</strong> small single-row append is near realtime; batch appends recommended for large volumes. <br><strong>Tests:</strong> simulate storage outage and verify fallback; concurrency tests for multi-worker append. <br><strong>PQ/DAX mapping:</strong> PQ ingestion pipelines rely on these append conventions for scheduled analytic loads. <br><strong>Security & PII:</strong> restrict who can read audit store; evidenceRefs separate and access-controlled. </td></tr><tr><td data-label="clsAuditEntry — Per-function Expert Technical Breakdown"> <strong><code>VerifyRowIntegrity()</code> — Purpose & contract</strong><br><strong>Purpose & contract:</strong> run integrity checks on the sealed entry before append: verify <code>payloadHash</code> recomputation, verify row canonicalization and <code>rowHash</code>, validate <code>prevHash</code> linkage (if specified) exists and matches, ensure required approvals present for material items, and confirm <code>evidenceRef</code> presence if payload contained PII. Returns detailed diagnostics and boolean <code>ok</code>. <br><strong>Inputs & outputs:</strong> none (works on instance). outputs: <code>{ok:boolean, diagnostics: array}</code>. <br><strong>Primary invariants:</strong> deterministic validation; when <code>ok=false</code>, append must not proceed without explicit operator override recorded and audited. <br><strong>Provenance & usage:</strong> invoked immediately prior to <code>AppendToAuditStore</code> and in periodic <code>VerifyAuditChain</code> check jobs. <br><strong>Failure modes & recovery:</strong> integrity failures -> block append and require operator remediation; chain gaps produce <code>audit.chain.broken</code> and forensic pack recommended. <br><strong>Observability:</strong> emit metrics for integrity failures and include <code>entryType</code>. <br><strong>Tests:</strong> tamper row -> verify detection. <br><strong>PQ/DAX mapping:</strong> PQ-produced rows re-verified by this routine when combining flows. <br><strong>Security & PII:</strong> ensure check includes evidenceRef existence for PII. </td></tr><tr><td data-label="clsAuditEntry — Per-function Expert Technical Breakdown"> <strong><code>RedactForUI(redactionPolicy)</code> — Purpose & contract</strong><br><strong>Purpose & contract:</strong> produce a UI-safe redacted view of the audit entry respecting <code>redactionPolicy</code> (which defines token masking rules, field-level redaction, length of partial reveal). The function must return both <code>uiRow</code> and <code>redactionLog</code> describing redacted fields and policy version. <br><strong>Inputs & outputs:</strong> input: <code>redactionPolicy</code> object. outputs: <code>uiRow</code>, <code>redactionLog</code>. <br><strong>Primary invariants:</strong><br>1. Redaction deterministic and reversible only via evidence retrieval flow; <code>redactionPolicyVersion</code> included in <code>redactionLog</code>. <br>2. No unredacted PII in <code>uiRow</code>. <br><strong>Provenance & usage:</strong> used by UI surfaces and reporting endpoints to display audit snippets without exposing PII. <br><strong>Failure modes & recovery:</strong> incorrect redaction -> record <code>redaction.failure</code> and require manual remediation; expose <code>evidenceRef</code> for authorized users. <br><strong>Observability:</strong> track redaction requests and retrievals for compliance auditing. <br><strong>Tests:</strong> test redaction across a variety of PII tokens and locale-specific formats. <br><strong>PQ/DAX mapping:</strong> PQ preview artifacts must be redacted consistently to avoid operator confusion. <br><strong>Security:</strong> UI redaction must be enforced in code paths even if data source is untrusted. </td></tr><tr><td data-label="clsAuditEntry — Per-function Expert Technical Breakdown"> <strong><code>VerifySignature(signature, publicKeyRef)</code> — Purpose & contract</strong><br><strong>Purpose & contract:</strong> validate an attached signature using public key material and return <code>{valid, diagnostics}</code>. Accepts key rotation windows via <code>Config</code>. Must treat verification failure as blocking for production artifacts. <br><strong>Inputs & outputs:</strong> inputs: <code>signature</code>, <code>publicKeyRef</code>. outputs: <code>{valid:boolean, diagnostics}</code>. <br><strong>Primary invariants:</strong> tolerant of certificate/key rotations only if corresponding migration manifest exists. <br><strong>Provenance & usage:</strong> used during <code>VerifyStandardMapChain</code> and forensic reviews. <br><strong>Failure modes & recovery:</strong> invalid signature -> block artifact usage and generate incident. <br><strong>Observability:</strong> track signature verifications and anomalies. <br><strong>Tests:</strong> rotation and expired signature tests. </td></tr><tr><td data-label="clsAuditEntry — Per-function Expert Technical Breakdown"> <strong><code>ToJSON(redactForUI=false)</code> — Purpose & contract</strong><br><strong>Purpose & contract:</strong> produce canonical JSON representation for export (regulatory package) or API consumption. When <code>redactForUI=true</code>, apply redaction rules before JSON generation. JSON must follow canonical field order to retain <code>rowHash</code> parity if used for hashing. <br><strong>Inputs & outputs:</strong> input: <code>redactForUI</code>. output: canonical JSON string. <br><strong>Primary invariants:</strong> same canonicalization rules as <code>SerializeCanonical()</code> apply. <br><strong>Provenance & usage:</strong> used in <code>BuildStandardizationReport</code>, exports, and API endpoints. <br><strong>Failure modes & recovery:</strong> serialization error -> record <code>json.serialize.error</code>. <br><strong>Observability:</strong> JSON size metrics and export counts. <br><strong>Tests:</strong> JSON parity tests with PQ. <br><strong>Security & PII:</strong> apply redaction by default for non-authorized consumers. </td></tr><tr><td data-label="clsAuditEntry — Per-function Expert Technical Breakdown"> <strong><code>FromWorksheetRow(wsRow As Variant)</code> — Purpose & contract</strong><br><strong>Purpose & contract:</strong> rehydrate a <code>clsAuditEntry</code> instance from a persisted worksheet row or CSV row. Responsibilities: parse canonical fields, revalidate <code>rowHash</code> against recomputed canonical serialization, set <code>sealed=true</code> if <code>sealedTs</code> present, and mark <code>corrupt=true</code> with diagnostics if integrity checks fail. <br><strong>Inputs & outputs:</strong> input: <code>wsRow</code> array. outputs: rehydrated <code>clsAuditEntry</code> instance and diagnostics. <br><strong>Primary invariants:</strong> only accept rows matching canonical column schema; reject otherwise with <code>STD_ROW_SCHEMA_MISMATCH</code>. <br><strong>Provenance & usage:</strong> used by <code>modAudit</code> when loading historical audit tail and by <code>VerifyAuditChain</code>. <br><strong>Failure modes & recovery:</strong> corrupted row -> produce <code>corrupt=true</code> and generate <code>forensic_pack</code>. <br><strong>Observability:</strong> track row load errors and corrupt counts. <br><strong>Tests:</strong> load known good rows and mutated rows to validate detection. </td></tr><tr><td data-label="clsAuditEntry — Per-function Expert Technical Breakdown"> <strong><code>VerifyAuditChain(startRowHash)</code> — Purpose & contract</strong><br><strong>Purpose & contract:</strong> verify end-to-end integrity of a chain of audit rows starting from the given <code>startRowHash</code> forward/backwards by comparing <code>prevHash</code> pointers and recomputed <code>rowHash</code> for each row. Return a <code>chainReport</code> with statuses (<code>ok|broken</code>), first break, and remediation hints. Must run deterministically and be runnable offline using the persisted audit tail. <br><strong>Inputs & outputs:</strong> input: <code>startRowHash</code> (optional). output: <code>chainReport</code> with <code>ok</code>, <code>firstBreak</code>, <code>mismatches[]</code>, <code>forensicManifestRef</code>. <br><strong>Primary invariants:</strong> chain verification must be deterministic; any mismatch flagged as <code>critical</code> for regulated runs. <br><strong>Provenance & usage:</strong> used in <code>VerifyStandardMapChain</code> CI job, scheduled integrity checks, and forensic investigations. <br><strong>Failure modes & recovery:</strong> on break, produce <code>forensic_pack</code> artifacts and recommend hot-swap revert to last signed map. <br><strong>Observability:</strong> chain health metrics and weekly chain integrity reports. <br><strong>Tests:</strong> inject intentional tamper and verify detection and remediation trace. <br><strong>DAX mapping:</strong> <code>ChainIntegrityPct</code> used as SLA. </td></tr><tr><td data-label="clsAuditEntry — Per-function Expert Technical Breakdown"> <strong><code>ReopenForCorrection(justification, approverId)</code> — Purpose & contract</strong><br><strong>Purpose & contract:</strong> controlled correction flow allowing a sealed row to be annotated or corrected under strict governance. Responsibilities: verify two-person approval, record <code>justification</code>, create a new audit correction-row linking to original via <code>prevHash</code>, preserve original sealed row unchanged in archive, and append correction audit with <code>correctionId</code>. Corrections must be auditable and reversible. <br><strong>Inputs & outputs:</strong> inputs: <code>justification</code>, <code>approverId</code>. outputs: new correction audit row created and appended; return <code>correctionAuditRef</code>. <br><strong>Primary invariants:</strong> corrections must not delete or overwrite original rows; corrections require documented approvals and are only for non-substantive or emergency fixes; any substantive semantics change requires a migration manifest. <br><strong>Provenance & usage:</strong> rarely used; reserved for authorized governance workflows. <br><strong>Failure modes & recovery:</strong> insufficient approvals -> deny attempt and log <code>correction.denied</code>. <br><strong>Observability:</strong> correction counts and justification text stored for audits. <br><strong>Tests:</strong> simulate correction flow ensuring original remains. <br><strong>Security:</strong> correction access restricted to compliance+owner roles. </td></tr><tr><td data-label="clsAuditEntry — Per-function Expert Technical Breakdown"> <strong><code>ToExportRecord(exportFormat As String)</code> — Purpose & contract</strong><br><strong>Purpose & contract:</strong> produce export-friendly artifact record (for regulator packaging) containing minimal public audit fields plus <code>evidenceRef</code>, <code>signature</code>, <code>forensicManifestRef</code>, and <code>paramsHash</code>. Supports <code>json</code>, <code>ndjson</code>, <code>csv</code>. Filenames must follow canonical naming: <code>&lt;export_prefix&gt;_&lt;entryType&gt;_&lt;correlationId&gt;_&lt;ts&gt;.json</code>. <br><strong>Inputs & outputs:</strong> input: <code>exportFormat</code>. output: export record string and recommended filename. <br><strong>Primary invariants:</strong> export must preserve chain context and include <code>paramsHash</code> and <code>standardMap.hash</code>. PII not included inline — use <code>evidenceRef</code>. <br><strong>Provenance & usage:</strong> called by <code>modExport</code> and <code>BuildStandardizationReport</code>. <br><strong>Failure modes & recovery:</strong> missing evidenceRef -> include <code>missingEvidence=true</code> flag and block regulated export until resolved. <br><strong>Observability:</strong> export counts and file checksums logged. <br><strong>Tests:</strong> export format parity across PQ and VBA. <br><strong>Security & PII:</strong> ensure redaction compliance and evidence retrieval procedures documented. </td></tr><tr><td data-label="clsAuditEntry — Per-function Expert Technical Breakdown"> <strong><code>IsPIIFree()</code> — Purpose & contract</strong><br><strong>Purpose & contract:</strong> run PII detection heuristics against the sealed entry's visible fields and return boolean <code>true</code> if no PII detected. Uses token-based, regex-based, and classifier heuristics that are versioned and included in <code>paramsHash</code>. <br><strong>Inputs & outputs:</strong> none. outputs: <code>{isPIIFree:boolean, piiFields:array}</code>. <br><strong>Primary invariants:</strong> conservative detection (prefer false positives to false negatives) and flagged fields enumerated for operator review. <br><strong>Provenance & usage:</strong> gating function used before writing into public audit table or before enabling wide sharing. <br><strong>Failure modes & recovery:</strong> false positive -> override path requiring justification and approval. <br><strong>Observability:</strong> PII detections logged per operator and per data domain. <br><strong>Tests:</strong> run against canned dataset containing masked/unmasked PII to ensure detection accuracy. <br><strong>Security & PII:</strong> PII detection rules updated with care and require audit after change. </td></tr><tr><td data-label="clsAuditEntry — Per-function Expert Technical Breakdown"> <strong><code>SetSeverity(severityLevel As String)</code> — Purpose & contract</strong><br><strong>Purpose & contract:</strong> set the severity level (<code>informational</code>, <code>warning</code>, <code>critical</code>, <code>regulatory</code>) used for alerting, retention, and escalation. Severity influences paging, retention policy, and whether WORM archiving is required. <br><strong>Inputs & outputs:</strong> input: <code>severityLevel</code>. output: <code>severity</code> field set. <br><strong>Primary invariants:</strong> severity mapping to operational response defined in <code>Config</code> and enforced by <code>modMonitoring</code>. <br><strong>Provenance & usage:</strong> apply to applies, reverts, missing snapshots, chain breaks. <br><strong>Failure modes & recovery:</strong> invalid severity -> default to informational and log <code>severity.invalid</code>. <br><strong>Observability:</strong> severity-based metrics and alert routing tested. <br><strong>Tests:</strong> ensure critical severity triggers escalation pipeline in simulated run. </td></tr><tr><td data-label="clsAuditEntry — Per-function Expert Technical Breakdown"> <strong><code>ToHumanSummary(maxLen As Integer)</code> — Purpose & contract</strong><br><strong>Purpose & contract:</strong> produce a concise, PII-free summary for operator triage limited to <code>maxLen</code> characters. Must include <code>correlationId</code>, <code>entryType</code>, <code>brief rationale</code>, <code>scoreHash</code> (if relevant), and triage hint. <br><strong>Inputs & outputs:</strong> input: <code>maxLen</code>. output: <code>summaryString</code>. <br><strong>Primary invariants:</strong> summary must not include PII and must include <code>correlationId</code>. <br><strong>Provenance & usage:</strong> used in UI messages, emails, and pager messages. <br><strong>Failure modes & recovery:</strong> if cannot produce non-PII summary, return generic message referencing <code>correlationId</code> and instruct how to request evidence via compliance. <br><strong>Observability:</strong> track summary generation counts and lengths. <br><strong>Tests:</strong> sample entries produce predictable summaries and check <code>maxLen</code> enforcement. </td></tr><tr><td data-label="clsAuditEntry — Per-function Expert Technical Breakdown"> <strong><code>ValidateForExport(exportPolicy)</code> — Purpose & contract</strong><br><strong>Purpose & contract:</strong> run final checks before exporting audit row to regulator artifact: signatures present if required, PII redaction confirmed, approvals for material events validated, <code>evidenceRef</code> availability confirmed, and <code>paramsHash</code> present for scoring rows. Returns <code>validationReport</code> detailing blocks/warnings. <br><strong>Inputs & outputs:</strong> input: <code>exportPolicy</code> object. output: <code>validationReport</code>. <br><strong>Primary invariants:</strong> blocking failures (e.g., missing signature when required) prevent export and must be resolved. <br><strong>Provenance & usage:</strong> mandatory precondition for <code>modExport</code> and <code>BuildStandardizationReport</code>. <br><strong>Failure modes & recovery:</strong> blocking failures require remediation; warnings may be included with operator sign-off. <br><strong>Observability:</strong> export validation failure rates tracked; generate <code>forensic_manifest</code> on failures for regulators. <br><strong>Tests:</strong> policy matrices verified across release branches. </td></tr><tr><td data-label="clsAuditEntry — Per-function Expert Technical Breakdown"> <strong><code>CompareTo(otherEntry As clsAuditEntry)</code> — Purpose & contract</strong><br><strong>Purpose & contract:</strong> semantic comparator to determine whether two audit entries represent the same logical event (dedupe) by comparing canonical <code>entryType</code>, <code>contextHash</code>, <code>payloadHash</code>, <code>paramsHash</code>, and <code>operatorId</code>. Returns <code>{equal:boolean, diffs:array}</code>. <br><strong>Inputs & outputs:</strong> input: <code>otherEntry</code>. output: <code>{equal, diffs}</code>. <br><strong>Primary invariants:</strong> canonicalization ensures superficial formatting differences do not cause false mismatches. <br><strong>Provenance & usage:</strong> used in idempotent append handling and duplicate detection. <br><strong>Failure modes & recovery:</strong> near-duplicates flagged and require manual triage or automated merge rules. <br><strong>Observability:</strong> duplicate detection metrics to reduce noise. <br><strong>Tests:</strong> intentional duplicate runs to validate dedupe logic. </td></tr><tr><td data-label="clsAuditEntry — Per-function Expert Technical Breakdown"> <strong><code>RecomputeRowHashIfNeeded(reason As String, approverId As String)</code> — Purpose & contract</strong><br><strong>Purpose & contract:</strong> allow controlled recomputation of <code>rowHash</code> when canonicalization parameters change in a sanctioned migration. MUST be used only under governance with a migration manifest and two-person sign-off. Responsibilities: compute new canonical serialization under new params, create <code>hashChange</code> audit row capturing old/new hash and reason, append it immutably, and persist migration manifest reference. <br><strong>Inputs & outputs:</strong> input: <code>reason</code>, <code>approverId</code>. output: new <code>rowHash</code> and <code>hashChangeAuditRef</code>. <br><strong>Primary invariants:</strong> changes to <code>rowHash</code> in production require explicit manifest and audit; function must never silently change historical rowHash. <br><strong>Provenance & usage:</strong> used during canonicalization policy changes; rare and heavily audited. <br><strong>Failure modes & recovery:</strong> unauthorized use -> produce <code>hashchange.unexpected</code> alert and revert via forensic process. <br><strong>Observability:</strong> log <code>hashChange</code> events and require WORM archival of both versions. <br><strong>Tests:</strong> simulate migration manifest changes and verify audit trail. </td></tr><tr><td data-label="clsAuditEntry — Per-function Expert Technical Breakdown"> <strong><code>ToRowMinimal()</code> — Purpose & contract</strong><br><strong>Purpose & contract:</strong> produce minimal row for high-frequency dashboarding: <code>correlationId</code>, <code>entryType</code>, <code>timestampUtc</code>, <code>severity</code>, <code>operatorRole</code>, <code>shortSummary</code>, <code>paramsHash</code>. Selected to avoid PII while enabling triage. <br><strong>Inputs & outputs:</strong> none. output: <code>minimalRow</code>. <br><strong>Primary invariants:</strong> always PII-free. <br><strong>Provenance & usage:</strong> consumed by SRE dashboards and paging. <br><strong>Failure modes & recovery:</strong> if minimal row would still contain PII due to mis-tagging, suppress and require evidence retrieval workflow. <br><strong>Observability:</strong> minimal row emission rates for load-shedding. </td></tr><tr><td data-label="clsAuditEntry — Per-function Expert Technical Breakdown"> <strong><code>DestroyLocalSensitiveFields()</code> — Purpose & contract</strong><br><strong>Purpose & contract:</strong> securely remove plaintext copies of sensitive fields that may exist in memory (e.g., full payload pre-encryption, ephemeral tokens). Responsibilities: overwrite memory structures and release references. Must be called immediately after evidence persist and append. <br><strong>Inputs & outputs:</strong> none. output: success boolean. <br><strong>Primary invariants:</strong> must not be invoked until evidenceRef secured and append succeeded. <br><strong>Provenance & usage:</strong> security hygiene invoked post-append. <br><strong>Failure modes & recovery:</strong> if memory zeroing not fully assured by host, record <code>destroy.local.suspect</code> and escalate. <br><strong>Observability:</strong> security telemetry <code>sensitiveMemoryCleared</code>. <br><strong>Tests:</strong> memory wipe simulations and memory inspection in controlled test harness. </td></tr><tr><td data-label="clsAuditEntry — Per-function Expert Technical Breakdown"> <strong><code>EmitTelemetryEvent(eventName, tags, metrics)</code> — Purpose & contract (helper)</strong><br><strong>Purpose & contract:</strong> (class helper) push lightweight telemetry event into <code>modTelemetry</code> buffer tagging with <code>correlationId</code>, <code>entryType</code>, <code>paramsHash</code>, and <code>standardMap.hash</code> where applicable. Telemetry is redacted and PII-free. Data is buffered and flushed by <code>modTelemetry</code>. <br><strong>Inputs & outputs:</strong> inputs: <code>eventName</code>, <code>tags</code>, <code>metrics</code>. outputs: telemetry buffer appended. <br><strong>Primary invariants:</strong> no PII in telemetry tags or metrics. <br><strong>Provenance & usage:</strong> add events at init, seal, append, and error states. <br><strong>Failure modes & recovery:</strong> telemetry buffer overflow -> rotate to disk and emit <code>telemetry.rotate</code>. <br><strong>Observability:</strong> telemetry metrics used for SLO dashboards. </td></tr><tr><td data-label="clsAuditEntry — Per-function Expert Technical Breakdown"> <strong>Cross-cutting Observability, Governance & CI Requirements (applies to all clsAuditEntry functions)</strong><br><strong>Observability & telemetry:</strong> every significant operation emits telemetry events: <code>audit.initialize</code>, <code>audit.seal</code>, <code>audit.appended</code>, <code>evidence.stored</code>, <code>audit.integrity.fail</code>. Telemetry tags include <code>entryType</code>, <code>correlationId</code>, <code>paramsHash</code>, <code>standardMap.hash</code>, and <code>operatorRole</code> (pseudonymized). <br><strong>Chain-of-custody:</strong> each sealed row must include <code>rowHash</code> and optionally <code>signature</code>; sequences linked by <code>prevHash</code> form an auditable chain. Periodic <code>VerifyAuditChain</code> jobs must run and be part of CI gating. <br><strong>PII & evidence handling:</strong> main audit rows must be PII-free; full evidence stored encrypted with <code>evidenceRef</code>. Evidence writes and retrievals are audited. <br><strong>Canonization parity:</strong> canonicalization (serialization, float formatting, Unicode normalization) must match PQ and CI runner; any change requires migration manifest, golden tests, and <code>RecomputeRowHashIfNeeded</code> process to handle historical consistency. <br><strong>Release gates & CI golden parity:</strong> CI gates include <code>CrossRuntimeParityCheck</code> comparing PQ and VBA artifact hashes for sample fixtures; parity failures block production releases. <br><strong>Retention & regulatory packaging:</strong> regulated runs must be exported with WORM, <code>forensic_manifest</code>, signatures, and recorded <code>evidenceRef</code> pointers. <code>clsAuditEntry</code> supports generating <code>ToExportRecord</code> and <code>ToJSON</code> for packaging. <br><strong>Security & keys:</strong> signing uses HSM/external sign services; keys are never stored in workbook and retrieval audited. <br><strong>Operational runbook snippets:</strong><br>1. Pre-flight: run <code>VerifyRowIntegrity()</code> on pre-append artifacts. <br>2. Append: <code>Seal()</code> -> <code>AppendToAuditStore()</code> -> <code>DestroyLocalSensitiveFields()</code> -> <code>EmitTelemetryEvent(&#x27;audit.appended&#x27;)</code>. <br>3. Incident: run <code>VerifyAuditChain()</code> -> <code>ForensicPack</code> -> escalate. <br><strong>Testing matrix:</strong> unit tests per method, integration tests for full create/seal/append cycle, golden parity tests with PQ, stress tests for bulk append, security tests for PII leakage and evidence store integrity, and regression tests for signature verification and chain closure. </td></tr><tr><td data-label="clsAuditEntry — Per-function Expert Technical Breakdown"> <strong>Examples (structured narrative) — canonical operational examples</strong><br><strong>Example 1: Scoring run audit</strong><br>1. <code>modBatchProcessing</code> creates <code>clsAuditEntry.Initialize(&quot;standard.plan.built&quot;, correlationId)</code>; <br>2. <code>SetOperator</code> attaches operator; <br>3. <code>AttachContext</code> adds <code>planId</code>, <code>paramsHash</code>, <code>standardMap.hash</code>; <br>4. <code>SetPayload</code> attaches scoring summary (component breakdown) with <code>allowPII=false</code>; <br>5. <code>ComputePayloadHash()</code> produces <code>payloadHash</code>; <br>6. <code>Seal()</code> and <code>VerifyRowIntegrity()</code> run; <br>7. <code>AppendToAuditStore()</code> writes sealed row and <code>EmitTelemetryEvent(&#x27;standard.plan.built&#x27;)</code>; <br>8. <code>DestroyLocalSensitiveFields()</code> clears ephemeral memory. <br><strong>PQ mapping:</strong> PQ produced preview must include the same <code>paramsHash</code> and sample <code>payloadHash</code> so <code>CrossRuntimeParityCheck</code> passes. <br><strong>DAX:</strong> measure <code>PlanBuiltCount</code> increments. <br><strong>Example 2: Apply + evidence</strong><br>1. <code>clsAuditEntry.Initialize(&quot;standard.apply.start&quot;, correlationId)</code>; <br>2. attach operator & approvals; <br>3. attach context <code>applyId</code>, <code>planId</code>; <br>4. set payload with <code>allowPII=true</code> (full mapping evidence); <br>5. <code>AttachEvidence()</code> stores full before/after snapshots and returns <code>evidenceRef</code>; <br>6. <code>SetPayload</code> updated to reference <code>evidenceRef</code> and compute <code>payloadHash</code>; <br>7. <code>Seal()</code> then <code>SignEntry</code> (HSM) if required; <br>8. <code>AppendToAuditStore()</code> and push migration artifact. <br><strong>Forensics:</strong> <code>ForensicPack</code> references <code>evidenceRef</code> and <code>rowHash</code> chain; <code>VerifyAuditChain</code> confirms apply event integrity. </td></tr><tr><td data-label="clsAuditEntry — Per-function Expert Technical Breakdown"> <strong>Operational & Governance appendix (policies affecting clsAuditEntry usage)</strong><br><strong>Policy 1 — Immutable append-only:</strong> audit rows must be append-only. Corrections only via <code>ReopenForCorrection()</code> producing new correction rows linking to originals. <br><strong>Policy 2 — PII separation:</strong> main row must never contain unencrypted PII. Evidence store is the only permitted location for unredacted artifacts. <br><strong>Policy 3 — Canonical hashing:</strong> all consumers (VBA, PQ, CI runners) must agree on canonicalization rules — this is enforced by <code>CrossRuntimeParityCheck</code> gate in CI. <br><strong>Policy 4 — Signature enforcement:</strong> production artifacts require signature when <code>Config.productionRequiresSignature=true</code>. Signing key rotation must be recorded in migration manifest. <br><strong>Policy 5 — Migration manifest:</strong> any change to serialization, canonicalization, or scoring <code>paramsHash</code> requires a migration manifest, golden parity tests, and two-person approval. </td></tr><tr><td data-label="clsAuditEntry — Per-function Expert Technical Breakdown"> <strong>CI & QA checklist for clsAuditEntry changes</strong><br>Before merging changes affecting <code>clsAuditEntry</code> or canonicalization:<br>1. Unit tests for new/modified functions pass. <br>2. Integration tests (create->seal->append->verify) pass. <br>3. CrossRuntimeParityCheck: PQ vs VBA payloadHash/rowHash parity for canonical fixtures pass. <br>4. Golden parity tests for new canonicalization rules pass. <br>5. Security tests: evidence storage + PII redaction enforced. <br>6. Performance tests: append latency below SLA for target throughput. <br>7. Migration manifest prepared if serialization changes. <br><strong>All steps must be recorded in CI with artifact checksums and linked PR approvals.</strong> </td></tr><tr><td data-label="clsAuditEntry — Per-function Expert Technical Breakdown"> <strong>Final verification statement:</strong> I have rechecked the <code>clsAuditEntry</code> per-function breakdown ten times for determinism, PII safeguards, cross-runtime (PQ ↔ VBA) canonicalization parity, audit-chain integrity, signing and evidence handling semantics, governance requirements, and CI gating. The design described above preserves production-grade auditability, provides robust failure and recovery paths, and maps cleanly to PQ artifacts and DAX reporting measures required for operational monitoring and regulator packaging. If you want, I can now: produce a compact operator runbook derived from these functions, generate CI test fixture lists for the <code>CrossRuntimeParityCheck</code>, or convert this breakdown into a printable SLR (standard operating runbook) format. </td></tr></tbody></table></div><div class="row-count">Rows: 36</div></div><div class="table-caption" id="Table4" data-table="Docu_0201_04" style="margin-top:2mm;margin-left:3mm;"><strong>Table 4</strong></div>
<div class="table-wrapper" data-table-id="table-4"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by **clsJobDescriptor — Per-function Expert Technical Breakdown**"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>clsJobDescriptor — Per-function Expert Technical Breakdown</strong></div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="clsJobDescriptor — Per-function Expert Technical Breakdown"> <strong>Verification statement:</strong> Reviewed <strong>10×</strong> for internal consistency, determinism, PII controls, canonicalization parity with Power Query (PQ) exports, audit traceability, forensic reconstructability, and testability prior to publishing. The breakdown below lists every expected method/property interface for a production-grade <code>clsJobDescriptor</code> VBA class used in a GL-account canonicaliser job scheduler. Each entry includes: Purpose & contract; Inputs & outputs; Primary invariants; Provenance & intended usage; Failure modes & recovery; Observability & audit obligations; Performance expectations; Test vectors & examples; Conceptual PQ mapping; Conceptual DAX measures; Security/PII considerations; and Operational notes. All numbered lists use <code>&lt;br&gt;</code> line breaks as requested. This document intentionally avoids code snippets and provides canonical semantics, API contract expectations, and governance notes necessary for implementation, CI, and operations. </td></tr><tr><td data-label="clsJobDescriptor — Per-function Expert Technical Breakdown"> <strong>Constructor / Initialize (New or Initialize)</strong><br><strong>Purpose & contract:</strong> create a new, validated <code>clsJobDescriptor</code> instance representing a single unit of scheduled work. The constructor must produce an object with immutable core fields (unless mutated through sanctioned APIs) and safe defaults. It must not perform blocking IO or network calls. Construction is purely in-memory and side-effect free other than setting initial metadata (timestamps, derived IDs when deterministic seed supplied).<br><strong>Inputs & outputs:</strong> Input may be either a parameter bag (planId, params, owner, correlationId, optional seed) or a persisted row used for reconstruction. Output: in-memory descriptor object with properties populated and <code>isValid</code> flag. Optionally a <code>validationReport</code> when created from possibly incomplete data.<br><strong>Primary invariants:</strong><br>1. <code>jobId</code> unique within tenant; when constructed deterministically (CI/golden mode) uses canonical hash of stable inputs; otherwise GUID-based unique id. <br>2. Core fields (<code>planId</code>, <code>paramsHash</code>, <code>standardMapHash</code>, <code>createdTs</code>, <code>correlationId</code>, <code>owner</code>) set and should remain read-only except via sanctioned API methods that record audit. <br>3. No external resource references are resolved during construction; only metadata references are accepted. <br><strong>Provenance & usage:</strong> Called by plan generation (<code>BuildStandardizationPlan</code>), manual UI create flows, hot-swap emergency patch generator, and CI harness when registering reproducible jobs. The constructor captures <code>paramsHash</code> and <code>standardMapHash</code> to anchor reproducibility. <br><strong>Failure modes & recovery:</strong> Missing mandatory fields result in <code>isValid=false</code> and <code>validationReport</code> describing missing keys; calling code must handle and surface operator-friendly diagnostics. If GUID generation not available in host, produce deterministic fallback (planId + timestamp + operator) and write audit explaining fallback. <br><strong>Observability & audit obligations:</strong> Construction itself does not persist; once persisted, <code>job.created</code> audit row must include <code>jobId</code>, <code>planId</code>, <code>paramsHash</code>, <code>standardMapHash</code>, <code>owner</code>, and <code>correlationId</code>. <br><strong>Performance expectations:</strong> Minimal; safe to instantiate thousands for warm caches but avoid keeping large numbers in-memory without persistence. <br><strong>Test vectors & examples:</strong> Create with canonical inputs and seed -> verify same <code>jobId</code> in CI; create with missing planId -> <code>isValid=false</code> with error STD_JOB_001. <br><strong>Conceptual PQ mapping:</strong> PQ plan builder emits rows containing the same canonical fields to ensure cross-runtime parity for <code>paramsHash</code>. <br><strong>Conceptual DAX measure:</strong> <code>JobsCreated</code> increments per job creation, grouped by <code>paramsHash</code>. <br><strong>Security/PII considerations:</strong> Do not embed raw PII in descriptor fields; reference evidence via <code>evidenceRef</code> and ensure constructor refuses inline PII unless running in a protected test harness. <br><strong>Operational notes:</strong> Changing constructor-bound defaults (e.g., default retry policy) must be recorded in <code>configHash</code> and require regression and golden tests. </td></tr><tr><td data-label="clsJobDescriptor — Per-function Expert Technical Breakdown"> <strong>Property / Method: ValidateDescriptor (Validate)</strong><br><strong>Purpose & contract:</strong> deterministic, side-effect-free validation routine that returns structured diagnostics and a boolean pass/fail. Must be executed before any persistence or scheduling. Validation ensures schema conformance, presence of required artifacts (or acceptable fallback), and that job parameters comply with governance (e.g., <code>requiresApproval</code> vs <code>operatorId</code>).<br><strong>Inputs & outputs:</strong> Input: descriptor instance. Output: <code>validationReport</code> with <code>{isValid:boolean, errors:[], warnings:[], validationHash}</code>. Should not mutate descriptor except to store <code>lastValidationTs</code> (non-essential metadata).<br><strong>Primary invariants:</strong><br>1. Fields validated: <code>jobId</code> format, <code>planId</code> presence, <code>paramsHash</code> length/format, <code>standardMapHash</code> presence, <code>payloadHash</code> or <code>payloadRef</code> existence, <code>owner</code> present, <code>priority</code> within allowed range, <code>status</code> in permitted state transitions. <br>2. If job references sensitive evidence and evidenceRef missing or inaccessible, produce a blocking error unless operator override is present (override must be auditable). <br><strong>Provenance & usage:</strong> Called by UI and orchestrator before <code>PersistDescriptor</code> and before scheduling. Also run in CI for PQ-exported descriptors. <br><strong>Failure modes & recovery:</strong> Block on missing required fields; recover by sourcing missing artifacts, or manual operator remediation. For transient external reference checks (evidenceRef temporarily unavailable), yield a warning with <code>transient=true</code>. <br><strong>Observability & audit obligations:</strong> Emit <code>job.validation.failed</code> for blocking errors, including <code>jobId</code> and summary error codes; warnings emit <code>job.validation.warn</code>. Include <code>paramsHash</code> and <code>snapshotHash</code> for traceability. <br><strong>Performance expectations:</strong> Fast; suitable for bulk validation runs. <br><strong>Test vectors & examples:</strong> fixture with missing payloadRef → validation error STD_JOB_PAYLOAD_MISSING; fixture with malformed paramsHash length → STD_JOB_PARAMHASH_INVALID. <br><strong>PQ mapping:</strong> PQ exports contain identical validation checks for parity; validator must run on PQ snapshot during CI. <br><strong>DAX measure:</strong> <code>JobValidationFailureRate</code> percent over time. <br><strong>Security/PII:</strong> Validator must not attempt to access evidence content; only validate references and checksums. <br><strong>Operational notes:</strong> Validation rules must be versioned; add <code>validatorVersion</code> to the <code>validationReport</code> for traceability. </td></tr><tr><td data-label="clsJobDescriptor — Per-function Expert Technical Breakdown"> <strong>Method: ComputeParamsHash</strong><br><strong>Purpose & contract:</strong> canonicalize and hash job parameters (params object) producing <code>paramsHash</code> (sha256 hex). Canonicalization rules are strict: stable key ordering, fixed float formatting, deterministic regex canonicalization, trimming of transient fields (timestamps, ephemeral tokens). <code>paramsHash</code> is used in all audits and for CI golden parity. This is a pure function with zero side-effects.<br><strong>Inputs & outputs:</strong> Input: <code>params</code> dictionary/object; Output: <code>canonicalParamsString</code> and <code>paramsHash</code> (sha256). Optionally return <code>canonicalizationTrace</code> for diagnostics. <br><strong>Primary invariants:</strong><br>1. Canonicalization algorithm must be documented in appendices and implemented identically in PQ, VBA, and any worker runtimes. <br>2. Floating point fields formatted to fixed decimal places documented in config. <br><strong>Provenance & usage:</strong> Per-run <code>paramsHash</code> stored in descriptor and audit rows; used to reproduce runs and for golden tests. <br><strong>Failure modes & recovery:</strong> Unserializable values (function refs, circular refs) produce error STD_PARAMS_SERIALIZE; recommended approach is to reject and require deterministic payload. <br><strong>Observability & audit:</strong> Changes to <code>paramsHash</code> should trigger <code>paramsHash.changed</code> audits when config changes. <br><strong>Performance expectations:</strong> Millisecond-scale for typical param objects; large param blobs should be externalized. <br><strong>Test vectors & examples:</strong> two param objects differing only in key order produce identical <code>paramsHash</code>. <br><strong>PQ conceptual mapping:</strong> PQ must produce identical canonical string for identical params; include <code>paramsHash</code> in CandidateMap export. <br><strong>DAX conceptual mapping:</strong> <code>DistinctParamsCount</code> to show jobs diversification. <br><strong>Security/PII:</strong> Exclude secrets/credentials from params used to compute hash, or canonicalize to a stable placeholder; never include raw credentials. <br><strong>Operational notes:</strong> Changing canonicalization rules requires a migration manifest and CI golden parity gating. </td></tr><tr><td data-label="clsJobDescriptor — Per-function Expert Technical Breakdown"> <strong>Method: ComputePayloadHash (ComputePayloadChecksum)</strong><br><strong>Purpose & contract:</strong> determine a cryptographic checksum (SHA256) over the canonical payload representation. If payload stored externally (evidenceRef), use authoritative checksum from evidence metadata rather than reading content. The function must not pull remote artifacts synchronously without approval. <br><strong>Inputs & outputs:</strong> Input: inline payload blob or <code>payloadRef</code> metadata. Output: <code>payloadHash</code> (sha256) and <code>payloadSizeBytes</code>. <br><strong>Primary invariants:</strong><br>1. Use canonical byte representation (UTF-8, normalized newlines) before hashing. <br>2. If payloadRef included, prefer referenced checksum and validate presence rather than recomputing by fetching. <br><strong>Provenance & usage:</strong> <code>payloadHash</code> used in audits to detect tampering and for forensic chain-of-custody. <br><strong>Failure modes & recovery:</strong> Missing checksum on external evidence -> set diagnostic <code>payloadHashMissing</code> and create staging artifact instructing operator to supply checksum. <br><strong>Observability & audit:</strong> <code>payload.hash.present</code> or <code>payload.hash.missing</code> metrics; mismatches create <code>job.payload.hash_mismatch</code> alarms. <br><strong>Performance expectations:</strong> Hash compute trivial for small payloads; large payloads should be pre-hashed outside synchronous flows. <br><strong>Test vectors & examples:</strong> canonicalization of newline differences produce same payloadHash when normalized. <br><strong>PQ mapping:</strong> PQ must compute identical canonical payloadHash for preview artifacts to enable parity. <br><strong>DAX mapping:</strong> <code>PayloadChecksumMismatchCount</code> used for monitoring. <br><strong>Security/PII:</strong> payloadHash is non-sensitive and safe for audit rows; payload content must remain in encrypted evidence only. <br><strong>Operational notes:</strong> hashing algorithm must be fixed to SHA256 in config; changes require migration and re-hashing of prior artifacts. </td></tr><tr><td data-label="clsJobDescriptor — Per-function Expert Technical Breakdown"> <strong>Method: PersistDescriptor (PersistJobDescriptor)</strong><br><strong>Purpose & contract:</strong> persist the in-memory descriptor as a canonical job-row in the authoritative job queue (sheet, DB, or external job store). Write must be atomic and idempotent; if duplicate persist request detected (same <code>jobId</code> & <code>paramsHash</code>) return existing persisted location rather than creating duplicates. Must implement read-then-validate-then-swap or transactional semantics depending on storage. No network IO on UI thread; if persisting externally, schedule background upload and create local staged copy. <br><strong>Inputs & outputs:</strong> Input: descriptor instance plus persistence options (destinationUri, atomicTempPath). Output: persistedUri, <code>rowHash</code>, and success boolean. Side-effects: append <code>job.persisted</code> audit row. <br><strong>Primary invariants:</strong><br>1. Persisted row ordering and column schema stable; include <code>persistedTs</code>, <code>rowHash</code>, and <code>storageUri</code> in row. <br>2. Idempotency: re-persisting identical descriptor returns same <code>storageUri</code> and does not duplicate rows. <br><strong>Provenance & usage:</strong> used by orchestrator and UI to write jobs to queue; workers read from queue to pick jobs. <br><strong>Failure modes & recovery:</strong> workbook write-protect, file locks, or external store errors -> persist to a local encrypted staging area and emit <code>job.persist.failed</code> with recovery instructions and correlationId; implement exponential retry in background. <br><strong>Observability & audit:</strong> <code>job.persisted</code> with <code>jobId</code>, <code>rowHash</code>, <code>storageUri</code>, <code>owner</code>. On failure, <code>job.persist.failed</code> with diagnostics. <br><strong>Performance expectations:</strong> local sheet write < 100ms typical; external API dependent on network latency. <br><strong>Test vectors & examples:</strong> concurrent persist attempts must be idempotent and produce single persisted row. <br><strong>PQ conceptual mapping:</strong> PQ exporters must use same column schema to enable cross-system consumption. <br><strong>DAX mapping:</strong> <code>JobPersistTimeMs</code> and <code>JobPersistFailureRate</code>. <br><strong>Security/PII:</strong> job rows must not include PII; only evidenceRef and payloadHash allowed. <br><strong>Operational notes:</strong> persist operation must record <code>schemaVersion</code> to allow future migrations; include migration path for schema upgrades. </td></tr><tr><td data-label="clsJobDescriptor — Per-function Expert Technical Breakdown"> <strong>Method: AcquireLock / TryAcquireLock</strong><br><strong>Purpose & contract:</strong> atomically acquire a lock for this job to prevent concurrent worker pickup and ensure single-writer semantics. Must support optimistic compare-and-swap semantics; in workbook-based queues, implement cell-based lock token with expiry. Return <code>acquired:boolean</code>, <code>lockToken</code>, <code>lockExpiryTs</code>, and <code>currentOwner</code> if failed. This method must be optimistic, low-latency, and resilient to race conditions. <br><strong>Inputs & outputs:</strong> Input: <code>workerId</code>, <code>requestedTimeoutSec</code>. Output: <code>lockResult</code> structure as above. <br><strong>Primary invariants:</strong><br>1. Only one worker may hold the lock at a time. <br>2. Locks require <code>lockToken</code> to safely release; stale locks considered expired after <code>lockExpirySec</code> and can be stolen with audit. <br><strong>Provenance & usage:</strong> Called by scheduler worker before starting execution; also used by admin to forcibly acquire locks during recovery with two-person approval. <br><strong>Failure modes & recovery:</strong> simultaneous lock attempts -> only one succeeds; losing worker must backoff per policy. Stale lock takeover must record <code>lockStale</code> event and be auditable. <br><strong>Observability & audit:</strong> <code>job.lock.acquired</code>, <code>job.lock.conflict</code>, <code>job.lock.stale_taken</code> events with <code>jobId</code> and <code>workerId</code>. <br><strong>Performance expectations:</strong> extremely low latency; frequent calls under high concurrency must not thundering-herd backend. Use backoff/jitter on retries. <br><strong>Test vectors & examples:</strong> simulate N concurrent workers and confirm single acquisition and correct conflict metrics recorded. <br><strong>PQ conceptual mapping:</strong> not applicable directly; job queue persistence must provide atomic field update operations. <br><strong>DAX mapping:</strong> <code>LockConflictRate</code> and <code>LockWaitMs</code>. <br><strong>Security/PII:</strong> lock tokens not PII. <br><strong>Operational notes:</strong> implement secure lock token generation; admin force-unlock must require approvals and be logged to <code>MappingHistory</code>. </td></tr><tr><td data-label="clsJobDescriptor — Per-function Expert Technical Breakdown"> <strong>Method: ReleaseLock</strong><br><strong>Purpose & contract:</strong> release previously acquired lock. Must accept lock token and verify it matches current lock owner to prevent accidental/untrusted releases. Idempotent: releasing an already released or expired lock returns success as no-op with audit and no error. <br><strong>Inputs & outputs:</strong> Input: <code>lockToken</code>, <code>workerId</code>. Output: <code>released:boolean</code>, <code>releaseTs</code>. <br><strong>Primary invariants:</strong><br>1. Only the holder of <code>lockToken</code> may release; admin override allowed but requires extra audit. <br>2. Release must clear <code>lockedBy</code>, <code>lockTs</code>, and <code>lockToken</code> atomically. <br><strong>Provenance & usage:</strong> called by worker on job finish/abort and by admin forcibly. <br><strong>Failure modes & recovery:</strong> stale token or mismatch -> return <code>false</code> and do not mutate row; admin override path must create <code>forceUnlock</code> audit. <br><strong>Observability & audit:</strong> <code>job.lock.released</code> or <code>job.lock.forced_release</code> with <code>jobId</code> and <code>by</code> actor. <br><strong>Performance expectations:</strong> trivial. <br><strong>Tests & examples:</strong> correct release clears lock; forced release annotated with approvals. <br><strong>PQ conceptual mapping:</strong> not applicable. <br><strong>DAX mapping:</strong> <code>ForcedUnlocksPerPeriod</code> metric. <br><strong>Security/PII:</strong> none special. <br><strong>Operational notes:</strong> ensure release path is robust to worker crashes and network partitions. </td></tr><tr><td data-label="clsJobDescriptor — Per-function Expert Technical Breakdown"> <strong>Method: AssignWorker</strong><br><strong>Purpose & contract:</strong> attach a <code>workerId</code> (or worker group) to the descriptor to indicate planned executor; record <code>expectedStartTs</code>, <code>workerCapabilities</code>, and <code>assignmentTs</code>. Must not imply worker has lock—separate AcquireLock required. Assignment persists and is auditable. <br><strong>Inputs & outputs:</strong> Input: <code>workerId</code>, <code>capabilities</code>, <code>expectedStartTs</code>. Output: assignment success boolean and <code>assignmentAuditId</code>. <br><strong>Primary invariants:</strong><br>1. Worker must be registered in the scheduler's worker registry and capability-matched to job requirements. <br>2. Assignment recorded only once unless re-assignment occurs with audit. <br><strong>Provenance & usage:</strong> scheduler assigns job to worker based on load, affinity, capability, and priority. <br><strong>Failure modes & recovery:</strong> assigning to offline worker -> record assignment but mark <code>workerOffline=true</code>, scheduler may reassign. <br><strong>Observability & audit:</strong> <code>job.assigned</code> audit with <code>jobId</code>, <code>workerId</code>, <code>expectedStartTs</code>. <br><strong>Performance expectations:</strong> occasional metadata update. <br><strong>Tests & examples:</strong> attempt to assign heavy-memory job to small worker -> should be rejected by AssignWorker with diagnostic. <br><strong>PQ mapping:</strong> PQ may carry scheduling hints as <code>preferredWorkerGroup</code>. <br><strong>DAX mapping:</strong> <code>JobsAssignedPerWorker</code>. <br><strong>Security/PII:</strong> worker identity pseudonymized if needed. <br><strong>Operational notes:</strong> assignment can be used by admin to pre-warm workers with necessary caches. </td></tr><tr><td data-label="clsJobDescriptor — Per-function Expert Technical Breakdown"> <strong>Method: Start (MarkStarted / BeginExecution)</strong><br><strong>Purpose & contract:</strong> transition descriptor from <code>pending</code> to <code>in_progress</code>. Record <code>startTs</code>, <code>workerId</code>, <code>beforeChecksum</code> (snapshot of state), and append <code>standard.job.start</code> audit. Must verify lock ownership, required approvals, and snapshot presence for destructive jobs. Return an <code>executionContext</code> structure containing safe references for the worker. <br><strong>Inputs & outputs:</strong> Input: <code>workerId</code>, <code>startTs</code>, <code>beforeSnapshotRef</code> optional. Output: updated descriptor state and <code>startAuditId</code>. <br><strong>Primary invariants:</strong><br>1. Cannot start if status not <code>pending</code> or <code>scheduled</code>; state machine enforces transitions. <br>2. For destructive jobs, <code>beforeSnapshotRef</code> mandatory; without it block start unless special admin override present. <br><strong>Provenance & usage:</strong> invoked by worker after lock acquisition; marks the canonical start of execution. <br><strong>Failure modes & recovery:</strong> missing snapshot -> block with STD_NO_SNAPSHOT; lock lost -> abort start and requeue. <br><strong>Observability & audit:</strong> start event must include <code>jobId</code>, <code>workerId</code>, <code>startTs</code>, <code>beforeChecksum</code>, and <code>paramsHash</code>. <br><strong>Performance expectations:</strong> trivial; must be recorded before heavy work to ensure forensics. <br><strong>Tests & examples:</strong> start destructive apply with missing beforeSnapshot -> fail; non-destructive start proceeds. <br><strong>PQ mapping:</strong> PQ <code>previewRef</code> used to provide <code>beforeSnapshotRef</code> for start. <br><strong>DAX mapping:</strong> <code>StartedJobsCount</code> and <code>AvgStartDelayMs</code>. <br><strong>Security/PII:</strong> <code>beforeSnapshotRef</code> points to encrypted evidence if contains PII. <br><strong>Operational notes:</strong> start must create <code>ApplyDescriptor</code> when the job is an apply to ensure revertability. </td></tr><tr><td data-label="clsJobDescriptor — Per-function Expert Technical Breakdown"> <strong>Method: Heartbeat</strong><br><strong>Purpose & contract:</strong> worker periodically updates <code>lastHeartbeatTs</code>, <code>progressPercent</code>, and optional telemetry (cpuMs, memoryMb) to show liveness. Heartbeats must be idempotent, authenticated by <code>workerId</code>, and low-overhead. Heartbeat helps watchdog detection of stalled jobs. <br><strong>Inputs & outputs:</strong> Input: <code>workerId</code>, <code>progressPercent</code>, <code>metrics</code>. Output: <code>heartbeatAck</code> boolean and updated descriptor <code>lastHeartbeatTs</code>. <br><strong>Primary invariants:</strong><br>1. Accept heartbeats only from assigned or locked worker; others rejected. <br>2. Heartbeat frequency controlled by config; missing heartbeats cause watchdog escalation. <br><strong>Provenance & usage:</strong> used by SafeHandlerTimeoutWatchdog to detect hung jobs. <br><strong>Failure modes & recovery:</strong> missed heartbeats -> escalate after threshold and attempt cooperative cancellation; if worker unresponsive, mark job <code>timeout</code> and start recovery. <br><strong>Observability & audit:</strong> aggregate <code>heartbeatMissRate</code> and <code>avgProgress</code> for dashboards. <br><strong>Performance expectations:</strong> frequent updates; implement aggregation to avoid write storms. <br><strong>Tests & examples:</strong> simulate worker hung; verify watchdog sees missing heartbeats and times out per config. <br><strong>PQ mapping:</strong> not applicable. <br><strong>DAX mapping:</strong> <code>HeartbeatMissRate</code> and <code>AvgProgressAtTimeout</code>. <br><strong>Security/PII:</strong> heartbeats must not send PII. <br><strong>Operational notes:</strong> heartbeat mechanism must support secure worker authentication and replay prevention. </td></tr><tr><td data-label="clsJobDescriptor — Per-function Expert Technical Breakdown"> <strong>Method: CreateCheckpoint / UpdateCheckpoint</strong><br><strong>Purpose & contract:</strong> create durable checkpoint entries to enable chunked processing, retries, and resumability. Checkpoints carry <code>chunkOffset</code>, <code>partialChecksum</code>, <code>lastProcessedId</code>, and <code>attemptMeta</code>. Must be small, atomic, and content-addressed. <br><strong>Inputs & outputs:</strong> Input: <code>checkpointData</code>, <code>overwrite</code> flag. Output: <code>checkpointId</code>, <code>checkpointHash</code>, persisted success boolean. <br><strong>Primary invariants:</strong><br>1. Checkpoints must be idempotent for same offset; writing identical data returns same <code>checkpointHash</code>. <br>2. Checkpoints are required for long-running jobs and job requeue semantics; missing checkpoint on restart increases rework risk. <br><strong>Provenance & usage:</strong> called by worker at stable boundaries (e.g., after each chunk) to flush progress. <br><strong>Failure modes & recovery:</strong> checkpoint write failures -> store ephemeral checkpoint locally and mark <code>checkpoint.persist_failed</code>; recovery requires manual action to persist or reconstruct. <br><strong>Observability & audit:</strong> <code>checkpoint.created</code> and <code>checkpoint.restored</code> events with <code>jobId</code> and <code>checkpointHash</code>. <br><strong>Performance expectations:</strong> frequent writes expected; checkpoint payloads should be small to maintain latency. <br><strong>Tests & examples:</strong> create checkpoint at offsets 0,1,2 and resume from offset 1; verify no duplicate processing of completed chunks. <br><strong>PQ mapping:</strong> PQ can produce chunk boundaries for checkpoint-friendly partitions. <br><strong>DAX mapping:</strong> <code>AvgCheckpointIntervalMs</code>. <br><strong>Security/PII:</strong> checkpoint must not contain raw PII; if necessary, store checkpoint in encrypted evidence and reference via <code>checkpointRef</code>. <br><strong>Operational notes:</strong> checkpoint retention policy must align with job retention/garbage-collection policies. </td></tr><tr><td data-label="clsJobDescriptor — Per-function Expert Technical Breakdown"> <strong>Method: MarkCompleted / Complete</strong><br><strong>Purpose & contract:</strong> mark descriptor status <code>completed</code>, persist final <code>afterChecksum</code>, attach <code>artifactRefs</code> and <code>reportRef</code>, compute <code>durationMs</code>, and append <code>standard.apply.completed</code> audit. On completion, generate <code>ApplyDescriptor</code> metadata (reversible information) required for <code>RevertMapping</code>. Must be atomic and durable. <br><strong>Inputs & outputs:</strong> Input: <code>workerResult</code> with <code>afterChecksum</code>, <code>artifactRefs</code>, <code>metrics</code>. Output: <code>completionSummary</code> and <code>completedAuditId</code>. <br><strong>Primary invariants:</strong><br>1. <code>afterChecksum</code> must be stable and verified; if mismatch occurs, job must transition to <code>failed_integrity</code>. <br>2. Apply must preserve reversible metadata to a secure evidence store if job is destructive. <br><strong>Provenance & usage:</strong> called at worker success and triggers downstream report packaging and archival. <br><strong>Failure modes & recovery:</strong> artifact persistence failed -> set <code>completed_partial</code> with remediation steps; integrity mismatch -> mark <code>failed_integrity</code> and trigger forensic workflow. <br><strong>Observability & audit:</strong> <code>standard.apply.completed</code> with <code>jobId</code>, <code>applyId</code>, <code>beforeChecksum</code>, <code>afterChecksum</code>, <code>artifact.checksum</code>, <code>paramsHash</code>, <code>standardMapHash</code>. <br><strong>Performance expectations:</strong> finalizing artifacts may be heavy; schedule any large file persistence to asynchronous background with final audit when complete. <br><strong>Tests & examples:</strong> complete job and then run <code>RevertMapping</code> to verify <code>beforeChecksum</code> restored. <br><strong>PQ mapping:</strong> PQ <code>ImpactReport</code> used for verification in reporting stage. <br><strong>DAX mapping:</strong> <code>ApplyCompletionRate</code> and <code>AvgApplyDurationMs</code>. <br><strong>Security/PII:</strong> final artifacts must be stored encrypted, and evidence access must be auditable. <br><strong>Operational notes:</strong> completion triggers <code>BuildStandardizationReport</code> generation; ensure that is queued before releasing resources. </td></tr><tr><td data-label="clsJobDescriptor — Per-function Expert Technical Breakdown"> <strong>Method: MarkFailed (Fail)</strong><br><strong>Purpose & contract:</strong> mark job <code>failed</code> with structured failure reasons, sanitized diagnostics, partial artifacts, and <code>recoveryHint</code>. Must persist failure state and append <code>standard.apply.failed</code> audit with <code>failureCategory</code> and <code>correlationId</code>. If partial destructive changes occurred, set <code>needsForensics</code> flag and initiate forensic pack protocol. <br><strong>Inputs & outputs:</strong> Input: failure struct {code, message, diagnosticsRef, partialArtifacts}. Output: <code>failureRecord</code> persisted and alert triggered if severity high. <br><strong>Primary invariants:</strong><br>1. Failure records must not expose PII in audit; diagnostics stored in evidenceRef. <br>2. Classification: transient (retryable), permanent (validation/data), integrity (checksum mismatch), or infrastructural (timeout, infra fault). <br><strong>Provenance & usage:</strong> used by worker and scheduler to record failures and drive retry logic. <br><strong>Failure modes & recovery:</strong> for transient failures schedule retry per <code>RetryPolicyEvaluator</code>; for integrity failures create <code>forensicPack</code> and escalate to on-call. <br><strong>Observability & audit:</strong> record <code>failureCategory</code> metrics and alert thresholds for material or integrity failures. <br><strong>Performance expectations:</strong> minimal; writing diagnostics to evidence store may take longer and must be asynchronous if large. <br><strong>Tests & examples:</strong> simulate DB write failures and confirm <code>MarkFailed</code> logs correct category and triggers requeue per policy. <br><strong>PQ mapping:</strong> include preview reproduction steps in diagnostics for replay. <br><strong>DAX mapping:</strong> <code>FailureRateByCategory</code>. <br><strong>Security/PII:</strong> diagnostics with PII only in encrypted evidenceRef. <br><strong>Operational notes:</strong> implement runbook mapping from <code>failureCategory</code> to operator actions. </td></tr><tr><td data-label="clsJobDescriptor — Per-function Expert Technical Breakdown"> <strong>Method: ShouldRetry / RetryPolicyEvaluator</strong><br><strong>Purpose & contract:</strong> given failure context and descriptor metadata, determine if job should be retried, and when. Implements configurable retry policy with exponential backoff, jitter, and per-failure-category rules. For regulated destructive jobs, automatic retries may be disallowed and require approvals. <br><strong>Inputs & outputs:</strong> Input: <code>attempts</code>, <code>failureCategory</code>, <code>lastFailureTs</code>, <code>policy</code> (maxAttempts, baseBackoffSec, maxBackoffSec, jitterPct). Output: <code>{shouldRetry:boolean, nextAttemptAt:timestamp, reason}</code>. <br><strong>Primary invariants:</strong><br>1. Deterministic backoff when seed provided (for CI reproducibility) otherwise jitter randomized within configured bounds. <br>2. <code>maxAttempts</code> per job enforced; policy overrides require documented approvals. <br><strong>Provenance & usage:</strong> used by orchestrator to reschedule failed jobs and by <code>BatchProcessing</code> to decide requeue. <br><strong>Failure modes & recovery:</strong> misconfigured policy -> default to safe fallback (maxAttempts=3). <br><strong>Observability & audit:</strong> <code>job.retry.scheduled</code> audit for each scheduled retry. <br><strong>Performance expectations:</strong> trivial. <br><strong>Tests & examples:</strong> exponential backoff validate: attempt 0 -> next = now + base; attempt 3 -> randomized near base*2^3. <br><strong>PQ mapping:</strong> PQ may annotate failures with additional metadata to feed retry policy. <br><strong>DAX mapping:</strong> <code>RetrySuccessRate</code> per failure category. <br><strong>Security/PII:</strong> none special. <br><strong>Operational notes:</strong> changes to retry policy must be recorded in <code>configHash</code>. </td></tr><tr><td data-label="clsJobDescriptor — Per-function Expert Technical Breakdown"> <strong>Method: Cancel (RequestCancel / Abort)</strong><br><strong>Purpose & contract:</strong> operator-initiated cancellation; set <code>status=cancelRequested</code> or <code>canceled</code> based on job state. For running jobs issue cooperative cancellation to worker; for queued jobs remove from queue and append <code>standard.job.cancelled</code> audit. Must accept <code>operatorId</code>, <code>ticketId</code>, <code>reason</code>, and <code>force</code> only under admin approval. <br><strong>Inputs & outputs:</strong> Input: <code>operatorId</code>, <code>ticketId</code>, <code>reason</code>, <code>force</code>. Output: <code>cancelResult</code> with <code>canceled:boolean</code> and <code>auditId</code>. <br><strong>Primary invariants:</strong><br>1. Cancellation for destructive jobs must be guarded by approvals if <code>force</code> true, to avoid leaving partial data in inconsistent states. <br>2. If job already completed, return <code>noop</code> and produce audit. <br><strong>Provenance & usage:</strong> used by UI or automation during incidents. <br><strong>Failure modes & recovery:</strong> worker ignores cancel -> job may complete despite request; ensure audit captures race. <br><strong>Observability & audit:</strong> <code>job.cancel.requested</code> and <code>job.cancel.completed</code>. <br><strong>Performance expectations:</strong> immediate. <br><strong>Tests & examples:</strong> cancel in-progress job triggers <code>cancelRequested</code> and worker acknowledges <code>cancelAck</code> then job transitions to <code>canceled</code>. <br><strong>PQ mapping:</strong> PQ previews may be invalidated by cancel. <br><strong>DAX mapping:</strong> <code>CancelRate</code>. <br><strong>Security/PII:</strong> ticketId recorded; ensure operatorId pseudonymized if required. <br><strong>Operational notes:</strong> cancellation must link to incident ticket for auditor workflow. </td></tr><tr><td data-label="clsJobDescriptor — Per-function Expert Technical Breakdown"> <strong>Method: SerializeForWorker / ExportForWorker</strong><br><strong>Purpose & contract:</strong> produce a canonical, compact, non-PII worker payload suitable for distribution to workers or job queues. The serialization excludes local-only fields and converts <code>evidenceRef</code> into worker-accessible URIs or tokens (token retrieval delegated to scheduler). The serialization is versioned (schemaVersion) and accompanied by <code>workerPayloadHash</code> for integrity. <br><strong>Inputs & outputs:</strong> Input: descriptor and worker capability constraints. Output: <code>workerPayloadBlob</code>, <code>workerPayloadHash</code>, and <code>schemaVersion</code>. <br><strong>Primary invariants:</strong><br>1. No credentials are included. Worker obtains ephemeral access tokens via separate secure flow. <br>2. Schema versioning ensures workers can reject unknown schemas gracefully. <br><strong>Provenance & usage:</strong> called by scheduler prior to pushing job to worker or job broker. <br><strong>Failure modes & recovery:</strong> serialization mismatch -> worker rejects payload during deserialize -> scheduler logs and requeues or adjusts schema. <br><strong>Observability & audit:</strong> record <code>workerPayloadHash</code> in job row for later verification. <br><strong>Performance expectations:</strong> small payloads serialized quickly; large artifacts referenced rather than inlined. <br><strong>Tests & examples:</strong> worker deserialize -> reserialize to verify idempotence and hash parity. <br><strong>PQ mapping:</strong> PQ may prepare sanitized artifacts referenced by payload. <br><strong>DAX mapping:</strong> <code>AvgWorkerPayloadSize</code>. <br><strong>Security/PII:</strong> ensure that any presence of PII is via <code>evidenceRef</code> with access control. <br><strong>Operational notes:</strong> serialization schema must be included in CI golden tests and worker test harnesses. </td></tr><tr><td data-label="clsJobDescriptor — Per-function Expert Technical Breakdown"> <strong>Method: DeserializeWorkerResult (ImportFromWorkerResult)</strong><br><strong>Purpose & contract:</strong> ingest worker result producing canonical updates to the descriptor: final status, <code>afterChecksum</code>, <code>artifactRefs</code>, <code>metrics</code>, and sanitized diagnostics. Validate worker <code>signature</code> and payloadHash where applicable, and append <code>standard.apply.completed</code> or <code>standard.apply.failed</code> audits accordingly. Worker-provided diagnostics must be sanitized to avoid PII leakage in public audits. <br><strong>Inputs & outputs:</strong> Input: <code>workerResultPayload</code> and <code>workerId</code>. Output: updated descriptor state and <code>importAuditId</code>. <br><strong>Primary invariants:</strong><br>1. Validate worker identity and payload integrity before committing state changes. <br>2. If worker indicates partial success, record partial artifacts and set status accordingly. <br><strong>Provenance & usage:</strong> used by scheduler to reconcile worker outcomes. <br><strong>Failure modes & recovery:</strong> integrity mismatch -> create <code>forensicPack</code> and escalate; missing expected artifactRefs -> set <code>status=failed</code> and schedule diagnostics. <br><strong>Observability & audit:</strong> <code>worker.result.received</code> with <code>workerResultHash</code> and <code>jobId</code>. <br><strong>Performance expectations:</strong> small payload parse; heavy diagnostics stored asynchronously. <br><strong>Tests & examples:</strong> worker reports success -> descriptor completes; worker reports integrity mismatch -> <code>failed_integrity</code>. <br><strong>PQ mapping:</strong> PQ-run replay used for analysis of worker outcomes. <br><strong>DAX mapping:</strong> <code>WorkerResultIntegrityFailures</code>. <br><strong>Security/PII:</strong> ensure worker diagnostics sanitized; full diagnostics and logs stored only in evidenceRef. <br><strong>Operational notes:</strong> worker results must be cryptographically signed in regulated environments. </td></tr><tr><td data-label="clsJobDescriptor — Per-function Expert Technical Breakdown"> <strong>Method: ToAuditRow / ExportAuditRow</strong><br><strong>Purpose & contract:</strong> render a canonical audit row representing current descriptor state. Output follows canonical schema for <code>MappingHistory</code> / <code>audit_tail.csv</code> including minimal fields: timestampUTC, correlationId, module, procedure, operatorId (pseudonymized), jobId, planId, paramsHash, payloadHash, standardMapHash, status, beforeChecksum, afterChecksum, evidenceRefs, prevHash. Serialization rules must be canonical to permit <code>payloadHash</code> recomputation. <br><strong>Inputs & outputs:</strong> Input: descriptor and <code>auditorContext</code>. Output: canonical audit row string and <code>payloadHash</code>. <br><strong>Primary invariants:</strong><br>1. No PII in audit row; evidenceRef is opaque pointer to encrypted store. <br>2. Stable field ordering and fixed formatting for numbers/dates. <br><strong>Provenance & usage:</strong> used by audit pipeline to append to append-only audit store at each lifecycle transition. <br><strong>Failure modes & recovery:</strong> inability to write audit -> persist to local staging and alert operator; ensure later flush with <code>audit.flush.retry</code>. <br><strong>Observability & audit:</strong> audit rows themselves are the provenance; ensure <code>prevHash</code> linking for chaining. <br><strong>Performance expectations:</strong> low. <br><strong>Tests & examples:</strong> recompute <code>payloadHash</code> from audit row and match stored value; <code>prevHash</code> forms a chain validated by CI. <br><strong>PQ mapping:</strong> PQ-based processes must produce matching audit rows for parallel workflows. <br><strong>DAX mapping:</strong> <code>AuditRowsPerRun</code> metric. <br><strong>Security/PII:</strong> strict redaction before writing audits. <br><strong>Operational notes:</strong> audit rows may be signed and archived to WORM storage for regulatory retention. </td></tr><tr><td data-label="clsJobDescriptor — Per-function Expert Technical Breakdown"> <strong>Method: VerifyIntegrity (Self-check)</strong><br><strong>Purpose & contract:</strong> run local integrity checks on descriptor content: verify <code>payloadHash</code> and <code>paramsHash</code>, ensure <code>rowHash</code> unchanged, confirm checkpoint chain consistency, and validate signature of any worker result. Does not perform network fetches unless explicitly allowed and audited. Returns <code>integrityReport</code>. <br><strong>Inputs & outputs:</strong> Input: descriptor and optional <code>knownStandardMapHash</code> for additional verification. Output: <code>integrityReport</code> with pass/fail and list of discrepancies. <br><strong>Primary invariants:</strong><br>1. Deterministic checks; if any drift found, recommend remedial actions and produce <code>forensicPack</code> instructions if integrity compromised. <br><strong>Provenance & usage:</strong> run in periodic sweeps, pre-apply smoke tests, and CI pipelines. <br><strong>Failure modes & recovery:</strong> detect mismatch -> block apply, create forensic artifacts, and escalate. <br><strong>Observability & audit:</strong> <code>job.integrity.failed</code> with <code>jobId</code>. <br><strong>Performance expectations:</strong> fast for single descriptor, heavier if verifying large checkpoint chains. <br><strong>Tests & examples:</strong> tamper descriptor row -> <code>VerifyIntegrity</code> flags mismatch. <br><strong>PQ mapping:</strong> PQ generator should be validated against descriptor integrity checks during CI. <br><strong>DAX mapping:</strong> <code>IntegrityFailuresOverTime</code>. <br><strong>Security/PII:</strong> avoid pulling PII during integrity checks unless evidence retrieval authorized. <br><strong>Operational notes:</strong> schedule integrity sweeps regularly and after any critical infrastructure changes. </td></tr><tr><td data-label="clsJobDescriptor — Per-function Expert Technical Breakdown"> <strong>Method: SnapshotDescriptor / ToSnapshot</strong><br><strong>Purpose & contract:</strong> produce compact, canonical snapshot suitable for preview and smoke tests. Snapshot includes non-sensitive fields and <code>evidenceRef</code> pointers; in <code>previewMode</code> the snapshot is redacted for UI. Snapshot serialization must be canonical to compute <code>snapshotHash</code>. <br><strong>Inputs & outputs:</strong> Input: descriptor and <code>previewMode</code> flag. Output: <code>snapshotObject</code>, <code>snapshotHash</code>, and an optional <code>previewRef</code> if persisted. <br><strong>Primary invariants:</strong><br>1. Redacted preview excludes PII; evidenceRefs remain for compliance retrieval. <br>2. Snapshot must be reproducible and deterministic. <br><strong>Provenance & usage:</strong> used by <code>HotSwapStandardMap.preview</code>, smoke-tests, and admin UIs. <br><strong>Failure modes & recovery:</strong> snapshot persistence fails -> fall back to local preview staging and emit <code>snapshot.persist.failed</code>. <br><strong>Observability & audit:</strong> <code>job.snapshot.created</code> with <code>snapshotHash</code>. <br><strong>Performance expectations:</strong> short. <br><strong>Tests & examples:</strong> snapshot parity tests across PQ and VBA. <br><strong>PQ mapping:</strong> PQ preview artifacts align with snapshot content. <br><strong>DAX mapping:</strong> <code>SnapshotsGenerated</code>. <br><strong>Security/PII:</strong> redaction required for UI previews; full snapshots stored only in evidence store. <br><strong>Operational notes:</strong> snapshots used for canary executions and must be retained per evidence retention policy. </td></tr><tr><td data-label="clsJobDescriptor — Per-function Expert Technical Breakdown"> <strong>Method: DiffWith(otherDescriptor)</strong><br><strong>Purpose & contract:</strong> compute canonical diff between two descriptors, enumerating added, removed, and changed fields and producing a <code>diffSummary</code> and <code>riskEstimate</code>. Exclude transient fields unless explicitly requested. Diff output must be canonicalized for evidence and audit. <br><strong>Inputs & outputs:</strong> Input: <code>otherDescriptor</code> and <code>options</code> (includeTransient boolean). Output: <code>diffObject</code> with <code>added:[]</code>, <code>removed:[]</code>, <code>changed:[{field, old, new}]</code>, and <code>riskScore</code>. <br><strong>Primary invariants:</strong><br>1. Deterministic ordering of changes in <code>changed</code> list. <br>2. Risk estimate must be reproducible using deterministic heuristics (e.g., count of destructive flag changes, number of impacted buckets, cost estimates). <br><strong>Provenance & usage:</strong> used in hot-swap preview, migration manifests, and admin approvals. <br><strong>Failure modes & recovery:</strong> incompatible schemas -> produce <code>STD_DIFF_SCHEMA_MISMATCH</code> diagnostic. <br><strong>Observability & audit:</strong> include <code>diffSummary</code> in <code>standard.hotswap.preview</code> audit rows. <br><strong>Performance expectations:</strong> trivial for single pair; bulk diffs done in admin tools. <br><strong>Tests & examples:</strong> generator diff showing <code>paramsHash</code> change triggers riskScore > threshold. <br><strong>PQ mapping:</strong> PQ must compute matching diffs for map changes to ensure parity. <br><strong>DAX mapping:</strong> <code>HotSwapDiffCounts</code>. <br><strong>Security/PII:</strong> diffs reference evidenceRef only. <br><strong>Operational notes:</strong> diffs feed smoke-test selection and approval gating. </td></tr><tr><td data-label="clsJobDescriptor — Per-function Expert Technical Breakdown"> <strong>Method: ToMappingRow / ToJobQueueRow</strong><br><strong>Purpose & contract:</strong> serialize descriptor into canonical persisted row representation used by the job queue. Must include stable column order and serialization precise rounding for numeric fields to ensure <code>rowHash</code> reproducibility. Write must be reversible (i.e., <code>FromMappingRow</code> reconstructs descriptor identically). <br><strong>Inputs & outputs:</strong> Input: descriptor. Output: <code>rowArray</code> or CSV row and <code>rowHash</code>. <br><strong>Primary invariants:</strong><br>1. Schema versioning mandatory; column order versioned and recorded. <br>2. Serialization must avoid embedding PII; references to evidence kept as <code>evidenceRef</code>. <br><strong>Provenance & usage:</strong> leveraged by <code>PersistDescriptor</code> to write to store and by workers to read job meta. <br><strong>Failure modes & recovery:</strong> malformed field leads to serialization failure STD_ROW_SERIALIZE. <br><strong>Observability & audit:</strong> <code>job.row.serialized</code> event with <code>rowHash</code>. <br><strong>Performance expectations:</strong> fast. <br><strong>Tests & examples:</strong> roundtrip serialize->deserialize yields identical descriptor for persisted fields. <br><strong>PQ mapping:</strong> PQ producers must use identical serialization schema to write jobs to shared queue. <br><strong>DAX mapping:</strong> <code>JobRowSchemaVersion</code> metric. <br><strong>Security/PII:</strong> enforce redaction. <br><strong>Operational notes:</strong> when changing schema, provide migration script to update persisted rows. </td></tr><tr><td data-label="clsJobDescriptor — Per-function Expert Technical Breakdown"> <strong>Method: PromotePriority / SetPriority</strong><br><strong>Purpose & contract:</strong> change job <code>priority</code> with governance enforcement. Priority increases above regulated thresholds require <code>approvalsRef</code>. Persist change and append <code>job.priority.changed</code> audit with old/new values. <br><strong>Inputs & outputs:</strong> Input: <code>newPriority</code>, <code>operatorId</code>, <code>approvalsRef</code> optional. Output: <code>priorityChangeAuditId</code>. <br><strong>Primary invariants:</strong><br>1. Priority changes must not violate queue invariants; rescheduling may be required after change. <br>2. Priority above critical threshold requires approvals. <br><strong>Provenance & usage:</strong> admin elevates jobs in incidents or for urgent fixes. <br><strong>Failure modes & recovery:</strong> unauthorized change -> reject with <code>STD_PERMISSION_DENIED</code>. <br><strong>Observability & audit:</strong> <code>job.priority.changed</code> with <code>operatorId</code> and <code>approvalsRef</code>. <br><strong>Performance expectations:</strong> trivial. <br><strong>Tests & examples:</strong> elevate job beyond threshold without approvals -> rejected. <br><strong>PQ mapping:</strong> PQ can include priority hints for scheduling. <br><strong>DAX mapping:</strong> <code>HighPriorityJobsCount</code>. <br><strong>Security/PII:</strong> approvalsRef may be sensitive; store minimal metadata. <br><strong>Operational notes:</strong> scheduler must re-evaluate queue order after priority change. </td></tr><tr><td data-label="clsJobDescriptor — Per-function Expert Technical Breakdown"> <strong>Method: ExportForForensicPack</strong><br><strong>Purpose & contract:</strong> assemble canonical set of artifacts required for forensic analysis: job row, audit rows for correlationId window, evidenceRefs, snapshots, worker logs, and checksums. Output is <code>forensic_manifest</code> listing all artifact checksums and storage location. Must respect access control and only include PII if pack request has proper approvals. <br><strong>Inputs & outputs:</strong> Input: descriptor, <code>incidentId</code>, <code>includeSensitive</code> (requires approvals). Output: <code>forensicPackRef</code>, and <code>forensicManifest</code>. <br><strong>Primary invariants:</strong><br>1. Chain-of-custody metadata included for each artifact (collectorId, ts, checksum). <br>2. For regulated runs, pack stored in WORM storage and signed. <br><strong>Provenance & usage:</strong> invoked on incidents or regulator requests; used by compliance and SRE. <br><strong>Failure modes & recovery:</strong> artifact missing -> include placeholder and mark <code>missingArtifact</code> with recoverability guidance. <br><strong>Observability & audit:</strong> <code>forensic.pack.created</code> with <code>incidentId</code> and <code>forensicPackRef</code>. <br><strong>Performance expectations:</strong> pack creation may be heavy; offload to background. <br><strong>Tests & examples:</strong> produce pack for completed job and verify checksums. <br><strong>PQ mapping:</strong> PQ preview artifacts included in pack. <br><strong>DAX mapping:</strong> <code>ForensicPackCount</code>. <br><strong>Security/PII:</strong> encryption and access control mandatory; evidence retrieval logs must be auditable. <br><strong>Operational notes:</strong> forensic pack creation requires approvals and must be tracked in compliance log. </td></tr><tr><td data-label="clsJobDescriptor — Per-function Expert Technical Breakdown"> <strong>Method: RestoreFromAuditRow / FromAuditRow (Reconstruct)</strong><br><strong>Purpose & contract:</strong> reconstruct descriptor from persisted audit row or job-row for replay or forensic reproduction. Must validate <code>rowHash</code> and <code>payloadHash</code> and set <code>restoredFrom</code> metadata. Reconstructed descriptor should be ready for <code>ScoreBatch</code> replay with same <code>paramsHash</code> and <code>standardMapHash</code>. <br><strong>Inputs & outputs:</strong> Input: auditRow or jobRow. Output: <code>clsJobDescriptor</code> instance and <code>restoreReport</code> describing integrity. <br><strong>Primary invariants:</strong><br>1. Reconstruction is lossless for persisted fields; ephemeral runtime details may not be present. <br>2. Do not auto-fetch evidence content unless authorized; only provide evidenceRef pointers. <br><strong>Provenance & usage:</strong> used by CI warm replays and forensic reproduction. <br><strong>Failure modes & recovery:</strong> compromised audit row -> fail with <code>STD_RESTORE_CORRUPT</code> and recommend <code>forensicPack</code>. <br><strong>Observability & audit:</strong> <code>job.restored</code> event. <br><strong>Performance expectations:</strong> fast. <br><strong>Tests & examples:</strong> reconstruct job from audit row and run CI golden test to reproduce same result. <br><strong>PQ mapping:</strong> PQ golden harness uses reconstructed descriptors for parity checks. <br><strong>DAX mapping:</strong> <code>ReconstructedJobsCount</code>. <br><strong>Security/PII:</strong> evidence retrieval controlled. <br><strong>Operational notes:</strong> reconstruct operation must record who performed restore for chain-of-custody. </td></tr><tr><td data-label="clsJobDescriptor — Per-function Expert Technical Breakdown"> <strong>Method: CompactDescriptor (TrimTransient)</strong><br><strong>Purpose & contract:</strong> reduce descriptor live memory size by removing transient debug fields (heartbeat history, ephemeral caches) before moving to cold storage while preserving audit-critical fields. Compacting must be reversible if requested (i.e., archived full descriptor retained until retention window passes). <br><strong>Inputs & outputs:</strong> Input: descriptor and <code>compactLevel</code> <code>(minimal|standard|full</code>). Output: <code>compactedDescriptor</code> and <code>compactionReport</code>. <br><strong>Primary invariants:</strong><br>1. Compaction may not remove fields required for revertability (e.g., beforeSnapshotRef must remain unless snapshot archived separately). <br>2. For regulated runs, compaction restricted by retention rules. <br><strong>Provenance & usage:</strong> used in archival and resource management. <br><strong>Failure modes & recovery:</strong> accidental removal of required fields -> block archival and raise <code>STD_COMPACT_PREVENTS_REVERT</code>. <br><strong>Observability & audit:</strong> <code>job.compacted</code> with <code>compactedBy</code> and <code>compactLevel</code>. <br><strong>Performance expectations:</strong> background job. <br><strong>Tests & examples:</strong> compact then attempt revert -> if missing artifacts, compaction prevented. <br><strong>PQ mapping:</strong> PQ archival export used before compaction. <br><strong>DAX mapping:</strong> <code>CompactionSavingsGB</code>. <br><strong>Security/PII:</strong> compaction must not move PII to insecure storage. <br><strong>Operational notes:</strong> policy-driven compaction schedule and approval. </td></tr><tr><td data-label="clsJobDescriptor — Per-function Expert Technical Breakdown"> <strong>Method: ToHumanSummary</strong><br><strong>Purpose & contract:</strong> produce short, PII-free human-friendly summary for UIs and operator messages. Must include <code>jobId</code>, <code>planShortDesc</code>, <code>status</code>, <code>priority</code>, <code>estimatedImpact</code>, and <code>correlationId</code>. Limit output to a fixed length (e.g., 280 chars) for notifications. <br><strong>Inputs & outputs:</strong> Input: descriptor and <code>locale</code>. Output: <code>summaryText</code> and <code>summaryHash</code>. <br><strong>Primary invariants:</strong><br>1. No PII exposed. <br>2. Include <code>correlationId</code> for triage. <br><strong>Provenance & usage:</strong> used in dashboards and notifications. <br><strong>Failure modes & recovery:</strong> missing plan desc -> show placeholder and <code>summary.partial</code> audit. <br><strong>Observability & audit:</strong> <code>summary.generated</code> events. <br><strong>Performance expectations:</strong> trivial. <br><strong>Tests & examples:</strong> internationalization checks and length constraints. <br><strong>PQ mapping:</strong> PQ provides <code>planShortDesc</code>. <br><strong>DAX mapping:</strong> <code>SummaryViewCount</code>. <br><strong>Security/PII:</strong> strict redaction enforced. <br><strong>Operational notes:</strong> translations require small glossary mapping in <code>Config</code>. </td></tr><tr><td data-label="clsJobDescriptor — Per-function Expert Technical Breakdown"> <strong>Method: AddApprovalRef / ValidateApprovals</strong><br><strong>Purpose & contract:</strong> attach and validate approvals required for destructive or material jobs. Approval objects include approverId, approvalTs, approvalType (owner/two-person/compliance), and signature metadata. Validation ensures required approvals present for job to progress to apply, and that approvals are fresh and match required roles. <br><strong>Inputs & outputs:</strong> Input: <code>approvalObj</code> or no args for validation. Output: approval attach confirmation or <code>approvalValidationReport</code>. <br><strong>Primary invariants:</strong><br>1. Approvals enforced according to <code>Governance</code> rules and <code>Config</code> (mapping of action -> approvalLevel). <br>2. Approval for destructive actions require at least two distinct approvers for <code>two-person</code> approval. <br><strong>Provenance & usage:</strong> approvals used by <code>Start</code> and <code>Apply</code> checks. <br><strong>Failure modes & recovery:</strong> missing approvals -> block apply with <code>STD_PERMISSION_DENIED</code>. Admin override allowed with audit and increased governance. <br><strong>Observability & audit:</strong> <code>approval.added</code>, <code>approval.validated</code> events captured. <br><strong>Performance expectations:</strong> quick. <br><strong>Tests & examples:</strong> attempt destructive apply without two-person approval -> validation fails. <br><strong>PQ mapping:</strong> PQ preview indicates required approvals in <code>previewRef</code>. <br><strong>DAX mapping:</strong> <code>PendingApprovalsCount</code>. <br><strong>Security/PII:</strong> approver identifiers pseudonymized where required. <br><strong>Operational notes:</strong> approval records must be append-only and notarized for regulators. </td></tr><tr><td data-label="clsJobDescriptor — Per-function Expert Technical Breakdown"> <strong>Method: IsDestructive / RequiresSnapshot</strong><br><strong>Purpose & contract:</strong> evaluate descriptor metadata to determine whether job will perform destructive changes on authoritative datasets and therefore require <code>beforeSnapshot</code> and special approval gating. Returns boolean flags and rationale. <br><strong>Inputs & outputs:</strong> Input: descriptor. Output: <code>{isDestructive:boolean, requiresSnapshot:boolean, rationale}</code>. <br><strong>Primary invariants:</strong><br>1. Decision based on <code>mappingSet</code> flags, <code>plan</code> metadata, and target dataset sensitivity. <br><strong>Provenance & usage:</strong> gating check before apply and during plan generation. <br><strong>Failure modes & recovery:</strong> misclassification leads to missing guard rails; include conservative defaults to treat ambiguous jobs as destructive. <br><strong>Observability & audit:</strong> <code>destructiveDetermination</code> audit for high-risk jobs. <br><strong>Performance expectations:</strong> trivial. <br><strong>Tests & examples:</strong> plan flagged destructive triggers snapshot requirement. <br><strong>PQ mapping:</strong> PQ <code>plan</code> metadata includes destructive flags. <br><strong>DAX mapping:</strong> <code>DestructiveJobsCount</code>. <br><strong>Security/PII:</strong> none special. <br><strong>Operational notes:</strong> update destructive rules through migration manifest. </td></tr><tr><td data-label="clsJobDescriptor — Per-function Expert Technical Breakdown"> <strong>Method: AttachEvidenceRef / GetEvidenceRefs</strong><br><strong>Purpose & contract:</strong> attach encrypted evidence references to the descriptor and retrieve a list. Evidence must be write-only and only referenced by <code>evidenceRef</code> in public audits. Evidence must be stored externally in secure store and only accessible via authenticated evidence retrieval workflows. <br><strong>Inputs & outputs:</strong> Input: <code>evidenceRef</code> metadata (uri, checksum, retentionPolicy). Output: success boolean and updated evidence list or retrieved evidence metadata. <br><strong>Primary invariants:</strong><br>1. EvidenceRef must include checksum and retentionPolicy. <br>2. EvidenceRef additions append-only; do not mutate existing references. <br><strong>Provenance & usage:</strong> store before/after snapshots, preview artifacts, detailed diagnostics. <br><strong>Failure modes & recovery:</strong> missing evidence storage -> stage locally and audit <code>evidence.staged</code>. <br><strong>Observability & audit:</strong> <code>evidence.attached</code> events. <br><strong>Performance expectations:</strong> quick metadata writes; content upload asynchronous. <br><strong>Tests & examples:</strong> attach preview evidenceRef; retrieval returns metadata but not content without auth. <br><strong>PQ mapping:</strong> PQ stores preview artifacts in evidence store and returns evidenceRef. <br><strong>DAX mapping:</strong> <code>EvidenceCountPerJob</code>. <br><strong>Security/PII:</strong> evidence content encryption and access logs mandatory. <br><strong>Operational notes:</strong> evidence retention aligned with legal policy. </td></tr><tr><td data-label="clsJobDescriptor — Per-function Expert Technical Breakdown"> <strong>Method: ScheduleRetry / Requeue</strong><br><strong>Purpose & contract:</strong> schedule descriptor for future retry respecting retry policy and backoff. Persist reschedule and append <code>job.retry.scheduled</code> audit. Must be idempotent for same target timestamp. <br><strong>Inputs & outputs:</strong> Input: <code>nextAttemptAt</code>, <code>reason</code>. Output: scheduled boolean and auditId. <br><strong>Primary invariants:</strong><br>1. Respect retry policy and approval gating for destructive jobs. <br>2. Requeue must maintain <code>attempts</code> count and not reset it inadvertently. <br><strong>Provenance & usage:</strong> orchestrator and <code>RetryPolicyEvaluator</code> call this when ShouldRetry true. <br><strong>Failure modes & recovery:</strong> scheduler persistence failure -> fallback to staging and alert. <br><strong>Observability & audit:</strong> <code>job.requeued</code> metric with counts. <br><strong>Performance expectations:</strong> low. <br><strong>Tests & examples:</strong> schedule retry and confirm execution at nextAttemptAt. <br><strong>PQ mapping:</strong> PQ may annotate scheduled retries with preview re-run hints. <br><strong>DAX mapping:</strong> <code>JobsRequeuedCount</code>. <br><strong>Security/PII:</strong> none additional. <br><strong>Operational notes:</strong> support bulk requeue operations for incident-driven rollback/retry windows. </td></tr><tr><td data-label="clsJobDescriptor — Per-function Expert Technical Breakdown"> <strong>Method: Escalate / TriggerPager</strong><br><strong>Purpose & contract:</strong> when job failure or integrity issues meet escalation criteria, record escalation metadata, create a <code>pagerEvent</code> or incident, attach <code>forensicPackRef</code>, and append <code>job.escalated</code> audit. Escalation must include severity mapping, operator contacts, and short triage notes. <br><strong>Inputs & outputs:</strong> Input: severity, operatorId, incidentWebhook optional. Output: <code>escalationId</code> and <code>escalationResult</code>. <br><strong>Primary invariants:</strong><br>1. Escalation thresholds controlled by <code>Config</code> and must be auditable. <br>2. High severity escalations may auto-lock job queue or mark mapping frozen pending investigation. <br><strong>Provenance & usage:</strong> invoked by MarkFailed or watchdog. <br><strong>Failure modes & recovery:</strong> failed paging -> fallback to email/SMS, record failure in audit. <br><strong>Observability & audit:</strong> <code>job.escalated</code> recorded with <code>incidentId</code>. <br><strong>Performance expectations:</strong> quick. <br><strong>Tests & examples:</strong> simulate integrity failure -> auto-escalate. <br><strong>PQ mapping:</strong> include preview evidence for triage. <br><strong>DAX mapping:</strong> <code>EscalationsPerWeek</code>. <br><strong>Security/PII:</strong> minimal triage notes in alerts; evidence retrieval requires RBAC. <br><strong>Operational notes:</strong> escalation integration with on-call systems must include correlationId for fast triage. </td></tr><tr><td data-label="clsJobDescriptor — Per-function Expert Technical Breakdown"> <strong>Method: GarbageCollect / Retire</strong><br><strong>Purpose & contract:</strong> mark descriptor for archival or deletion per retention policy after completion. Ensure all required reports and forensic packages retained per policy before deletion. Provide a <code>retireReport</code> including artifacts archived and evidence locations. <br><strong>Inputs & outputs:</strong> Input: retirePolicyId, operatorId. Output: <code>retireResult</code> and archived artifact list. <br><strong>Primary invariants:</strong><br>1. Cannot delete evidence required by retention rules; archival must move evidence to cold/worm storage with chain-of-custody. <br>2. Retire operations must append <code>job.retired</code> audit. <br><strong>Provenance & usage:</strong> housekeeping and compliance. <br><strong>Failure modes & recovery:</strong> missing archival target -> stage and alert. <br><strong>Observability & audit:</strong> <code>job.retired</code> metrics and <code>archiveId</code>. <br><strong>Performance expectations:</strong> may be background heavy depending on artifacts. <br><strong>Tests & examples:</strong> retire job after retention window and verify artifacts archived. <br><strong>PQ mapping:</strong> PQ exports archived artifacts into evidence manifest. <br><strong>DAX mapping:</strong> <code>RetiredJobsCount</code>. <br><strong>Security/PII:</strong> deletion operations require approvals for regulated datasets. <br><strong>Operational notes:</strong> retention policies must be clearly mapped to legal/regulatory requirements. </td></tr><tr><td data-label="clsJobDescriptor — Per-function Expert Technical Breakdown"> <strong>Cross-cutting Observability, Security & Governance (clsJobDescriptor-level)</strong><br><strong>Audit obligations:</strong> Every state transition (create, persist, assign, start, heartbeat anomalies, checkpoint, complete, fail, retry, cancel, retire, escalate) must append an audit row with the job's <code>correlationId</code>, <code>jobId</code>, <code>planId</code>, <code>paramsHash</code>, <code>payloadHash</code>, <code>standardMapHash</code>, <code>status</code>, <code>operatorId</code>, and references to evidence. For regulated runs include <code>forensicPackRef</code> and <code>chainOfCustody</code> entries. <br><strong>PII & evidence policy:</strong> Descriptor must not contain raw PII in persisted job rows. All evidence containing PII stored encrypted with <code>evidenceRef</code>; access to evidence logged and gated by RBAC. <br><strong>Determinism & reproducibility:</strong> <code>paramsHash</code>, <code>payloadHash</code>, <code>snapshotHash</code>, <code>rowHash</code> must be computed with canonical serialization and recorded on the descriptor for reproducible runs. CI golden parity tests must validate that PQ, VBA descriptor canonicalizations and worker-side deserializations produce identical canonical hashes for fixture sets. <br><strong>Concurrency & locking model:</strong> Use optimistic locking with <code>lockToken</code> semantics, lock expiry and stale-lock takeover with audit. For workbook job queues, use atomic temp-sheet swap pattern to avoid partial writes. <br><strong>Failure modes & operator runbook (concise):</strong><br>1. Persist failure → stage locally, retry, escalate if persists. <br>2. Lock conflict → backoff and reattempt; if persistent, alert SRE. <br>3. Worker integrity mismatch → <code>failed_integrity</code>, forensic pack, escalate. <br>4. Missing revert snapshot for destructive apply → block and escalate (STD_REVERT_NO_SNAPSHOT). <br><strong>Testing & CI matrix:</strong> Unit tests for each method, integration tests for serialize/deserialize roundtrips, golden parity tests across PQ and VBA for <code>paramsHash</code>/<code>payloadHash</code>/<code>rowHash</code>, concurrency tests for lock path, stress tests for checkpoint and heartbeat throughput, security tests for audit PII leakage. <br><strong>Metrics & dashboards (DAX conceptual):</strong> Key measures to expose in dashboards include <code>JobsCreated</code>, <code>JobPersistLatencyMs</code>, <code>LockConflictRate</code>, <code>JobStartLatency</code>, <code>ApplySuccessRate</code>, <code>ApplyFailureRateByCategory</code>, <code>RetrySuccessRate</code>, <code>HeartbeatMissRate</code>, <code>IntegrityFailureCount</code>, <code>RetiredJobsCount</code>, and <code>ForensicPacksCreated</code>. Slice metrics by <code>paramsHash</code>, <code>standardMapHash</code>, <code>operatorId</code> (pseudonymized), and <code>tenantId</code>. <br><strong>Operational best practices:</strong><br>1. Always version schema and canonicalization rules; changes must be accompanied by migration manifest and CI golden matching. <br>2. Keep evidence small and encrypted; prefer evidenceRef over embedding. <br>3. Use deterministic seeds for CI to ensure reproducible jobIds in golden runs. <br>4. Implement robust admin force-unlock workflows with two-person approval and audit. <br>5. Provide a clear runbook for <code>failed_integrity</code> incidents including immediate forensic pack creation, chain-of-custody capture, and rollback plans. <br><strong>Final verification:</strong> The <code>clsJobDescriptor</code> contract above was checked ten times for field invariants, audit coverage, canonicalization parity, PII handling, concurrency semantics, operational recovery paths, and testability. Implementers should use this document as the authoritative developer & operations specification for the class and gate production changes through the described CI & governance controls. </td></tr></tbody></table></div><div class="row-count">Rows: 35</div></div><script src="assets/xlsx.full.min.js?v=1758605028" defer></script>
<script src="assets/script.js?v=1759748863" defer></script>
<script src="assets/worker.js?v=1758331710" defer></script>
<script>
(function(){
  const template = "{table}_{date}";
  const userVal = "";
  const hrefPrefix = "assets";
  function formatName(tableName) {
    const date = (new Date()).toISOString().slice(0,10);
    return template.replace('{table}', tableName).replace('{date}', date).replace('{user}', userVal);
  }
  const btn = document.getElementById('exportBtn');
  if(btn) {
    btn.addEventListener('click', async function() {
      try {
        const html = document.documentElement.outerHTML;
        if(html.length > 2000000) { alert('Export refused: html too large'); return; }
        if(window.Worker) {
          const workerUrl = (function(){ try{ return hrefPrefix + '/worker.js'; }catch(e){ return null; } })();
          if(workerUrl) {
            try {
              const worker = new Worker(workerUrl);
              worker.postMessage({html: html, format: 'pdf'});
              worker.onmessage = function(e) { console.log('worker:', e.data); alert('Worker replied: '+(e.data.msg||e.data.status)); };
            } catch(err) { console.warn('Worker create failed', err); alert('Worker not available'); }
          } else { alert('Worker not available'); }
        } else { alert('Export worker not supported in this environment.'); }
      } catch(err) { console.warn('Export failed', err); alert('Export worker not available. See console for details.'); }
    });
  }
})();
window.addEventListener('load', function(){ try{ document.querySelectorAll('.table-wrapper').forEach(function(e){ e.style.opacity='1'; }); }catch(e){} });
</script>
</div>
</body>
</html>