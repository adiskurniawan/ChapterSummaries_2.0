<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='utf-8'><meta name='viewport' content='width=device-width,initial-scale=1'>
<title>Tables Viewer v2.1</title>
<style>:root{--main-header-height:56px;} .table-wrapper{opacity:0;min-height:24px;transition:opacity .12s linear}</style>
<link rel="stylesheet" href="assets/style.css?v=1769960840">
<link rel="stylesheet" href="assets/overrides.css?v=1771754174">
</head><body>
<div id="tables-viewer" role="region" aria-label="Tables viewer">
<div id="stickyMainHeader">
<div id="tv-header">
<div><h1>Tables Viewer v2.1</h1></div>
<div style="display:flex;gap:8px;align-items:center;">
<input id="searchBox" class="tv-search" type="search" placeholder="Search" aria-label="Search tables" />
<button id="modeBtn" type="button" onclick="toggleMode()" aria-label="Toggle theme">Theme</button>
<button id="toggleAllBtn" type="button" aria-label="Toggle all" onclick="toggleAllTables()">Collapse All Tables</button>
<button id="copyAllPlainBtn" class="copy-btn" type="button" onclick="copyAllTablesPlain()" aria-label="Copy all tables as plain text">Copy All Tables (Plain Text)</button>
<button id="copyAllTablesBtn" class="copy-all-btn" type="button" onclick="copyAllTablesMarkdown()" aria-label="Copy All Tables (Markdown)">Copy All Tables (Markdown)</button>
<button id="copyAllMdBtn" style="display:none" aria-hidden="true"></button>
<button id="resetAllBtn" type="button" onclick="resetAllTables()" aria-label="Reset All Tables">Reset All Tables</button>
</div></div>
<script>
(function(){
  function ensureDelegation(){
    try {
      var vis = document.getElementById('copyAllTablesBtn');
      var alias = document.getElementById('copyAllMdBtn');
      if(!vis || !alias) return;
      alias.addEventListener = function(type, listener, options){
        vis.addEventListener(type, listener, options);
      };
      alias.removeEventListener = function(type, listener, options){
        vis.removeEventListener(type, listener, options);
      };
      Object.defineProperty(alias, 'onclick', {
        set: function(fn){ vis.onclick = fn; },
        get: function(){ return vis.onclick; },
        configurable: true
      });
      alias.focus = function(){ vis.focus(); };
      alias.blur = function(){ vis.blur(); };
    } catch(e) {
      try{ console && console.warn && console.warn('alias delegation failed', e); }catch(_){} 
    }
  }
  if(document.readyState === 'loading'){
    document.addEventListener('DOMContentLoaded', ensureDelegation);
  } else {
    ensureDelegation();
  }
})();
</script>
<noscript><div style='color:#b91c1c'>JavaScript is disabled. Tables will be shown statically. For large tables enable JS for virtualization.</div></noscript>
<div id="tocBar" role="navigation" aria-label="Table of contents"><ul><li class="toc-item"><a class="toc-link" href="#Table1">Table 1</a></li>
<li class="toc-item"><a class="toc-link" href="#Table2">Table 2</a></li>
<li class="toc-item"><a class="toc-link" href="#Table3">Table 3</a></li>
<li class="toc-item"><a class="toc-link" href="#Table4">Table 4</a></li>
<li class="toc-item"><a class="toc-link" href="#Table5">Table 5</a></li>
<li class="toc-item"><a class="toc-link" href="#Table6">Table 6</a></li></ul></div></div>
<div class="table-caption" id="Table1" data-table="YJKN_0016_01" style="margin-top:2mm;margin-left:3mm;"><strong>Table 1</strong></div>
<div class="table-wrapper" data-table-id="table-1"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Introduction"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Introduction</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Introduction"><strong>BPJS Kesehatan — langkah penting (untuk pemberi kerja & karyawan)</strong><br>- Persiapan dokumen (pemberi kerja): akta pendirian/NPWP perusahaan, NIB/SIUP/izin usaha, KTP penanggung jawab, surat domisili (jika diminta).<br>- Persiapan karyawan: fotokopi e-KTP, Kartu Keluarga, kartu keluarga/akta lahir, paspor untuk WNA.<br>- Daftarkan badan usaha / pemberi kerja di portal BPJS Kesehatan (pendaftaran badan usaha → setuju syarat & ketentuan). Setelah akun aktif, lakukan entri peserta: manual melalui aplikasi new e-DABU atau upload formulir 34 kolom (pendataan massal).<br>- Verifikasi & approval: BPJS memverifikasi data perusahaan dan peserta; pemberi kerja harus memastikan data valid sebelum submit. Setelah disetujui, peserta terdaftar dan mendapatkan nomor peserta.<br>- Penetapan kelas & perhitungan iuran: tentukan kelas peserta (I/II/III) sesuai ketentuan; hitung iuran karyawan dan pemberi kerja (kewajiban pemotongan dan kontribusi perusahaan). (Periksa tarif iuran terbaru pada sumber resmi sebelum bayar.)<br>- Pembayaran iuran: lakukan pembayaran bulanan melalui Virtual Account atau kanal pembayaran resmi BPJS Kesehatan; simpan bukti bayar untuk rekonsiliasi.<br>- Mutasi / penambahan / pengurangan peserta: laporkan karyawan baru, resign, atau mutasi (mutasi antar fasilitas/kota) melalui e-DABU atau menu perubahan data.<br>- Klaim pelayanan & rujukan: peserta menggunakan faskes tingkat pertama; rujukan ke rumah sakit mengikuti mekanisme rujukan dan sistem klaim INA-CBG di rumah sakit. (Pemberi kerja tidak mengurus klaim medis langsung kecuali untuk verifikasi administrasi peserta.)<br>- Sanksi & kepatuhan: pastikan laporan dan pembayaran tepat waktu untuk menghindari denda atau sanksi administratif. </td></tr><tr><td data-label="Introduction"><strong>BPJS Ketenagakerjaan — langkah penting (untuk pemberi kerja & karyawan)</strong><br>- Persiapan dokumen (pemberi kerja): formulir pendaftaran pemberi kerja, NPWP perusahaan, NPP (jika sudah), KTP pemilik/pengelola, dokumen karyawan (KTP, KK).<br>- Daftar pemberi kerja & buat akun SIPP / portal BPJamsostek: daftar perusahaan (NPP) → buat user login di SIPP/ES BPJamsostek (sistem pendaftaran online untuk pemberi kerja dan entri data pekerja). Alternatif ada pendaftaran BPU untuk bukan penerima upah.<br>- Entri data pekerja: isi data tiap pekerja (upah, jabatan, NIK, jenis perlindungan yang diikutkan: JHT, JKK, JKM, JP, JKP). Upload formulir laporan iuran (laporan rinci) jika diperlukan.<br>- Penentuan iuran & potongan: hitung iuran sesuai persentase untuk program (mis. JHT, JKK, JP, JKP) — potongan/pemotongan gaji karyawan dan kontribusi pemberi kerja sesuai aturan terbaru. Selalu verifikasi tarif iuran terbaru pada sumber resmi.<br>- Pembayaran iuran & pelaporan bulanan: bayar iuran melalui kanal pembayaran resmi (VA/korporat) dan laporkan iuran bulanan melalui SIPP/ES. Simpan bukti pembayaran untuk audit.<br>- Klaim manfaat (JHT, JKK, JKM, JP, JKP): prosedur klaim berbeda per program — umumnya: persiapkan dokumen peserta (kartu peserta, KTP, KK, surat keterangan kerja/PHK), ajukan klaim melalui kantor cabang BPJamsostek atau aplikasi resmi (mis. JMO/JHT online bila tersedia). Untuk klaim pensiun/benefit berkala ada persyaratan tambahan.<br>- Mutasi/terminasi peserta: laporkan pemutusan kerja/terminasi sehingga status peserta diperbarui; untuk pekerja keluar pastikan data keluar (terminate) agar perhitungan iuran dan hak (JHT) dapat diproses.<br>- Kepatuhan & audit: sediakan dokumentasi lengkap; responif terhadap permintaan data dari auditor atau BPJS untuk menghindari sanksi atau denda. </td></tr><tr><td data-label="Introduction"><strong>Quick checklist operasional (ringkas)</strong><br>- Siapkan dokumen perusahaan + dokumen karyawan.<br>- Daftarkan perusahaan → buat akun admin di portal masing-masing (e-DABU untuk Kesehatan; SIPP/ES untuk Ketenagakerjaan).<br>- Entri/upload data peserta (form 34 kolom atau laporan rinci).<br>- Hitung & bayarkan iuran bulanan melalui VA/kanal resmi; simpan bukti.<br>- Lakukan mutasi/terminate saat ada perubahan status; bantu karyawan proses klaim saat diperlukan. </td></tr><tr><td data-label="Introduction"><strong>BPJS & Employment Resources</strong><br><strong>BPJS Ketenagakerjaan</strong><br>- <a href="https://sipp.bpjsketenagakerjaan.go.id/" target="_blank">SIPP (Sistem Informasi Pelaporan Perusahaan) — portal login & upload</a><br>- <a href="https://sipp.bpjsketenagakerjaan.go.id/panduan-pengguna" target="_blank">Panduan Pengguna SIPP (cara unduh template & upload)</a><br>- <a href="https://www.bpjsketenagakerjaan.go.id/" target="_blank">Situs resmi BPJS Ketenagakerjaan — informasi program & pendaftaran</a><br><strong>BPJS Kesehatan</strong><br>- <a href="https://edabu.bpjs-kesehatan.go.id/" target="_blank">e-DABU BPJS Kesehatan — portal Badan Usaha (login / upload peserta)</a><br>- <a href="https://sipp.bpjs-kesehatan.go.id/sipp/#/home/dashboard" target="_blank">SIPP BPJS Kesehatan — portal dashboard</a><br>- <a href="https://www.bpjs-kesehatan.go.id/" target="_blank">Situs resmi BPJS Kesehatan — panduan & layanan kepesertaan</a><br>- <a href="https://play.google.com/store/apps/details?hl=id&id=id.go.bpjskesehatan.mobilebu" target="_blank">Edabu Mobile (Google Play) — aplikasi untuk HR Badan Usaha</a><br>- <a href="https://kerjoo.com/blog/e-dabu-bpjs-kesehatan-panduan-lengkap/" target="_blank">Panduan e-DABU (panduan langkah demi langkah untuk perusahaan)</a><br><strong>Kementerian Ketenagakerjaan (Kemnaker)</strong><br>- <a href="https://panduan.kemnaker.go.id/panduan/29" target="_blank">Panduan WLKP (Wajib Lapor Ketenagakerjaan) — petunjuk pelaporan perusahaan</a><br>- <a href="https://panduan.kemnaker.go.id/uploads/1686709335-Panduan-WLKP-2.0.0.pdf" target="_blank">Panduan WLKP (PDF) — contoh template & alur pendaftaran</a><br>- <a href="https://wajiblapor.kemnaker.go.id" target="_blank">Portal WLKP / Wajib Lapor Ketenagakerjaan (siapkerja / wajiblapor)</a><br><strong>Templates & Upload Help</strong><br>- <a href="https://employers.glints.com/id-id/blog/sipp-online-bpjs-tk/" target="_blank">Cara unduh & isi template Excel SIPP (glints: ringkasan langkah upload massal)</a><br>- <a href="https://www.ptgasi.co.id/perubahan-formulir-upload-badan-usaha-pada-aplikasi-e-dabu-bpjs-kesehatan/" target="_blank">Perubahan formulir upload e-DABU (catatan versi/formulir)</a><br>- <a href="https://id.scribd.com/document/614134182/Tata-Cara-Update-Data-Pekerja-Melalui-Sipp-Online-1" target="_blank">Contoh panduan prosedur upload SIPP (file contoh / tata cara)</a><br><strong>Guides, Tutorials & Videos</strong><br>- <a href="https://www.youtube.com/watch?v=37dflxqPdiU" target="_blank">Tutorial e-DABU (BPJS Kesehatan) — video resmi / demo</a><br>- <a href="https://www.youtube.com/watch?v=gunN9ZEH4EY" target="_blank">Cara update gaji di e-DABU (video langkah-langkah Excel → upload)</a><br>- <a href="https://tanya.zemangat.com/kb/cara-upload-data-karyawan/" target="_blank">Panduan umum upload data karyawan (contoh tata letak template)</a><br><strong>Third-party integrators & HR tools</strong><br>- <a href="https://kerjoo.com/blog/e-dabu-bpjs-kesehatan-panduan-lengkap/" target="_blank">Artikel & toolset HR — cara integrasi data ke e-DABU</a><br>- <a href="https://employers.glints.com/id-id/blog/sipp-online-bpjs-tk/" target="_blank">Glints Employers — panduan SIPP untuk HR</a><br>- <a href="https://bansos.medanaktual.com/ini-cara-update-data-di-sipp-bpjs-ketenagakerjaan-untuk-dapat-bsu-rp600-ribu-tahun-2025-simak-info-lengkapnya/" target="_blank">Contoh prosedur pengkinian data & unduh template (media lokal)</a></td></tr></tbody></table></div><div class="row-count">Rows: 4</div></div><div class="table-caption" id="Table2" data-table="YJKN_0016_02" style="margin-top:2mm;margin-left:3mm;"><strong>Table 2</strong></div>
<div class="table-wrapper" data-table-id="table-2"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by LANGKAH-LANGKAH UPDATE DATA MELALUI SIPP BPJS"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">LANGKAH-LANGKAH UPDATE DATA MELALUI SIPP BPJS</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="LANGKAH-LANGKAH UPDATE DATA MELALUI SIPP BPJS"> <strong>Tujuan ringkasan:</strong><br>Bagi HRD perusahaan yang ingin memastikan seluruh data pekerja telah sesuai untuk menerima BSU, menjelaskan tahapan pembaruan data di SIPP. </td></tr><tr><td data-label="LANGKAH-LANGKAH UPDATE DATA MELALUI SIPP BPJS"> <strong>Ringkasan singkat proses:</strong><br>1. Login ke situs resmi SIPP menggunakan akun perusahaan.<br>2. Pilih menu “BSU Tahun 2025”.<br>3. Akses sub-menu “Pengkinian Data BSU”.<br>4. Unduh template Excel yang tersedia.<br>5. Lengkapi informasi sesuai kolom yang diminta.<br>6. Upload kembali file Excel yang telah diisi ke dalam sistem.<br>7. Tunggu proses verifikasi dan persetujuan data dari sistem. </td></tr><tr><td data-label="LANGKAH-LANGKAH UPDATE DATA MELALUI SIPP BPJS"> <strong>Peran dan akses yang diperlukan:</strong><br>HRD / PIC perusahaan dengan akun perusahaan terdaftar dan hak untuk mengakses menu BSU/Pengkinian Data. </td></tr><tr><td data-label="LANGKAH-LANGKAH UPDATE DATA MELALUI SIPP BPJS"> <strong>Template & format file:</strong><br>Gunakan template Excel yang tersedia pada laman unduhan SIPP; isi sesuai kolom yang diminta oleh template. </td></tr><tr><td data-label="LANGKAH-LANGKAH UPDATE DATA MELALUI SIPP BPJS"> <strong>Prasyarat sebelum update:</strong><br>Akun perusahaan aktif dan terdaftar di SIPP; akses ke menu BSU Tahun 2025; file Excel terisi lengkap sesuai template. </td></tr><tr><td data-label="LANGKAH-LANGKAH UPDATE DATA MELALUI SIPP BPJS"> <strong>Proses verifikasi:</strong><br>Setelah upload, sistem melakukan verifikasi — menunggu persetujuan/konfirmasi dari sistem sebelum perubahan dianggap final. </td></tr><tr><td data-label="LANGKAH-LANGKAH UPDATE DATA MELALUI SIPP BPJS"> <strong>Catatan penting:</strong><br>Di laman unduhan tersedia panduan pengisian untuk menghindari kesalahan teknis saat input data; ikuti panduan tersebut sebelum upload. </td></tr></tbody></table></div><div class="row-count">Rows: 7</div></div><div class="table-caption" id="Table3" data-table="YJKN_0016_03" style="margin-top:2mm;margin-left:3mm;"><strong>Table 3</strong></div>
<div class="table-wrapper" data-table-id="table-3"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by IMPORT DATA"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">IMPORT DATA</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="IMPORT DATA"> <strong>Tujuan ringkasan:</strong><br>menjelaskan langkah, format, validasi, risiko, troubleshooting, dan best-practice untuk unggah/impor data peserta perusahaan ke sistem BPJS (penerapan umum untuk kedua layanan). </td></tr><tr><td data-label="IMPORT DATA"> <strong>Ringkasan singkat proses:</strong><br>1) persiapkan data sesuai template resmi<br>2) validasi & bersihkan (duplikat, NIK, tanggal)<br>3) uji di sandbox atau subset<br>4) unggah massal via portal resmi<br>5) cek hasil / perbaiki eror<br>6) monitoring & audit. </td></tr><tr><td data-label="IMPORT DATA"> <strong>Peran dan akses yang diperlukan:</strong><br>Akun admin perusahaan / PIC yang tercatat, hak untuk unggah massal, akses ke laporan hasil impor, dan otorisasi untuk mengoreksi data. </td></tr><tr><td data-label="IMPORT DATA"> <strong>Prasyarat sebelum impor:</strong><br>Akun perusahaan aktif; nomor pemberi kerja/registrasi (NPP/NIK perusahaan sesuai portal); akses ke template resmi; koneksi aman (TLS/HTTPS); backup data lama. </td></tr><tr><td data-label="IMPORT DATA"> <strong>Template & format file:</strong><br>Gunakan template Excel/CSV resmi dari portal; jangan ubah header; simpan dalam encoding UTF-8 (untuk CSV); jaga konsistensi format tanggal. </td></tr><tr><td data-label="IMPORT DATA"> <strong>Nama file & versi:</strong><br>Gunakan pola nama yang jelas, contoh: IMPOR_BPJS_<jenis><em><YYYYMMDD></em><v1>.xlsx. Simpan salinan arsip sebelum unggah. </td></tr><tr><td data-label="IMPORT DATA"> <strong>Kolom wajib (umum, per baris):</strong><br>NIK; Nama Lengkap; Tgl Lahir; Jenis Kelamin; Tgl Mulai Kerja/Keaktifan; Status Karyawan (aktif/cuti/keluar); Upah (jika untuk ketenagakerjaan); Jabatan/Unit; Alamat; No. Telepon/email PIC. </td></tr><tr><td data-label="IMPORT DATA"> <strong>Kolom sering diminta khusus BPJS Kesehatan:</strong><br>Kelas perawatan; jenis peserta (pekerja utama/anggota); FKTP rujukan awal (jika ada). </td></tr><tr><td data-label="IMPORT DATA"> <strong>Kolom sering diminta khusus BPJS Ketenagakerjaan:</strong><br>Gaji/upah bulanan; kode cabang/UP; jenis penerima manfaat; tanggal PHK (jika keluar). </td></tr><tr><td data-label="IMPORT DATA"> <strong>Format NIK:</strong><br>16 digit numerik — pastikan tanpa spasi, tanpa tanda baca; verifikasi checksum dasar (panjang + numerik). </td></tr><tr><td data-label="IMPORT DATA"> <strong>Format tanggal:</strong><br>Tentukan format yang konsisten (disarankan YYYY-MM-DD) dan pastikan portal menyetujui format tersebut; beberapa portal menerima DD/MM/YYYY. </td></tr><tr><td data-label="IMPORT DATA"> <strong>Validasi nama:</strong><br>Hilangkan karakter kontrol; batasi panjang (mis. ≤100 char); cek karakter ilegal (emoji/karakter non unicode standar). </td></tr><tr><td data-label="IMPORT DATA"> <strong>Validasi duplikat:</strong><br>Deteksi NIK duplikat dan karyawan ganda; tandai untuk review manual sebelum unggah massal. </td></tr><tr><td data-label="IMPORT DATA"> <strong>Validasi upah:</strong><br>Rentang logis (tidak negatif, dalam mata uang IDR); jika ada aturan kelas upah per threshold — laporkan nilai outlier >3× rata-rata unit. </td></tr><tr><td data-label="IMPORT DATA"> <strong>Mapping kolom:</strong><br>Siapkan peta kolom (sheet terpisah) yang menunjukkan kolom Excel → field portal; susun dokumentasi mapping untuk tim IT. </td></tr><tr><td data-label="IMPORT DATA"> <strong>Field optional vs mandatory:</strong><br>Pisahkan kolom opsional (mis. NPWP, alamat lengkap) dari wajib; kosongkan opsional bila tidak tersedia. </td></tr><tr><td data-label="IMPORT DATA"> <strong>Penanganan baris error:</strong><br>Portal biasanya mengembalikan file laporan error; buat proses otomatis untuk mem-parsing laporan dan menandai baris yang gagal. </td></tr><tr><td data-label="IMPORT DATA"> <strong>Contoh entri sampel (satu baris):</strong><br><code>NIK=1234567890123456 | Nama=Budi Santoso | TglLahir=1985-05-20 | JK=L | TglMulai=2020-01-15 | Upah=7500000 | Kelas=III | Unit=Finance</code></td></tr><tr><td data-label="IMPORT DATA"> <strong>Batas ukuran & jumlah baris:</strong><br>Periksa batas portal (mis. max rows per upload / max filesize); jika besar → bagi menjadi batch lebih kecil. </td></tr><tr><td data-label="IMPORT DATA"> <strong>Preflight checks (otomatis):</strong><br>Cek header cocok template; jumlah kolom benar; semua mandatory terisi; NIK numerik 16 digit; tanggal valid; tidak ada duplikat internal. </td></tr><tr><td data-label="IMPORT DATA"> <strong>Backup & rollback:</strong><br>Sebelum unggah, simpan snapshot data peserta saat ini; jika import merusak, gunakan prosedur rollback/restore atau minta portal melakukan revert jika tersedia. </td></tr><tr><td data-label="IMPORT DATA"> <strong>Uji sandbox:</strong><br>Lakukan unggah percobaan pada subset data di environment uji (jika tersedia) untuk memeriksa pemetaan dan eror. </td></tr><tr><td data-label="IMPORT DATA"> <strong>Logging & audit trail:</strong><br>Simpan log upload (user, timestamp, file name, jumlah baris sukses/gagal, id job dari portal) untuk audit internal. </td></tr><tr><td data-label="IMPORT DATA"> <strong>Komunikasi & SOP internal:</strong><br>Tetapkan checklist persiapan, penanggung jawab koreksi, SLA untuk menindaklanjuti eror, dan saluran eskalasi ke pihak berwenang. </td></tr><tr><td data-label="IMPORT DATA"> <strong>Keamanan & perlindungan data:</strong><br>Enkripsi file sensitif saat transit (HTTPS) dan at-rest; batasi akses ke file mentah; hapus file temporer setelah sukses. </td></tr><tr><td data-label="IMPORT DATA"> <strong>Privasi data:</strong><br>Patuhi aturan perlindungan data karyawan (pembagian NIK/identitas cukup untuk tujuan administratif). </td></tr><tr><td data-label="IMPORT DATA"> <strong>Notifikasi & konfirmasi:</strong><br>Setelah upload, verifikasi status job (success/partial/failed) dan kirim ringkasan ke stakeholder (HR, payroll, compliance). </td></tr><tr><td data-label="IMPORT DATA"> <strong>Penanganan anggota yang sudah terdaftar:</strong><br>Jika NIK sudah terdaftar dengan nomor peserta, putuskan apakah melakukan update atau skip — siapkan rule prioritas (update if newer). </td></tr><tr><td data-label="IMPORT DATA"> <strong>Merge vs Replace:</strong><br>Tentukan apakah unggah massal harus men-merge (update sebagian field) atau replace penuh; hati-hati jika replace dapat menghapus data penting. </td></tr><tr><td data-label="IMPORT DATA"> <strong>Error umum & solusinya:</strong><br>Format tanggal salah → perbaiki format; NIK tidak valid → verifikasi KTP; kolom hilang → gunakan template resmi; ukuran file → pecah batch. </td></tr><tr><td data-label="IMPORT DATA"> <strong>Monitoring pasca-unggah:</strong><br>Cek laporan tagihan/kontribusi bulan pertama setelah perubahan; pastikan perubahan tercermin di laporan payroll dan potongan. </td></tr><tr><td data-label="IMPORT DATA"> <strong>Test case minimal sebelum go-live:</strong><br>1) baris sukses sederhana<br>2) baris dengan NIK duplikat<br>3) baris dengan format tanggal berbeda<br>4) baris gagal karena mandatory kosong<br>5) batch besar. </td></tr><tr><td data-label="IMPORT DATA"> <strong>SOP koreksi:</strong><br>Unduh laporan error → perbaiki di file sumber → ulang upload hanya baris yang gagal; jangan re-upload seluruh file kecuali dimaksudkan. </td></tr><tr><td data-label="IMPORT DATA"> <strong>Komunikasi dengan pihak portal:</strong><br>Siapkan bukti (log, file sumber, screenshot job id) saat eskalasi. Catat jam, pesan error, dan nomor tiket/ID job. </td></tr><tr><td data-label="IMPORT DATA"> <strong>Integrasi payroll & HRIS:</strong><br>Buat pipeline ETL yang men-generate file import dari HRIS secara otomatis setelah validasi, termasuk aturan mapping & normalisasi. </td></tr><tr><td data-label="IMPORT DATA"> <strong>Automasi validasi:</strong><br>Gunakan skrip (Python/VBA) atau tools ETL untuk checklists: NIK, tanggal, duplikat, karakter set, konsistensi upah, referensi kode cabang. </td></tr><tr><td data-label="IMPORT DATA"> <strong>Dokumentasi:</strong><br>Simpan manual operasional, template contoh, mapping, dan changelog tiap kali struktur kolom berubah. </td></tr><tr><td data-label="IMPORT DATA"> <strong>Kontrol versi & change management:</strong><br>Untuk template/skrip, gunakan version control (git) dan tandai release yang digunakan untuk setiap upload. </td></tr><tr><td data-label="IMPORT DATA"> <strong>Rekomendasi pelaksanaan:</strong><br>Lakukan unggah pada akhir hari kerja/low-traffic window; upload pertama pada subset kecil; gunakan batching; pastikan tim on-call saat upload besar. </td></tr><tr><td data-label="IMPORT DATA"> <strong>Checklist quick-run sebelum upload:</strong><br>backup data lama; jalankan preflight script; cek user access; beri tahu stakeholder; simpan nama file & versi; catat job id. </td></tr><tr><td data-label="IMPORT DATA"> <strong>Risiko & mitigasi:</strong><br>Kehilangan data karena replace → gunakan backup; korupsi karakter → gunakan UTF-8 & cek; saldo iuran tidak sinkron → cross-check laporan kontribusi. </td></tr><tr><td data-label="IMPORT DATA"> <strong>Frequently Asked Points:</strong><br>Selalu gunakan template resmi; jangan edit header; validasi NIK; simpan laporan hasil; koreksi hanya baris gagal bila memungkinkan. </td></tr><tr><td data-label="IMPORT DATA"> <strong>Catatan akhir:</strong><br>Selalu konfirmasi format terakhir di portal resmi sebelum unggah — template resmi bisa berubah; dokumentasikan setiap perubahan prosedur. </td></tr></tbody></table></div><div class="row-count">Rows: 43</div></div><div class="table-caption" id="Table4" data-table="YJKN_0016_04" style="margin-top:2mm;margin-left:3mm;"><strong>Table 4</strong></div>
<div class="table-wrapper" data-table-id="table-4"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by **Form Excel 34 BPJS Kesehatan Guidance**"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>Form Excel 34 BPJS Kesehatan Guidance</strong></div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"><strong>Purpose & objective</strong><br>Form Excel 34 (Format 34 kolom) is the canonical batch/collective registration template employers use to enrol <strong>new</strong> BPJS Kesehatan participants (employees and dependents who do not yet have a BPJS membership number). Its objective is to capture all required company-level and participant-level data in a fixed schema so BPJS can validate, create membership records, assign faskes and process contributions. Use the official Form_34.xlsx from e-DABU/SVBU and follow the template’s “kamus data 34 kolom” for authoritative field definitions. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"><strong>Who prepares & submits (roles & responsibilities)</strong> <br><strong>Primary:</strong><br>- Employer (HR / payroll PIC) or authorised HR/payroll vendor with BU e-DABU access.<br><strong>Support:</strong><br>- BPJS branch/account manager can assist initial submissions or exceptions.<br><strong>Reviewer:</strong><br>- independent HR/Finance reviewer (not preparer) must sample-check before upload.<br><strong>Uploader:</strong><br>- BU-account user on e-DABU/SVBU performs the actual upload.<br><br>Baca: <a href="https://www.ptgasi.co.id/gasi-newsletter-gasi-nl-115-0719-penutupan-aplikasi-new-e-dabu-versi-3-1-0/" target="_blank">PENUTUPAN APLIKASI NEW E-DABU VERSI 3.1.0 - PT GASI</a> </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"><strong>Where to obtain the official template & authoritative references</strong> <br>- <strong>Official e-DABU (BU portal)</strong> — download <strong>Form_34.xlsx</strong> from the e-DABU referensi/download area inside your BU account; the template includes the sheet <strong>“kamus data 34 kolom”</strong> (valid codes, required fields and formats).<br>- <strong>BPJS Kesehatan</strong> — program information, helpdesk and BU/SIPP portal: <a href="https://sipp.bpjs-kesehatan.go.id/" target="_blank">SIPP - BPJS Kesehatan</a> <code>|</code> <a href="https://www.bpjs-kesehatan.go.id/" target="_blank">bpjs-kesehatan.go.id</a><br>- <strong>BPJS Ketenagakerjaan (SIPP)</strong> — separate system for employment/social security; do NOT use Form 34 for BPJS Ketenagakerjaan (use SIPP templates for TK submissions): <a href="https://sipp.bpjsketenagakerjaan.go.id/" target="_blank">sipp.bpjsketenagakerjaan.go.id</a><br>- <strong>Kemnaker WLKP guidance</strong> — employer reporting guidance and useful cross-checks for workforce reporting: <a href="https://panduan.kemnaker.go.id/uploads/1686709335-Panduan-WLKP-2.0.0.pdf" target="_blank">1686709335-Panduan-WLKP-2.0.0.pdf</a><br>- <strong>Community / sample copies (mirrors)</strong> — useful for examples but always verify against the official e-DABU Form_34.xlsx before uploading: <a href="https://app.box.com/s/ofx22fzls73gesyfs3po9pxycxf8a4uo" target="_blank">Form Excel 34</a> <code>|</code> <a href="https://app.box.com/s/ub0as9r0ogvt0vga5rdm8viy753zat2o" target="_blank">Form Excel 34 Dictionary</a> <code>|</code> <a href="https://app.box.com/s/p8etm1ktjz4l5r3uavo33lsm6b1vzpm3" target="_blank">Form Excel 34 Sample</a> </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"><strong>SIPP (BPJS Ketenagakerjaan)</strong><br>SIPP (Sistem Informasi Pelaporan Perusahaan) BPJS Ketenagakerjaan adalah platform daring resmi untuk memudahkan perusahaan mengelola administrasi kepegawaian secara cepat dan akurat. Ini mencakup pendaftaran pekerja, pembaruan data upah/mutasi, serta perhitungan iuran JHT, JKM, JKK, dan JP secara realtime. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"><strong>SIPP BPJS Kesehatan (Saluran Informasi & Pengaduan Peserta JKN-KIS)</strong><br>SIPP BPJS Kesehatan merupakan sistem yang di sediakan untuk peserta JKN-KIS mendapatkan informasi dan menyampaikan pengaduan seputar program JKN yang terhubung langsung oleh Kantor Cabang yang dapat memberikan informasi dan menangani pengaduan sesuai kebutuhan peserta secara realtime. Saluran Informasi Penanganan Pengaduan dibuat untuk memberikan kemudahan, kecepatan dan kepastian layanan informasi dan penanganan pengaduan peserta yang dapat diakses kapan saja dan dimana saja. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"><strong>Quick operational checklist (one-line tasks)</strong> <br>1. Obtain official <strong>Form_34.xlsx</strong> from e-DABU.<br>2. Read <strong>kamus data</strong> sheet.<br>3. Map payroll export → 34 columns.<br>4. Format ID fields as Text (NIK/KK).<br>5. Validate NIK vs Dukcapil export.<br>6. Validate faskes/kecamatan codes per kamus data.<br>7. Save .xlsx; upload via e-DABU.<br>8. Fix portal upload report errors; reupload until clean.<br>9. Archive final file + upload report + payment proof.</td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"><strong>Header (company) fields — what appears at top of sheet</strong><br>Nama Instansi/Badan/Perusahaan (as registered); Virtual Account / VA code; Bank tempat pembayaran iuran; Tanggal Registrasi / TMT (DD/MM/YYYY); Nomor PKS / Kode PKS / Unit Kerja / Kode Cabang (if requested). These identify the BU and determine upload context. Use official company records (NPWP, NIB) for exact spelling. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"><strong>Field-by-field summary (typical 34 columns — use kamus data for exact order)</strong> <br>Use the kamus data sheet inside the official Form_34.xlsx for exact column names/order. Below are the commonly-present columns and practical notes:<br>• No / Serial — internal row number.<br>• No. KK (Kartu Keluarga) — 16 digits; set cell format Text to preserve leading zeros.<br>• NIK / KITAS / KITAP — 16 digits for NIK; for WNA use passport/KITAS/KITAP as applicable.<br>• Nama Lengkap — as on KTP/passport.<br>• Hubungan keluarga — code (1=Peserta,2=Istri,3=Suami,4=Anak, etc.).<br>• Tempat Lahir / Tanggal Lahir (DD/MM/YYYY).<br>• Jenis Kelamin (1=Lk,2=Pr).<br>• Status Kawin (codes per kamus data).<br>• Alamat lengkap; RT/RW; Kode Pos.<br>• Kode Kecamatan / Nama Kecamatan; Kode Desa / Nama Desa (use codes from kamus data).<br>• Kode & Nama Faskes Tk.I (primer) — authoritative by code; ensure you pick a puskesmas/klinik/GP (not hospital) for primary faskes.<br>• Faskes dokter gigi (optional column in some templates).<br>• Telepon / Email (if available).<br>• NPP / No. Pegawai / Jabatan / Status Pegawai (internal company identifiers).<br>• Kelas rawat (I/II/III) — follow company policy and statutory rules for class assignment; salary basis may determine class.<br>• Gaji pokok + tunjangan — used for salary-change mutation uploads where required.<br>• Kewarganegaraan / Passport (for WNA).<br>• NPWP / Asuransi lain / No Polis / Notes.<br>• Kolom 26–34 — flags/attachments/internal notes depending on template version. Always follow the kamus data for permitted values and lengths.<br><br>Baca: <a href="https://app.box.com/s/ub0as9r0ogvt0vga5rdm8viy753zat2o" target="_blank">Kamus Data 34 Kolom</a> </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"><strong>Kamus data & code validation</strong><br>Open the kamus data sheet in Form_34.xlsx: it lists valid values for relationship, gender, employment status, faskes codes, kecamatan/desa codes and other lookup values.<br>Use those exact codes — portal validation is code-based, not name-based.<br>Build internal lookup tables from kamus data to validate exports before upload. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"><strong>Practical file preparation steps (detailed)</strong> <br>1. <strong>Download</strong> Form_34.xlsx from your BU e-DABU account (Referensi → Download File Excel). <br>2. <strong>Read</strong> kamus data to identify mandatory columns per mutation type.<br>3. <strong>Map</strong> payroll export fields → template columns; create a repeatable mapping script or ETL transform.<br>4. <strong>Format</strong> NIK/KK as Text; set date columns to DD/MM/YYYY; remove formulas (paste as values).<br>5. <strong>Validate</strong> internally: NIK length = 16; KK present for dependents; faskes code exists; mandatory columns filled for the chosen mutation code.<br>6. <strong>Split</strong> large batches to respect portal row/size limits; test upload with a small batch (10–50 rows).<br>7. <strong>Save</strong> final file as .xlsx; keep an unauthored working copy and an archived final copy.  </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"><strong>Upload procedure (e-DABU / SVBU / branch)</strong><br>• <strong>Preferred:</strong> upload via e-DABU (Peserta → Upload Peserta) and choose the 34-kolom template type; specify unit kerja if portal asks. <br>• <strong>Validation:</strong> portal returns an upload report (row-level errors). Fix reported rows in the source file and reupload until no errors remain. Save upload reports and screenshots for audit trail.<br>• <strong>Offline:</strong> if BU cannot access e-DABU, deliver Form_34.xlsx to local BPJS Kesehatan branch per branch instructions; branch will advise on processing and any required attachments. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"><strong>Common upload errors & remediation</strong><br>- <strong>NIK mismatch/invalid:</strong> correct NIK to match Dukcapil records or attach supporting documents and reupload.<br>- <strong>Invalid faskes code:</strong> replace with code from kamus data (code is authoritative).<br>- <strong>Wrong date format / formulas present:</strong> convert to DD/MM/YYYY and paste values.<br>- <strong>Template structure changed:</strong> restore original column order/name from official Form_34.xlsx.<br>- <strong>File size / row limits:</strong> split into smaller batches and reupload. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"><strong>Required attachments & special cases</strong> <br>- <strong>Penonaktifan (terminate):</strong> attach paklaring / termination letter, death certificate, or divorce decree where applicable.<br>- <strong>NIK/KK corrections:</strong> include Dukcapil-issued correction documents when available.<br>- <strong>WNA entries:</strong> include passport/KITAS details in the passport/KITAS column.<br>- <strong>Large employers:</strong> coordinate with your BPJS account manager for high-volume submissions and possible staged migrations. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"><strong>Who must act & suggested RACI (brief)</strong><br> <strong>HR/Payroll (PIC):</strong> prepare mapping, populate Form_34 and run validations. <br><strong>Independent reviewer (HR/Finance):</strong> sample checks and sign-off. <br><strong>BU e-DABU uploader:</strong> upload and manage upload reports. <br><strong>BPJS branch/account manager:</strong> assist and finalise offline cases. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"><strong>Timing, deadlines & payment linkages</strong><br>Register new hires promptly (ideally before first payroll or within the month hired) to avoid coverage gaps; monthly contributions are subject to statutory deadlines (employer must remit on schedule — check current tariff before payment).<br><br>After successful upload and membership creation, perform payroll reconciliation: participants created ↔ payroll list ↔ iuran/JHT payment. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"><strong>HR pre-upload validation checklist (detailed)</strong> <br> ☐ Official Form_34.xlsx from e-DABU downloaded and kamus data reviewed. <br>☐ Mapping spreadsheet (source → target) created and versioned. <br>☐ NIK/KK formatted as Text; date formats standardized. <br>☐ Internal validation scripts run (NIK length, faskes code existence, mandatory columns). <br>☐ Small test upload performed and portal errors analysed. <br>☐ Final upload performed and upload report saved. <br>☐ Payment of iuran scheduled; proof archived. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"><strong>Large-volume & integration notes</strong><br>For recurring bulk loads automate mapping and validations; maintain a change log for mapping adjustments.<br>Where possible integrate payroll/HCM exports to produce a Form_34 output directly to reduce manual copy/paste errors.<br>For very large migrations coordinate with BPJS account manager to avoid rate-limits and to obtain guidance on staging. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>Audit trail & retention</strong><br> Archive the final Form_34.xlsx, all intermediate versions, upload reports, evidence of supporting documents (termination letters, passport copies where required), and proof of payment. Retain per local statutory retention rules (commonly 6–10 years) and organisation policy. Maintain an index linking payroll runs and upload batches for auditor sampling.</td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>Practical tips & best practices</strong> <br> <strong>Template integrity</strong><br>- Never edit, rename, reorder, or delete header columns in the official template.<br>- Preserve original data formats (dates, numeric fields, IDs) to avoid upload rejection.<br><br><strong>Version control & file governance</strong><br>- Maintain a locked “golden” copy of Form_34.xlsx (unchanged) as the master template.<br>- Perform all transformations and edits only on controlled export copies.<br>- Use clear version naming (e.g., Form_34_YYYYMMDD_v01.xlsx).<br><br><strong>Pre-validation controls</strong><br>- Run automated pre-check scripts to detect missing mandatory fields and format errors before upload.<br>- Validate NIK, BPJS number, and date fields against required formats.<br>- Ensure no hidden characters, extra spaces, or merged cells exist.<br><br><strong>Upload documentation & audit trail</strong><br>- Record the upload report ID for every submission.<br>- Capture and archive portal confirmation screenshots.<br>- Store submitted files and error reports in a structured archive folder.<br><br><strong>Operational readiness (HR users)</strong><br>- Develop a concise FAQ/SOP covering common validation errors and portal error codes.<br>- Provide step-by-step guidance for frequent mutasi cases (new hire, termination, salary update).<br><br><strong>Further development</strong><br>Reference technical specification: <a href="https://adiskurniawan.github.io/ChapterSummaries_2.0/summaries/YJKN_0017.html" target="_blank">BPJS Data Import & Governance Control Framework</a>  — includes system architecture, validation controls, batching logic, reconciliation design, logging standards, and governance framework for regulated data processing</td></tr></tbody></table></div><div class="row-count">Rows: 19</div></div><div class="table-caption" id="Table5" data-table="YJKN_0016_05" style="margin-top:2mm;margin-left:3mm;"><strong>Table 5</strong></div>
<div class="table-wrapper" data-table-id="table-5"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by **Form Excel 34 BPJS Kesehatan Guidance**"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>Form Excel 34 BPJS Kesehatan Guidance</strong></div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>Introduction — purpose & audience</strong><br>This single-column master reference is a comprehensive operational manual for preparing, validating, and uploading the canonical employer bulk submission template (referred to hereafter as <em>Form_34 template</em>) used to register or mutate participants in the national health insurance portal. It is written for HR/PIC, payroll engineers, HRIS integrators, payroll vendors, and regional portal officers. It assumes you will use the official employer portal template (the <code>Form_34.xlsx</code> with its <strong>kamus data</strong> sheet) as the authoritative source. The guide covers end-to-end process: source extraction, mapping, automated validation, portal upload, error triage, attachments handling, governance, audit evidence, rollback procedures, and continuous-improvement controls. Use this as an operational playbook to embed Form_34 processing into a repeatable ETL pipeline with traceable artifacts and clearly defined SLAs. This expanded edition includes deeper operational examples, additional validation matrices, enhanced pseudocode, expanded testing and regression guidance, further manifest and attachment schemas, and extended troubleshooting playbooks for common portal error classes. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>Executive summary (one-paragraph mission)</strong><br>Deliver accurate, auditable, and repeatable bulk submission of employee and dependent registrations/mutations to the employer portal using the official 34-column Excel template. Minimize portal rejections by enforcing deterministic mappings, robust pre-upload validation, and systematic governance. Preserve privacy and auditability by storing versioned templates, checksums, upload reports, and attachment manifests with strict access controls. Instrument KPIs, automate safe remediations, and document exceptions with supporting evidence to ensure reproducible corrective operations. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>Who must do this & legal context (role mapping)</strong><br>Employers (or authorized third-party payroll/HR vendors) are legally responsible for enrolling and updating participant data. Responsibilities: HR collects personal data and initial verification; Payroll certifies wages and contribution basis; HRIS/Integration engineers build and maintain transformation pipelines; Compliance/GDPR teams define PII controls and retention policies; regional office handles exceptional processing and portal support. Maintain sign-off evidence (HR + Payroll) for every production upload and follow retention rules required by company policy and applicable regulations. Where population registry reconciliation is available, use the registry API for NIK validation and preserve reconciliation output as audit evidence. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>Authoritative template source & version control (policy)</strong><br>Always download the <code>Form_34.xlsx</code> directly from the employer account area of the employer upload portal or obtain the file from the regional account manager. Do not use third-party copies without verification. Keep a <code>template_registry.xlsx</code> with columns: <code>template_file</code>, <code>portal_version</code>, <code>downloaded_by</code>, <code>downloaded_on</code>, <code>sha256</code>, <code>validated_by</code>, <code>notes</code>. Before any production run, confirm the template hash and kamus data version; if the portal updates the template, freeze production uploads until you run regression tests against golden files and update mapping scripts. Record template changes in change-control tickets and link to CI runs. Maintain a public changelog for operators showing dates and high-level impact (e.g., new mandatory columns, renamed enumerations). </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>Template structure — high-level layout</strong><br>(A) a company/header block (company name, virtual account, payment bank, registration/TMT date, contract or account codes).<br>(B) a participant sheet with one row per person (34 participant columns).<br>(C) a <strong>kamus data</strong> sheet listing valid enumerations for fields (facility lists, area codes, mutation codes, employment status codes).<br>The kamus is the single source of truth for code-to-name mappings. Always use codes where the template expects them; names are secondary.<br>Version the kamus locally and store the portal-published kamus dump with the same sha256 approach.<br>If the portal provides effective dates for kamus entries, include those in the registry and use the kamus snapshot that matches your reporting period. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>Canonical 34-column mapping — authoritative details (expanded)</strong><br>1) <strong>RowID / No.</strong> — sequential numeric index for internal reference; useful in error reports and mapping diffs. Include original source reference (e.g., <code>source_row_id</code>) for bi-directional traceability.<br>2) <strong>Nomor Kartu Keluarga (KK)</strong> — 16-digit family card number (format as Text); identical for family rows. Validate group integrity: flag KK groups missing a head of family or groups where multiple heads exist contrary to policy. Maintain a <code>kk_group_consistency</code> field during preprocessing and a <code>kk_group_id</code> used across parts to partition uploads safely.<br>3) <strong>NIK / KITAS / KITAP</strong> — primary identity: 16-digit NIK for nationals; for foreigners use KITAS/KITAP/passport per template rules. Store as Text to prevent numeric truncation. Where registry reconciliation is possible, perform batch lookup and capture <code>dukcapil_response_code</code>, <code>dukcapil_name</code>, and <code>dukcapil_match_confidence</code>. For exceptions, attach supporting documents and log operator attestation.<br>4) <strong>Nama Lengkap</strong> — full legal name exactly as official ID; preserve diacritics using UTF-8 NFC normalization; enforce maximum field length per template and document truncation handling policy. Maintain a <code>name_normalized</code> column for matching algorithms (e.g., <code>strip_accents(lowercase)</code> used only for matching, not for submission).<br>5) <strong>Hubungan (relationship)</strong> — enumerated code: 1=Peserta, 2=Istri, 3=Suami, 4=Anak, etc. Use kamus codes to ensure correct hierarchical attachment; validate exactly one primary participant per family group when policy requires it. Implement family-level validations: ensure dependent DOB and relationship are consistent (e.g., spouse age plausibility checks).<br>6) <strong>Tempat Lahir</strong> — place of birth (city/regency) — store label and optional administrative code; normalize via kamus mapping where available. For ambiguous names include <code>place_of_birth_confidence</code> to capture fuzzy mapping state.<br>7) <strong>Tanggal Lahir (DOB)</strong> — dd/mm/yyyy; normalize and validate impossible dates (DOB in future, DOB < 1900) and compute derived ages used in coverage eligibility logic. For newborn registrations use policy-defined windows (e.g., allow <code>TMT</code> backdated by X days) and document exceptions.<br>8) <strong>Jenis Kelamin</strong> — code (1=Laki-laki, 2=Perempuan). If template includes other codes or <code>unknown</code> semantics, capture original source and map conservatively, flagging for HR review where required.<br>9) <strong>Status Perkawinan</strong> — code per kamus (e.g., 1=Belum Kawin, 2=Kawin, 3=Cerai). Cross-check with dependent relationships and NIK gender where policy requires.<br>10) <strong>Alamat Lengkap</strong> — full residential address as used in HR records; preserve punctuation where necessary; normalize abbreviations (Jl, Jln) consistently. Keep <code>raw_address</code> in staging for audit; use <code>address_parsed</code> fields for geocoding and kamus mapping.<br>11) <strong>RT / RW</strong> — neighborhood codes (treat as Text to preserve leading zeros). Validate against kecamatan/desa hierarchy.<br>12) <strong>Kode Pos</strong> — 5-digit postal code; verify against postal table where available and flag outliers for manual verification.<br>13) <strong>Kode Kecamatan</strong> — official administrative code; map via kamus lookup and validate kecamatan–desa consistency.<br>14) <strong>Kode Desa / Kelurahan</strong> — village code matching kecamatan; ensure hierarchy validation across codes and use geo-fallback if mapping fails (manual review queue).<br>15) <strong>Kode Faskes Tk.I</strong> — primary facility code (clinic/puskesmas/GP). The code is authoritative — the portal uses it to assign primary facility; map strictly to kamus and capture <code>faskes_mapping_confidence</code> for manual review flows.<br>16) <strong>Nama Faskes Tk.I</strong> — optional; code is primary. Do not rely on name alone. When both code and name present verify consistency and flag mismatches for operator action.<br>17) <strong>Kode Faskes Dokter Gigi</strong> — if template asks; map via kamus and handle optionality per mutasi type.<br>18) <strong>Telepon / Email</strong> — optional participant contact fields; format telephone as Text to preserve leading plus signs and country codes; validate email syntax using a pragmatic regex (RFC-5322-lite) and capture invalid contacts as warnings, not critical failures unless required by policy.<br>19) <strong>Status Pegawai</strong> — employment status code (1=Tetap, 2=Kontrak, 3=Harian/Perjam etc.). Map company internal statuses to template codes and persist mapping table in version control with approver metadata.<br>20) <strong>NPP (No. Pegawai)</strong> — internal employee number used to reconcile with payroll; ensure uniqueness within company and persist previous identifiers for mutation flows. Use <code>npp_history</code> table for audit linking of past identifiers.<br>21) <strong>TMT Kerja (start date)</strong> — dd/mm/yyyy effective employment date; necessary to calculate coverage and eligibility windows. Cross-check <code>TMT</code> vs <code>DOB</code> and <code>Gaji</code> where business rules apply (e.g., probation periods affecting class selection).<br>22) <strong>Kelas Rawat</strong> — I/II/III; compute deterministically from salary bands if the employer sets classes, or leave blank if portal computes by rule. Always store the <code>kelas_decision_source</code> (e.g., <code>company_policy_v3</code> plus approver signature).<br>23) <strong>Gaji Pokok (monthly)</strong> — numeric integer, no separators. Use payroll gross or employer-declared base used for contributions; confirm regional rules for contribution basis and apply rounding/truncation rules consistently. Capture <code>contribution_basis_components</code> when multiple elements contribute to the base.<br>24) <strong>Tunjangan / Komponen Upah Lainnya</strong> — optional columns for allowances if template includes them and if those components determine contribution basis; document which components are included in base and version this mapping.<br>25) <strong>NPWP</strong> — tax ID (format as Text); validate structure and optionally cross-check with tax authority records where integration exists. Flag malformed NPWP but treat as non-blocking unless required for a specific mutasi.<br>26) <strong>No Passport</strong> — for foreigners; include issuing country indicator where requested. Store <code>passport_expiry</code> for eligibility checks and renewals.<br>27) <strong>Kewarganegaraan</strong> — nationality code (1=WNI, 2=WNA) per kamus. For dual nationals capture both identifiers and treat with policy-defined precedence rules.<br>28) <strong>Kode Mutasi / Operation</strong> — operation code: DAFTAR_BARU (new registration), PERUBAHAN_IDENTITAS, PERUBAHAN_FASKES, PERUBAHAN_GAJI, PENONAKTIFAN, etc. Each code activates mandatory columns — automate required-field masks per mutasi type and ensure the validation engine enforces these masks before upload.<br>29) <strong>Tanggal Mutasi / Pelaporan</strong> — dd/mm/yyyy effective date for the upload/mutation; ensure timezone and reporting-period alignment with payroll cutoffs. For batch processing include <code>reporting_period_month</code> and <code>reporting_cutoff</code> artifacts to prove throughput timeliness.<br>30) <strong>Kode Alasan Penonaktifan</strong> — code indicating reason (resignation, termination, death). Attach supporting document references per manifest mapping and ensure the retention of supporting evidence for audit windows mandated by policy.<br>31) <strong>Nomor Dokumen Pendukung</strong> — internal identifier for supporting document, used in the attachments manifest to pair scanned docs to rows; use deterministic naming (e.g., <code>&lt;NIK&gt;_&lt;reason&gt;_&lt;YYYYMMDD&gt;.pdf</code>). Capture both <code>attachment_sha256</code> and <code>attachment_storage_path</code> in the manifest for integrity checks.<br>32) <strong>Keterangan</strong> — free-text notes (avoid sensitive PII here). Limit length to template-supported max to prevent truncation and use <code>keterangan_coded</code> when structured reasons are preferred for analytics.<br>33) <strong>Flag Verifikasi / PIC</strong> — internal code for the operator who prepared or verified the row. Useful for audits and triage priorities; include <code>operator_id</code> and <code>operator_signoff_timestamp</code>.<br>34) <strong>Control / Checksum / System Field</strong> — some template versions include a non-editable control column; do not alter. If present, read kamus for policy. Where present, capture <code>control_checksum</code> and keep read-only handling in ETL; do not regenerate unless explicitly requested by portal support. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>Field-level validation rules & best-practice checks (expanded)</strong><br>- <strong>NIK/KK</strong>: store as Text; enforce <code>^\d{16}$</code> and ensure numeric-only. Implement a batch population cross-check via the official population registry when available and maintain audit records of any exceptions and supporting documents. Flag NIKs that exist but are assigned to a different name in registry results for manual review; create a <code>DCA</code> (discrepancy confirmation attestation) workflow for employee-provided proof. <br>- <strong>Cross-field consistency</strong>: validate <code>Hubungan</code> vs <code>DOB</code> (e.g., "Anak" must have DOB younger than the head by reasonable margin); ensure <code>KK</code> groups have exactly one primary "Peserta" row unless multi-participant family policy applies. Build family-level rulesets to detect orphan dependents and inconsistent family compositions. <br>- <strong>Faskes</strong>: map using official facility codes from the kamus; update canonical lists when portal publishes new kamus versions and snapshot the kamus per upload for reproducibility. For facilities not found in kamus, create a <code>faskes_exception</code> ticketing flow with a required SLA for resolution and an interim manual mapping if permissible. <br>- <strong>Salary → Kelas mapping</strong>: encode salary band-to-class rules in a single source-of-truth table with effective dates; always log the effective mapping version and approver. Include rollback procedures when mapping logic changes. <br>- <strong>Date validation</strong>: use a reliable date parser and business rules (DOB < today; TMT cannot be in the future unless allowed; mutasi date within reporting window). Include leap-year checks and confirm date locale (DD/MM/YYYY) is enforced. Normalize Excel date serials to strings before upload if the portal requires text-formatted dates. <br>- <strong>Character encoding</strong>: enforce UTF-8 NFC. Strip control characters (ZWJ, ZWNJ, BOM) before upload and normalize whitespace sequences. Replace non-printable characters with a placeholder and flag rows for operator review. Maintain an <code>encoding_errors</code> audit file. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>Preprocessing — exact sequence before mapping (detailed)</strong><br>1. <strong>Source export</strong>: extract HRIS/payroll master containing required fields (IDs, names, addresses, contract details, salary elements). Export as CSV or direct DB query to canonical staging area. Ensure export captures payroll period and extraction timestamp and store <code>export_metadata.json</code> with <code>query</code>, <code>executed_by</code>, <code>execution_time</code>, and <code>rowcount</code> fields. <br>2. <strong>Normalization</strong>: normalize Unicode to NFC; trim leading/trailing whitespace; replace repeated whitespace with single spaces; remove non-printable control characters and normalize punctuation (smart quotes, long dashes). Run a <code>sanity_stats</code> pass to capture distributions of key columns (e.g., unique NIK count, empty email %, invalid postal codes). <br>3. <strong>ID formatting</strong>: coerce KK and NIK to Text; apply <code>Text.PadStart</code> for records missing leading zeros due to system export issues; add a <code>source_id</code> and <code>extraction_time</code> column for traceability. Use <code>id_coercion_report.csv</code> to show coerced records and operator approvals when automatic padding applied. <br>4. <strong>Address geocoding</strong>: map address components to kecamatan/desa codes using kamus; if names are ambiguous, store both name and mapped code and include manual review flags; preserve original raw address in a <code>raw_address</code> column for audit. Where address geocoding fails, optionally attempt administrative unit inference from postal code then set <code>address_inference_reason</code>. <br>5. <strong>Faskes mapping</strong>: map HRIS facility names to official facility codes using fuzzy matching; log low-confidence matches for human approval and track <code>similarity_score</code> and approver in mapping table. Maintain a <code>faskes_override_log</code> to record any manual override with <code>who</code>, <code>why</code>, <code>timestamp</code>, and <code>previous_value</code>. <br>6. <strong>Date normalization</strong>: parse all date inputs and format to <code>DD/MM/YYYY</code>, detect impossible dates and add them to the errors file; include parser source and line number for reproducibility. Build an exception list for borderline dates (e.g., Excel serials misinterpreted) and include transformation rules applied. <br>7. <strong>Salary cleansing</strong>: remove thousand separators, validate numeric ranges, and compute derived contribution bases where needed. Apply rounding/truncation rules and record the rule version used in <code>salary_transformation_log</code>. For negative or zero salaries, send to <code>salary_investigation_queue</code>. <br>8. <strong>Duplicate detection</strong>: identify duplicate NIKs, duplicates within same file, and duplicates compared to recent accepted uploads. If duplicates are detected, apply rule: if NIK exists in current active list treat as mutation rather than new registration; log prior active upload IDs for reference, and set <code>action_suggested</code> = <code>mutation</code> or <code>merge</code>. <br>9. <strong>Row-level validation</strong>: run automated checks referencing kamus and mutation rules; create <code>errors_report.csv</code> and a <code>Validation</code> sheet listing <code>rowid + error codes + human instructions</code>. Classify errors as <code>critical</code> (block), <code>major</code> (HR fix), or <code>info</code> (document-only). Provide remediation hints per error code (e.g., <code>ERR_FASKES_MAP</code>: list top 3 candidate faskes codes with similarity scores). <br>10. <strong>Package</strong>: assemble final <code>Form_34_ready.xlsx</code> with canonical column order, a <code>Validation</code> sheet, and a <code>manifest.txt</code> with sha256 checksum before upload. Require sign-off by HR + Payroll recorded through a digital signature or logged SSO event prior to portal upload. Store all artifacts in an encrypted, access-controlled archive with retention metadata. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>Transformation automation — Power Query / ETL design (practical)</strong><br>Recommended approach: implement an idempotent ETL script or a small ETL job to make the transformation reproducible. Steps:<br>- <strong>Parameters</strong>: company code, payroll period, template version, kamus source path, operator_id.<br>- <strong>Source</strong>: connect to HRIS export (CSV, DB).<br>- <strong>Cleansing</strong>: normalize text (Trim, NormalizeUnicode), change types (IDs=Text, dates=Date, salary=Decimal).<br>- <strong>Lookups</strong>: join with versioned kamus tables for facilities and area codes. <br>- <strong>Derived</strong>: compute kelas_rawat by salary bands (single source of truth table) and set <code>kelas_reason</code> fields.<br>- <strong>Validation</strong>: add a computed column <code>valid_row</code> with boolean result and <code>error_notes</code> string listing failures. <br>- <strong>Output</strong>: write to Excel with exact column order and a <code>Validation</code> sheet. <br>Store the M script in source control and include unit tests that validate mapping on sample records. Use scheduled CI jobs to run golden-file regressions when kamus/template changes. Ensure ETL is idempotent: running it repeatedly on the same input should produce the same <code>Form_34_ready.xlsx</code> and the same sha256 checksum unless inputs or configuration changed. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>Pre-upload validation matrix (automatable checks)</strong><br>Implement these checks as boolean rules that produce an errors file for remediation before upload: <code>NIK_present</code>, <code>NIK_regex_ok</code>, <code>KK_present</code>, <code>DOB_valid</code>, <code>Faskes_code_mapped</code>, <code>Kecamatan_mapped</code>, <code>Salary_numeric</code>, <code>Mutasi_code_valid</code>, <code>Mandatory_fields_for_mutasi_present</code>, <code>Duplicate_NIK_flag</code>. Develop a scoring mechanism: rows with <code>score &gt;= threshold</code> pass; rows <code>score &lt; threshold</code> are routed to manual remediation. Critical failures block upload; warnings are logged and require documented HR sign-off to proceed. Include automated KPI capture for acceptance reasons. Provide machine-readable output (JSON) and human-readable reports (CSV, Excel). </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>Regex & lightweight validators (PCRE)</strong><br>- NIK: <code>^\d{16}$</code><br>- KK: <code>^\d{16}$</code><br>- Date (dd/mm/yyyy simple): <code>^([0-2]\d|3[0-1])\/(0\d|1[0-2])\/\d{4}$</code> (use date parser to validate real dates)<br>- Postal: <code>^\d{5}$</code><br>- Salary: <code>^\d+$</code> (no separators)<br>- Email (simple): <code>^[^\s@]+@[^\s@]+\.[^\s@]+$</code><br>Keep regexes in a centralized validators file and version them. For critical checks use deterministic parsers rather than regexes alone (e.g., date parsing library). Unit-test regexes with edge-case examples and include test vectors in repository. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>Fuzzy-match mapping strategy (practical)</strong><br>When HRIS stores facility names as free text, build a canonical mapping table with fields: <code>source_name</code>, <code>canonical_faskes_code</code>, <code>canonical_faskes_name</code>, <code>similarity_score</code>, <code>manual_override_flag</code>, <code>approved_by</code>, <code>approved_on</code>. Use trigram similarity or Jaro-Winkler with a conservative threshold (e.g., 0.85); below threshold, queue for manual review. Persist approved mappings into the canonical table to reduce future manual work. Record who approved each mapping and the timestamp for audit. Maintain a <code>mapping_decay_policy</code> to periodically re-evaluate mappings (e.g., quarterly) against updated kamus files. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>Upload process — step-by-step (operational, expanded)</strong><br>1. Ensure <code>Form_34_ready.xlsx</code> is final and signed-off (HR + Payroll). Use a digital signature or recorded SSO approval event. <br>2. Save final copy with canonical filename and compute <code>sha256</code> checksum; record it in <code>manifest.txt</code> and in <code>upload_log.csv</code> with extraction_time and operator. <br>3. Log into employer upload portal using the BU account (use 2FA and company SSO where available). <br>4. Navigate to Upload → choose "Format 34 kolom" → select unit kerja and upload file. Preserve portal navigation screenshots with timestamps for audit. <br>5. Portal returns validation report (immediate or background). Download and analyze the report: it lists row-level errors & warnings. Automate parsing of portal report to map portal error codes to internal error taxonomy. <br>6. Correct errors in source and regenerate <code>Form_34_ready.xlsx</code> (or produce a corrected delta file for failed rows). Maintain delta files and cross-reference them to original <code>part</code> files. <br>7. Reupload until acceptance; if partial acceptance occurs, note accepted rows and resubmit failed rows in a separate file (use <code>part##</code> naming). Ensure <code>part</code> files are non-overlapping and include <code>rows_range</code> in the manifest. <br>8. Download portal acceptance confirmation and acceptance ID; archive the acceptance report with the signed-off manifest, acceptance sha256, and screenshots or portal ticket. Create <code>acceptance_record.json</code> containing <code>upload_id</code>, <code>accepted_rows</code>, <code>rejected_rows</code>, <code>timestamp</code>, <code>operator_id</code>. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>Interpreting portal error codes & remedies (expanded)</strong><br>Map portal errors into three remediation buckets: Data Correction, Kamus/Mapping Update, Portal/Process Issue. Provide standard remedies with concrete steps:<br>- <strong>ERR_NIK_NOT_FOUND / NIK_MISMATCH</strong> — cause: NIK does not match registry or is invalid. Remedy: verify KTP with employee, run registry validation, submit corrected row or escalate with supporting scanned ID. Maintain evidence chain (scan, HR attestation). If registry indicates multiple records, prepare a reconciliation ticket including <code>dukcapil_response</code> and employee affidavit. <br>- <strong>ERR_FASKES_INVALID</strong> — cause: facility code not found or mismatched name. Remedy: check kamus, map to correct code, reupload. If facility missing in kamus, open <code>kamus_extension</code> request with required metadata. <br>- <strong>ERR_LAYOUT</strong> — cause: header changes or missing columns. Remedy: restore canonical template column order and re-map; always run a header-checker pre-upload. Keep a script <code>header_checker.py</code> that validates case-sensitive header equality. <br>- <strong>ERR_DUPLICATE</strong> — cause: duplicate NIK or multiple registrations. Remedy: verify if existing record is active; if so, use mutation flow instead of new registration; include prior acceptance IDs in remediation notes. For duplicate rows within same file apply a deterministic rule (e.g., prefer the most recently updated row based on <code>source_timestamp</code>) and log override. <br>- <strong>ERR_DATE_INVALID</strong> — cause: malformed date or out-of-range. Remedy: correct date format and business logic. For systemic date parsing problems, check Excel serial vs string formatting and ensure locale is dd/mm/yyyy. <br>- <strong>ERR_ATTACHMENT_REQUIRED</strong> — cause: mutasi requires supporting doc. Remedy: attach scan per manifest mapping and reupload; use password-protected zip if portal lacks secure attachment support and record password exchange method. <br>- <strong>ERR_KAMUS_MISMATCH</strong> — value not in kamus for field. Remedy: consult latest kamus; if discrepancy confirmed, open a support ticket with portal and snapshot your kamus version and evidence for the requested addition. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>Attachment standards, manifest design & secure transfer</strong><br>Attach supporting documents only as required. Bundle attachments into a zip: <code>BU_&lt;Code&gt;_Form34_Attachments_&lt;YYYYMMDD&gt;.zip</code>. Include <code>manifest.csv</code> with columns: <code>RowID</code>, <code>NIK</code>, <code>AttachmentFileName</code>, <code>DocumentType</code>, <code>SignedBy</code>, <code>Date</code>, <code>sha256</code>. When emailing attachments (only if portal lacks attachment support), use password-protected zip with password shared via a separate secure channel. Prefer portal upload to avoid email PII exposure. Encrypt stored archives and limit access to authorized roles. Retain attachment logs for audit matching to upload acceptance IDs. For each attachment maintain metadata: <code>uploader_id</code>, <code>upload_time</code>, <code>validation_status</code> (virus scan, integrity check), and <code>retention_end_date</code>. Keep an <code>attachment_access_log</code> for forensic traces. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>File naming conventions, partitioning, and checksums</strong><br>Use deterministic file names for traceability: <code>BU_&lt;CompanyCode&gt;_Form34_&lt;MutasiCode&gt;_&lt;YYYYMM&gt;_part&lt;NN&gt;_v&lt;V&gt;.xlsx</code>. For large populations split into sequential <code>part</code> files (e.g., <code>part01</code>, <code>part02</code>) with row ranges recorded in the manifest. Generate an SHA-256 checksum for each artifact and store in <code>manifest.txt</code> for integrity verification. Keep a <code>parts_index.csv</code> documenting row-range, rowcount, checksum, and upload outcome. Automate an S3-style storage layout that includes <code>company_code/year/month/upload_id/</code> and maintain lifecycle policies to move older archives to cold storage. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>Testing strategy, golden files & regression</strong><br>Maintain golden test files that exercise edge cases (foreign nationals, large families, diacritics, long names, boundary salary values, unmatched facilities). On template or kamus updates, automatically re-run the ETL transforms on golden files and validate results against expected acceptance behavior. Record diffs and escalate unexpected failures. Use CI to gate release of updated mapping pipelines. Add automated smoke uploads (sample 10 rows) to the staging portal when available and include an automated acceptance verification step that parses the portal response. Keep <code>regression_dashboard</code> showing pass/fail per golden test across template versions. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>Acceptance criteria before production upload</strong><br>- Sample staging upload: 0 critical errors on the sample.<br>- Unique NIK coverage: expected headcount matches unique NIKs.<br>- Mutation-specific mandatory fields populated (per kamus).<br>- Payroll sign-off confirmed with timestamped manifest.<br>- Validation report snapshot saved and attached to the upload artifact. Maintain a gating checklist artifact in the CI pipeline to block production runs until all criteria satisfied. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>Rollback & correction workflow (controlled)</strong><br>If erroneous data is accepted: prepare a corrective <code>Form_34</code> using appropriate mutasi code and include required supporting documents. Upload corrective file for affected rows only. Maintain rollback ledger: <code>erroneous_upload_id</code> → <code>corrective_upload_id</code> → <code>reason</code> → <code>operator</code> → <code>timestamp</code>. For large-scale revert requests, coordinate with the regional office — do not attempt destructive operations in portal without explicit guidance. Track rollback metrics and time-to-correct KPIs and provide weekly summaries to stakeholders. Where allowed, automate corrective file generation using diffs between accepted dataset and source master and route to HR for sign-off. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>Post-acceptance reconciliation & QA</strong><br>After acceptance: reconcile accepted rows against payroll and HRIS master; sample-check 1–2% of accepted records against source documents; verify contribution calculations for changed salaries in payroll; compute KPIs (AcceptanceRate, TimeToAcceptance) for continuous improvement; archive acceptance report, manifest, and signed approvals. Automate periodic reconciliation jobs comparing portal accepted dataset to local master and flag divergences. Maintain <code>reconciliation_run</code> artifacts with <code>run_id</code>, sample_size, pass_rate, and remediation actions. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>KPIs to monitor operationally (with measurement guidance)</strong><br>Track: UploadSuccessRate = accepted/total, AvgValidationErrorsPer1kRows, AvgTimeToAcceptanceHours, RepeatRejectionRate, PercentOfUploadsAutomated, and Top5RejectionReasons. Create an operations dashboard with drilldown to row-level errors and trends by error category across weeks/months. Set targets (e.g., AcceptanceRate ≥ 98%, MeanTimeToAccept ≤ 48 hours). Store weekly snapshots and retain historical KPI data for trend analysis. Provide alerts for KPI degradation and automated tickets for repeated error patterns. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>Security & privacy controls (detailed)</strong><br>- Restrict upload and artifact access to authorized HR PICs & system accounts; use RBAC and 2FA.<br>- Keep artifacts (Excel, attachments, manifests) encrypted at rest and in transit; use company KMS for key management.<br>- Use pseudonymized test data in non-production environments.<br>- Maintain an incident response plan for PII exposure; notify Legal and Compliance when incidents occur per company policies and log incident IDs.<br>- Ensure retention schedules reflect regulatory needs; purge temporary staging copies after confirmed acceptance and retention period elapses. Perform periodic access reviews and record role changes. Maintain <code>pii_access_audit</code> and a <code>pii_purge_log</code>. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>Escalation matrix & RO coordination (operational)</strong><br>Define a 4-level escalation path: (1) HR PIC (first contact), (2) Payroll Lead (internal escalation), (3) regional office Account Manager (technical & policy exceptions), (4) Legal/Compliance (regulatory disputes). When escalating to the regional office include: sample rows, validation report, template SHA-256, upload timestamp, and any portal screenshots; log ticket IDs and track to closure. Keep an escalation playbook with email templates and required attachments. For critical portal outages include a cross-functional war room with defined roles and triage script. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>Governance: roles, responsibilities & SLAs</strong><br>- <strong>HR PIC</strong>: prepare data, correct personal information, request employee verification for mismatches (SLA: respond to validation queries within 24 hours).<br>- <strong>Payroll Lead</strong>: confirm salary basis and approve uploads (SLA: sign-off within 48 hours pre-upload).<br>- <strong>HRIS Engineer</strong>: maintain ETL, mapping, and run validations (SLA: fix automation issues within 3 business days).<br>- <strong>Compliance</strong>: manage PII policies and retention; approve exceptional releases. Record all approvals and maintain an audit trail accessible for internal/external audits. Include an annual review cycle for SOPs and responsibilities. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>Change control & template update policy (governance)</strong><br>When portal updates the <code>Form_34</code> template or kamus, follow controlled steps: download new template → record sha256 → run automated regression on golden files → adjust mapping scripts → run staging upload → gather HR + Payroll sign-off → update <code>template_registry.xlsx</code> and SOP → re-enable production uploads. Do not upload to production without passing regression tests. Maintain a communication calendar for scheduled portal maintenance windows and publish impact notices to stakeholders when template or kamus changes are detected. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>Common pitfalls & mitigation tactics (expanded)</strong><br>- <strong>Editing headers manually</strong>: always map programmatically to canonical template. <br>- <strong>Leading-zero loss</strong>: enforce Text or pad programmatically. <br>- <strong>Facility name mismatches</strong>: maintain canonical mapping table + fuzzy-review flow. <br>- <strong>Large one-file uploads</strong>: split files and track parts in manifest. <br>- <strong>Unencrypted PII via email</strong>: use portal attachment or encrypted packages only. <br>- <strong>Stale kamus</strong>: implement kamus sync checks and alerting when portal kamus hash differs from local. <br>- <strong>Time-zone mismatches</strong>: ensure upload timestamps are recorded in UTC and local timezone conversion is logged. <br>- <strong>Unchecked manual fixes</strong>: require manual remediation to be recorded in the validation sheet with operator initials. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>Operational SOP (concise checklist for HR Ops)</strong><br>1) Export HRIS/payroll population for target period.<br>2) Run validation script (produce <code>errors_report.csv</code>).<br>3) Fix flagged records.<br>4) Run ETL (Power Query/M or scripted) to map to <code>Form_34</code>.<br>5) Produce <code>Form_34_ready.xlsx</code> and compute sha256; attach manifest.<br>6) HR + Payroll sign-off.<br>7) Upload to portal; download validation report; fix and reupload until accepted.<br>8) Archive acceptance report, manifest, and signed approvals in audit folder.<br><br>Use this checklist as a gating artifact in the CI/CD pipeline for automated runs. For each step, record <code>actor_id</code>, <code>timestamp</code>, and <code>artifact_path</code>. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>Troubleshooting playbook (triage flow)</strong><br>1) Reproduce error locally on single row.<br>2) Categorize: schema/layout, kamus mismatch, data value, or portal bug.<br>3) Fix source row(s) and re-run local validation.<br>4) Reupload sample.<br>5) If valid rows fail systematically, escalate to regional office with artifacts.<br>6) Log resolution and update mapping/GOLDEN tests with lessons learned.<br>Capture time-to-resolution and update root-cause taxonomy for trend analysis.<br>Create a <code>triage_ticket</code> template with required attachments for each escalation level. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>Automation opportunities & prioritized investments</strong><br>1) <strong>High value</strong>: automated ETL pipeline (HRIS → transforms → Form_34) with scheduled runs and validations, integrated with SSO and sign-off flows.<br>2) <strong>Medium</strong>: fuzzy-match UI for HR to resolve mappings and approve low-confidence facility matches.<br>3) <strong>Medium</strong>: CI gating to run golden-file regressions on template updates and automated smoke uploads to staging.<br>4) <strong>Low</strong>: API-based uploads if regional office supports tokenized automation.<br><br>Start with ETL + validation automation — highest ROIs. Consider building an internal microservice that validates, maps, and packages artifacts and exposes a review web UI for manual remediation tasks. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>Testing & regression framework</strong><br>- Maintain golden sample sets (small/medium/large) for each template version. <br>- Automate regression: run PQ transforms on golden sets when kamus/template changes; compare validation outputs vs expected; block production until passing. <br>- Keep unit tests for mapping rules (salary→kelas mapping, faskes mapping, date validators). Integrate test runs into CI and record test artifacts' checksums. Maintain a <code>test_failures_registry</code> and require triage tickets for persistent failures. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>Sample synthetic test rows (5 anonymized, safe for staging)</strong><br>Row A (employee): RowID=1; KK=0123456789012345; NIK=3201123456789012; Name="Test Employee A"; Hub=1; DOB=01/01/1990; Gender=1; Address="Jalan Mawar 1"; RT=001; RW=002; KodePos=12110; KecamatanCode=320112; DesaCode=32011201; FaskesCode=FK000123; StatusPegawai=1; NPP=EMP0001; TMT=01/02/2026; Kelas=II; Gaji=8500000; Mutasi=DAFTAR_BARU; Notes="Synthetic test row".<br>Row B (dependent): RowID=2; same KK as Row A; NIK=3201123456789013; Name="Test Child B"; Hub=4; DOB=15/05/2015; Gender=2; FaskesCode=FK000123; Mutasi=DAFTAR_BARU.<br>Row C (WNA): RowID=3; KK=0000000000000000; NIK= (blank); Passport=Z1234567; Name="Test Foreigner C"; Kewarganegaraan=2; NoPassport=Z1234567; FaskesCode=FK000999; Mutasi=DAFTAR_BARU.<br>Row D (edge salary): RowID=4; KK=9876543210987654; NIK=3201123456789020; Name="Boundary Salary D"; Hub=1; DOB=10/10/1985; Gaji=4999999; Kelas=III (per policy boundary).<br>Row E (diacritics): RowID=5; KK=0123400000000001; NIK=3201123456789031; Name="Siti Núñez-Åström"; Hub=1; DOB=20/08/1988; FaskesCode=FK000123; Mutasi=DAFTAR_BARU. Use only for staging; persist these rows in golden-tests and exclude from production exports. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>Deliverables available on immediate request</strong><br>Select one or more to generate in this session (embedded as content):<br><strong>A</strong> = exact <code>Form_34</code> CSV header row (34 canonical columns) for copy/paste;<br><strong>B</strong> = synthetic <code>Form_34.xlsx</code> (5 anonymized rows + Validation sheet) packaged as an embedded spreadsheet preview;<br><strong>C</strong> = Power Query (M) skeleton script to map HRIS export → Form_34 with kamus lookups;<br><strong>D</strong> = Python (pandas) validation script producing <code>errors_report.csv</code> and <code>Form_34_ready.xlsx</code>.<br><br>Reply with letters (e.g., "A" or "A,B") and the requested artifacts will be generated directly in this session.<br>(Note: generated spreadsheets or scripts will be parameterized and include comments and sample data for immediate testing.) </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>Appendix — validation sheet structure (recommended layout)</strong><br>Create a <code>Validation</code> sheet in the upload workbook with columns: <code>RowID</code>, <code>NIK</code>, <code>KK</code>, <code>Check_NIK_regex</code>, <code>Check_KK_regex</code>, <code>Check_Faskes_mapped</code>, <code>Check_DOB_valid</code>, <code>Check_Salary_numeric</code>, <code>Mutasi_code_valid</code>, <code>Attachments_needed</code>, <code>Error_codes</code> (comma-separated), <code>Operator_notes</code>. Use this sheet as the canonical remediation tracker and require operators to clear <code>Error_codes</code> before authorizing reupload. Include <code>resolution_by</code> and <code>resolution_timestamp</code> fields for auditability; consider a <code>resolution_evidence_path</code> to link scans and ticket IDs. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>Sample email templates (operational use)</strong><br>1) <strong>Pre-check request to regional office</strong> — Subject: <code>Pre-check Request — Form_34 sample upload — BU_&lt;Company&gt;</code> Body: include company code, PIC contact, sample file attached (max 50 rows), and request for format validation prior to full upload. Attach <code>template_registry</code> snapshot and <code>manifest_meta.json</code>.<br>2) <strong>Post-acceptance internal</strong> — Subject: <code>Form_34 Upload Accepted — BU_&lt;Company&gt; — &lt;Date&gt;</code> Body: attach acceptance report and summary: accepted rows, rejected rows (0), acceptance ID, and actions for payroll. Keep templates in a central comms folder and version them. Provide canned follow-up for common rejection reasons to speed operator response. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>Appendix — manifest & archive contents (what to store)</strong><br>For each upload store: <code>hr_export_source.csv</code> + checksum; <code>Form_34_ready.xlsx</code> used; <code>validation_report</code> (portal); <code>acceptance_confirmation</code> (portal acceptance ID); <code>attachments</code> zip + <code>manifest.csv</code>; <code>template_registry</code> snapshot; <code>signed_approval.pdf</code> (HR+Payroll); <code>rollback_log</code> entries (if any); <code>audit_readme.txt</code> describing the upload. Encrypt archives and keep limited access to authorized auditors. Use storage path convention: <code>/archive/&lt;company_code&gt;/Form34/&lt;YYYY-MM&gt;/upload_&lt;upload_id&gt;/</code> and include <code>retention_policy</code> metadata. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>FAQs (operational)</strong><br>Q: Can we edit template headers to match payroll export?<br>A: No. Always map into the canonical template programmatically.<br>Q: What if NIKs are rejected en masse?<br>A: Run population batch validation, obtain KTP proof from employees where needed, escalate to regional office for systemic mismatch with documentation.<br>Q: Does Form_34 apply to employment social security templates?<br>A: No — employment social security uses different templates and portals; keep uploads separate and maintain separate mapping pipelines for each social scheme.<br>Q: Are attachments mandatory for death or termination cases?<br>A: Typically yes — follow the <code>KodeAlasanPenonaktifan</code> policy in the kamus; attach supporting documentation per the portal rules. </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>Final operational note (executive)</strong><br>Embed Form_34 processing in a deterministic ETL pipeline with versioned templates, automated validation, golden-file regressions, and an auditable artifact trail. Treat the kamus data as authoritative, automate what is safe to automate, and escalate early for portal or kamus inconsistencies. Maintain continuous improvement metrics and allocate resources to the highest ROI automation opportunities (ETL & validation, fuzzy-match approvals, CI regressions). </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>Appendix — canonical Form_34 CSV header row (copy/paste)</strong><br><code>RowID,KK,NIK,NamaLengkap,Hubungan,TempatLahir,TanggalLahir,JenisKelamin,StatusPerkawinan,AlamatLengkap,RT,RW,KodePos,KodeKecamatan,KodeDesa,KodeFaskes1,NamaFaskes1,KodeFaskesGigi,Telepon,Email,StatusPegawai,NPP,TMTKerja,KelasRawat,GajiPokok,Tunjangan,NPWP,NoPassport,Kewarganegaraan,KodeMutasi,TanggalMutasi,KodeAlasanPenonaktifan,NoDokumenPendukung,Keterangan,FlagVerifikasi,ControlChecksum</code> </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>Appendix — manifest.csv example (fields & sample row)</strong><br><code>file_name,part,rows,rows_range,sha256,created_by,created_on,upload_id</code><br><code>BU_ABC_Form34_DAFTAR_202602_part01.xlsx,part01,500,1-500,3f2a1e...,ops.user,2026-02-20,</code> </td></tr><tr><td data-label="Form Excel 34 BPJS Kesehatan Guidance"> <strong>Appendix — common portal error codes (expanded quick-reference)</strong><br><code>ERR_NIK_REGEX</code> — NIK must match 16 digits.<br><code>ERR_KK_REGEX</code> — KK must match 16 digits.<br><code>ERR_FASKES_MAP</code> — facility code missing or invalid.<br><code>ERR_DUPLICATE_NIK</code> — NIK duplicates within file or portal.<br><code>ERR_LAYOUT</code> — file header mismatch.<br><code>ERR_DATE_INVALID</code> — malformed or out-of-range date.<br><code>ERR_ATTACHMENT_REQUIRED</code> — missing supporting doc for this mutation.<br><code>ERR_MUTASI_MANDATORY</code> — required column(s) for mutation not present.<br><code>ERR_KAMUS_MISMATCH</code> — value not in kamus for field.<br><code>ERR_NIK_NOT_FOUND</code> — NIK not present in population registry (requires manual reconciliation).<code>   **Appendix — recommended </code>manifest.csv<code> schema for attachments**&lt;br&gt;</code>RowID,NIK,AttachmentFileName,DocumentType,SignedBy,Date,sha256,portal_attachment_id<code>   **Appendix — minimal portal upload troubleshooting checklist**&lt;br&gt;1. Confirm template hash matches </code>template_registry.xlsx<code>.&lt;br&gt;2. Confirm column headers equal canonical header row (case-sensitive).&lt;br&gt;3. Confirm no formula cells or merged cells in participant sheet. &lt;br&gt;4. Confirm all date cells are text in </code>DD/MM/YYYY<code> format or Excel date type recognized by portal. &lt;br&gt;5. Confirm zipped attachments manifest matches file names and sha256 sums. &lt;br&gt;6. Confirm the file size is within portal limits and parts are properly partitioned if necessary.   **Appendix — deterministic filename examples (operational)**&lt;br&gt;</code>BU_ACME_Form34_DAFTAR_202602_part01_v1.xlsx<code>&lt;br&gt;</code>BU_ACME_Form34_DAFTAR_202602_part01_v1_manifest.txt<code>&lt;br&gt;</code>BU_ACME_Form34_Attachments_20260201.zip<code>   **Appendix — quick-reference remediation flow for ERR_NIK_NOT_FOUND**&lt;br&gt;1) Run registry batch check and capture response IDs.&lt;br&gt;2) If registry data shows mismatch, request KTP scan from employee and update HRIS.&lt;br&gt;3) Create corrective </code>Form_34<code> row with </code>PERUBAHAN_IDENTITAS<code> if previously accepted.&lt;br&gt;4) Archive KTP scan with </code>NoDokumenPendukung<code> and reference in manifest.&lt;br&gt;&lt;br&gt;Maintain </code>dukcapi_recon_log<code> with </code>employee_id<code>, </code>dukcapil_response_code<code>, and </code>action_taken<code>.   **Appendix — acceptance evidence packaging (recommended structure)**&lt;br&gt;</code>/archive/BU_ACME/Form34/2026-02/<code>&lt;br&gt;- </code>hr_export_source.csv<code>&lt;br&gt;- </code>Form_34_ready_part01_v1.xlsx<code>&lt;br&gt;- </code>manifest.txt<code> (sha256 list)&lt;br&gt;- </code>portal_validation_report.pdf<code>&lt;br&gt;- </code>portal_acceptance_confirmation.pdf<code>&lt;br&gt;- </code>attachments.zip<code>&lt;br&gt;- </code>signed_approval_HR_Payroll.pdf<code>   **Appendix — lightweight audit checklist (to attach to each upload)**&lt;br&gt;- Template sha256 recorded: Y/N&lt;br&gt;- HR sign-off timestamp: YYYY-MM-DD HH:MM&lt;br&gt;- Payroll sign-off timestamp: YYYY-MM-DD HH:MM&lt;br&gt;- Errors resolved: critical=0&lt;br&gt;- Attachment manifest included: Y/N&lt;br&gt;- Acceptance ID archived: Y/N&lt;br&gt;- Reconciliation run scheduled: Y/N   **Appendix — change log template for template/kamus updates (recommended)**&lt;br&gt;</code>change_id,changed_by,change_date,template_version,kamus_version,description,tests_run,regression_result,approved_by,approval_date<code>&lt;br&gt;</code>CHG-2026-001,hris.engineer,2026-02-18,v2.1,kamus-202602,Add new faskes codes and rename header field,GoldenTestRun: PASS,approved_by: payroll.lead,2026-02-19<code>   **Appendix — sample operator remediation instructions (template for Validation sheet)**&lt;br&gt;</code>ERR_CODE: ERR_FASKES_MAP<code>&lt;br&gt;</code>Instruction: Check faskes_name against kamus sheet. If similar facility exists, set KodeFaskes1 to <recommended_code>. If not found, create ticket to request kamus update with supporting evidence (facility address, phone, registration number). Mark record as pending and attach operator comment.<code>   **Appendix — glossary (concise)**&lt;br&gt;</code>Form_34 template<code> — canonical 34-column employer upload workbook used for bulk participant registration/mutation.&lt;br&gt;</code>kamus<code> — portal-published lookup tables used for enumerated fields (facility codes, area codes, statuses).&lt;br&gt;</code>ETL<code> — extract-transform-load pipeline used to convert HRIS exports into the canonical template.&lt;br&gt;</code>Golden files<code> — small curated test datasets used for regression and acceptance testing.   **Appendix — recommended escalation email template (RO support)**&lt;br&gt;Subject: </code>Support Request — Portal Validation Failure — BU_<Company> — Upload_<upload_id><code>&lt;br&gt;Body: Provide </code>upload_id<code>, </code>sha256<code>, sample rows (CSV), portal validation report (PDF), error codes, and steps already attempted. Attach </code>template_registry<code> and </code>manifest_meta.json<code>. Request root-cause analysis and indicate urgency level. Include contact details and preferred SLA.   **Appendix — monitoring &amp; alerting recommendations**&lt;br&gt;Instrument the pipeline to emit structured events: </code>extraction_completed<code>, </code>validation_completed<code>, </code>upload_attempted<code>, </code>portal_response_received<code>, </code>acceptance_received<code>. Use these events to drive alerts when a threshold is breached (e.g., AcceptanceRate &lt; 95% over 24h or RepeatRejectionRate &gt; 5%). Integrate alerting with incident management (PagerDuty/Slack) for on-call rotations.   **Appendix — recommended retention policy (example)**&lt;br&gt;- Temporary staging exports: retain 30 days.&lt;br&gt;- Accepted upload artifacts and acceptance confirmations: retain 10 years or per regulatory requirement.&lt;br&gt;- Attachment archives with supporting documents: retain per local regulation (typical 5–10 years).&lt;br&gt;- Golden files and regression artifacts: indefinite with versioning.   **Appendix — sample metrics report fields (weekly)**&lt;br&gt;</code>week_start,uploads_total,uploads_accepted,uploads_rejected,acceptance_rate,avg_validation_errors_per_1k,top3_rejection_reasons,mean_time_to_accept_hours<code>   **Appendix — recommended RBAC matrix (minimal)**&lt;br&gt;- </code>hr_ops<code> — prepare data, view validations, request manual mappings, cannot upload to production.&lt;br&gt;- </code>payroll_lead<code> — review salary mappings, sign-off uploads, cannot change kamus.&lt;br&gt;- </code>hris_engineer<code> — manage ETL, run tests, maintain mappings in version control.&lt;br&gt;- </code>uploader<code> — perform portal uploads and download reports (2FA required).&lt;br&gt;- </code>auditor<code> — readonly access to archives and logs.   **Appendix — sample audit evidence checklist to attach to compliance review**&lt;br&gt;- </code>template_registry<code> snapshot with sha256.&lt;br&gt;- </code>manifest_meta.json<code> with operator sign-offs.&lt;br&gt;- </code>portal_validation_report.pdf<code> and </code>portal_acceptance_confirmation.pdf<code>.&lt;br&gt;- </code>attachments.zip<code> and its </code>manifest.csv<code> with sha256 sums.&lt;br&gt;- </code>reconciliation_run<code> output and sampled document scans.   **A. Expanded field edge-cases &amp; handling guidance**&lt;br&gt;- **NIK with leading/trailing whitespace or non-breaking spaces**: detect and normalize before regex validation. Log original and normalized values in </code>normalization_audit.csv<code> for traceability. &lt;br&gt;- **Multiple passport entries**: if HRIS contains multiple passport fields, apply precedence rules (</code>passport_current<code> &gt; </code>passport_previous<code>) and capture </code>passport_source<code> and </code>passport_source_date<code>. &lt;br&gt;- **Name suffix/prefix handling**: support extraction of honorifics (e.g., &quot;Dr.&quot;, &quot;H.&quot;, &quot;Ir.&quot;) into separate fields when present; do not submit honorifics in </code>NamaLengkap<code> if portal prohibits them—document any truncation in </code>name_transform_log<code>. &lt;br&gt;- **Compound family names and diacritics**: when name length exceeds template limit, store full name in </code>name_truncation_log<code> and require HR sign-off for accepted truncation policy; prefer sending untruncated UTF-8 where portal accepts it. &lt;br&gt;- **Temporary IDs for newborns**: adopt deterministic placeholder NIK or KK conventions for initial registration only if portal accepts placeholders; otherwise require HR to submit as mutation once NIK assigned. Capture mapping to final NIK in </code>newborn_resolution_log<code>.   **B. Expanded error code catalog (operational RFC)**&lt;br&gt;- </code>ERR_INVALID_NPP<code> — NPP invalid or non-unique. Remedy: check internal employee registry; ensure unique constraint and correct format.&lt;br&gt;- </code>ERR_INVALID_KELAS<code> — KelasRawat value outside accepted enumerations. Remedy: recalc from salary bands or leave blank if portal computes.&lt;br&gt;- </code>ERR_EMAIL_INVALID<code> — Email not matching pattern or domain restrictions. Remedy: treat as warning unless required for contact flows.&lt;br&gt;- </code>ERR_PHONE_FORMAT<code> — detection of local vs international formats; standardize to </code>+62<code> rule where applicable.&lt;br&gt;- </code>ERR_POSTAL_MISMATCH<code> — postal code not in kecamatan; remedy: cross-check address inference and flag for manual review.&lt;br&gt;- </code>ERR_ATTACHMENT_HASH_MISMATCH<code> — manifest sha256 differs from stored zip; remedy: quarantine and re-generate attachments zip.   **C. Additional synthetic test rows (5 more for stress &amp; edge-case coverage)**&lt;br&gt;Row F (duplicate within file): RowID=6; KK=0123456789012345; NIK=3201123456789012; Name=&quot;Test Employee A Dup&quot;; Mutasi=DAFTAR_BARU — used to test duplicate detection and override rules.&lt;br&gt;Row G (future DOB edge): RowID=7; KK=1111111111111111; NIK=3201129999999999; Name=&quot;Future Born&quot;; DOB=01/01/2030; Mutasi=DAFTAR_BARU — validate DOB future rejection.&lt;br&gt;Row H (long address &amp; special chars): RowID=8; Address contains 1024 characters with punctuation and emojis (for encoding tests).&lt;br&gt;Row I (salary zero): RowID=9; Gaji=0; Mutasi=DAFTAR_BARU — triggers </code>salary_investigation_queue<code>.&lt;br&gt;Row J (expired passport): RowID=10; Passport=Z7654321; PassportExpiry=01/01/2020; Kewarganegaraan=2; Mutasi=DAFTAR_BARU — check expiry business rules.   **D. Expanded remediation playbooks with decision trees (concise)**&lt;br&gt;- **If portal returns &gt;10% overall rejection rate on a full upload**:&lt;br&gt;(1) Stop further uploads.&lt;br&gt;(2) Run a row-level rejection distribution.&lt;br&gt;(3) Pick top 3 error categories.&lt;br&gt;(4) Create triage tickets for each with sample 20 rows.&lt;br&gt;(5) Re-run mapping/regression on golden files aligned to portal kamus version.&lt;br&gt;(6) Schedule a war-room if root cause not identified within SLA.&lt;br&gt;- **If faskes mismatch is systemic**:&lt;br&gt;(1) Snapshot your kamus and portal kamus.&lt;br&gt;(2) Compute diff and candidate mappings.&lt;br&gt;(3) Open </code>kamus_extension<code> with portal and attach authoritative metadata (facility registration numbers, addresses).&lt;br&gt;(4) Create interim manual mapping table and mark affected rows as </code>pending_kamus<code> until resolution.   **E. Expanded KPIs &amp; suggested alert thresholds (operational)**&lt;br&gt;- </code>AcceptanceRate<code> — alert if &lt; 95% over rolling 24h. &lt;br&gt;- </code>AvgValidationErrorsPer1kRows<code> — alert if &gt; 50. &lt;br&gt;- </code>RepeatRejectionRate<code> — alert if &gt; 5% of total rejections are repeats within 30 days. &lt;br&gt;- </code>AttachmentIntegrityFailures<code> — alert immediately on any hash mismatch. &lt;br&gt;- **Notes on measurement**: compute KPIs both per-upload and aggregated per-day/week with annotations for template/kamus changes that could explain spikes.   **F. Expanded artifact examples (machine-readable templates)**&lt;br&gt;- **manifest_meta.json (example fields)**: </code>{"upload_id":"UPL-202602-0001","company_code":"BU_ABC","operator_id":"ops.user","created_on":"2026-02-20T07:12:00Z","template_sha256":"3f2a1e...", "kamus_version":"kamus-202602","rowcount":500}<code>. &lt;br&gt;- **acceptance_record.json (example)**: </code>{"upload_id":"UPL-202602-0001","accepted_rows":495,"rejected_rows":5,"acceptance_id":"ACC-202602-7890","timestamp":"2026-02-20T08:01:00Z"}<code>. Keep these JSON artifacts with every upload for audit and machine parsing.   **G. Operational checklist for regional office escalations (detailed)**&lt;br&gt;When opening a ticket include: (1) canonical </code>Form_34_ready.xlsx<code> (sha256); (2) </code>manifest_meta.json<code>; (3) </code>portal_validation_report.pdf<code>; (4) 10-20 sample rows in CSV with </code>rowid<code> and portal error codes; (5) </code>template_registry<code> snapshot; (6) screenshots showing portal error messages; (7) contact details and preferred SLA. Use subject line </code>URGENT: PortalValidation — <company_code> — <upload_id><code>.   **H. Expanded guidance on partial acceptance &amp; idempotency**&lt;br&gt;- If portal accepts partial rows, mark accepted </code>rowids<code> in </code>parts_index.csv<code> and remove them from reupload sets to avoid duplicates. &lt;br&gt;- For idempotency, every upload file should contain a </code>trace_id<code> and non-overlapping </code>part<code> row ranges to allow safe retries. Record all </code>trace_id<code> → </code>portal_upload_id<code> mappings.   **I. Additional governance artifacts to maintain**&lt;br&gt;- </code>operator_roster.csv<code> with </code>operator_id<code>, </code>role<code>, </code>contact<code>, </code>enabled_since<code>, </code>last_access_review<code>.&lt;br&gt;- </code>kamus_sync_log.csv<code> capturing </code>kamus_sha256<code>, </code>downloaded_on<code>, </code>downloaded_by<code>, </code>diff_summary<code>.&lt;br&gt;- </code>golden_tests_registry.csv<code> listing </code>test_id<code>, </code>description<code>, </code>expected_result<code>, </code>last_run<code>, </code>last_result<code>.   **J. Extended FAQs (additions)**&lt;br&gt;Q: How to handle mass surname changes due to legal name changes?&lt;br&gt;A: Use </code>PERUBAHAN_IDENTITAS<code> with supporting court/registry documents attached and include mapping from </code>old_name<code> → </code>new_name<code> in </code>name_change_manifest.csv<code>. HR sign-off required.&lt;br&gt;Q: Portal temporary outages during uploads — what to preserve?&lt;br&gt;A: Keep local copies of the final </code>Form_34_ready.xlsx<code>, manifest, and the portal session logs/screenshots. Do not re-run automatic retries without changing </code>trace_id<code>; instead schedule a manual retry with RO coordination after outage clearance.   **K. Additional recommended low-friction automations**&lt;br&gt;- Automatic generation of the </code>Validation<code> sheet and pre-filled remediation instructions for common error codes.&lt;br&gt;- Auto-splitting logic for large batches to create </code>partNN<code> files under portal size limits with consistent naming and indexed row ranges.&lt;br&gt;- Slack/PagerDuty notifications for uploads that exceed threshold rejections.   **L. Extended operator quick-reference cards (one-line rules)**&lt;br&gt;- Always verify template sha256 before mapping.&lt;br&gt;- Do not edit headers manually; use mapping scripts.&lt;br&gt;- Treat kamus as authoritative; do not invent facility codes.&lt;br&gt;- Log every manual override with </code>who/why/when<code>.&lt;br&gt;- Archive acceptance artifacts immediately after successful upload.   **M. Additional appendices (samples &amp; templates)**&lt;br&gt;- </code>name_change_manifest.csv<code> columns: </code>old_name,new_name,nik,doc_file,doc_sha256,approved_by,approval_date<code>.&lt;br&gt;- </code>faskes_override_log.csv<code> columns: </code>source_name,override_code,override_name,approved_by,approved_on,comment<code>.&lt;br&gt;- </code>reconciliation_run<code> JSON example: </code>{"run_id":"REC-202602-01","upload_id":"UPL-202602-0001","sample_size":50,"pass_rate":0.96,"notes":"2 mismatches due to name normalization"}<code>.   **N. Closing operational guidance (concise)**&lt;br&gt;Continue using the checklists, golden tests, and automated validations. Prioritize automations that remove human error (header-matching, id coercion, faskes fuzzy mapping). Maintain auditability: do not delete or overwrite manifests, acceptance reports, or sign-off records. Review and update the SOP annually or whenever the portal publishes a template or kamus update.   **O. Legal &amp; compliance checklist (concise actionable items)**&lt;br&gt;- Confirm legal basis for processing (contractual necessity / legal obligation).&lt;br&gt;- Maintain DPIA summary for Form_34 pipeline if PII processing scale &gt; threshold or if special category data present.&lt;br&gt;- Ensure data-processing agreements (DPAs) with third-party payroll vendors include: permitted purposes, subprocessors, data retention, incident notification timelines, and audit rights.&lt;br&gt;- Maintain a register of lawful bases per country/region for cross-border transfers; log data transfers to external processors and their legal mechanism (SCCs, adequacy, consent).&lt;br&gt;- Require revocation &amp; correction procedures documented and tested annually; store evidence of user requests and outcome.   **P. Data quality monitoring &amp; alerting (expanded metrics and triggers)**&lt;br&gt;- Implement continuous data-quality jobs producing: </code>NIK_completeness_pct<code>, </code>KK_completeness_pct<code>, </code>Faskes_map_rate<code>, </code>address_geo_match_rate<code>, </code>email_valid_pct<code>, </code>phone_normalization_rate<code>.&lt;br&gt;- Define automatic alerts: </code>NIK_completeness_pct < 98%<code>, </code>Faskes_map_rate < 95%<code>, </code>repeat_rejection_rate > 5% (7-day rolling)<code>.&lt;br&gt;- Capture drift metrics comparing current kamus mappings to historical (e.g., new facility additions per week) and alert when drift exceeds baseline.&lt;br&gt;- Maintain a </code>dq_dashboard<code> that links alerts to remediation tickets and SLA owner.   **Q. KPI definitions with exact formulas (machine-friendly)**&lt;br&gt;- </code>AcceptanceRate = accepted_rows / total_rows<code>.&lt;br&gt;- </code>AvgValidationErrorsPer1kRows = (total_validation_errors / total_rows) * 1000<code>.&lt;br&gt;- </code>RepeatRejectionRate = rejected_rows_that_were_previously_rejected / total_rejected_rows<code>.&lt;br&gt;- </code>MeanTimeToAcceptHours = mean(acceptance_timestamp - upload_timestamp)<code>; compute in UTC and express to two decimal places.&lt;br&gt;- Persist KPI provenance: </code>upload_id<code>, </code>kamus_version<code>, </code>template_sha256<code>, </code>kpi_algorithm_version<code>.   **R. Incident &amp; outage runbook (step-by-step)**&lt;br&gt;1) Detect: pipeline alert or operator report. Record incident start timestamp (UTC).&lt;br&gt;2) Triage: reproduce with 1–5 sample rows; classify (portal outage / validation spike / mapping regression).&lt;br&gt;3) Mitigate: stop automated retries, isolate affected </code>part<code> files, notify stakeholders per escalation matrix. &lt;br&gt;4) Communicate: publish interim status to RO + internal channels with </code>incident_id<code>, </code>known_impact<code>, </code>next_update_in<code>. &lt;br&gt;5) Recover: if portal confirms fix, run controlled smoke sample (10 rows) in staging; on green, resume parts with unique </code>trace_id<code>. &lt;br&gt;6) Postmortem: publish timeline, root cause, corrective actions, and verify fixes in golden tests. Attach </code>incident_postmortem.json<code>.   **S. Traceability &amp; metadata schema (recommended minimal fields)**&lt;br&gt;- </code>trace_id<code> (UUID), </code>upload_id<code>, </code>file_name<code>, </code>part<code>, </code>rowid_start<code>, </code>rowid_end<code>, </code>created_by<code>, </code>created_on<code> (UTC ISO8601), </code>template_sha256<code>, </code>kamus_sha256<code>, </code>kamus_version<code>, </code>validation_score<code>, </code>acceptance_id<code> (if accepted), </code>resolution_ticket<code> (if any).&lt;br&gt;- Store this metadata as </code>upload_manifest.json<code> alongside artifacts and index it for search.   **T. Attachment chain-of-custody &amp; integrity controls (operational)**&lt;br&gt;- For each attachment record: </code>attachment_id<code> (UUID), </code>rowid<code>, </code>original_file_name<code>, </code>storage_path<code>, </code>uploader_id<code>, </code>uploaded_on<code> (UTC), </code>sha256<code>, </code>virus_scan_status<code>, </code>access_list<code> (role-based), </code>retention_end_date<code>.&lt;br&gt;- Implement automated integrity checks (verify sha256) on restore &amp; prior to any portal resubmission; flag mismatches and quarantine.   **U. Retention, anonymization &amp; archival templates (practical rules)**&lt;br&gt;- Short-lived staging exports: retain 30 days, auto-delete via lifecycle rule. &lt;br&gt;- Accepted upload artifacts &amp; acceptance confirmations: retain per legal minimum (example: 10 years) then move to cold encrypted storage for remainder of retention or deletion per legal hold. &lt;br&gt;- When exporting reports for analytics, pseudonymize NIK/KK with salted HMAC using company KMS key; retain re-identification mapping only in secure vault with restricted access and audit trail.   **V. Lightweight API integration guidance (if portal offers API)**&lt;br&gt;- Prefer token-based OAuth2 client credentials with scoped tokens limited to upload scope; avoid long-lived static credentials. &lt;br&gt;- Implement idempotency keys (</code>trace_id<code>) per file to allow safe retries. &lt;br&gt;- For each API upload response, store both HTTP payload and parsed portal acceptance IDs; map portal error codes to internal taxonomy automatically. &lt;br&gt;- When API supports attachments, prefer multipart upload with server-side integrity checks; if not supported, use secure ephemeral storage links and include link in manifest.   **W. Localization &amp; accessibility considerations**&lt;br&gt;- Ensure date parsing strictly enforces </code>DD/MM/YYYY<code> for all locales or convert early in ETL with explicit locale metadata. &lt;br&gt;- Validate character-encoding end-to-end as UTF-8 NFC; create automated checks for unsupported characters and provide operator-friendly reports that highlight offending rows and display hex codepoints. &lt;br&gt;- For operator UIs and mapping tools, support Indonesian (id-ID) and English (en-US) labels; ensure right-to-left or other locale concerns are documented if supporting other locales.   **X. Change-communications &amp; stakeholder notice templates (short)**&lt;br&gt;- **Template Update Notice (subject)**: </code>ACTION REQUIRED: Form_34 Template Update — BU_<Company> — <YYYY-MM-DD><code> Body must include: </code>template_sha256<code>, </code>kamus_version<code>, </code>expected_impact<code> (mandatory columns changed? kamus changes?), </code>test_plan<code>, </code>deadline for sign-off<code> and </code>contact<code>. Attach </code>golden_test_report<code>.&lt;br&gt;- Use templated subject lines and track acknowledgements via SSO-backed responses.   **Y. Lightweight governance checklist for CI/CD gating**&lt;br&gt;- Pre-merge checks: header_checker passes, unit tests for mapping rules pass, kamus snapshot included, golden-file smoke tests pass.&lt;br&gt;- Pre-release: run staging sample upload (10 rows) and verify acceptance report has zero critical errors; record acceptance snapshot in PR. &lt;br&gt;- Require two approvers for mapping changes that alter salary→kelas or faskes canonical mappings.   **Z. Quick operator decision matrix (one-line rules for triage)**&lt;br&gt;- If </code>ERR_LAYOUT<code> → do not edit file; restore canonical template and re-generate. &lt;br&gt;- If </code>Faskes_map_rate < 90%<code> → pause full upload, remediate mapping table, re-run. &lt;br&gt;- If </code>AcceptanceRate < 95%<code> on full upload → stop further uploads and execute outage triage runbook. &lt;br&gt;- If </code>attachment_hash_mismatch<code> → quarantine attachments, re-generate ZIP and re-verify sha256 before reupload.   **AA. Suggested incremental automations (prioritized)**&lt;br&gt;1) Auto-generate </code>Validation<code> sheet with remediation hints and pre-filled recommended </code>kode<code> values for top-5 frequent errors. &lt;br&gt;2) Auto-splitting &amp; </code>partNN<code> naming with non-overlapping row-range manifest generation. &lt;br&gt;3) Faskes fuzzy-mapping UI for HR to approve low-confidence matches with one-click approvals and audit logging. &lt;br&gt;4) Automated smoke staging uploads on template/kamus change and CI gating.   **AB. Additional appendices to add to master reference on request**&lt;br&gt;- Full </code>header_checker.py<code> (small executable snippet) with unit tests; exportable sample </code>Form_34_ready.xlsx<code> with embedded </code>Validation<code> sheet; step-by-step operator screenshots for portal upload flows; templated SLA emails for escalations; sample </code>incident_postmortem.json<code> schema. (Request letters: A,B,C,D map to artifacts; reply with letters to generate.)   **AC. Data provenance &amp; lineage (practical model)**&lt;br&gt;- Capture full lineage for each row from </code>source_system<code> → </code>extraction_id<code> → </code>etl_trace_id<code> → </code>form34_part<code> → </code>portal_upload_id<code>.&lt;br&gt;- Store provenance as structured JSON per row: </code>{"trace_id","source_system","source_query","extraction_time","etl_job_id","transformations":[{"name","version","params"}],"output_file","rowid"}<code>.&lt;br&gt;- Index provenance store for rapid audit lookups (query by </code>nik<code>, </code>trace_id<code>, or </code>upload_id<code>).&lt;br&gt;- Provide tooling to reconstruct the exact transformation applied to a row (replayable ETL run) and to regenerate the submitted artifact bit-for-bit using archived inputs and transformation parameters.   **AD. Role-based dashboards &amp; operator views (UX requirements)**&lt;br&gt;- Operator dashboard views: </code>PendingRemediation<code>, </code>RecentUploads<code>, </code>AcceptanceTrend<code>, </code>TopErrorsByCategory<code>.&lt;br&gt;- Provide per-operator queues showing assigned rows with </code>error_codes<code>, remediation hints, and one-click apply for common fixes (with mandatory reason &amp; audit log).&lt;br&gt;- Include an approvals dashboard for HR/Payroll that shows required sign-offs per upload with SSO-backed action buttons and timestamped records. Support CSV/Excel export of any view.   **AE. Scaling &amp; performance guidance (large populations)**&lt;br&gt;- Shard processing by deterministic key (e.g., </code>company_code<code> + </code>part_number<code>) to enable parallel ETL workers.&lt;br&gt;- Stream validation checks where possible (row-by-row) rather than materializing full datasets in memory; use chunked pandas processing or database-backed staging for &gt;1M rows.&lt;br&gt;- Precompute kamus lookups into high-performance in-memory maps (hash maps) or local SQLite caches for fast joins. Measure throughput targets (e.g., 100k rows/min) and instrument latencies per transformation stage.   **AF. Cost &amp; storage optimization (artifact lifecycle)**&lt;br&gt;- Implement tiered storage lifecycle: staging (30 days, hot), archive (10 years, cold), long-term retention if legal hold applies (separate bucket with restricted access).&lt;br&gt;- Deduplicate attachments before storage; store attachments in content-addressed storage (sha256 keyed) and reference by </code>attachment_id<code> to avoid duplicates across uploads.&lt;br&gt;- Compress archives with lossless algorithms; maintain manifest with checksums to permit integrity verification without decompressing.   **AG. Training materials &amp; operator playbooks (operational readiness)**&lt;br&gt;- Produce 1-page quick-reference cards: </code>Header Match<code>, </code>Top 5 Errors<code>, </code>How to Attach Docs<code>, </code>Rollback Steps<code>.&lt;br&gt;- Provide interactive runbooks with sample flows (video screencasts) for portal upload and error triage. Include short quizzes and a sandbox upload environment for operator certification. Maintain training records per operator.   **AH. Legal hold &amp; e-discovery process (controlled)**&lt;br&gt;- When a legal hold triggers, snapshot all artifacts under the hold&#x27;s scope and flag them in storage metadata to prevent deletion. Record </code>hold_id<code>, </code>issued_by<code>, </code>issued_on<code>, </code>cases_affected<code>, and </code>hold_expiry<code> (if any).&lt;br&gt;- Provide automated export functionality for legal requests that packages </code>hr_export_source.csv<code>, </code>Form_34_ready.xlsx<code>, </code>manifest_meta.json<code>, </code>attachments.zip<code>, and </code>acceptance_record.json<code> with a chain-of-custody report.   **AI. Command-line utilities &amp; scripting conventions (examples)**&lt;br&gt;- Provide small CLI tools: </code>header-checker<code>, </code>compute-sha256<code>, </code>split-parts<code>, </code>generate-manifest<code>, </code>faskes-map-review<code>.&lt;br&gt;- Conventions: exit codes (0=ok, 1=warning, 2=critical), machine-parsable JSON output for CI integration, and idempotent behaviors. Document usage and include automated shell completion for common environments.   **AJ. Data anonymization &amp; safe-sharing patterns (teams &amp; auditors)**&lt;br&gt;- Use deterministic salted HMAC for NIK/KK pseudonymization when sharing with analysts: </code>pseudonym = HMAC_SHA256(salt, NIK)<code> and store salt in KMS-protected vault.&lt;br&gt;- Provide reversible encryption only when strictly necessary and protect keys with KMS and strict RBAC; prefer irreversible pseudonymization for analytics exports. Log re-identification requests with approvals.   **AK. Security review checklist (pre-deployment)**&lt;br&gt;- Threat model assessment completed; identify PII flows.&lt;br&gt;- Secrets handling: no credentials in code; use vault/KMS.&lt;br&gt;- Penetration test scheduled for ETL endpoints and operator UI.&lt;br&gt;- Access reviews: ensure least privilege for operator roles; enforce 2FA for upload accounts.&lt;br&gt;- Automated dependency vulnerability scanning integrated into CI.   **AL. Vendor selection criteria for third-party integrators**&lt;br&gt;- Required: experience with national employer portals, data residency compliance, SOC2 or equivalent, secure attachment handling, API support, and documented incident response.&lt;br&gt;- Evaluate: ETL repeatability features, audit artifact retention guarantees, SLA for mapping updates, and support for golden-file regression testing. Include contractual SLAs on acceptance/triage timelines.   **AM. Audit readiness drill (quarterly checklist)**&lt;br&gt;- Select a recent upload and perform an end-to-end audit: re-run ETL from archived </code>hr_export_source.csv<code>, recompute sha256, verify </code>manifest_meta.json<code>, validate </code>portal_acceptance_confirmation.pdf<code>, and confirm attachments integrity.&lt;br&gt;- Produce an </code>audit_playbook<code> document with timelines, responsible persons, and required artifacts. Track drill outcomes and remediate gaps.   **AN. SLO &amp; SLA definitions (practical examples)**&lt;br&gt;- Operator SLA: remediation of critical validation errors within 24 hours.&lt;br&gt;- System SLO: ETL job success rate ≥ 99.5% over 30 days.&lt;br&gt;- Upload SLA: mean </code>TimeToAcceptanceHours<code> ≤ 48 hours for automated pipelines (excluding external portal outages). Record SLO measurement windows and alert thresholds.   **AO. Monthly maintenance runbook (tasks &amp; cadence)**&lt;br&gt;- Weekly: kamus sync check and diff report.&lt;br&gt;- Monthly: golden-file regression run, review </code>faskes_override_log<code>, and prune staging artifacts older than 30 days.&lt;br&gt;- Quarterly: access review, training refreshers, and penetration scan remediation follow-ups. Document maintenance actions in </code>maintenance_log.csv<code>.   **AP. Regulatory alignment notes (high-level)**&lt;br&gt;- Track local data protection rules that apply to employee PII and follow cross-border transfer requirements when attachments or processing move outside jurisdiction.&lt;br&gt;- Maintain a compliance register tying retention durations to regulatory obligations and update when laws change. Consult Legal for jurisdiction-specific obligations before automation of deletion or long-term retention.   **AQ. Extended monitoring signals (operational telemetry)**&lt;br&gt;- Emit structured events for: </code>etl_started<code>, </code>etl_completed<code>, </code>validation_failed_row_count<code>, </code>upload_initiated<code>, </code>portal_response_received<code>, </code>acceptance_processed<code>.&lt;br&gt;- Capture rich metadata with each event: </code>upload_id<code>, </code>operator_id<code>, </code>kamus_sha256<code>, </code>template_sha256<code>, </code>rows_processed<code>, </code>critical_errors_count<code>. Use these to power alerts and dashboards.   **AR. Continuous improvement backlog &amp; prioritization matrix**&lt;br&gt;- Maintain a backlog ranked by ROI: items include </code>auto-attachment-hash-check<code>, </code>faskes-mapping-ml-improvement<code>, </code>auto-splitting-by-size<code>, </code>staging-smoke-upload-automation<code>.&lt;br&gt;- Include cost estimates, expected error-reduction impact, and required approvers. Reprioritize quarterly based on KPI trends.   **If you want an artifact generated now** — respond with letters for deliverables (A = header row, B = synthetic </code>.xlsx`, C = Power Query script, D = Python validation script). The chosen artifacts will be produced directly in this session. </td></tr></tbody></table></div><div class="row-count">Rows: 40</div></div><div class="table-caption" id="Table6" data-table="YJKN_0016_06" style="margin-top:2mm;margin-left:3mm;"><strong>Table 6</strong></div>
<div class="table-wrapper" data-table-id="table-6"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by **Form_34 Technical &amp; Operational Standards Specification**"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>Form_34 Technical & Operational Standards Specification</strong></div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>1. Expanded field edge-cases & handling guidance</strong><br><strong>NIK whitespace & normalization</strong><br>Trim, remove NBSP (<code>\u00A0</code>), normalize Unicode to NFC; reject if length ≠ 16 after normalization. Record <code>original_value</code>, <code>normalized_value</code>, <code>normalization_rule</code>, <code>operator_id</code>, <code>timestamp</code> in <code>normalization_audit.csv</code>.<br><strong>Implementation notes (expert)</strong><br>Use <code>^\d{16}$</code> after normalization; sample pseudocode: <code>value = unicodedata.normalize(&#x27;NFC&#x27;, value).replace(&#x27;\u00A0&#x27;,&#x27;&#x27;).strip()</code>; log diffs when rule applied. Add unit tests that assert idempotence (running normalization twice yields same result).<br><strong>Multiple passports</strong><br>Apply precedence <code>passport_current &gt; passport_previous &gt; passport_legacy</code>; store <code>passport_source</code>, <code>passport_source_date</code>, <code>passport_verified</code> (boolean), and <code>passport_validation_score</code>. If <code>passport_source</code> is external, persist <code>source_assertion_id</code> and <code>source_sha256</code> for audit.<br><strong>Honorifics & suffixes</strong><br>Extract to <code>name_prefix</code>, <code>name_suffix</code> using curated list (<code>dr, prof, h, s</code> etc.), preserve <code>NamaLengkap</code> as canonical ID string. If truncation occurs, write entry to <code>name_transform_log</code> with <code>truncation_reason</code>, <code>original_length</code>, <code>truncated_value</code>, <code>approved_by</code> (HR).<br><strong>Diacritics & long names</strong><br>Prefer UTF-8 NFC; if template field length < full name, persist full name into <code>name_truncation_log</code> and submit truncated form with <code>operator_attestation_file</code>. Add automatic warning when non-BMP codepoints found; include hex codepoints in operator report.<br><strong>Newborn temporary IDs</strong><br>Use deterministic placeholder <code>TMP-&lt;source&gt;-YYYYMMDD-HHMMSS-&lt;counter&gt;</code>; persist mapping to final NIK in <code>newborn_resolution_log</code> and require reconciliation evidence (<code>recon_evidence_id</code>) before automated conversion. Add weekly reconciliation job that fails if >0 unresolved <code>TMP-</code> mappings older than 30 days. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>2. Expanded error code catalog (operational RFC)</strong><br><strong>ERR_INVALID_NPP</strong><br>Invalid/non-unique NPP → run <code>SELECT npp, count(*) FROM employee_registry GROUP BY npp HAVING count(*)&gt;1</code>; generate <code>ERR_INVALID_NPP</code> ticket with sample rows and candidate canonical NPPs. Enforce DB unique constraint and migration-safe deploy (defer to maintenance window).<br><strong>ERR_INVALID_KELAS</strong><br>Recalc from salary bands with deterministic mapping table <code>salary_to_kelas</code> (versioned). If salary outside known bands, set <code>kelas=null</code> and tag for <code>kelas_review_queue</code>.<br><strong>ERR_EMAIL_INVALID</strong><br>Pattern-check using <code>^[^\s@]+@[^\s@]+\.[^\s@]+$</code>; classify as WARNING unless <code>email_required=true</code> in manifest. Provide operator hint with likely-correction suggestions (typo domains: <code>gmaill</code>→<code>gmail</code>).<br><strong>ERR_PHONE_FORMAT</strong><br>Normalize to E.164; prefer <code>+62</code> when local; sample normalizer: remove non-digits, if startswith <code>0</code> → replace with <code>+62</code>. Store <code>phone_normalized</code> and <code>phone_normalizer_rule</code>.<br><strong>ERR_POSTAL_MISMATCH</strong><br>Cross-check <code>postal_code</code> against <code>kecamatan/desa</code> lookups with fuzzy threshold (levenshtein ≤2); if mismatch, add <code>postal_suspect</code> flag and recommended <code>postal_suggestion</code>.<br><strong>ERR_ATTACHMENT_HASH_MISMATCH</strong><br>Quarantine attachment bundle; re-generate zip using canonical order, recompute <code>sha256</code> (<code>sha256sum</code>), compare; attach remediation SLA and pre-filled ticket template with steps: <code>quarantine -&gt; regenerate -&gt; rescan -&gt; reupload</code>. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>3. Additional synthetic test rows (stress & edge-case coverage)</strong><br><strong>RowID=6</strong><br>Duplicate NIK within file — expected: reject duplicates, log <code>duplicate_group_id</code> and sample rows to <code>dedupe_review</code>.<br><strong>RowID=7</strong><br>Future DOB (<code>2030-01-01</code>) — expected: <code>ERR_DOB_FUTURE</code>; test must assert pipeline rejects and adds operator remediation hint.<br><strong>RowID=8</strong><br>Long address (1024 chars + emojis) — expected: pass through ETL but truncated for portal field; full value stored in <code>address_full_blob</code> with <code>address_truncation_log</code> entry.<br><strong>RowID=9</strong><br>Salary zero — expected: route to <code>salary_investigation_queue</code> and tag <code>possible_unpaid</code> with SLA 72h.<br><strong>RowID=10</strong><br>Expired passport — expected: <code>ERR_PASSPORT_EXPIRED</code>; provide remediation checklist for operator (request scan, attach renewed passport).<br><strong>RowID=11</strong><br>Ambiguous faskes name — expected: <code>FASKES_FUZZY_MATCH</code> with top-3 suggestions and confidence scores; store operator choice. <br><strong>RowID=12</strong><br>Non-numeric postal — expected: <code>ERR_POSTAL_FORMAT</code>; apply auto-suggestion if numeric substring exists. Store all tests in <code>golden_tests_registry.csv</code> with schema: <code>test_id,description,input_row,expected_errors,expected_action,last_run,pass_rate</code>. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>4. Expanded remediation playbooks (decision trees — concise steps)</strong><br><strong>If >10% rejection rate</strong><br>1. Halt uploads (<code>set system.write_enabled=false</code>). 2. Produce rejection distribution by error code (top-20). 3. Isolate top-3 causes and sample 20 rows each. 4. Create triage tickets auto-populated (SLA 24h). 5. Run golden-file regression against <code>kamus_snapshot_v{N}</code>; diff results. 6. If unresolved beyond SLA, escalate to war-room with executive summary and impact matrix.<br><strong>If systemic faskes mismatch</strong><br>Snapshot <code>kamus</code> (<code>kamus_sha256</code>), run algorithmic diff, produce suggested <code>kamus_extension</code> proposals (JSON patch), create interim mapping overrides with <code>approved_by</code>, TTL, and audit logs; tag affected rows <code>pending_kamus</code> and schedule re-run when override deployed. Include rollback plan for overrides. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>5. Expanded KPIs & alert thresholds</strong><br><strong>AcceptanceRate</strong><br>Definition: <code>accepted_rows / total_rows</code> (24h rolling). Alert: <code>&lt;95%</code>. Owner: <code>data-ops</code>.<br><strong>AvgValidationErrorsPer1kRows</strong><br>Definition & alert: <code>&gt;50</code> per 1k rows. Use Prometheus: <code>sum(validation_errors)/sum(rows_processed)*1000 &gt; 50</code>.<br><strong>RepeatRejectionRate</strong><br>Alert if <code>&gt;5%</code> repeats within 30 days. Add dashboard widget showing repeat offenders and remediation owner.<br><strong>AttachmentIntegrityFailures</strong><br>Immediate paging to on-call via PagerDuty. Each KPI must include <code>template_sha256</code>, <code>kamus_version</code> and <code>upload_id</code> in provenance for root-cause analysis. Maintain runbook links in alerts. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>6. Expanded artifact examples (machine-readable templates)</strong><br><strong>manifest_meta.json (schema excerpt)</strong><br><code>json {&quot;upload_id&quot;:&quot;uuid&quot;,&quot;company_code&quot;:&quot;ABC&quot;,&quot;operator_id&quot;:&quot;op-123&quot;,&quot;created_on&quot;:&quot;2026-02-22T10:00:00Z&quot;,&quot;template_sha256&quot;:&quot;...&quot;,&quot;kamus_version&quot;:&quot;v2026-02&quot;,&quot;rowcount&quot;:15000,&quot;parts_index&quot;:[{&quot;part&quot;:&quot;part01&quot;,&quot;row_start&quot;:1,&quot;row_end&quot;:5000}]}&lt;br&gt;**acceptance_record.json (schema excerpt)**&lt;br&gt;json {&quot;upload_id&quot;:&quot;uuid&quot;,&quot;accepted_rows&quot;:14700,&quot;rejected_rows&quot;:300,&quot;acceptance_id&quot;:&quot;acc-20260222-01&quot;,&quot;timestamp&quot;:&quot;2026-02-22T11:02:00Z&quot;,&quot;portal_version&quot;:&quot;v2.4.1&quot;}</code><br>Include <code>checksum</code> and <code>signature</code> fields (e.g., detached JWS) for legal integrity. Archive with index file (<code>archive_index.csv</code>) for fast lookup. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>7. Operational checklist for regional office escalations (detailed)</strong><br>Package: canonical <code>Form_34_ready.xlsx</code> (include <code>sha256</code>), <code>manifest_meta.json</code>, portal validation PDF, 10–20 sample CSV rows with error codes, <code>template_registry</code> snapshot, screenshots, contact info, preferred SLA, and escalation subject <code>URGENT: PortalValidation — &lt;company_code&gt; — &lt;upload_id&gt;</code>.<br>Attach <code>diff_summary</code> between local and portal <code>kamus</code> plus recommended temporary mappings. Include an executive one-pager with impact, top 5 errors, suggested fixes, and estimated remediation time. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>8. Partial acceptance & idempotency guidance</strong><br>Mark accepted <code>rowids</code> in <code>parts_index.csv</code>; when reuploading, produce non-overlapping parts and include <code>trace_id</code> and <code>part</code> metadata in filename and manifest. Persist <code>trace_id</code>→<code>portal_upload_id</code> mapping in <code>reconciliation_store</code>. Implement idempotency via server-side token: <code>Idempotency-Key: trace_id</code>. Provide conflict-resolution policy: if <code>rowid</code> already accepted, skip and log. Provide CLI: <code>generate_reupload --upload_id uuid --accepted=1-2000 &gt; part02.csv</code>. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>9. Governance artifacts to maintain</strong><br><code>operator_roster.csv</code>: <code>operator_id,fullname,email,roles,last_access,review_due</code>.<br><code>kamus_sync_log.csv</code>: <code>kamus_sha256,version,source_url,applied_on,diff_summary</code>.<br><code>golden_tests_registry.csv</code>: <code>test_id,scenario,expected_result,last_run,pass_rate,owner</code>.<br>Apply RBAC and require signed approvals (<code>approver_id,approved_on</code>) for camus changes. Retain artifacts according to retention policy. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>10. FAQs (additions)</strong><br><strong>Q: mass surname changes?</strong><br>Use <code>PERUBAHAN_IDENTITAS</code> workflow; require court/registry docs; create <code>name_change_manifest.csv</code> with <code>old_name,new_name,nik,doc_file,doc_sha256,approved_by,approval_date</code>; schedule batch revalidation with <code>trace_id</code> and audit trail.<br><strong>Q: portal outage — what to keep?</strong><br>Do NOT retry with same <code>trace_id</code>; preserve final <code>Form_34_ready.xlsx</code>, manifest, session logs, screenshots; create <code>outage_incident</code> with <code>incident_id</code> and attach evidence; coordinate manual retry with regional office after outage cleared. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>11. Appendix quick-reference remediation flow for ERR_NIK_NOT_FOUND</strong><br>1. Run registry batch check capturing response IDs and <code>dukcapil_response_code</code>.<br>2. If mismatch, request KTP scan and update HRIS; record <code>NoDokumenPendukung</code> and archive scan with <code>doc_sha256</code>.<br>3. Create corrective Form_34 row using <code>PERUBAHAN_IDENTITAS</code> if previously accepted; reference supporting docs in manifest. Maintain <code>dukcapil_recon_log</code> with <code>employee_id,dukcapil_response_code,action_taken,operator_id,timestamp</code>. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>12. Low-friction automations to implement</strong><br>Auto-generate <code>Validation</code> sheet with remediation hints and one-click "create ticket" buttons; auto-split large batches into <code>partNN</code> and generate manifests; Slack + PagerDuty integration for alerts; basic auto-approval for faskes mappings above confidence threshold (e.g., >0.95) with human-in-loop review queue for 24h. Provide phased rollout: P0 core validation → P1 auto-split → P2 faskes ML → P3 full automation; include ROI estimate and acceptance criteria per phase. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>13. Operator quick-reference cards (one-line rules)</strong><br>Verify template <code>sha256</code> before upload; never edit headers manually; treat <code>kamus</code> as authoritative; log manual overrides with <code>who/why/when</code>; archive acceptance artifacts immediately. Provide printable laminated cards and a searchable quick-code index for error codes. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>14. Appendices (samples & templates)</strong><br><code>name_change_manifest.csv</code> columns: <code>old_name,new_name,nik,doc_file,doc_sha256,approved_by,approval_date</code>.<br><code>faskes_override_log.csv</code> columns: <code>source_name,override_code,override_name,approved_by,approved_on,comment</code>.<br><code>reconciliation_run</code> JSON template with fields: <code>run_id,upload_id,sample_rows,results,actions</code>. Provide example filled rows and unit-test fixtures for onboarding. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>15. Closing operational guidance</strong><br>Maintain checklists, golden tests, and automated validations; prioritize automations that reduce manual error; preserve manifests and acceptance reports; update SOP annually or on portal/<code>kamus</code> change. Schedule quarterly drills, record outcomes, and maintain remediation owners. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>16. Legal & compliance checklist (concise actionable items)</strong><br>Confirm legal basis for processing (contract/consent/legal obligation); trigger DPIA when PII batch size or sensitivity threshold exceeded; require DPAs with subprocessors specifying purpose, retention, incident timelines, audit rights; document cross-border transfer legal mechanism (SCCs, adequacy, consent); run annual revocation & correction tests and store evidence in <code>compliance_archive</code> with access control. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>17. Data-quality monitoring & alerting (metrics & triggers)</strong><br>Implement continuous jobs for <code>NIK_completeness_pct</code>, <code>KK_completeness_pct</code>, <code>Faskes_map_rate</code>, <code>address_geo_match_rate</code>, <code>email_valid_pct</code>, <code>phone_normalization_rate</code>. Define Prometheus alerts, e.g.: <code>alert if (sum(nik_missing)/sum(rows))*100 &gt; 2</code>. Link alerts to remediation tickets and SLA owners; include weekly trend export to stakeholders. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>18. KPI definitions (machine-friendly formulas)</strong><br><strong>AcceptanceRate</strong> = <code>accepted_rows / total_rows</code>.<br><strong>AvgValidationErrorsPer1kRows</strong> = <code>(total_validation_errors / total_rows) * 1000</code>.<br><strong>RepeatRejectionRate</strong> = <code>previously_rejected_rows / total_rejected_rows</code>.<br><strong>MeanTimeToAcceptHours</strong> = <code>mean(acceptance_timestamp - upload_timestamp)</code> (UTC, 2 decimals). Persist <code>upload_id,kamus_version,template_sha256,kpi_algorithm_version</code> for reproducibility. Version KPI code in CI. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>19. Incident & outage runbook (step-by-step)</strong><br>1. Detect & record UTC start and <code>incident_id</code>.<br>2. Triage with 1–5 sample rows, capture stack traces and portal responses.<br>3. Mitigate: stop retries, isolate parts, set system to readonly if needed.<br>4. Communicate interim status with <code>incident_id</code> and ETA to stakeholders.<br>5. Recover: run smoke sample in staging, validate golden tests, resume uploads with unique <code>trace_id</code> and monitor closely.<br>6. Postmortem: publish timeline, RCA, fixes, verify golden tests, attach <code>incident_postmortem.json</code>, assign owners and deadlines, and close only after regression window passes. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>20. Traceability & metadata schema (minimal recommended fields)</strong><br><code>trace_id</code> (UUID), <code>upload_id</code>, <code>file_name</code>, <code>part</code>, <code>rowid_start</code>, <code>rowid_end</code>, <code>created_by</code>, <code>created_on</code> (UTC), <code>template_sha256</code>, <code>kamus_sha256</code>, <code>kamus_version</code>, <code>validation_score</code>, <code>acceptance_id</code>, <code>resolution_ticket</code>. Store as <code>upload_manifest.json</code> alongside artifacts and index for search; ensure manifest entries are append-only and signed (detached JWS) for legal audit. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>21. Attachment chain-of-custody & integrity controls</strong><br>Per attachment store <code>attachment_id</code> (UUID), <code>rowid</code>, <code>original_file_name</code>, <code>storage_path</code>, <code>uploader_id</code>, <code>uploaded_on</code> (UTC), <code>sha256</code>, <code>virus_scan_status</code>, <code>access_list</code>, <code>retention_end_date</code>. Automate integrity checks (<code>sha256sum</code>) pre-submission; if mismatch, <code>quarantine -&gt; notify -&gt; regenerate -&gt; rescan</code>. Keep chain-of-custody reports per <code>hold_id</code>. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>22. Retention, anonymization & archival templates (practical rules)</strong><br>Staging exports: retain 30 days; Accepted artifacts: retain per legal minimum (example 10 years) then move to cold encrypted storage. For analytics, pseudonymize NIK/KK via salted HMAC stored in KMS: <code>HMAC_SHA256(salt, NIK)</code>; store salt in KMS and re-id mapping only in a secure vault with strict RBAC and audit logs. Provide deletion-proofs (WORM logs) when records removed. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>23. Lightweight API integration guidance (if portal supports API)</strong><br>Use OAuth2 client credentials with scoped tokens; implement idempotency keys (<code>trace_id</code>) and exponential backoff with jitter for retries (<code>backoff = min(initial*2^n, max)</code> + jitter). Store raw API payloads and parsed acceptance IDs; prefer multipart server-side attachment integrity; if unavailable, use ephemeral secure links and record in manifest. Provide example <code>curl</code> for token: <code>curl -d &quot;grant_type=client_credentials&quot; -u client:secret https://auth/token</code>. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>24. Localization & accessibility considerations</strong><br>Enforce <code>DD/MM/YYYY</code> parsing or convert early with explicit <code>locale</code> metadata; enforce UTF-8 NFC end-to-end and highlight unsupported codepoints (hex) in operator reports; Localize operator UI labels for <code>id-ID</code> and <code>en-US</code>; provide ARIA labels and keyboard navigation; test with screen readers during release cycle. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>25. Change-communications & stakeholder notice templates</strong><br>Template Update Notice must include <code>template_sha256</code>, <code>kamus_version</code>, <code>expected_impact</code>, <code>test_plan</code>, <code>sign-off_deadline</code>, contact; attach <code>golden_test_report</code>; track acknowledgements via SSO timestamped responses; provide rollback plan and quick remediation checklist for operators in the header. Use standardized subject: <code>TEMPLATE_UPDATE: &lt;component&gt; v&lt;ver&gt; — action required</code>. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>26. Governance checklist for CI/CD gating</strong><br>Pre-merge: header checker, unit tests for mapping, <code>kamus_snapshot</code> included, golden-file smoke tests pass. Pre-release: staging sample upload (10 rows) with zero critical errors. Require two approvers for changes to <code>salary→kelas</code> or <code>faskes</code> canonical mappings. Automate canary release and automated rollback on KPI regressions. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>27. Quick operator decision matrix (triage rules)</strong><br><code>ERR_LAYOUT</code> → Restore canonical template and retry part; <code>Faskes_map_rate &lt; 90%</code> → Pause uploads and remediate mapping table; <code>AcceptanceRate &lt; 95%</code> → Stop uploads and execute outage runbook; <code>attachment_hash_mismatch</code> → Quarantine and re-generate ZIP before re-upload. Provide flowcharts and auto-generated ticket templates linked from UI. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>28. Suggested incremental automations (prioritized)</strong><br>1) Auto-generate <code>Validation</code> sheet with remediation hints and suggested fixes.<br>2) Auto-splitting & <code>partNN</code> naming with manifest generation.<br>3) Faskes fuzzy-mapping UI with one-click approvals and audit trail.<br>4) Automated staging smoke uploads and CI gating with canary metrics. Include acceptance criteria, estimated ROI, owner, and backlog priority per increment. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>29. Data provenance & lineage (practical model)</strong><br>Record lineage JSON per row: <code>{&quot;trace_id&quot;,&quot;source_system&quot;,&quot;source_query&quot;,&quot;extraction_time&quot;,&quot;etl_job_id&quot;,&quot;transformations&quot;:[{&quot;name&quot;,&quot;version&quot;,&quot;params&quot;}],&quot;output_file&quot;,&quot;rowid&quot;}</code>; Index by <code>nik</code>, <code>trace_id</code>, <code>upload_id</code>. Provide replay tooling (<code>replay --trace_id &lt;id&gt;</code>) to regenerate submitted artifact bit-for-bit; persist transform versions and container image tags for reproducibility. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>30. Role-based dashboards & operator views (UX requirements)</strong><br>Provide views: <code>PendingRemediation</code>, <code>RecentUploads</code>, <code>AcceptanceTrend</code>, <code>TopErrorsByCategory</code>. Per-operator queue with remediation hints and audit-logged one-click fixes; approvals dashboard with SSO-backed sign-off buttons and CSV export. Add filters (company, date, error), saved bookmarks, SLA indicators, and an action-log per row. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>31. Scaling & performance guidance (large populations)</strong><br>Shard by <code>company_code+part_number</code>; stream-row validation; use chunked pandas or DB staging for >1M rows and backpressure via queue length. Use in-memory <code>kamus</code> hash caches (LRU) or local SQLite for low-latency lookups. Instrument per-stage latencies; set throughput targets (e.g., 100k rows/min) and autoscaling policies. Include smoke-test with sample 1M row file in CI. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>32. Cost & storage optimization (artifact lifecycle)</strong><br>Tiered lifecycle: staging (30d hot), archive (10y cold), legal-hold bucket with retention exceptions. Deduplicate attachments via content-addressed storage (sha256 keying), compress archives losslessly, and maintain <code>manifest_index</code> for integrity checks without decompression. Track storage cost per upload and generate monthly cost reports. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>33. Training materials & operator playbooks (operational readiness)</strong><br>1-page quick cards: <code>Header Match</code>, <code>Top5 Errors</code>, <code>Attach Docs</code>, <code>Rollback Steps</code>. Provide interactive runbooks, video screencasts, sandbox environment, short quizzes, and maintain operator certification records with expiry and refresher cadence. Include onboarding checklist and test accounts. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>34. Legal hold & e-discovery process (controlled)</strong><br>On hold snapshot artifacts, flag metadata (<code>hold_id,issued_by,issued_on,cases_affected,hold_expiry</code>). Provide automated export package for legal requests (including chain-of-custody report) with secure transfer methods (SFTP/GPG) and audit logs. Maintain access logs and retention exceptions. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>35. Command-line utilities & scripting conventions (examples)</strong><br>Small CLIs: <code>header-checker</code>, <code>compute-sha256</code>, <code>split-parts</code>, <code>generate-manifest</code>, <code>faskes-map-review</code>. Exit codes: <code>0=ok</code>, <code>1=warning</code>, <code>2=critical</code>. Output JSON for CI; document usage and ensure idempotent behavior (re-running produces same outputs). Include <code>--dry-run</code> and <code>--sample</code> modes. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>36. Data anonymization & safe-sharing patterns</strong><br>Pseudonymize NIK/KK via <code>HMAC_SHA256(salt, NIK)</code> with salt stored in KMS. Sample command: <code>echo -n &quot;$NIK&quot; | openssl dgst -sha256 -hmac &quot;$SALT&quot;</code>. Use reversible encryption only when necessary; store re-identification mapping in a KMS-protected vault with strict RBAC and logged access. Provide sample scripts and review checklist for safe sharing. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>37. Security review checklist (pre-deployment)</strong><br><strong>Threat model completed</strong> — list assets and mitigations.<br><strong>Secrets management</strong> — KMS/vault, no hardcoded credentials, rotate keys quarterly.<br><strong>Dependency scanning</strong> — run SCA and remediate critical/high findings before release.<br><strong>Auth & access control</strong> — enforce least-privilege, RBAC, 2FA for operator/admin accounts.<br><strong>Pentest</strong> — schedule external tests and track remediation timeline.<br><strong>Logging & backup</strong> — centralize logs, enable integrity protection and periodic restore tests. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>38. Vendor selection criteria for third-party integrators</strong><br>Required: national portal experience, data residency compliance, SOC2 (or equivalent), secure attachments, API support, documented incident response. Evaluate ETL repeatability, mapping update SLA, golden-file regression support. Insist on contract SLAs for triage timelines and clear responsibility matrices. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>39. Audit readiness drill (quarterly checklist)</strong><br>Pick recent upload, re-run ETL from archived <code>hr_export_source.csv</code>, recompute <code>sha256</code>, verify <code>manifest_meta.json</code>, validate <code>portal_acceptance_confirmation.pdf</code>, confirm attachments integrity. Publish <code>audit_playbook</code> with findings, assign owners, and remediate gaps within defined SLAs. Retain audit artifacts for legal retention window. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>40. SLO & SLA definitions (practical examples)</strong><br>Operator SLA: critical validation remediation within 24h.<br>System SLO: ETL success rate ≥99.5% over 30 days.<br>Upload SLA: mean <code>TimeToAcceptanceHours</code> ≤48 for automated pipelines (excluding portal outages). Define measurement windows, alert thresholds, and reporting cadence. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>41. Monthly maintenance runbook (tasks & cadence)</strong><br>Weekly: <code>kamus</code> sync & diff report (email to owners).<br>Monthly: golden-file regression & <code>faskes_override_log</code> review.<br>Quarterly: access review, training refreshers, pentest remediation. Log actions in <code>maintenance_log.csv</code> and automate reminders with evidence collection links. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>42. Regulatory alignment notes (high-level)</strong><br>Maintain a compliance register mapping retention rules to local laws; record cross-border transfer legal mechanisms (SCCs, adequacy, consent); consult Legal before automating deletions or long-term retention changes; prepare dossiers for DPAs and audits. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>43. Extended monitoring signals (operational telemetry)</strong><br>Emit structured events: <code>etl_started</code>, <code>etl_completed</code>, <code>validation_failed_row_count</code>, <code>upload_initiated</code>, <code>portal_response_received</code>, <code>acceptance_processed</code>. Enrich events with <code>upload_id</code>, <code>operator_id</code>, <code>kamus_sha256</code>, <code>template_sha256</code>, <code>rows_processed</code>, <code>critical_errors_count</code>. Feed into observability stack and create long-term trend reports. </td></tr><tr><td data-label="Form_34 Technical &amp; Operational Standards Specification"> <strong>44. Continuous improvement backlog & prioritization matrix</strong><br>Maintain backlog ranked by ROI (e.g., <code>auto-attachment-hash-check</code>, <code>faskes-mapping-ml-improvement</code>, <code>auto-splitting</code>, <code>staging-smoke-upload-automation</code>). For each item: include <code>cost_estimate</code>, <code>expected_error_reduction</code>, <code>owner</code>, <code>dependencies</code>, and <code>acceptance_criteria</code>. Re-prioritize quarterly based on KPI trends and incident history. </td></tr></tbody></table></div><div class="row-count">Rows: 44</div></div><script src="assets/xlsx.full.min.js?v=1758605028" defer></script>
<script src="assets/script.js?v=1759748863" defer></script>
<script src="assets/worker.js?v=1758331710" defer></script>
<script>
(function(){
  const template = "{table}_{date}";
  const userVal = "";
  const hrefPrefix = "assets";
  function formatName(tableName) {
    const date = (new Date()).toISOString().slice(0,10);
    return template.replace('{table}', tableName).replace('{date}', date).replace('{user}', userVal);
  }
  const btn = document.getElementById('exportBtn');
  if(btn) {
    btn.addEventListener('click', async function() {
      try {
        const html = document.documentElement.outerHTML;
        if(html.length > 2000000) { alert('Export refused: html too large'); return; }
        if(window.Worker) {
          const workerUrl = (function(){ try{ return hrefPrefix + '/worker.js'; }catch(e){ return null; } })();
          if(workerUrl) {
            try {
              const worker = new Worker(workerUrl);
              worker.postMessage({html: html, format: 'pdf'});
              worker.onmessage = function(e) { console.log('worker:', e.data); alert('Worker replied: '+(e.data.msg||e.data.status)); };
            } catch(err) { console.warn('Worker create failed', err); alert('Worker not available'); }
          } else { alert('Worker not available'); }
        } else { alert('Export worker not supported in this environment.'); }
      } catch(err) { console.warn('Export failed', err); alert('Export worker not available. See console for details.'); }
    });
  }
})();
window.addEventListener('load', function(){ try{ document.querySelectorAll('.table-wrapper').forEach(function(e){ e.style.opacity='1'; }); }catch(e){} });
</script>
</div>
</body>
</html>