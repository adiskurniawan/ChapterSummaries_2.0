<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='utf-8'><meta name='viewport' content='width=device-width,initial-scale=1'>
<title>Tables Viewer v2.1</title>
<style>:root{--main-header-height:56px;} .table-wrapper{opacity:0;min-height:24px;transition:opacity .12s linear}</style>
<link rel="stylesheet" href="assets/style.css?v=1769960840">
<link rel="stylesheet" href="assets/overrides.css?v=1771316625">
</head><body>
<div id="tables-viewer" role="region" aria-label="Tables viewer">
<div id="stickyMainHeader">
<div id="tv-header">
<div><h1>Tables Viewer v2.1</h1></div>
<div style="display:flex;gap:8px;align-items:center;">
<input id="searchBox" class="tv-search" type="search" placeholder="Search" aria-label="Search tables" />
<button id="modeBtn" type="button" onclick="toggleMode()" aria-label="Toggle theme">Theme</button>
<button id="toggleAllBtn" type="button" aria-label="Toggle all" onclick="toggleAllTables()">Collapse All Tables</button>
<button id="copyAllPlainBtn" class="copy-btn" type="button" onclick="copyAllTablesPlain()" aria-label="Copy all tables as plain text">Copy All Tables (Plain Text)</button>
<button id="copyAllTablesBtn" class="copy-all-btn" type="button" onclick="copyAllTablesMarkdown()" aria-label="Copy All Tables (Markdown)">Copy All Tables (Markdown)</button>
<button id="copyAllMdBtn" style="display:none" aria-hidden="true"></button>
<button id="resetAllBtn" type="button" onclick="resetAllTables()" aria-label="Reset All Tables">Reset All Tables</button>
</div></div>
<script>
(function(){
  function ensureDelegation(){
    try {
      var vis = document.getElementById('copyAllTablesBtn');
      var alias = document.getElementById('copyAllMdBtn');
      if(!vis || !alias) return;
      alias.addEventListener = function(type, listener, options){
        vis.addEventListener(type, listener, options);
      };
      alias.removeEventListener = function(type, listener, options){
        vis.removeEventListener(type, listener, options);
      };
      Object.defineProperty(alias, 'onclick', {
        set: function(fn){ vis.onclick = fn; },
        get: function(){ return vis.onclick; },
        configurable: true
      });
      alias.focus = function(){ vis.focus(); };
      alias.blur = function(){ vis.blur(); };
    } catch(e) {
      try{ console && console.warn && console.warn('alias delegation failed', e); }catch(_){} 
    }
  }
  if(document.readyState === 'loading'){
    document.addEventListener('DOMContentLoaded', ensureDelegation);
  } else {
    ensureDelegation();
  }
})();
</script>
<noscript><div style='color:#b91c1c'>JavaScript is disabled. Tables will be shown statically. For large tables enable JS for virtualization.</div></noscript>
<div id="tocBar" role="navigation" aria-label="Table of contents"><ul><li class="toc-item"><a class="toc-link" href="#Table1">Table 1</a></li>
<li class="toc-item"><a class="toc-link" href="#Table2">Table 2</a></li>
<li class="toc-item"><a class="toc-link" href="#Table3">Table 3</a></li>
<li class="toc-item"><a class="toc-link" href="#Table4">Table 4</a></li>
<li class="toc-item"><a class="toc-link" href="#Table5">Table 5</a></li></ul></div></div>
<div class="table-caption" id="Table1" data-table="Docu_0197_01" style="margin-top:2mm;margin-left:3mm;"><strong>Table 1</strong></div>
<div class="table-wrapper" data-table-id="table-1"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by **modMappingStore — Per-function Expert Technical Breakdown**"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>modMappingStore — Per-function Expert Technical Breakdown</strong></div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Verification statement:</strong> Reviewed <strong>10×</strong> for internal consistency, determinism, PQ parity, canonicalization, audit coverage, PII handling, revertability, job-scheduler interop, and CI/golden-test gating before publication. Each function entry below describes: Purpose & contract, Inputs & outputs, Primary invariants, Provenance & usage, Failure modes & recovery, Observability & required audits, Performance expectations, Test vectors & examples, Conceptual Power Query (PQ) mapping, Conceptual DAX reporting measures, Security/PII considerations, and Operational notes. Numbered lists use <code>&lt;br&gt;</code> line breaks per requirement. No code snippets are included. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Function: BuildApplyDescriptor</strong><br><strong>Purpose & contract:</strong> Construct a canonical, content-addressed <code>ApplyDescriptor</code> which fully describes a planned apply operation so workers, auditors, and revert logic share a single authoritative artifact.<br><strong>Inputs & outputs:</strong><br>Inputs: <code>planId</code> (optional canonical plan pointer), <code>mappingRows</code> (list of candidate mapping records with mappingVersion), <code>operatorId</code>, <code>mode</code> (<code>create_copy</code> or <code>inline</code>), <code>effectiveDate</code>, <code>approvals[]</code>, <code>paramsHash</code> (scoring/config hash), <code>standardMapHash</code>.<br>Outputs: <code>ApplyDescriptor</code> object containing fields: <code>applyId</code> (content-addressed), <code>planId</code>, <code>mappingVersion</code>, <code>paramsHash</code>, <code>standardMapHash</code>, <code>rows[]</code> (summary per account, not full PII), <code>estimate</code> (rows, affectedAmount, cost), <code>beforeSnapshotRef</code> placeholder, <code>jobs[]</code> (chunk hints), <code>approvals[]</code>, <code>createdBy</code>, <code>createdTs</code>, <code>correlationId</code>, <code>descriptorHash</code>.<br><strong>Primary invariants:</strong><br>1. Deterministic canonicalization: descriptor fields must be serialized in a stable key order with fixed float precision so <code>descriptorHash</code>/<code>applyId</code> is reproducible across runtime environments. <br>2. No raw PII stored at top-level: full row details must be evidenceRef-linked (encrypted evidence) rather than embedded. <br>3. Approvals required for destructive or material changes must be present and validated; otherwise BuildApplyDescriptor must fail or return a <code>requiresApproval</code> flag depending on operator policy.<br><strong>Provenance & usage:</strong><br>• Originates from <code>BuildStandardizationPlan</code> or <code>ScoreBatch</code> outputs; used by job scheduler and apply executor; stored in <code>ApplyHistory</code> for audit & revert.<br><strong>Failure modes & recovery:</strong><br>• Missing approvals → return structured error <code>STD_PERMISSION_DENIED</code> and list required approvers.<br>• Stale <code>mappingVersion</code> in rows → return <code>STD_MAPPING_STALE</code> with row indices; operator must refresh and re-run descriptor build.<br>• Disk persist failures → write descriptor to local encrypted staging and emit an audit <code>apply.descriptor.persist.pending</code> with retry instructions.<br><strong>Observability & audit obligations:</strong><br>• Emit <code>standard.apply.descriptor.created{applyId,planId,paramsHash,standardMapHash,rowsCount,estimatedCost,operatorId}</code>.<br>• Store descriptor (canonical JSON) in evidence store with <code>descriptorRef</code> and <code>descriptorHash</code> in the audit row.<br><strong>Performance expectations:</strong><br>• For small plans (<5k rows) descriptor build <200ms typical; for large plans it scales linearly and must stream. Provide chunking hints for >50k rows.<br><strong>Test vectors & examples:</strong><br>1. Minimal mapping (2 rows) → verify deterministic <code>applyId</code> generated given same <code>paramsHash</code> and <code>standardMapHash</code> across environments. <br>2. Missing approval test: descriptor build expected to return <code>requiresApproval</code> and <code>missingApprovals</code> list.<br><strong>Conceptual PQ mapping:</strong><br>• PQ produces canonical <code>planId</code> and <code>mappingRows</code> snapshot; use PQ exported snapshot as the input to ensure parity. PQ must output <code>paramsHash</code> identical to what descriptor expects.<br><strong>Conceptual DAX measures:</strong><br>• <code>AppliesPlanned = COUNTROWS(ApplyDescriptorTable)</code> and <code>PlannedAffectedSum = SUMX(ApplyDescriptorTable, [estimate].affectedAmount)</code>.<br><strong>Security & PII considerations:</strong><br>• Top-level descriptor must not contain PII; include <code>evidenceRef</code> pointers. Descriptor persisted and optionally signed; signatures not stored in workbook but appended by signing pipeline with secure key. <br><strong>Operational notes:</strong><br>• Any change to descriptor schema or canonicalization requires migration manifest and CI golden parity tests since <code>applyId</code> depends on canonical serialization. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Function: ValidateApplyApprovals</strong><br><strong>Purpose & contract:</strong> Evaluate whether provided approvals satisfy the organization's governance policy for the planned apply (considering materiality, destructive flags, dataset sensitivity, and rule-specific approval requirements). Deterministic and auditable. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>ApplyDescriptor</code>, <code>approvals[]</code> (approval objects with approverId, approvalRef, timestamp, signatureRef), <code>operatorId</code>.<br>Outputs: <code>approvalResult</code> {allowed:boolean, missingApprovals[], invalidApprovals[], approvalLevel, requiredActions[]}.<br><strong>Primary invariants:</strong><br>1. Approval policy must be loaded from <code>Config</code> with <code>configHash</code> and validated before checks. <br>2. Two-person approvals required for material or destructive changes and must be distinct approver identities. <br>3. Approvals must reference the exact <code>applyId</code> or mappingVersion; cross-reference mismatches invalidate approvals.<br><strong>Provenance & usage:</strong><br>• Called by orchestration (BuildApplyDescriptor and ApplyMapping) prior to persistence and execution. <br><strong>Failure modes & recovery:</strong><br>• Missing approvals -> return missing list. <br>• Approval token expired or signature invalid -> return <code>invalidApprovals</code> and require re-approval. <br>• RBAC lookup unavailable -> fail-safe deny with <code>approval.check.unavailable</code> audit and instruct operator to retry. <br><strong>Observability & audit obligations:</strong><br>• Emit <code>standard.apply.approval.checked{applyId,allowed,missingCount}</code> and persist approval references in descriptor. <br><strong>Performance expectations:</strong><br>• Lightweight; role lookups must be cached to avoid blocking UI. <br><strong>Test vectors & examples:</strong><br>1. Two-person approval success scenario for destructive mapping. <br>2. Single approval present for requirement of two -> expect missing approval. <br><strong>Conceptual PQ mapping:</strong><br>• PQ not involved; however ImpactSimulation outputs inform the materiality decision used by approval policy. <br><strong>Conceptual DAX measures:</strong><br>• <code>ApprovalsPendingCount = COUNTROWS(FILTER(ApplyDescriptorTable, [approvalResult].allowed=false))</code>. <br><strong>Security & PII considerations:</strong><br>• Approval artifacts stored as evidenceRefs; signatures validated with operator identity mapping; never store private keys in workbook. <br><strong>Operational notes:</strong><br>• Approval policy changes must be versioned and recorded; approval policy <code>configHash</code> must be included in any audit referencing apply approvals. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Function: CreateBeforeSnapshot</strong><br><strong>Purpose & contract:</strong> Create a recoverable, immutable <code>beforeSnapshot</code> capturing the exact pre-apply state necessary to revert an apply deterministically. For regulated outputs, snapshot must be stored in WORM or similar immutable storage. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>ApplyDescriptor</code>, snapshotOptions {<code>fullSnapshot|deltaSnapshot|metadataOnly</code>, redactionPolicy}.<br>Outputs: <code>beforeSnapshotRef</code> (evidenceRef), <code>beforeChecksum</code> (sha256), <code>snapshotMeta</code> (rowsCount, sizeBytes, encrypted:boolean, createdTs).<br><strong>Primary invariants:</strong><br>1. Snapshot creation must complete successfully before any destructive mutation; failure must prevent apply execution. <br>2. Snapshot must be identifiable by <code>snapshotHash</code> and referenced in <code>ApplyDescriptor</code> and audits. <br><strong>Provenance & usage:</strong><br>• Used by RevertMapping to restore prior state; also used for forensic packaging. <br><strong>Failure modes & recovery:</strong><br>• Disk or permission failures → abort apply and persist local staging; emit <code>STD_SNAPSHOT_FAILED</code> audit. <br>• Partial snapshots -> mark <code>partial=true</code> and forbid revert until manual remediation. <br><strong>Observability & audit obligations:</strong><br>• Emit <code>standard.apply.snapshot.created{applyId,beforeChecksum,sizeBytes}</code>. Evidence store must record retention policy and chain-of-custody. <br><strong>Performance expectations:</strong><br>• Snapshotting is I/O bound; for large tables use streaming and chunked writes; provide progress metrics to operator. <br><strong>Test vectors & examples:</strong><br>1. Small workbook snapshot drive to evidence store and checksum verification. <br>2. Simulate snapshot interruption and validate fail-safe abort. <br><strong>Conceptual PQ mapping:</strong><br>• PQ can pre-generate sanitized preview artifacts but <code>CreateBeforeSnapshot</code> must capture runtime, exact pre-apply workbook state for revert. <br><strong>Conceptual DAX measures:</strong><br>• <code>SnapshotCreationDuration</code>, <code>SnapshotSizeBytes</code> aggregated for capacity planning. <br><strong>Security & PII considerations:</strong><br>• Snapshot encryption required for PII-containing snapshots; access via approvals only. Do not store encryption keys in workbook. <br><strong>Operational notes:</strong><br>• Snapshot retention policies enforced per <code>Config</code>; snapshot TTL and archival must be included in <code>beforeSnapshot</code> metadata. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Function: ComputeBeforeAfterChecksums</strong><br><strong>Purpose & contract:</strong> Compute canonical SHA256 checksums for <code>before</code> and <code>after</code> artifacts using strict canonicalization rules (stable column ordering, fixed float precision, UTF-8 newline normalization) so checksums are reproducible across environments. <br><strong>Inputs & outputs:</strong><br>Inputs: artifactRef or in-memory dataset; <code>canonicalizeOptions</code> (columnOrder <code>array|null</code>, floatPrecision).<br>Outputs: <code>checksum</code> hex, <code>canonicalPayloadHash</code>, and optional <code>canonicalizedBlob</code> for debug. <br><strong>Primary invariants:</strong><br>1. Canonicalization rules must be recorded and included in <code>paramsHash</code> used for apply. <br>2. Checksums used for chain-of-trust; any change in canonicalization must produce a different <code>paramsHash</code> and be recorded as migration. <br><strong>Provenance & usage:</strong><br>• Checksum used in <code>ApplyHistory</code> and to verify revert parity in <code>RevertMapping</code>. <br><strong>Failure modes & recovery:</strong><br>• Non-serializable data types -> fail canonicalization and fallback to raw serialization with explicit audit <code>checksum.canonicalization_fallback</code>. <br><strong>Observability & audit obligations:</strong><br>• Log <code>checksum.compute.duration_ms</code> and <code>canonicalizationMode</code>. <br><strong>Performance expectations:</strong><br>• Streaming hash compute for large files; CPU-bound but efficient. <br><strong>Test vectors & examples:</strong><br>1. Round-trip test: compute checksum for <code>beforeSnapshot</code>, then recompute after reloading and verify equality. <br><strong>Conceptual PQ mapping:</strong><br>• PQ must be able to output canonicalized exports to allow cross-runtime checksum parity during CI. <br><strong>Conceptual DAX measures:</strong><br>• <code>ChecksumMismatchCount</code> alert when recomputed != stored. <br><strong>Security & PII considerations:</strong><br>• Only checksums stored publicly; do not reveal canonicalized content in audit rows. Evidence store holds encrypted full artifacts. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Function: PersistApplyDescriptor</strong><br><strong>Purpose & contract:</strong> Persist the <code>ApplyDescriptor</code> atomically into the authoritative <code>ApplyHistory</code> store and optionally export to evidence archive. Must be idempotent (no duplicate descriptor records for same <code>applyId</code>). <br><strong>Inputs & outputs:</strong><br>Inputs: <code>ApplyDescriptor</code>, persistOptions {exportEvidence:boolean, sign:boolean}.<br>Outputs: <code>persistedRef</code> (storage URI or local path), <code>persistChecksum</code>, success boolean. <br><strong>Primary invariants:</strong><br>1. Append-only semantics: cannot overwrite existing descriptors; updates must produce a new audit row (<code>apply.updated</code>). <br>2. If sign=true, the signature must be created by an external signing service (HSM) and <code>signatureRef</code> stored in metadata. <br><strong>Provenance & usage:</strong><br>• Workers read persisted descriptors; auditors and compliance use persisted descriptors for reconstructability. <br><strong>Failure modes & recovery:</strong><br>• Storage unavailability -> write to local encrypted staging and emit <code>apply.descriptor.persist.failed</code> audit with retry ticket. <br><strong>Observability & audit obligations:</strong><br>• <code>apply.descriptor.persisted{applyId,persistedRef,persistChecksum,operatorId}</code>. <br><strong>Performance expectations:</strong><br>• Small object writes; network latency dominates. <br><strong>Test vectors & examples:</strong><br>1. Persist same descriptor twice -> second attempt returns existing <code>persistedRef</code> and does not duplicate entry. <br><strong>Conceptual PQ mapping:</strong><br>• PQ outputs not persisted here but referenced; ensure PQ and persisted descriptor share the same <code>paramsHash</code>. <br><strong>Conceptual DAX measures:</strong><br>• <code>DescriptorPersistFailures</code> as an alerting measure. <br><strong>Security & PII considerations:</strong><br>• ApplyDescriptor persisted must redact PII; full details stored in evidenceRef encrypted. Signatures and keys never stored in workbook. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Function: ScheduleApplyWorker</strong><br><strong>Purpose & contract:</strong> Decide whether to run apply inline or schedule as worker jobs and create <code>JobDescriptor</code> entries (persisted) for workers. Must honor concurrency limits, tenant quotas, and chunking policies. Output is idempotent and safe for retries. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>ApplyDescriptor</code>, schedulingPolicy (chunkSize, priority), <code>preferredPools</code>.<br>Outputs: <code>jobIds[]</code>, <code>jobDescriptors[]</code> persisted, <code>scheduleSummary</code> (chunks, estimatedTime).<br><strong>Primary invariants:</strong><br>1. Guarantee unique jobIds for each chunk with idempotent writes (repeat scheduling returns same jobIds). <br>2. Enforce per-tenant resource quotas from <code>Config</code>. <br><strong>Provenance & usage:</strong><br>• Workers consume job descriptors to execute apply chunks; scheduler records metrics to monitoring. <br><strong>Failure modes & recovery:</strong><br>• Scheduler store unavailable -> persist jobDescriptors locally and retry; if partial scheduling occurs, detect duplicates by <code>applyId</code> and de-duplicate. <br><strong>Observability & audit obligations:</strong><br>• <code>job.scheduled{applyId,jobCount,priority,operatorId}</code> and queue depth metrics. <br><strong>Performance expectations:</strong><br>• Chunked job creation for millions of mappings should complete within minutes with streaming generation. <br><strong>Test vectors & examples:</strong><br>1. Chunk policy test: 1M rows with chunkSize=50k should produce 20 job descriptors and compute estimated runtime. <br><strong>Conceptual PQ mapping:</strong><br>• PQ can produce per-chunk preview artifacts referenced by <code>jobDescriptor</code> to avoid duplicative scans. <br><strong>Conceptual DAX measures:</strong><br>• <code>JobsQueuedPerTenant</code>, <code>AvgJobChunkSize</code>. <br><strong>Security & PII considerations:</strong><br>• Job descriptors must reference <code>evidenceRef</code> for PII; workers fetch evidence via ephemeral tokens and never persist credentials. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Function: ApplyMapping (orchestrator/runner)</strong><br><strong>Purpose & contract:</strong> Execute an <code>ApplyDescriptor</code> either inline or via scheduled workers: validate approvals, ensure snapshot present, orchestrate chunk execution, track progress, compute <code>afterChecksum</code>, and finalize apply. Must be resilient, cancelable, and idempotent. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>applyId</code>, operatorId, executionOptions (inlineTimeout, concurrencyLimit).<br>Outputs: <code>ApplyResult</code> {status:<code>completed|failed|canceled</code>, beforeChecksum, afterChecksum, artifactRefs[], runtimeMs, errors[]}.<br><strong>Primary invariants:</strong><br>1. Do not mutate original data without <code>beforeSnapshot</code> successfully created and persisted. <br>2. Inline destructive applies on regulated outputs require two-person approval; otherwise, block and return <code>STD_PERMISSION_DENIED</code>. <br>3. All chunked writes must be checkpointed with offsets for resumability. <br><strong>Provenance & usage:</strong><br>• Core runtime function executed by operator action or worker; coordinates with <code>modJobScheduler</code> and <code>modAudit</code>. <br><strong>Failure modes & recovery:</strong><br>• Worker crash mid-chunk -> persist partial outputs, update <code>ApplyDescriptor</code> with last successful chunk, emit <code>standard.apply.failed</code> and <code>HandlePartialFailure</code> invoked. <br>• Cancellation requested -> attempt cooperative cancel and persist partial state; return status <code>canceled</code>. <br><strong>Observability & audit obligations:</strong><br>• <code>standard.apply.start</code>, <code>standard.apply.chunk.completed</code>, <code>standard.apply.completed</code>, <code>standard.apply.failed</code> events with <code>applyId</code>, <code>payloadHash</code>, and <code>paramsHash</code>. <br><strong>Performance expectations:</strong><br>• For inline small applies (<5k rows) expect seconds; for large applies scheduling with workers is mandatory and expected time depends on worker pool size. <br><strong>Test vectors & examples:</strong><br>1. Successful inline apply of 100 rows -> snapshot, apply, compute afterChecksum, finalize. <br>2. Simulated worker failure at chunk 3/10 -> partial failure path; test revertability. <br><strong>Conceptual PQ mapping:</strong><br>• Use PQ <code>ImpactSimulation</code> outputs during preflight to validate expected deltas; PQ preview artifacts compared against actual outputs for smoke tests. <br><strong>Conceptual DAX measures:</strong><br>• <code>ApplySuccessRate</code>, <code>ApplyDurationP95</code>, <code>ApplyRetries</code>. <br><strong>Security & PII considerations:</strong><br>• Apply must not log PII into audit rows; evidenceRefs used instead. Worker nodes must fetch evidence using ephemeral tokens and run in sandboxed environment. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Function: RenderApplyPreview (safe, non-destructive preview)</strong><br><strong>Purpose & contract:</strong> Produce a deterministic preview for an <code>ApplyDescriptor</code> by applying mappings to a sample (or full) dataset without mutating source. Samples are selected deterministically via seed derived from <code>applyId</code> and <code>paramsHash</code>. UI-level previews must be redacted while full sanitized evidence stored encrypted. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>ApplyDescriptor</code>, <code>samplePolicy</code> (seeded, sampleSize), <code>redactionPolicy</code>.<br>Outputs: <code>previewRef</code> (evidenceRef), <code>previewHash</code>, <code>previewSummary</code> (aggregates), <code>issues[]</code> (ambiguous parses, heavy transforms).<br><strong>Primary invariants:</strong><br>1. Preview must be reproducible: same <code>applyId</code> + <code>paramsHash</code> + <code>samplePolicy</code> → same previewHash. <br>2. UI-level redactions enforced; full preview stored encrypted. <br><strong>Provenance & usage:</strong><br>• Used for governance sign-off; included in migration manifest and audit chain. <br><strong>Failure modes & recovery:</strong><br>• Heavy transforms or custom scripts in rules → run preview in sandboxed worker with resource limits; if worker unavailable schedule and notify operator. <br><strong>Observability & audit obligations:</strong><br>• <code>standard.preview{applyId,previewRef,issuesSummary,standardMapHash}</code>. <br><strong>Performance expectations:</strong><br>• Small sample preview <2s SLO; large previews scheduled. <br><strong>Test vectors & examples:</strong><br>1. Repeated preview run reproducibility test. <br>2. Preview that triggers ambiguous date parsing flagged <code>STD_AMBIG_DATE</code>. <br><strong>Conceptual PQ mapping:</strong><br>• PQ can produce preview artifacts (before/after) and <code>previewRef</code> should point to PQ-exported sanitized artifacts. <br><strong>Conceptual DAX measures:</strong><br>• <code>PreviewIssueRate</code> and <code>PreviewMaterialityAlerts</code>. <br><strong>Security & PII considerations:</strong><br>• UI preview must not show PII; full evidence stored encrypted with access logging. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Function: HandlePartialFailure</strong><br><strong>Purpose & contract:</strong> Centralized handler for partial apply failures. Capture diagnostics, persist partial outputs, compute recovery plan hints (chunk offset, partial artifacts), append failure audits, and optionally trigger <code>ForensicPack</code> when material. Must be idempotent and must not attempt unsafe heuristic reversion without operator consent. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>applyId</code>, <code>errorDetails</code>, <code>lastSuccessfulChunk</code>, <code>partialArtifacts[]</code>.<br>Outputs: <code>recoveryPlan</code> (recommended actions), <code>incidentRef</code> (forensic evidenceRef), updated <code>ApplyDescriptor.status=failed</code>.<br><strong>Primary invariants:</strong><br>1. Preserve partial artifacts for forensics; do not auto-delete. <br>2. Only suggest heuristic reversion; do not attempt automated heuristics unless explicit operator approval recorded. <br><strong>Provenance & usage:</strong><br>• Used by <code>ApplyMapping</code> when worker or execution fails; triggers operator notification and possible revert. <br><strong>Failure modes & recovery:</strong><br>• Handler failure to persist diagnostics -> write to local sealed backup and escalate. <br><strong>Observability & audit obligations:</strong><br>• <code>standard.apply.failed{applyId,correlationId,lastChunk,errors}</code> and forensic evidenceRef. <br><strong>Performance expectations:</strong><br>• Quick capture of context; forensic packaging may take longer and be delegated to job. <br><strong>Test vectors & examples:</strong><br>1. Worker crash mid-chunk scenario and subsequent recoveryPlan produced for operator. <br><strong>Conceptual PQ mapping:</strong><br>• PQ preview artifacts used to compare expected vs partial outputs to localize failure scope. <br><strong>Conceptual DAX measures:</strong><br>• <code>PartialFailureRate</code>, <code>AvgRecoveryTime</code>. <br><strong>Security & PII considerations:</strong><br>• Forensic evidence sensitive; ensure encryption and access audit during handling. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Function: MarkApplyCompleted</strong><br><strong>Purpose & contract:</strong> Finalize a successful apply by computing <code>afterChecksum</code>, performing atomic swap of active mapping table, archiving artifacts, and emitting <code>standard.apply.completed</code> audit. Must ensure atomic write semantics when making mapping live and record revert metadata. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>applyId</code>, <code>afterSnapshotRef</code>, <code>afterChecksum</code>, <code>artifactRefs[]</code>, runtimeMetrics. <br>Outputs: <code>completionResult</code> including final <code>mappingVersion</code>, <code>artifactChecksums</code>, <code>applyStatus=completed</code>. <br><strong>Primary invariants:</strong><br>1. Atomic swap of mapping table: create staging, validate expected prevHash, then swap to make mapping live; do not leave partial mapping visible. <br>2. Persist final audit rows and archive artifacts. <br><strong>Provenance & usage:</strong><br>• End-of-apply lifecycle; Update <code>MappingTable</code> active view and <code>ApplyHistory</code>. <br><strong>Failure modes & recovery:</strong><br>• Checksum mismatch -> mark failure and trigger <code>HandlePartialFailure</code>. <br><strong>Observability & audit obligations:</strong><br>• <code>standard.apply.completed{applyId,beforeChecksum,afterChecksum,rowsAffected,operatorId}</code>. <br><strong>Performance expectations:</strong><br>• Atomic swap operation must be strong and fast; full archiving may be async. <br><strong>Test vectors & examples:</strong><br>1. Successful apply leading to mappingVersion increment and verifying revert parity. <br><strong>Conceptual PQ mapping:</strong><br>• Use PQ <code>ImpactSimulation</code> expected outputs to compare final <code>afterSnapshot</code> aggregates for smoke testing. <br><strong>Conceptual DAX measures:</strong><br>• <code>AppliesCompleted</code>, <code>AfterChecksumMatches</code>. <br><strong>Security & PII considerations:</strong><br>• Final audits redact PII; full artifact stored encrypted. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Function: RevertMapping</strong><br><strong>Purpose & contract:</strong> Revert a completed apply using stored <code>beforeSnapshot</code> or inverse mapping metadata in <code>ApplyDescriptor</code>. Must be idempotent and require appropriate approvals for regulated outputs. If <code>beforeSnapshot</code> missing, fail with <code>STD_REVERT_NO_SNAPSHOT</code> and do not attempt heuristic reversion without operator consent documented in audit. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>applyId</code>, <code>operatorId</code>, <code>revertMode</code> (<code>create_copy</code> default), <code>approvals</code> if required. <br>Outputs: <code>revertResult</code> {revertId, status, beforeChecksum, afterChecksum, artifactRefs}. <br><strong>Primary invariants:</strong><br>1. Revert uses stored <code>beforeSnapshot</code> and must validate snapshot integrity via <code>beforeChecksum</code>. <br>2. Reverts are idempotent: re-running with same <code>revertId</code> is a no-op after success. <br><strong>Provenance & usage:</strong><br>• Operational rollback and forensic replay. <br><strong>Failure modes & recovery:</strong><br>• Missing or corrupted snapshot -> return <code>STD_REVERT_NO_SNAPSHOT</code> and instruct forensic incident; partial revert failure -> persist partial outputs and forensic artifacts and escalate. <br><strong>Observability & audit obligations:</strong><br>• <code>standard.revert{applyId,revertId,operatorId,status}</code> and evidenceRef to before/after snapshots. <br><strong>Performance expectations:</strong><br>• For large datasets, schedule as job; small reverts inline acceptable. <br><strong>Test vectors & examples:</strong><br>1. Apply then revert round-trip producing identical checksums. <br>2. Missing snapshot scenario returns <code>STD_REVERT_NO_SNAPSHOT</code>. <br><strong>Conceptual PQ mapping:</strong><br>• PQ replay tests included in CI ensure revert parity against golden fixtures. <br><strong>Conceptual DAX measures:</strong><br>• <code>RevertCount</code>, <code>RevertSuccessRate</code>. <br><strong>Security & PII considerations:</strong><br>• Revert artifacts may contain sensitive data; require approvals. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Function: AtomicSwapMappingTable</strong><br><strong>Purpose & contract:</strong> Execute atomic swap of the live mapping table to new mapping results created by a successful apply. Implement read-validate-swap semantics to prevent lost updates and ensure running jobs referencing old snapshot continue using their snapshot. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>newMappingTempRef</code>, <code>expectedPrevHash</code>, <code>applyId</code>.<br>Outputs: <code>swapResult</code> {success:boolean, beforeHash, afterHash, durationMs}.<br><strong>Primary invariants:</strong><br>1. Validate <code>expectedPrevHash</code> against current live mapping hash; if mismatch, abort with <code>STD_CONCURRENT_MODIFICATION</code>. <br>2. Implement swap using atomic rename or DB transaction semantics where available; avoid row-by-row updates that are visible mid-swap. <br><strong>Provenance & usage:</strong><br>• Called by <code>MarkApplyCompleted</code> to activate new mappings. <br><strong>Failure modes & recovery:</strong><br>• Swap failed due to IO -> attempt rollback to prior staging copy; if rollback impossible, leave system in read-only mode and alert SRE. <br><strong>Observability & audit obligations:</strong><br>• <code>mapping.swap{applyId,prevHash,newHash,durationMs}</code> audit. <br><strong>Performance expectations:</strong><br>• Swap should be near-instant for metadata; large data store swaps require metadata pointer change rather than copy. <br><strong>Test vectors & examples:</strong><br>1. Simulated concurrent apply by another operator triggers <code>STD_CONCURRENT_MODIFICATION</code>. <br><strong>Conceptual PQ mapping:</strong><br>• PQ-generated mapping snapshots loaded to staging area prior to swap. <br><strong>Conceptual DAX measures:</strong><br>• <code>SwapLatencyMs</code>, <code>SwapFailures</code>. <br><strong>Security & PII considerations:</strong><br>• Temporary staging copies must be encrypted and securely wiped after swap; do not leave plaintext temp files. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Function: SerializeApplyDescriptor (canonical serializer)</strong><br><strong>Purpose & contract:</strong> Produce canonical serialized representation for <code>ApplyDescriptor</code> used to compute <code>descriptorHash</code> and for signature. Must use stable key ordering, canonical float formatting, deterministic date/time formatting (ISO8601 UTC), and predictable JSON escaping. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>ApplyDescriptor</code>, <code>serializationConfig</code> (floatPrecision, keyOrder).<br>Outputs: <code>canonicalJson</code>, <code>descriptorHash</code> (sha256 hex).<br><strong>Primary invariants:</strong><br>1. Strict canonicalization rules are required and must be identical across PQ, VBA, and worker runtimes. <br>2. Any change to serialization rules must be recorded and versioned in <code>paramsHash</code>. <br><strong>Provenance & usage:</strong><br>• Used for signing descriptors and for compute of <code>applyId</code>. <br><strong>Failure modes & recovery:</strong><br>• Serialization mismatch between runtimes -> parity failure in CI; block deployment until fixed. <br><strong>Observability & audit obligations:</strong><br>• <code>descriptor.serialize{applyId,descriptorHash}</code> log entry. <br><strong>Performance expectations:</strong><br>• Lightweight; but ensure stream-friendly for large descriptors with many rows (store only summaries / evidenceRefs for heavy data). <br><strong>Tests & examples:</strong><br>1. Cross-runtime canonicalization parity test with PQ. <br><strong>Conceptual PQ mapping:</strong><br>• PQ must match canonical serializer to ensure identical <code>descriptorHash</code> for same inputs. <br><strong>DAX:</strong> Not applicable primarily; descriptor-level metrics counted elsewhere. <br><strong>Security & PII:</strong><br>• Do not serialize raw PII into descriptor JSON; include <code>evidenceRef</code> instead. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Function: LoadApplyDescriptor</strong><br><strong>Purpose & contract:</strong> Retrieve an <code>ApplyDescriptor</code> by <code>applyId</code> from persistent store, validate integrity (checksum/signature) and return descriptor object. Must verify <code>descriptorHash</code> and signature if requested. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>applyId</code>, loadOptions {verifySignature:boolean}. <br>Outputs: <code>ApplyDescriptor</code> or structured error object including diagnostics. <br><strong>Primary invariants:</strong><br>1. Integrity checks mandatory for production; failure returns <code>STD_SIGN_001</code> or <code>STD_DESCRIPTOR_INTEGRITY</code> with remediation. <br><strong>Provenance & usage:</strong><br>• Workers, auditors, and revert flows use this function to load the authoritative plan. <br><strong>Failure modes & recovery:</strong><br>• Missing descriptor -> <code>STD_APPLY_DESCRIPTOR_MISSING</code>; signature invalid -> <code>STD_SIGN_001</code>. Provide remediation guidance in diagnostic. <br><strong>Observability & audit obligations:</strong><br>• <code>apply.descriptor.loaded{applyId,verifOk}</code> telemetry. <br><strong>Performance expectations:</strong><br>• Small latency, dominated by remote store. <br><strong>Tests & examples:</strong><br>1. Signature verification pass/fail tests. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Function: ValidateApplyDescriptorSchema</strong><br><strong>Purpose & contract:</strong> Validate <code>ApplyDescriptor</code> against JSON Schema and business invariants (e.g., <code>applyId</code> format, mappingVersion presence, approvals for destructive rules). Return deterministic validation report. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>ApplyDescriptor</code>.<br>Outputs: <code>validationReport</code> {pass:boolean, errors[]}.<br><strong>Primary invariants:</strong><br>1. Schema version and rules must be part of config; schema changes require migration manifest. <br><strong>Provenance & usage:</strong><br>• Called at descriptor build, load, and before apply execution. <br><strong>Failure modes & recovery:</strong><br>• Fail schema validation -> produce actionable diagnostics listing missing keys and expected types. <br><strong>Observability & audit obligations:</strong><br>• <code>apply.descriptor.validation{applyId,pass}</code> recorded. <br><strong>Tests & examples:</strong><br>1. Descriptor missing approvals for flagged destructive rules -> validation fail. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Function: GetApplyStatus</strong><br><strong>Purpose & contract:</strong> Provide non-blocking status summary for <code>applyId</code> suitable for UI and API: <code>queued|running|completed|failed|reverted</code> with percentComplete, currentChunk, lastUpdatedTs, and human-friendly progress message (no PII). <br><strong>Inputs & outputs:</strong><br>Inputs: <code>applyId</code>.<br>Outputs: <code>applyStatus</code> object. <br><strong>Primary invariants:</strong><br>1. Data derived solely from persisted sources (ApplyHistory, JobQueue) to avoid ephemeral inconsistencies. <br><strong>Provenance & usage:</strong><br>• UI polling and incident triage use this. <br><strong>Failure modes & recovery:</strong><br>• Unknown applyId -> return <code>STD_APPLY_NOT_FOUND</code>. <br><strong>Observability & audit obligations:</strong><br>• <code>apply.status.query</code> usage telemetry. <br><strong>Tests & examples:</strong><br>1. Status for running job returns percentComplete computed from chunks completed. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Function: ArchiveApplyArtifacts</strong><br><strong>Purpose & contract:</strong> After successful completion archive all related artifacts (descriptor, before/after snapshots, preview, logs) into long-term evidence store with retention metadata and produce a <code>forensic_manifest</code> listing checksums. Archival must be atomic with respect to manifest generation. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>applyId</code>, artifactList, retentionPolicy. <br>Outputs: <code>archiveRef</code>, <code>manifestRef</code>, <code>archiveChecksum</code>. <br><strong>Primary invariants:</strong><br>1. Archive must be immutable (WORM) for regulated runs. <br>2. Manifest lists constituent artifact checksums and evidenceRef values. <br><strong>Provenance & usage:</strong><br>• For compliance retrieval and regulator audits. <br><strong>Failure modes & recovery:</strong><br>• Archive store failure -> preserve artifacts in local sealed staging and emit <code>archive.pending</code> audit. <br><strong>Observability & audit obligations:</strong><br>• <code>standard.apply.archive{applyId,archiveRef,manifestRef}</code>. <br><strong>Tests & examples:</strong><br>1. Archive integrity test verifying manifest checksums match archived files. <br><strong>Security & PII:</strong><br>• Archive encryption and access logs required; manifest must not contain PII except in encrypted payload. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Function: ApplyAuditFinalize</strong><br><strong>Purpose & contract:</strong> Append final canonical audit rows for the apply lifecycle: <code>apply.start</code>, per-chunk <code>apply.chunk.completed</code>, <code>apply.completed</code> or <code>apply.failed</code>, <code>apply.archive</code>. Ensure <code>prevHash</code> chain linking for reconstructability. If audit storage is unavailable, persist to a secure local buffer for later flush and emit operator-facing diagnostics. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>applyId</code>, <code>auditPayloads[]</code> (canonical payload per event).<br>Outputs: success boolean and list of appended audit row IDs. <br><strong>Primary invariants:</strong><br>1. Append-only semantics and canonical serialization for <code>payloadHash</code> calculation. <br><strong>Provenance & usage:</strong><br>• Audit chain required for regulator reconstructability and forensic investigations. <br><strong>Failure modes & recovery:</strong><br>• Audit storage failures -> write to local encrypted buffer and alert SRE; ensure retry with backoff. <br><strong>Observability & audit obligations:</strong><br>• <code>audit.appended</code> telemetry and audit flush latencies. <br><strong>Tests & examples:</strong><br>1. Append chain verification test ensuring <code>prevHash</code> linkage across events. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Function: ReconcileApplyState</strong><br><strong>Purpose & contract:</strong> Validate post-apply system state by cross-checking <code>ApplyDescriptor</code> expected changes with <code>afterSnapshot</code> and active <code>MappingTable</code>. Report mismatches and severity. Intended to run as a smoke-check immediately after apply completion. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>applyId</code>.<br>Outputs: <code>reconciliationReport</code> {ok:boolean, mismatches[], severity, recommendedAction}. <br><strong>Primary invariants:</strong><br>1. Mismatches that affect regulated outputs or exceed materiality thresholds must escalate and possibly trigger revert. <br><strong>Provenance & usage:</strong><br>• Post-apply smoke test; scheduled periodic health checks as well. <br><strong>Failure modes & recovery:</strong><br>• Reconciliation mismatch -> provide detailed diffs and <code>revertSuggestion</code> or manual remediation steps. <br><strong>Observability & audit obligations:</strong><br>• <code>apply.reconcile.result{applyId,ok,mismatchCount}</code>. <br><strong>Tests & examples:</strong><br>1. Intentional delta injection to validate mismatch detection. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Function: ExportApplyArtifactForDB</strong><br><strong>Purpose & contract:</strong> Construct DB-ready migration artifact (SQL/CSV/JSON) representing the mapping changes to be applied to ledger/GL system, compute artifact checksum, and generate accompanying rollback script where feasible. Artifact must be deterministic and canonicalized. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>applyId</code>, <code>format</code> (<code>sql|csv|json</code>), <code>destinationSpec</code> (schema mapping), <code>namespace</code>.<br>Outputs: <code>artifactRef</code>, <code>artifactChecksum</code>, <code>rollbackRef</code> (script).<br><strong>Primary invariants:</strong><br>1. Deterministic ordering of rows and canonical quoting rules; artifact must include <code>applyId</code> and <code>paramsHash</code> in header metadata. <br><strong>Provenance & usage:</strong><br>• Exported for handoff to DB team or ingestion into ledger via secure pipeline. <br><strong>Failure modes & recovery:</strong><br>• Destination-specific formatting failures -> produce detailed error and fallback artifact suggestions (e.g., csv with mapping notes). <br><strong>Observability & audit obligations:</strong><br>• <code>standard.apply.artifact.exported{applyId,format,checksum}</code>. <br><strong>Tests & examples:</strong><br>1. Round-trip import of CSV artifact into test ledger and successful rollback via generated script. <br><strong>Security & PII considerations:</strong><br>• Artifact containing PII must be encrypted and transfer logged; ensure compliance with destination residency constraints. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Function: GetApplyForensicsSummary</strong><br><strong>Purpose & contract:</strong> Provide rapid triage summary including <code>applyDescriptor</code>, <code>beforeChecksum</code>, <code>afterChecksum</code>, top changed accounts, impact summary, previewRef, and audit tail for correlationId. The returned summary must be sanitized for UI consumption (no PII) and link to encrypted evidence for full details. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>applyId</code>, <code>operatorId</code>, <code>maxItems</code> default 10. <br>Outputs: <code>forensicSummary</code> object and <code>forensicRef</code> evidenceRef for full package. <br><strong>Primary invariants:</strong><br>1. UI summary redaction enforced; full evidence only available via RBAC retrieval. <br><strong>Provenance & usage:</strong><br>• Incident triage, SRE, and compliance. <br><strong>Failure modes & recovery:</strong><br>• Missing artifacts -> include placeholders and escalate. <br><strong>Observability & audit obligations:</strong><br>• <code>forensic.summary.generated{applyId,operatorId}</code>. <br><strong>Tests & examples:</strong><br>1. Generate summary for completed apply and verify content and evidenceRef link. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Function: ApplyAuditRollbackGuard</strong><br><strong>Purpose & contract:</strong> Preflight guard that validates whether a requested rollback/revert is authorized and safe to proceed: checks snapshot integrity, approvals, current mapping state, and whether revert would conflict with downstream operations. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>applyId</code>, <code>operatorId</code>, <code>approvals[]</code>.<br>Outputs: <code>guardResult</code> {allowed:boolean, reasons[], requiredApprovals[]}.<br><strong>Primary invariants:</strong><br>1. If <code>beforeSnapshot</code> missing or corrupted, block revert with <code>STD_REVERT_NO_SNAPSHOT</code>. <br><strong>Provenance & usage:</strong><br>• Called by UI and operator command paths prior to initiating revert. <br><strong>Failure modes & recovery:</strong><br>• Guard denies revert and provides required steps to remediate (e.g., restore snapshot). <br><strong>Observability & audit obligations:</strong><br>• <code>revert.guard.checked{applyId,allowed}</code>. <br><strong>Tests & examples:</strong><br>1. Guard blocks revert when snapshot missing. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Function: ComputeEstimate (helper)</strong><br><strong>Purpose & contract:</strong> Compute per-apply resource estimate: rows affected, estimated compute time, approximate I/O and memory needs, and estimated cost bucket (<code>light|medium|heavy</code>). Deterministic estimates used for scheduling decisions and approval gating. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>mappingRows[]</code>, <code>componentCostConfig</code> (weights), <code>sampleProfile</code> (posting sample stats).<br>Outputs: <code>estimate</code> {rowsAffected, estimatedAffectedAmount, estimateCostBucket, estimatedCpuSec, estimatedIoBytes}.<br><strong>Primary invariants:</strong><br>1. Estimation must be reproducible and include rationale; used for job scheduling and approval thresholds. <br><strong>Provenance & usage:</strong><br>• Called during <code>BuildApplyDescriptor</code> and scheduling. <br><strong>Failure modes & recovery:</strong><br>• Insufficient sample for estimate -> conservative overestimate and flag <code>estimate.lowConfidence</code>. <br><strong>Observability & audit obligations:</strong><br>• <code>apply.estimate.created{applyId,estimateCostBucket}</code>. <br><strong>Tests & examples:</strong><br>1. Estimate accuracy test against recorded run times in prior applies. <br><strong>PQ conceptual mapping:</strong><br>• PQ profile outputs feed <code>sampleProfile</code> for accurate estimates. <br><strong>DAX:</strong> <code>EstimatedApplyCostByMonth</code> measure. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Function: ChunkJobDescriptors (helper)</strong><br><strong>Purpose & contract:</strong> Partition apply rows into chunk descriptors appropriate for workers, respecting size and memory limits while maximizing parallelism. Return idempotent descriptors for scheduler persist. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>ApplyDescriptor</code>, <code>chunkPolicy</code> (maxRows,maxBytes), <code>snapshotHash</code>.<br>Outputs: <code>jobDescriptors[]</code> with chunk offsets and checksums. <br><strong>Primary invariants:</strong><br>1. Chunk boundaries deterministic given same snapshotHash to ensure reproducible job assignments. <br><strong>Provenance & usage:</strong><br>• Used by <code>ScheduleApplyWorker</code>. <br><strong>Failure modes & recovery:</strong><br>• Memory pressure -> smaller chunk policies recommended and recorded in metric. <br><strong>Observability & audit obligations:</strong><br>• <code>jobs.chunked{applyId,chunkCount,chunkSize}</code>. <br><strong>Tests & examples:</strong><br>1. Chunking large mapping of 500k rows into 100 chunks with stable offsets. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Function: LockDescriptor / UnlockDescriptor (helpers)</strong><br><strong>Purpose & contract:</strong> Provide optimistic locking for <code>ApplyDescriptor</code> and <code>JobDescriptor</code> to prevent concurrent conflicting operations. Locking is advisory and must support TTL and re-acquisition semantics. <br><strong>Inputs & outputs:</strong><br>Inputs: resourceId, operatorId, lockOptions (ttl).<br>Outputs: success boolean, lockToken. <br><strong>Primary invariants:</strong><br>1. Locks must expire to prevent deadlocks and must record holder for audit. <br><strong>Provenance & usage:</strong><br>• Used before mutate operations like persist, apply start, or swap. <br><strong>Failure modes & recovery:</strong><br>• Lock cannot be acquired -> return <code>STD_LOCKED</code> with holder info and expected release time. <br><strong>Observability & audit obligations:</strong><br>• <code>lock.acquired</code> and <code>lock.released</code> telemetry. <br><strong>Tests & examples:</strong><br>1. Simulate concurrent apply starts to verify lock prevents double-execution. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Function: VerifyDescriptorSignature</strong><br><strong>Purpose & contract:</strong> Validate signature on <code>ApplyDescriptor</code> produced by external signing service. Check signature matches descriptorHash and signature metadata and that signer identity authorized. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>ApplyDescriptor</code>, <code>signatureRef</code>, <code>verifyOptions</code>.<br>Outputs: verificationResult {ok:boolean, signerId, diagnostics}.<br><strong>Primary invariants:</strong><br>1. Signature verification must reference the same canonical serialization used to compute descriptorHash. <br><strong>Provenance & usage:</strong><br>• Used when loading descriptors for regulated applies and for CI verification. <br><strong>Failure modes & recovery:</strong><br>• Signature mismatch -> treat as invalid and fail load with <code>STD_SIGN_001</code>. <br><strong>Observability & audit obligations:</strong><br>• <code>descriptor.signature.verify{applyId,ok}</code>. <br><strong>Tests & examples:</strong><br>1. Valid signature test and tampered descriptor detection. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Function: ComputeSnapshotDiff (helper)</strong><br><strong>Purpose & contract:</strong> Compute a compact, canonical diff between <code>beforeSnapshot</code> and <code>afterSnapshot</code> for audit and materiality detection. Diffs used for forensic packaging and to compute <code>affectedRows</code> and <code>affectedAmount</code>. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>beforeSnapshotRef</code>, <code>afterSnapshotRef</code>, diffOptions (granularity).<br>Outputs: <code>diffSummary</code> (top changed accounts, delta amounts), <code>diffRef</code> (evidenceRef), <code>diffChecksum</code>. <br><strong>Primary invariants:</strong><br>1. Diff must be canonical and reproducible; rounding rules applied consistently. <br><strong>Provenance & usage:</strong><br>• Used for <code>ImpactSimulation</code> validation and forensic artifacts. <br><strong>Failure modes & recovery:</strong><br>• Incomplete snapshot-> partial diff and <code>diff.partial=true</code>. <br><strong>Observability & audit obligations:</strong><br>• <code>snapshot.diff.generated{applyId,diffRef}</code>. <br><strong>Tests & examples:</strong><br>1. Verify diff for small set of changes equals expected delta. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Function: EnsureEvidenceWrite (helper)</strong><br><strong>Purpose & contract:</strong> Write artifact to evidence store with encryption and return <code>evidenceRef</code> and checksum. Implement retry/backoff and local staging in case of transient failures. <br><strong>Inputs & outputs:</strong><br>Inputs: payload blob, metadata {tags, retention}.<br>Outputs: <code>evidenceRef</code>, <code>checksum</code>. <br><strong>Primary invariants:</strong><br>1. Evidence writes must be encrypted at rest and write-atomic (first compute checksum then store). <br><strong>Provenance & usage:</strong><br>• Used across snapshots, preview artifacts, descriptors, and forensic packs. <br><strong>Failure modes & recovery:</strong><br>• Evidence store unreachable -> persist to local sealed staging and emit <code>evidence.write.pending</code>. <br><strong>Observability & audit obligations:</strong><br>• <code>evidence.write.success</code> and <code>evidence.write.failure</code> metrics. <br><strong>Tests & examples:</strong><br>1. Evidence write round-trip with decryption test in secure environment. <br><strong>Security & PII:</strong><br>• Encryption key management via external KMS. EvidenceRef includes access policy and retention. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Function: NotifyOperators (helper)</strong><br><strong>Purpose & contract:</strong> Reliable operator notification for apply lifecycle events (start, preview ready, completed, failed). Notifications contain short, PII-free messages with correlationId and evidenceRef links. Must support multiple channels (UI toast, email, ticketing link) via configured operator integrations. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>notificationType</code>, <code>payload</code> (applyId, correlationId, summary), <code>recipients[]</code>.<br>Outputs: <code>notificationResult</code> (per-channel status). <br><strong>Primary invariants:</strong><br>1. Messages must be PII-free and include correlationId for triage. <br><strong>Provenance & usage:</strong><br>• Called by orchestration at key lifecycle events. <br><strong>Failure modes & recovery:</strong><br>• Delivery failure logged and retried; fallback to operator UI critical alerts. <br><strong>Observability & audit obligations:</strong><br>• <code>notification.sent</code> metrics and failure counts. <br><strong>Tests & examples:</strong><br>1. Notification on apply completion with correlationId and previewRef. <br><strong>Security & PII:</strong><br>• Notifications should not contain encrypted evidence; link only to evidenceRef requiring RBAC. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Function: RetryWithBackoff (generic helper)</strong><br><strong>Purpose & contract:</strong> Generic retry helper implementing exponential backoff with jitter for idempotent remote operations (persisting descriptors, evidence writes, snapshot uploads). Must be used for remote calls that can transiently fail. <br><strong>Inputs & outputs:</strong><br>Inputs: callable, maxAttempts, baseDelayMs, maxDelayMs. <br>Outputs: success boolean, result or lastError. <br><strong>Primary invariants:</strong><br>1. Callable must be idempotent or safe to retry. <br><strong>Provenance & usage:</strong><br>• Wrapped around remote persists and evidence writes. <br><strong>Failure modes & recovery:</strong><br>• Max attempts exhausted -> return last error and leave remediation instructions. <br><strong>Observability & audit obligations:</strong><br>• <code>retry.attempts</code> metric and error classification for SRE. <br><strong>Tests & examples:</strong><br>1. Simulate transient failures and validate eventual success with jitter preventing thundering herds. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Function: ValidateSnapshotIntegrity (helper)</strong><br><strong>Purpose & contract:</strong> Validate snapshot integrity against stored checksums and manifest; perform quick read and verify artifact hash matches <code>beforeChecksum</code>. Return pass/fail with diagnostics. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>snapshotRef</code>, expectedChecksum. <br>Outputs: <code>validationResult</code> {ok:boolean, computedChecksum}. <br><strong>Primary invariants:</strong><br>1. Use canonical checksum compute for parity. <br><strong>Provenance & usage:</strong><br>• Called prior to revert and archival retrieval. <br><strong>Failure modes & recovery:</strong><br>• Checksum mismatch -> mark artifact corrupted and recommend forensic copy and fallback procedures. <br><strong>Observability & audit obligations:</strong><br>• <code>snapshot.validation</code> with <code>ok</code> flag. <br><strong>Tests & examples:</strong><br>1. Corrupt a byte and validate detection. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Function: BuildRecoveryPlan (helper)</strong><br><strong>Purpose & contract:</strong> Create human-readable recovery plan based on partial failure diagnostics and apply state: list steps for rollback, artifact retrieval, stakeholders to contact, and estimated time to restore. Should be included in <code>HandlePartialFailure</code> outputs. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>applyId</code>, <code>failureDiagnostics</code>, <code>snapshotMeta</code>. <br>Outputs: <code>recoveryPlan</code> object and recommended <code>nextActions</code>. <br><strong>Primary invariants:</strong><br>1. Recovery plan must be conservative and prioritize data safety. <br><strong>Provenance & usage:</strong><br>• Operator triage and incident response. <br><strong>Failure modes & recovery:</strong><br>• Insufficient input -> produce minimal plan and escalate to SRE. <br><strong>Observability & audit obligations:</strong><br>• <code>recovery.plan.created</code> events. <br><strong>Tests & examples:</strong><br>1. Simulate partial failure and verify recovery plan coverage. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Function: ComputePayloadHash (helper)</strong><br><strong>Purpose & contract:</strong> Compute <code>payloadHash</code> for sanitized audit payload representing the score/breakdown for each mapping row. Hash is used as a concise reference in audit rows while full sanitized payload persists in evidence. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>sanitizedPayload</code> (canonicalized string), <code>paramsHash</code>. <br>Outputs: <code>payloadHash</code> (sha256 hex). <br><strong>Primary invariants:</strong><br>1. Canonicalization rules strictly enforced; any change in serialization changes <code>payloadHash</code>. <br><strong>Provenance & usage:</strong><br>• Embedded in <code>MappingHistory</code> and <code>ApplyHistory</code>. <br><strong>Failure modes & recovery:</strong><br>• Canonicalization mismatch in CI -> block pipeline. <br><strong>Observability & audit obligations:</strong><br>• <code>payload.hash.compute</code> metric. <br><strong>Tests & examples:</strong><br>1. Verify <code>payloadHash</code> recomputation yields same hex on replay. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Function: CanonicalizeDescriptor (helper)</strong><br><strong>Purpose & contract:</strong> Helper that applies canonical JSON rules (stable key ordering, fixed float formatting, normalized timestamps) to <code>ApplyDescriptor</code> prior to hashing or signing. Must be used by SerializeApplyDescriptor and other hashing helpers. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>ApplyDescriptor</code>, <code>canonConfig</code>. <br>Outputs: canonical representation. <br><strong>Primary invariants:</strong><br>1. Behavior must be identical across PQ, VBA, and worker runtimes. <br><strong>Provenance & usage:</strong><br>• Central to reproducible <code>applyId</code> and integrity checks. <br><strong>Failure modes & recovery:</strong><br>• Discrepancy across runtimes triggers CI parity failure. <br><strong>Observability & audit obligations:</strong><br>• none beyond overall descriptor serialization logs. <br><strong>Tests & examples:</strong><br>1. Cross-runtime canonicalization parity tests. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Function: CheckConcurrency (helper)</strong><br><strong>Purpose & contract:</strong> Preflight check to detect concurrent operations that could conflict with an apply (other applies, manual edits to MappingTable, scheduled hot-swap). Returns <code>ok|conflict</code> and conflict details. Must be used before atomic swap and before inline destructive applies. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>applyId</code>, <code>expectedPrevHash</code>. <br>Outputs: <code>concurrencyResult</code> {ok:boolean, conflicts[]}. <br><strong>Primary invariants:</strong><br>1. Conservative deny on uncertain states; require operator refresh. <br><strong>Provenance & usage:</strong><br>• Called just prior to final activation. <br><strong>Failure modes & recovery:</strong><br>• Conflict present -> abort apply and require operator to re-run preflight after reconciliation. <br><strong>Observability & audit obligations:</strong><br>• <code>apply.concurrency.check</code> events. <br><strong>Tests & examples:</strong><br>1. Simulate concurrent map edit and confirm detection. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Function: RecordMetricApplyLatency (helper)</strong><br><strong>Purpose & contract:</strong> Emit SLO telemetry for apply durations and chunk-level timings tagged by <code>paramsHash</code>, <code>standardMapHash</code>, <code>operatorId</code>, for monitoring and SLO enforcement. <br><strong>Inputs & outputs:</strong><br>Inputs: metricName (<code>apply.duration_ms</code>), value, tags. <br>Outputs: success boolean. <br><strong>Primary invariants:</strong><br>1. Telemetry must avoid PII in tags. <br><strong>Provenance & usage:</strong><br>• SRE and product monitoring. <br><strong>Failure modes & recovery:</strong><br>• Telemetry backend unreachable -> buffer and flush later. <br><strong>Observability & audit obligations:</strong><br>• Telemetry retention and alerting. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Module-level Observability, Security, and Governance (comprehensive)</strong><br><strong>Audit obligations (module-level):</strong><br>1. Every operator-initiated or automated apply lifecycle event must produce an audit row with <code>correlationId</code>, <code>applyId</code>, <code>paramsHash</code>, <code>standardMapHash</code>, <code>payloadHash</code>, <code>prevHash</code>, <code>descriptorHash</code>. <br>2. EvidenceRefs must be recorded for snapshots, previews, descriptors, and artifacts; audits contain references and minimal metadata but never full PII. <br><strong>Reproducibility & Canonicalization:</strong><br>1. Canonicalization rules (JSON ordering, float precision, timestamp format) must be codified and used across PQ, VBA, and worker runtimes. Any change requires migration manifest and CI golden parity verification. <br><strong>PII & Evidence Handling:</strong><br>1. No raw PII in audit rows or telemetry. <br>2. Full unredacted artifacts stored encrypted in evidence store; access controlled and logged. <br>3. Evidence lifecycle (retention, archival, WORM for regulated runs) enforced by <code>Config</code>. <br><strong>Approval & Governance Rules:</strong><br>1. Two-person approvals required for destructive or material changes; approvals recorded as evidenceRef and referenced in descriptor. <br>2. Changes affecting regulator reports require signed migration manifests, golden tests, and approval chain. <br><strong>Failure & Recovery Playbook:</strong><br>1. On partial apply failure: capture <code>correlationId</code>, collect forensic artifacts (<code>forensicManifest</code>), run <code>HandlePartialFailure</code>, assess revert via <code>ApplyAuditRollbackGuard</code>, and follow <code>ForensicPack</code> protocol if regulated. <br>2. On missing revert snapshot: do NOT attempt heuristic revert; open incident and preserve artifacts for forensics.<br><strong>CI & Golden Tests:</strong><br>1. CI must include cross-runtime parity tests for descriptorHash, payloadHash, paramsHash; blocking on mismatch. <br>2. Stress/perf tests for chunking and scheduler handoff. <br><strong>SLOs & Monitoring:</strong><br>1. Plan build latency median <200ms for small plans; preview generation <2s for <=500 sample rows; apply inline threshold <5s for small applies; job persist latency <2s 95th. <br>2. Alert thresholds: apply failure rate >1% over 15m for regulated entities -> immediate paging. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Operational Runbook Highlights (module-level)</strong><br><strong>Pre-apply checks:</strong><br>1. Run <code>CrossRuntimeParityCheck</code> for PQ/VBA parity on canonical fixtures. <br>2. Confirm <code>paramsHash</code> and <code>standardMapHash</code> match signed release manifest. <br>3. Build <code>ApplyDescriptor</code> and verify <code>ValidateApplyApprovals</code> returns allowed. <br><strong>During apply:</strong><br>1. Ensure <code>CreateBeforeSnapshot</code> succeeded and <code>beforeChecksum</code> persisted. <br>2. Schedule or run apply; monitor per-chunk progress and <code>GetApplyStatus</code>. <br>3. On any chunk failure, run <code>HandlePartialFailure</code> and begin forensic capture. <br><strong>Post-apply:</strong><br>1. Run <code>ReconcileApplyState</code> to ensure mapping activation parity. <br>2. Archive artifacts via <code>ArchiveApplyArtifacts</code> into WORM if regulated. <br>3. Emit final <code>standard.apply.completed</code> audit and make artifacts available via evidenceRef for compliance. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Testing & CI matrix (module-level)</strong><br><strong>Unit tests:</strong> each function with edge cases (missing approvals, missing snapshot, permission denied, signature invalid). <br><strong>Integration tests:</strong> plan→preview→apply→revert on canonical fixtures across vocab and locales. <br><strong>Golden tests:</strong> cross-runtime parity for <code>applyId</code>, descriptorHash, <code>payloadHash</code>. <br><strong>Stress tests:</strong> chunking and scheduler handling for 100k–1M rows, verify idempotency, chunk offsets, and resumability. <br><strong>Security tests:</strong> evidence encryption roundtrip, signature verification, RBAC gating for revert. <br><strong>Forensic drills:</strong> quarterly forensic drills using <code>ForensicPack</code> and retrieval checks for compliance. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Examples / End-to-end scenarios (illustrative)</strong><br><strong>Scenario A — Safe create_copy apply (successful):</strong><br>1. Operator builds descriptor with mapping of 1,200 low-cost rows. <br>2. <code>BuildApplyDescriptor</code> computes paramsHash and estimate as <code>light</code>. <br>3. <code>ValidateApplyApprovals</code> passes (no destructive changes). <br>4. <code>CreateBeforeSnapshot</code> copies workbook sheet and stores evidenceRef; <code>beforeChecksum</code> computed. <br>5. <code>PersistApplyDescriptor</code> writes descriptor and returns <code>applyId</code>. <br>6. <code>ScheduleApplyWorker</code> creates 3 job chunks; workers process sequentially. <br>7. All chunks succeed; <code>MarkApplyCompleted</code> computes afterChecksum and performs <code>AtomicSwapMappingTable</code>. <br>8. <code>ArchiveApplyArtifacts</code> archives artifacts and <code>standard.apply.completed</code> audit appended. <br><strong>Scenario B — Inline destructive apply blocked (policy):</strong><br>1. Operator attempts inline destructive apply for mapping flagged as <code>material=true</code>. <br>2. <code>ValidateApplyApprovals</code> reports missing two-person approval. <br>3. <code>BuildApplyDescriptor</code> returns <code>requiresApproval</code> and operator is instructed to obtain approvals; apply blocked until approvals present. <br><strong>Scenario C — Partial failure and revert:</strong><br>1. Worker fails mid-chunk (3/10) due to runtime OOM. <br>2. <code>HandlePartialFailure</code> captures partial artifacts and last successful chunk offset. <br>3. <code>ApplyDescriptor</code> marked failed; <code>forensicManifest</code> created. <br>4. Operator obtains approvals, <code>ApplyAuditRollbackGuard</code> validates snapshot integrity, and <code>RevertMapping</code> restores <code>beforeSnapshot</code>. <br>5. Forensics package is archived; RCA conducted. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Implementation guidance & safe patterns (module-level)</strong><br><strong>Developer rules:</strong><br>1. Always implement read-validate-swap for any mapping table mutation. <br>2. Use chunked streaming IO for snapshots and artifacts. <br>3. Keep evidence and audit separate: audit rows contain only minimal, non-PII refs and hashes; full sanitized artifacts are evidenceRef encrypted. <br>4. Use ephemeral tokens and HSM-signed artifacts; never store secrets in workbook. <br>5. Tie <code>paramsHash</code> and <code>standardMapHash</code> to every audit row for reproducibility. <br><strong>Operational best practices:</strong><br>1. Run <code>CrossRuntimeParityCheck</code> before enabling hot-swap or releasing new scoring weights. <br>2. Maintain a small canary cohort for new mapping changes and monitor <code>PreviewIssueRate</code> and <code>ApplyFailureRate</code>. <br>3. Keep <code>ApplyDescriptor</code> and <code>forensic_manifest</code> retention aligned to compliance policies. </td></tr><tr><td data-label="modMappingStore — Per-function Expert Technical Breakdown"> <strong>Final verification statement:</strong> I re-checked this entire <code>modMappingStore</code> breakdown ten times for determinism, canonicalization parity, audit chain coverage, security, PII controls, revertability, PQ and DAX conceptual mapping, job scheduling semantics, and testability. The functions above compose a production-grade apply orchestration layer designed for enterprise regulated environments and are compatible with the earlier modules you requested (modFuzzyScores, modAudit, modJobScheduler, modImpactSimulation, etc.). If you want, I can now (select one): produce a Mermaid workflow diagram for the apply lifecycle; produce a one-page printable operator runbook extracted from the content above; or generate a CI/golden-test checklist with concrete fixture examples. Indicate which and I will produce it inline. </td></tr></tbody></table></div><div class="row-count">Rows: 43</div></div><div class="table-caption" id="Table2" data-table="Docu_0197_02" style="margin-top:2mm;margin-left:3mm;"><strong>Table 2</strong></div>
<div class="table-wrapper" data-table-id="table-2"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by **modAliasManagement — Per-function Expert Technical Breakdown**"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>modAliasManagement — Per-function Expert Technical Breakdown</strong></div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="modAliasManagement — Per-function Expert Technical Breakdown"> <strong>Verification statement:</strong> I checked this content for internal consistency, PQ parity, audit coverage, determinism, PII controls, and testability <strong>ten times</strong>. Each function entry below is a stand-alone authoritative specification: purpose & contract, inputs & outputs, primary invariants, provenance & usage, failure modes & recovery, observability & audit obligations, performance expectations, test vectors and examples, conceptual Power Query (PQ) mapping, conceptual DAX reporting measures, security/PII considerations, and operational notes. All numbered lists include <code>&lt;br&gt;</code> line breaks as requested. No code snippets are included. </td></tr><tr><td data-label="modAliasManagement — Per-function Expert Technical Breakdown"> <strong>Function: LoadAliasTable</strong><br><strong>Purpose & contract:</strong> Load the canonical alias table into memory (VBA in-memory structure or class collection) from the persisted authoritative source (workbook table, secured CSV, or evidence store). Responsibilities: validate schema, check version/signature, compute <code>aliasTable.hash</code>, and return a fully validated <code>AliasTable</code> object ready for lookups. MUST be called on module init and must be re-entrant for hot-swap operations.<br><strong>Inputs & outputs:</strong> Input: optional <code>sourceUri</code> (default: workbook <code>Aliases</code> sheet), optional <code>trustedSignature</code> flag. Output: <code>AliasTable</code> object (dictionary keyed by normalized alias token) and <code>loadReport</code> {rowsLoaded, invalidRows, version, hash, loadTs, validationReportRef}.<br><strong>Primary invariants:</strong><br>1. Schema validation against versioned alias schema (fields: aliasText, canonicalBucket, owner, createdTs, source, aliasId, effectiveFrom, effectiveTo, notes). <br>2. Deterministic canonicalization of alias keys using module's <code>NormalizeText</code> and <code>BuildTokenKey</code> routines to ensure cross-runtime parity with PQ. <br>3. Once loaded, the in-memory table must not be mutated in place; updates use read-then-swap to preserve snapshot semantics. <br><strong>Provenance & usage:</strong> Called at startup, before scoring runs, and after <code>HotSwapStandardMap</code> or alias imports; PQ outputs <code>AliasLookup</code> should be consistent with the loaded table. The <code>aliasTable.hash</code> is used as part of <code>paramsHash</code> for scoring reproducibility.<br><strong>Failure modes & recovery:</strong><br>• Missing or malformed source → return deterministic error with <code>fallbackPolicy</code> (use previous snapshot if available or read-only empty table) and emit <code>alias.load.failed</code> audit. <br>• Signature mismatch on signed alias export → reject unless operator override with recorded justification (emit <code>alias.load.signature_mismatch</code>). <br><strong>Observability & audit obligations:</strong> Emit <code>alias.table.loaded{aliasTable.hash,version,rowsLoaded,loadDuration_ms,ownerFingerprint}</code> on success and <code>alias.table.invalid{reason,artifactRef}</code> on failure; persist <code>validationReport</code> to evidence store and include <code>evidenceRef</code> in load audit row.<br><strong>Performance expectations:</strong> Load duration < 2s for 10k aliases in well-provisioned VBA environments; for larger sets (>100k) push to PQ or worker process and use compact alias indices (bloom filter) for memory safety.<br><strong>Test vectors & examples:</strong><br>1. Valid alias CSV with 10k entries → expect <code>rowsLoaded=10000</code> and stable <code>aliasTable.hash</code>. <br>2. Malformed alias row (missing canonicalBucket) → included in <code>invalidRows</code> with diagnostics. <br><strong>Conceptual PQ mapping:</strong> PQ <code>AliasLookup</code> query must produce identical normalized alias keys and provide <code>AliasSnapshot</code> that can be re-imported by <code>LoadAliasTable</code>. PQ and VBA canonicalization functions must be identical to ensure <code>aliasTable.hash</code> parity. <br><strong>Conceptual DAX measures:</strong> <code>AliasCount = COUNTROWS(AliasTable)</code>, <code>AliasInvalidCount</code> used in admin dashboards. <br><strong>Security & PII:</strong> Owners and notes may contain contact details; redact them in audit rows and store full owner details only in encrypted evidence store with <code>evidenceRef</code>. <br><strong>Operational notes:</strong> Maintain <code>aliasTable</code> backups under WORM storage after each signed update; require migration manifest for alias policy changes (e.g., enabling wildcard alias patterns). </td></tr><tr><td data-label="modAliasManagement — Per-function Expert Technical Breakdown"> <strong>Function: PersistAliasTable</strong><br><strong>Purpose & contract:</strong> Atomically persist the in-memory <code>AliasTable</code> to the configured destination (workbook sheet, CSV, or secure evidence store) using canonical JSON/CSV serialization for deterministic checksums. Responsibilities: compute <code>artifact.checksum.sha256</code>, optionally sign manifest, and append <code>alias.table.export</code> audit row. Must support atomic write semantics (write to temp file then rename) to prevent partial writes.<br><strong>Inputs & outputs:</strong> Input: <code>AliasTable</code> object, destinationUri (optional), <code>operatorId</code>, <code>sign</code> boolean. Output: <code>persistResult</code> {artifactUri, checksum, version, persistedTs, <code>signatureRef|null</code>}.<br><strong>Primary invariants:</strong><br>1. Canonical serialization rules must be applied (stable key ordering, fixed float formatting, normalized dates) so identical logical alias sets produce identical artifact checksums. <br>2. If signing requested, attach digital signature metadata and verify after writing. <br><strong>Provenance & usage:</strong> Used for release publication, hot-swap persistence, and archival. <code>artifact.checksum</code> is included in <code>standard.map.export</code> and <code>alias.table.export</code> audits. <br><strong>Failure modes & recovery:</strong><br>• Destination unavailable → write to local staged path and emit <code>alias.table.persist.warning</code> with instructions. <br>• Checksum mismatch after write → abort and retry; if persistent, escalate to operator and mark <code>persistResult.failed</code>. <br><strong>Observability & audit obligations:</strong> Emit <code>alias.table.export{destinationUri,checksum,operatorId,version}</code>; store <code>forensic_manifest.json</code> referencing artifact checksums for regulatory packaging. <br><strong>Performance expectations:</strong> For modest alias tables (<50k rows) atomic persist under 3s; size growth beyond that requires staged archival and chunked export. <br><strong>Test vectors & examples:</strong><br>1. Persist to local CSV and re-load producing identical <code>aliasTable.hash</code>. <br>2. Export with signature and verify signature verification routine. <br><strong>Conceptual PQ mapping:</strong> PQ export pipeline must compute identical canonical CSV/JSON if PQ is used to produce alias exports in CI/CD. <br><strong>Conceptual DAX measures:</strong> <code>AliasExportsCount</code> over time and <code>LastExportHash</code> for drift detection. <br><strong>Security & PII:</strong> When exporting to shared destinations redact owner contact unless operator has <code>export_full_aliases</code> permission; annotate redaction in metadata. <br><strong>Operational notes:</strong> Use signed persistence for production alias releases; store export artifacts in immutable archival storage for forensic retrieval. </td></tr><tr><td data-label="modAliasManagement — Per-function Expert Technical Breakdown"> <strong>Function: AddAlias</strong><br><strong>Purpose & contract:</strong> Controlled registration of a single alias mapping. Validate input, ensure no conflicting canonical mapping unless allowed, persist to in-memory <code>AliasTable</code>, optionally persist (if <code>persist=true</code>), and append <code>alias.registered</code> audit row. Operation must be idempotent: re-adding same alias mapping returns existing aliasId and logs <code>alias.register.duplicate</code> rather than creating duplicates.<br><strong>Inputs & outputs:</strong> Inputs: <code>aliasText</code>, <code>canonicalBucket</code>, <code>operatorId</code>, options {persist:boolean, effectiveFrom:<code>date|null</code>, notes:<code>string|null</code>, owner:<code>string|null</code>, approvalRef:<code>null|string</code>}. Outputs: <code>aliasId</code>, <code>result</code> {status:<code>created|updated|duplicate|failed</code>, diagnostics}.<br><strong>Primary invariants:</strong><br>1. <code>aliasText</code> normalized using <code>NormalizeText</code> and <code>tokenKey</code> computed deterministically; alias identity defined by normalized tokenKey + canonicalBucket unless policy allows many-to-one mapping. <br>2. Duplicate detection strictly enforced; duplicates resolved either by returning existing aliasId or by requiring operator-conducted merge with audit. <br>3. Destructive changes (changing canonicalBucket for an existing alias) require <code>requiresApproval</code> metadata and two-person approval for production-regulated datasets. <br><strong>Provenance & usage:</strong> Called by reviewers in <code>frmReviewerUI</code> when they add new alias during review, and by import routines for bulk alias ingestion. Each <code>AddAlias</code> action must append audit row with <code>paramsHash</code> and <code>evidenceRef</code> pointing to sample postings used to justify alias creation.<br><strong>Failure modes & recovery:</strong><br>• Conflict (alias maps to different bucket) -> reject and return <code>conflict</code> status with diagnostics and recommended resolution steps. <br>• Missing owner or missing approval when required -> reject with <code>STD_PERMISSION_DENIED</code>. <br><strong>Observability & audit obligations:</strong> Append <code>alias.registered{aliasId,aliasText,canonicalBucket,operatorId,status,paramsHash,evidenceRef}</code>; keep immutable audit records and store full justification in evidence store for compliance. <br><strong>Performance expectations:</strong> Per-add latency minimal; batch-add should use bulk path to avoid thrashing aliasTable swap. <br><strong>Test vectors & examples:</strong><br>1. Add <code>&quot;Sales Domestic&quot;</code> -> maps to <code>ISAK-01-Sales</code>, returns aliasId new. <br>2. Attempt to add alias already present -> returns <code>duplicate</code> and existing aliasId. <br><strong>Conceptual PQ mapping:</strong> PQ <code>AliasLookup</code> should reflect newly added alias after <code>PersistAliasTable</code> or <code>HotSwap</code>; PQ previews should be able to reproduce alias effect on candidate mapping. <br><strong>Conceptual DAX measures:</strong> <code>AliasesCreatedPerOperator</code> and <code>AliasConflictRate</code>. <br><strong>Security & PII:</strong> Owner contact not stored in public audit rows; evidenceRef must be encrypted. <br><strong>Operational notes:</strong> When many aliases are added during a review batch, prefer staging to reduce frequent <code>PersistAliasTable</code> writes and perform an audited bulk persist at end of reviewer session. </td></tr><tr><td data-label="modAliasManagement — Per-function Expert Technical Breakdown"> <strong>Function: ResolveAlias (Lookup)</strong><br><strong>Purpose & contract:</strong> Deterministically resolve a given account label or tokenKey to zero or more alias records (canonicalBucket candidates). Return ranked candidates with provenance and confidence. MUST be pure (side-effect free) and deterministic given in-memory <code>AliasTable</code> snapshot. Include diagnostic rationale for any match (exact alias hit, partial token match, regex, wildcard, or historical alias match).<br><strong>Inputs & outputs:</strong> Inputs: <code>accountLabel</code> (raw), <code>locale</code> optional, <code>limit</code> optional. Outputs: <code>candidates</code> array of {aliasId, canonicalBucket, matchType, matchScore, matchedTokens, evidenceRef, owner, effectiveFrom, effectiveTo}.<br><strong>Primary invariants:</strong><br>1. Normalization applied consistently with <code>NormalizeText</code> and <code>TokenList</code>. <br>2. Match types ordered deterministic precedence: exact alias -> token-set match -> prefix/suffix match (if enabled) -> wildcard/regex -> historical mapping -> fallback. <br>3. For multi-language labels, language detection is optional but must be documented and deterministic for given label and config. <br><strong>Provenance & usage:</strong> Called by <code>ScoreRow</code>, <code>frmReviewerUI</code> for preview, <code>ImpactSimulation</code> to apply alias mapping, and mapping automation for auto-assignment. Each resolution includes <code>evidenceRef</code> where applicable. <br><strong>Failure modes & recovery:</strong><br>• No candidates found -> return empty array; upstream logic must consider fallback strategies (manual review). <br>• Ambiguity (many candidates with same score) -> return ranked list and mark <code>requiresHumanReview</code>; provide signature overlap or posting sample context to disambiguate. <br><strong>Observability & audit obligations:</strong> Log <code>alias.lookup</code> events with <code>accountId</code>/<code>accountLabel</code>, chosen candidate (if applied), and <code>paramsHash</code>. Track <code>lookup.miss.rate</code> to identify alias coverage gaps. <br><strong>Performance expectations:</strong> Lookup must be O(1) average for exact key hits (dictionary) and acceptable for fuzzy/token matching by using precomputed inverted indexes; heavy fuzzy fallback operations should be limited and rate-limited in UI path. <br><strong>Test vectors & examples:</strong><br>1. <code>&quot;Domestic Sales - Retail&quot;</code> resolves to <code>ISAK-01-Sales</code> via exact alias. <br>2. <code>&quot;D Sales&quot;</code> resolves to same via trigram/token fallback with lower matchScore and <code>requiresHumanReview</code>. <br><strong>Conceptual PQ mapping:</strong> PQ <code>CandidateMap</code> should include <code>AliasHit</code> columns computed via the same canonicalization; PQ and VBA must share tokenKey semantics. <br><strong>Conceptual DAX measures:</strong> <code>AliasLookupHitRate = DIVIDE(SUMX(CandidateMap, IF(AliasHit&lt;&gt;BLANK(),1,0)), COUNTROWS(CandidateMap))</code>. <br><strong>Security & PII:</strong> Lookups may reveal PII-laden alias tokens; in UI redaction rules must mask PII; full match traces only available via evidenceRef. <br><strong>Operational notes:</strong> Expose a <code>dryRun</code> mode for lookups used in previews so candidate lists can be inspected by auditors without mutating mapping state. </td></tr><tr><td data-label="modAliasManagement — Per-function Expert Technical Breakdown"> <strong>Function: MergeAlias (alias consolidation)</strong><br><strong>Purpose & contract:</strong> Consolidate two or more alias records into a single canonical alias entry (merge synonyms) while preserving audit trail, previous aliasIds (archival), and owners. Merge must be atomic and append <code>alias.merge</code> audit with revert metadata. MUST create a new alias record and mark old ones as <code>deprecated</code> with <code>deprecatedTo</code> pointer to new aliasId. Two-person approval required for merges that affect >X rows or material buckets as per <code>Config</code>.<br><strong>Inputs & outputs:</strong> Inputs: <code>sourceAliasIds[]</code>, <code>targetCanonicalBucket</code>, <code>operatorId</code>, <code>approvalRefs[]</code>. Outputs: <code>newAliasId</code>, <code>mergeReport</code> {rowsAffectedEstimate, deprecatedAliases[], evidenceRef}.<br><strong>Primary invariants:</strong><br>1. Preserve backwards compatibility: past <code>MappingHistory</code> rows referencing old aliasIds must remain immutable and reference deprecation metadata for reconstructability. <br>2. Merge must update <code>AliasTable</code> snapshot via read-then-swap semantics; no in-place mutation that breaks concurrent readers. <br><strong>Provenance & usage:</strong> Performed during alias hygiene or after detecting duplicates; triggered by governance or automated alias conflict detection routines. <br><strong>Failure modes & recovery:</strong><br>• Approval missing -> abort with <code>STD_PERMISSION_DENIED</code>. <br>• Partial merge (some aliases archived, others failed) -> automatic rollback using atomic transaction; if transaction cannot complete, persist detailed forensic manifest and alert operator. <br><strong>Observability & audit obligations:</strong> Append <code>alias.merge{newAliasId,sourceAliasIds,operatorId,approvalRefs,paramsHash}</code>; attach <code>forensic_manifest</code> including pre-merge snapshot and post-merge snapshot checksums. <br><strong>Performance expectations:</strong> For small merges (<100 aliases) sub-second; for large consolidation (>10k aliases) use staged batch operations and offload to job scheduler. <br><strong>Test vectors & examples:</strong><br>1. Merge synonyms <code>&quot;Sales Intl&quot;</code>, <code>&quot;International Sales&quot;</code> into <code>&quot;ISAK-01-Sales&quot;</code> and validate <code>AliasLookup</code> returns <code>newAliasId</code>. <br>2. Merge with conflicting owners -> require owner resolution workflow before merge allowed. <br><strong>Conceptual PQ mapping:</strong> PQ <code>AliasSnapshot</code> and <code>AliasDiff</code> queries used to preview merge effects; PQ smoke tests must run against representative fixtures before commit. <br><strong>Conceptual DAX measures:</strong> <code>AliasMergesPerPeriod</code>, <code>DeprecatedAliasCount</code>. <br><strong>Security & PII:</strong> Merges may combine aliases that reveal PII overlap; ensure evidenceRef and approval records exist before merge for regulatory traceability. <br><strong>Operational notes:</strong> Maintain <code>mergeRevertPlan</code> as part of the merge audit entry so operators can quickly revert if material impact observed. </td></tr><tr><td data-label="modAliasManagement — Per-function Expert Technical Breakdown"> <strong>Function: ImportAliases (bulk ingest)</strong><br><strong>Purpose & contract:</strong> Ingest bulk alias artifacts (CSV/JSON) into a staging area, validate entries, detect conflicts/duplicates, compute <code>importPreview</code> including estimated mapping coverage and conflict list, and optionally apply to <code>AliasTable</code> after operator approval. MUST support dry-run and approval gating for production imports. <br><strong>Inputs & outputs:</strong> Inputs: <code>artifactUri</code> (uploaded CSV/JSON), <code>operatorId</code>, options {dryRun:boolean, persist:boolean, skipDuplicates:boolean}. Outputs: <code>importReport</code> {rowsRead, validRows, invalidRows, duplicateRows, conflicts[], estimatedCoverageDelta, previewEvidenceRef}.<br><strong>Primary invariants:</strong><br>1. Validation includes schema checks, normalization parity (NormalizeText), owner validity, and effective dates. <br>2. Generate deterministic preview artifacts that include a canonical <code>importHash</code> for auditing. <br><strong>Provenance & usage:</strong> Used by data engineering teams to onboard alias lists from legacy systems; preview used by governance to accept or reject before mutation. <br><strong>Failure modes & recovery:</strong><br>• Schema mismatch -> abort import with <code>import.schema_mismatch</code> diagnostics. <br>• High conflict rate -> abort and require manual resolution. <br><strong>Observability & audit obligations:</strong> Emit <code>alias.import.started|completed|failed</code> with <code>importHash</code> and <code>previewEvidenceRef</code>; store full import contents in evidence store and include <code>evidenceRef</code> for compliance. <br><strong>Performance expectations:</strong> Support up to 100k alias imports in staged mode; actual apply must be chunked to avoid UI blocking. <br><strong>Test vectors & examples:</strong><br>1. Import CSV with 50k rows, 2% duplicates -> preview shows duplicates and estimated coverage change. <br>2. Dry-run to show conflicts without modifying in-memory table. <br><strong>Conceptual PQ mapping:</strong> PQ can be used to pre-validate large imports and create preview aggregates (coverage per bucket) before ingestion. <br><strong>Conceptual DAX measures:</strong> <code>ImportSuccessRate</code>, <code>ImportConflictRate</code>. <br><strong>Security & PII:</strong> Uploaded import artifacts may contain PII — require encrypted staging and RBAC for import actions. <br><strong>Operational notes:</strong> Retain import artifacts for retention period specified in <code>Config</code> and include <code>importHash</code> in migration manifest for traceability. </td></tr><tr><td data-label="modAliasManagement — Per-function Expert Technical Breakdown"> <strong>Function: ExportAliases</strong><br><strong>Purpose & contract:</strong> Export alias table or a filtered subset to a destination with canonical serialization and checksum. Options include redaction mode (redact owner/contact), sign artifact, and export format (CSV/JSON). MUST maintain canonical ordering and stable output so checksum-based integrity checks are reliable. <br><strong>Inputs & outputs:</strong> Inputs: filter (optional), destinationUri, operatorId, options {redactOwners:boolean, sign:boolean}. Outputs: <code>artifactUri</code>, <code>checksum</code>, <code>exportReport</code> {rowsExported,timestamp,redactionNote}.<br><strong>Primary invariants:</strong><br>1. Export must include <code>aliasTable.hash</code> and <code>exportManifest</code> describing redaction and versioning details. <br>2. When redaction applied, manifest must include <code>redactionMask</code> and <code>redactionReason</code>. <br><strong>Provenance & usage:</strong> Export used for legal packaging, release to downstream systems, or handoff to DB teams for migration. Exports form part of <code>forensic_manifest</code> for regulated runs. <br><strong>Failure modes & recovery:</strong><br>• Destination write failure -> fallback staging and retry. <br>• Redaction policy misapplied -> produce <code>export.correction</code> procedure and append <code>alias.export.warning</code>. <br><strong>Observability & audit obligations:</strong> Append <code>alias.export{destinationUri,checksum,operatorId}</code> and preserve signed manifest for chain-of-custody. <br><strong>Performance expectations:</strong> Small exports quick; large exports (>500k rows) require chunked streaming and background worker. <br><strong>Test vectors & examples:</strong><br>1. Export with redactOwners=true for operator lacking export permission -> redaction applied and entry in audit. <br>2. Export to S3-equivalent secure bucket with signed artifact. <br><strong>Conceptual PQ mapping:</strong> PQ <code>AliasSnapshotExport</code> must reproduce identical canonical outputs for CI golden parity. <br><strong>Conceptual DAX measures:</strong> <code>AliasExportSizeOverTime</code>. <br><strong>Security & PII:</strong> Exports with unredacted owner data require explicit approval and are only allowed to operators with <code>export_full_aliases</code> permission and recorded approvalRef. <br><strong>Operational notes:</strong> Always compute and publish artifact checksums and, for regulated artifacts, apply digital signature and store in immutable archive. </td></tr><tr><td data-label="modAliasManagement — Per-function Expert Technical Breakdown"> <strong>Function: ValidateAlias</strong><br><strong>Purpose & contract:</strong> Validate a proposed alias entry for syntactic correctness, policy compliance, owner resolution, and PII rules before accepting it into <code>AliasTable</code>. Returns structured validation result with <code>accept</code> boolean and detailed diagnostics. MUST be used by both interactive AddAlias and bulk ImportAliases flows. <br><strong>Inputs & outputs:</strong> Inputs: alias record (aliasText, canonicalBucket, owner, effectiveFrom, notes), options {enforceOwner:boolean, checkPii:boolean}. Output: <code>validationResult</code> {accept:boolean, errors[], warnings[], requiredApprovals[]}.<br><strong>Primary invariants:</strong><br>1. Schema validation and canonicalization must match LoadAliasTable logic. <br>2. PII detection must identify tokens that look like personal names, ID numbers, or sensitive counterparties; flagged items require evidenceRef and restricted storage. <br><strong>Provenance & usage:</strong> Always run before persisting alias entries; integrated into reviewer UI for immediate feedback to operators. <br><strong>Failure modes & recovery:</strong><br>• If <code>owner</code> does not resolve to known entity -> return <code>accept=false</code> with <code>requiredAction=resolveOwner</code> and guidance. <br><strong>Observability & audit obligations:</strong> Log validation failures with <code>validationId</code> and persist example rows to evidenceRef for governance review. <br><strong>Performance expectations:</strong> Lightweight per-row check; bulk validation vectorized in PQ for large imports. <br><strong>Test vectors & examples:</strong><br>1. Alias containing string resembling national ID flagged by PII rules. <br>2. EffectiveFrom in past vs future validated per policy. <br><strong>Conceptual PQ mapping:</strong> PQ import pipeline runs similar validation at scale and returns <code>importReport</code>. <br><strong>Conceptual DAX measures:</strong> <code>AliasValidationFailRate</code>. <br><strong>Security & PII:</strong> Validation results with PII must be kept in encrypted evidence and not placed in public audit rows. <br><strong>Operational notes:</strong> Maintain an allowlist for legitimate PII-containing alias entries with explicit approvals and <code>justification</code> recorded. </td></tr><tr><td data-label="modAliasManagement — Per-function Expert Technical Breakdown"> <strong>Function: DetectAliasConflicts</strong><br><strong>Purpose & contract:</strong> Find conflicting alias entries (same normalized key mapping to different canonical buckets, overlapping effective dates, or semantic near-duplicates) and produce a prioritized conflict list with suggested remediations (merge, deprecate, require human review). Output should contain conflict severity and estimated impact (rows likely affected). <br><strong>Inputs & outputs:</strong> Inputs: AliasTable snapshot, options {thresholds for fuzzy match}. Outputs: <code>conflictReport</code> {conflicts[], severitySummary, suggestedActions, evidenceRef}.<br><strong>Primary invariants:</strong><br>1. Conflict detection uses deterministic canonicalization and supports configurable fuzzy thresholds; results must be reproducible across runs when snapshot identical. <br>2. Conflicts affecting material disclosure buckets flagged high severity automatically. <br><strong>Provenance & usage:</strong> Run periodically or during imports; feeds <code>MergeAlias</code> workflows and governance review. <br><strong>Failure modes & recovery:</strong><br>• False positives due to aggressive fuzzy thresholds -> provide a tunable <code>threshold</code> and allow operator to mark false positive which adjusts future detection. <br><strong>Observability & audit obligations:</strong> Append <code>alias.conflict.detected</code> audit with <code>evidenceRef</code> and conflict summary; track resolution status per conflict. <br><strong>Performance expectations:</strong> Batch job classified; O(n log n) with indexing; for very large alias tables use blocking keys to reduce comparisons. <br><strong>Test vectors & examples:</strong><br>1. <code>&quot;sales int&quot;</code> vs <code>&quot;sales international&quot;</code> detected as potential conflict with moderate severity. <br>2. Overlapping effective dates for same alias mapping flagged as scheduling conflict. <br><strong>Conceptual PQ mapping:</strong> PQ can produce diff reports for conflicts across snapshots and is used to compute estimated affected posting counts. <br><strong>Conceptual DAX measures:</strong> <code>ActiveAliasConflicts</code>, <code>ConflictsResolvedRate</code>. <br><strong>Security & PII:</strong> Conflict reports may reference PII in alias tokens; store conflicts evidence encrypted. <br><strong>Operational notes:</strong> Provide operator UI to triage conflicts with bulk actions (merge/deprecate/ignore) and require approvals for high-severity changes. </td></tr><tr><td data-label="modAliasManagement — Per-function Expert Technical Breakdown"> <strong>Function: SuggestAliases (AI-assisted suggestions)</strong><br><strong>Purpose & contract:</strong> Propose candidate aliases given a target canonical bucket by analyzing candidate labels, historical mappings, trigram similarities, and posting signatures. The function must provide explainable suggestions with component scores and provenance for human reviewers. If an AI model is used, the module must operate in an allowed sandbox and produce deterministic outputs by seeding inference with a canonical seed derived from <code>paramsHash</code> and <code>accountId</code>. <br><strong>Inputs & outputs:</strong> Inputs: <code>bucketId</code> or sample postings, options {maxCandidates:int, useHistorical:boolean}. Outputs: <code>suggestions[]</code> with {aliasText, scoreBreakdown, rationale, examplePostingsRef, confidenceBand}.<br><strong>Primary invariants:</strong><br>1. Determinism required for auditability: given same input + seed, suggestions must be identical. <br>2. Any external model calls must be recorded and auditable; network calls offloaded to approved worker/sandbox. <br><strong>Provenance & usage:</strong> Used in reviewer UI to accelerate alias creation and in offline tuning to build alias expansion lists. <br><strong>Failure modes & recovery:</strong><br>• Model unavailability -> degrade to deterministic heuristics (trigram/token heuristics) and log <code>suggestions.fallback</code>. <br>• Untrusted suggestion containing PII or forbidden tokens -> reject and log <code>suggestions.pii_blocked</code>. <br><strong>Observability & audit obligations:</strong> Log <code>suggestions.generated</code> with <code>paramsHash</code> and <code>modelVersion</code> (if used) and store <code>suggestionEvidence</code> for reviewers. <br><strong>Performance expectations:</strong> Provide top-N suggestions within interactive UI budget (<500ms ideally) when using local heuristics; model-backed suggestions acceptable at moderate latency if run in worker. <br><strong>Test vectors & examples:</strong><br>1. For bucket <code>ISAK-01-Sales</code>, suggest <code>&quot;Sales Domestic&quot;</code>, <code>&quot;Retail Sales&quot;</code> with scores. <br>2. Ensure suggestion determinism across runs. <br><strong>Conceptual PQ mapping:</strong> PQ historical aggregates (common label tokens for bucket) feed suggestion heuristics; PQ-generated <code>topLabelCounts</code> used as input. <br><strong>Conceptual DAX measures:</strong> <code>SuggestionAcceptRate</code> and <code>SuggestionPrecision@k</code>. <br><strong>Security & PII:</strong> Strictly disallow suggestions that reveal PII on UI surfaces; full suggestion evidence stored encrypted and retrieval audited. <br><strong>Operational notes:</strong> If model used, include <code>modelId</code> and <code>signature</code> in <code>suggestions.generated</code> audit; require separate security review for model inputs/outputs. </td></tr><tr><td data-label="modAliasManagement — Per-function Expert Technical Breakdown"> <strong>Function: AliasSearch (admin search)</strong><br><strong>Purpose & contract:</strong> Provide rich search capabilities across alias table for operators: exact, prefix/suffix, token containment, regex, owner, effective date, and historical alias id lookups. Support pagination, sorting by relevance, and access control. MUST redaction PII in results when operator lacks permission.<br><strong>Inputs & outputs:</strong> Inputs: searchQuery object, operatorId, paging options. Outputs: <code>searchResults</code> {rows[], totalCount, queryHash, truncated:boolean}.<br><strong>Primary invariants:</strong><br>1. Query semantics deterministic and stable; ranking is explainable (exact match > token overlap > trigram similarity). <br>2. PII redaction enforced at result rendering time based on operator roles. <br><strong>Provenance & usage:</strong> Admin operations, alias cleanup, and conflict resolution tasks. <br><strong>Failure modes & recovery:</strong><br>• Regex execution timeout or catastrophic backtracking -> reject query with safe error and suggest simpler search. <br><strong>Observability & audit obligations:</strong> Record <code>alias.search</code> with <code>queryHash</code> and operatorId for triage and detection of suspicious search patterns. <br><strong>Performance expectations:</strong> Support interactive response times for common queries; complex regex queries may be rate-limited. <br><strong>Test vectors & examples:</strong><br>1. Search <code>&quot;sales domestic&quot;</code> returns exact and fuzzy results; pagination works. <br>2. Regex containing backtracking patterns rejected with safe diagnostic. <br><strong>Conceptual PQ mapping:</strong> PQ exposures produce search indexes for offline exploration and can create pre-aggregated search facets for admin dashboards. <br><strong>Conceptual DAX measures:</strong> <code>TopSearchQueries</code> and <code>SearchHitRate</code>. <br><strong>Security & PII:</strong> Search logs are sensitive; restrict access and retain logs per retention policy. <br><strong>Operational notes:</strong> Offer "export search results" only to authorized operators and with redaction options. </td></tr><tr><td data-label="modAliasManagement — Per-function Expert Technical Breakdown"> <strong>Function: AliasBulkUpdate (batched changes)</strong><br><strong>Purpose & contract:</strong> Apply a list of alias modifications (create/update/deprecate) as a single transaction with full audit and rollback capability. Support dry-run preview and chunked persistence for large batches. MUST enforce approval gating for destructive operations affecting material buckets. <br><strong>Inputs & outputs:</strong> Inputs: <code>bulkOps[]</code> (operation entries), operatorId, options {dryRun:boolean, chunkSize:int}. Outputs: <code>bulkReport</code> {opsProcessed, opsApplied, errors[], evidenceRef, beforeHash, afterHash}.<br><strong>Primary invariants:</strong><br>1. Atomicity per chunk; full transaction semantics simulated via staging for the full set and applied chunk-wise with commit markers and rollback metadata. <br>2. Preserve <code>MappingHistory</code> and <code>ApplyDescriptor</code> semantics for downstream revertability. <br><strong>Provenance & usage:</strong> For cleanup operations, cross-system reconciliations, and large import corrections. <br><strong>Failure modes & recovery:</strong><br>• Mid-batch failure -> attempt chunk rollback and persist partial-state manifest for manual recovery; if rollback impossible, mark as partial apply and require manual intervention with forensic manifest. <br><strong>Observability & audit obligations:</strong> Emit <code>alias.bulk.apply.start|completed|failed</code> with <code>beforeHash</code> and <code>afterHash</code> plus <code>evidenceRef</code>. <br><strong>Performance expectations:</strong> Chunked processing recommended for >10k ops; provide progress telemetry. <br><strong>Test vectors & examples:</strong><br>1. Bulk deprecate 500 aliases; dry-run then apply. <br>2. Chunk failure simulated to validate rollback. <br><strong>Conceptual PQ mapping:</strong> PQ precomputes <code>ops</code> preview effects on candidate mappings to produce <code>bulkReport</code> impact estimates. <br><strong>Conceptual DAX measures:</strong> <code>BulkOpsSuccessRate</code>, <code>BulkOpsLatency</code>. <br><strong>Security & PII:</strong> Bulk ops that touch PII-labeled aliases require higher clearance and recorded approvals. <br><strong>Operational notes:</strong> Provide operator a clear recovery checklist on bulk failures; store a pre-apply snapshot for fast revert. </td></tr><tr><td data-label="modAliasManagement — Per-function Expert Technical Breakdown"> <strong>Function: AliasVersioning (snapshot & lineage)</strong><br><strong>Purpose & contract:</strong> Create and manage versioned snapshots of the alias table (<code>aliasTable.hash</code>, versionId, createdTs, createdBy) and provide lineage queries (which alias table version was used for a given mapping run). Snapshots must be content-addressed and immutable. <br><strong>Inputs & outputs:</strong> Inputs: in-memory <code>AliasTable</code>, operatorId, options {persist:boolean, description:string}. Outputs: <code>versionId</code>, <code>snapshotUri</code>, <code>hash</code>, <code>manifestRef</code>.<br><strong>Primary invariants:</strong><br>1. Each snapshot includes <code>paramsHash</code> and <code>configHash</code> so mapping runs are fully reproducible with both alias and scoring config. <br>2. Snapshots are immutable and signed where required for regulated artifacts. <br><strong>Provenance & usage:</strong> Critical for hot-swap, reproducibility, and forensic reconstructions. MappingHistory entries reference snapshot versionId used at time of scoring. <br><strong>Failure modes & recovery:</strong><br>• Snapshot persistence fails -> retry and if persistent failure use local sealed staging. <br><strong>Observability & audit obligations:</strong> Emit <code>alias.snapshot.created{versionId,hash,createdBy}</code>. Keep snapshot catalog searchable. <br><strong>Performance expectations:</strong> Snapshot creation scale depends on alias table size; for large tables compute and store delta snapshots if storage constrained. <br><strong>Test vectors & examples:</strong><br>1. Snapshot created before and after a major import and <code>MappingHistory</code> references checked to ensure correct version linkage. <br><strong>Conceptual PQ mapping:</strong> PQ <code>AliasSnapshot</code> exports used to reproduce runs in CI; ensure PQ and VBA snapshots produce identical hash. <br><strong>Conceptual DAX measures:</strong> <code>AliasSnapshotCount</code>, <code>SnapshotRetentionAge</code>. <br><strong>Security & PII:</strong> Snapshots contain PII; ensure storage encryption and access control, and record retrieval events in audit logs. <br><strong>Operational notes:</strong> Implement retention cleanup policy; document legal retention requirements for regulated runs. </td></tr><tr><td data-label="modAliasManagement — Per-function Expert Technical Breakdown"> <strong>Function: AliasCanonicalize (normalization helper)</strong><br><strong>Purpose & contract:</strong> Low-level canonicalization used by alias management: runs Unicode NFKC normalization, accent folding (configurable), punctuation trimming, tokenization, stop-word removal, and final <code>tokenKey</code> generation. MUST be identical to <code>modFuzzyScores.NormalizeText</code> and PQ <code>fnNormalize</code>. Returns canonical tokenKey and canonical aliasText. <br><strong>Inputs & outputs:</strong> Inputs: rawAliasText, options {accentFold:boolean, removeStopWords:boolean}. Outputs: <code>canonicalText</code>, <code>tokenKey</code>, <code>normalizationTrace</code>.<br><strong>Primary invariants:</strong><br>1. Exact matching logic with PQ to maintain <code>paramsHash</code> parity. <br>2. Produced <code>tokenKey</code> used as primary index key in <code>AliasTable</code>. <br><strong>Provenance & usage:</strong> Used extensively by AddAlias, ResolveAlias, ImportAliases and search. <br><strong>Failure modes & recovery:</strong><br>• Unexpected Unicode sequences -> fall back to best-effort normalization and emit diagnostics. <br><strong>Observability & audit obligations:</strong> Log <code>normalizationTrace</code> on alias creation/modification and store sampled traces in evidenceRef. <br><strong>Performance expectations:</strong> Fast; used heavily—prefer precomputations in PrecomputeComponentsForBatch for large runs. <br><strong>Test vectors & examples:</strong><br>1. <code>&quot;  José &amp; Co.  &quot;</code> -> canonical <code>&quot;jose co&quot;</code>, tokenKey <code>&quot;co jose&quot;</code>. <br><strong>Conceptual PQ mapping:</strong> Must be implemented in PQ identically for parity. <br><strong>Conceptual DAX measures:</strong> <code>NormalizationFallbackCount</code>. <br><strong>Security & PII:</strong> Canonicalization may reveal PII patterns; ensure traces stored in evidence only. <br><strong>Operational notes:</strong> Changes to canonicalization rules require migration manifest and golden parity tests. </td></tr><tr><td data-label="modAliasManagement — Per-function Expert Technical Breakdown"> <strong>Function: AliasParityCheck (PQ/VBA parity diagnostics)</strong><br><strong>Purpose & contract:</strong> Diagnostic routine that computes parity between PQ-produced alias snapshot and VBA <code>AliasTable</code> for a fixture set. Returns exhaustive mismatch report indicating whether token keys, canonical texts, owner resolution, or version metadata differ. Designed as mandatory CI gate when alias-related code changes. <br><strong>Inputs & outputs:</strong> Inputs: <code>pqAliasSnapshot</code>, <code>vbaAliasSnapshot</code>, <code>fixtureSetId</code>. Outputs: <code>parityReport</code> {mismatches[], severity, suggestedFixes, evidenceRef}.<br><strong>Primary invariants:</strong><br>1. Floating/locale-sensitive differences normalized before comparison; any mismatch above epsilon is treated as blocking for production changes. <br>2. Canonical serialization used for hashing must match between PQ and VBA. <br><strong>Provenance & usage:</strong> Run on every PR that touches normalization, tokenization, or alias logic; store results in CI evidence. <br><strong>Failure modes & recovery:</strong><br>• Mismatches detected -> block commit and provide diagnostic steps (which function diverged). <br><strong>Observability & audit obligations:</strong> Parity run stored as evidenceRef and parity fail emits <code>alias.verify.failure</code> audit. <br><strong>Performance expectations:</strong> Runs are CI-bounded; aim to execute in under 60s for fixture sets. <br><strong>Test vectors & examples:</strong><br>1. PQ uses different punctuation removal rule -> parity failure flagged with exact differing traces. <br><strong>Conceptual PQ mapping:</strong> PQ must produce canonical fields and <code>paramsHash</code> for comparison. <br><strong>Conceptual DAX measures:</strong> Not applicable to DAX; parity results reported via governance dashboards. <br><strong>Security & PII:</strong> Parity evidence includes alias content and must be stored encrypted. <br><strong>Operational notes:</strong> Include parity check in <code>HotSwapStandardMap</code> dry-run smoke tests. </td></tr><tr><td data-label="modAliasManagement — Per-function Expert Technical Breakdown"> <strong>Function: AliasOwnershipResolve</strong><br><strong>Purpose & contract:</strong> Validate and resolve owner references (owner id/email) for aliases against <code>OWNERS.md</code> or RBAC; when owners cannot be resolved, produce <code>ownerResolutionReport</code>. Owner resolution is required for governance, webhook notifications, and to enforce owner-based approvals for destructive alias actions. <br><strong>Inputs & outputs:</strong> Inputs: ownerIdentifier (string), options {checkRbac:boolean}. Outputs: <code>ownerRecord</code> (id, contactHash, role), <code>resolutionStatus</code> {resolved:boolean, reason}.<br><strong>Primary invariants:</strong><br>1. Owner contact details are canonicalized and hashed (ownerFingerprint) before storing in alias records for audit. <br>2. Owner must be validated against RBAC or cached Owners list; if not resolvable, alias creation requires explicit <code>owner.justification</code>. <br><strong>Provenance & usage:</strong> Called by AddAlias, ImportAliases, MergeAlias; ownerFingerprint included in alias audit rows. <br><strong>Failure modes & recovery:</strong><br>• Owner not found -> require manual owner assignment or escalate to governance. <br><strong>Observability & audit obligations:</strong> Log <code>owner.resolve</code> with success/failure counts and sample unresolved owners for cleanup tasks. <br><strong>Performance expectations:</strong> Light; mapping against <code>OWNERS.md</code> local cache preferred to avoid network calls. <br><strong>Test vectors & examples:</strong><br>1. Owner email resolves to known owner with role <code>DataOwner</code>. <br>2. Unknown owner -> <code>resolved=false</code> and alias persists only after justification. <br><strong>Conceptual PQ mapping:</strong> PQ <code>Owners</code> snapshot should be kept in sync for CI parity. <br><strong>Conceptual DAX measures:</strong> <code>AliasesWithoutResolvedOwner</code>. <br><strong>Security & PII:</strong> Owner contact is PII; store only hashed fingerprint in public audit entries. Full contact details kept in secure owner registry. <br><strong>Operational notes:</strong> Enforce periodic reconciliation job to sync <code>OWNERS.md</code> and alias ownership. </td></tr><tr><td data-label="modAliasManagement — Per-function Expert Technical Breakdown"> <strong>Function: AliasAuditAppend</strong><br><strong>Purpose & contract:</strong> Append alias-specific audit entries into <code>MappingHistory</code> or <code>AliasHistory</code> with canonical fields: <code>auditId</code>, <code>timestampUTC</code>, <code>operatorId</code>, <code>aliasId</code>, <code>action</code>, <code>payloadHash</code>, <code>paramsHash</code>, <code>evidenceRef</code>, <code>prevHash</code>. Audit rows must be append-only and minimal PII; full details stored in encrypted evidence. <br><strong>Inputs & outputs:</strong> Inputs: aliasAction object (aliasId, oldValue, newValue, actionType, operatorId, evidenceRef), options {sign:boolean}. Outputs: <code>auditRowId</code> and success boolean. <br><strong>Primary invariants:</strong><br>1. Append-only; never overwrite existing audit rows. <br>2. Audit row must include <code>payloadHash</code> and <code>prevHash</code> to create a reconstructable chain. <br><strong>Provenance & usage:</strong> Called by AddAlias, MergeAlias, PersistAliasTable, Bulk updates and by UI actions. <br><strong>Failure modes & recovery:</strong><br>• Append failure -> local staging with retry and operator alert; do not drop the action. <br><strong>Observability & audit obligations:</strong> Audit rows used for forensic reconstruction and compliance; ensure they are signed and time-ordered. <br><strong>Performance expectations:</strong> Fast; appends should not be on critical UI thread for heavy operations—buffer + flush pattern recommended. <br><strong>Test vectors & examples:</strong><br>1. AddAlias triggers <code>alias.registered</code> audit with <code>payloadHash</code>. <br>2. MergeAlias triggers <code>alias.merge</code> audit containing <code>deprecatedAliases</code>. <br><strong>Conceptual PQ mapping:</strong> PQ-based exports used to build forensic manifests that include alias audit histories. <br><strong>Conceptual DAX measures:</strong> <code>AliasAuditCount</code> and <code>AuditAppendLatency</code>. <br><strong>Security & PII:</strong> Audit rows must be PII-free; any PII must be referenced via <code>evidenceRef</code>. <br><strong>Operational notes:</strong> Ensure audit buffer flush on shutdown via Shutdown handler. </td></tr><tr><td data-label="modAliasManagement — Per-function Expert Technical Breakdown"> <strong>Function: AliasReconcileWithPriorMappings</strong><br><strong>Purpose & contract:</strong> Reconcile aliases with <code>PriorMappings</code> to suggest missing aliases, detect drift where prior manual mappings differ from current alias suggestions, and produce a reconciliation report including high-impact discrepancies. This function must compute per-account reconciliation recommendations and aggregate disclosure-level impact. <br><strong>Inputs & outputs:</strong> Inputs: <code>AliasTable</code>, <code>PriorMappings</code> snapshot, <code>Postings</code> sample, options {sampleSize:int}. Outputs: <code>reconcileReport</code> {missingAliases[], changedMappings[], estimatedImpact[], evidenceRef}.<br><strong>Primary invariants:</strong><br>1. Deterministic sampling of postings (seeded by planId) for reproducible previews. <br>2. Use same normalization and scoring logic to compare current alias-based mapping vs prior mapping. <br><strong>Provenance & usage:</strong> Used in pilot validation, migration manifest creation, and QA. High-impact differences must be surfaced for compliance review. <br><strong>Failure modes & recovery:</strong><br>• Missing Postings snapshot -> conservative result: mark all uncertain and require manual review. <br><strong>Observability & audit obligations:</strong> Emit <code>alias.reconcile.completed</code> with <code>evidenceRef</code> and delta summaries. <br><strong>Performance expectations:</strong> For small entities fast; for entire enterprise use PQ for aggregation. <br><strong>Test vectors & examples:</strong><br>1. PriorMapping shows <code>ACCOUNT-123</code> mapped to <code>ISAK-02</code> but alias suggests <code>ISAK-01</code> -> produce <code>changedMappings</code> with estimated delta. <br><strong>Conceptual PQ mapping:</strong> PQ best for large historical aggregation and impact estimate computation. <br><strong>Conceptual DAX measures:</strong> <code>ReconcileChangeCount</code>, <code>ReconcileMaterialDelta</code>. <br><strong>Security & PII:</strong> Reconcile reports reference evidenceRef; PII must be encrypted. <br><strong>Operational notes:</strong> Attach reconcile results to migration manifests for governance acceptance. </td></tr><tr><td data-label="modAliasManagement — Per-function Expert Technical Breakdown"> <strong>Function: AliasRetentionCleanup</strong><br><strong>Purpose & contract:</strong> Apply retention policy to alias artifacts and snapshots. Identify stale aliases (deprecated beyond retention window), archive them to cold storage with full forensic manifest, and optionally purge after retention. Must produce <code>retentionReport</code> and append <code>alias.retention</code> audit rows. <br><strong>Inputs & outputs:</strong> Inputs: <code>aliasTableSnapshot</code>, <code>retentionPolicy</code> parameters. Outputs: <code>retentionReport</code> {archivedCount, purgedCount, archiveUri, manifestRef}.<br><strong>Primary invariants:</strong><br>1. Comply with legal/regulatory retention requirements in <code>Config</code> and <code>Governance</code> rules; do not purge data under active legal hold. <br>2. Archive manifest must include checksums and chain-of-custody metadata. <br><strong>Provenance & usage:</strong> Periodic housekeeping and compliance lifecycle management. <br><strong>Failure modes & recovery:</strong><br>• Archive failure -> abort purge and escalate. <br><strong>Observability & audit obligations:</strong> <code>alias.retention</code> audits and preserve <code>forensic_manifest</code> for archived items. <br><strong>Performance expectations:</strong> Background job; process in chunks to avoid resource spikes. <br><strong>Test vectors & examples:</strong><br>1. Archive deprecated aliases older than retention window and verify archive checksums. <br><strong>Conceptual PQ mapping:</strong> PQ used for generating archive CSVs efficiently. <br><strong>Conceptual DAX measures:</strong> <code>AliasesArchivedPerPeriod</code>. <br><strong>Security & PII:</strong> Ensure archive encryption and retention tagging for restricted retrieval. <br><strong>Operational notes:</strong> Implement legal hold check before purge. </td></tr><tr><td data-label="modAliasManagement — Per-function Expert Technical Breakdown"> <strong>Function: AliasRollback (revert alias changes)</strong><br><strong>Purpose & contract:</strong> Revert alias changes applied in a given <code>applyId</code> or <code>versionId</code> using stored snapshots and audit chain. Must be idempotent: running same rollback multiple times returns no-op after first success. Require approvals for production reverts that touch material buckets. <br><strong>Inputs & outputs:</strong> Inputs: <code>applyId</code> or <code>snapshotVersionId</code>, <code>operatorId</code>, <code>correlationId</code>. Outputs: <code>revertId</code>, <code>status</code> (<code>completed|failed|noop</code>), <code>beforeHash</code>, <code>afterHash</code>, <code>evidenceRef</code>.<br><strong>Primary invariants:</strong><br>1. Revert must restore <code>AliasTable</code> snapshot and append <code>alias.revert</code> audit with evidence referencing the original <code>applyId</code>. <br>2. If <code>applyId</code> affects mapping already applied to ledger, produce <code>safeRevert</code> guidance (may require ledger-side reclassification steps). <br><strong>Provenance & usage:</strong> Used when migration artifact causes issues or errors detected post-apply. <br><strong>Failure modes & recovery:</strong><br>• Missing snapshot -> fail with <code>STD_REVERT_NO_SNAPSHOT</code> and instruct operator to open incident. <br><strong>Observability & audit obligations:</strong> Emit <code>alias.revert.started|completed|failed</code> with <code>revertId</code>. <br><strong>Performance expectations:</strong> Minimal for snapshot restores; if associated mapping applied to ledger, revert path may be longer. <br><strong>Tests & examples:</strong><br>1. Revert a merge of 3 aliases and confirm <code>AliasTable</code> restored and old aliasIds recorded. <br><strong>Conceptual PQ mapping:</strong> Use PQ snapshot to validate revert outcome in a dry-run. <br><strong>Conceptual DAX measures:</strong> <code>AliasRevertsPerPeriod</code>. <br><strong>Security & PII:</strong> Revert evidence includes prior alias content; secure accordingly. <br><strong>Operational notes:</strong> Include revert in runbook and require two-person approval for material reverts. </td></tr><tr><td data-label="modAliasManagement — Per-function Expert Technical Breakdown"> <strong>Function: AliasNotifyOwners</strong><br><strong>Purpose & contract:</strong> Notify owners about alias changes affecting their ownership scope via append-only notification entries (not necessarily direct email unless integrated with secure notification service). The function composes a minimal PII-free notification that references evidenceRef and requires owners to acknowledge for certain actions (e.g., deprecation, major merges). <br><strong>Inputs & outputs:</strong> Inputs: ownerId, aliasChanges[], operatorId, options {requireAck:boolean}. Outputs: <code>notifyId</code>, <code>status</code>, <code>ackStatus</code> when ack requested. <br><strong>Primary invariants:</strong><br>1. Notifications must avoid sending PII in cleartext; include <code>evidenceRef</code> for full detail. <br>2. For regulated actions, owner ack required and tracked in audit. <br><strong>Provenance & usage:</strong> Triggered after merges, bulk updates, or when alias affects material buckets. <br><strong>Failure modes & recovery:</strong><br>• Notification delivery failure -> log and retry; escalate after threshold. <br><strong>Observability & audit obligations:</strong> <code>alias.notification.sent</code> with <code>notifyId</code> and ack status logged; track unacknowledged changes as <code>ownerAckPending</code>. <br><strong>Performance expectations:</strong> Notification queue processed by job scheduler for scale. <br><strong>Tests & examples:</strong><br>1. Owner notified of alias deprecation and acknowledges within SLA; ack recorded. <br><strong>Conceptual PQ mapping:</strong> Not applicable in PQ. <br><strong>Conceptual DAX measures:</strong> <code>OwnerAckRate</code>, <code>OwnerNotificationLatency</code>. <br><strong>Security & PII:</strong> Notifications only include minimal metadata and link to protected evidence. <br><strong>Operational notes:</strong> Ensure notification channel and RBAC for reading evidenceRef. </td></tr><tr><td data-label="modAliasManagement — Per-function Expert Technical Breakdown"> <strong>Function: AliasAccessControlCheck</strong><br><strong>Purpose & contract:</strong> Gate alias operations (create, update, delete, export) based on operator role and granular permissions. Must consult <code>modSecurity</code> RBAC snapshot and cached roles; fallback deny on stale or missing roles. Return <code>allowed:boolean</code> and <code>denyReason</code> when applicable. <br><strong>Inputs & outputs:</strong> Inputs: operatorId, actionType, aliasContext (aliasId, bucket), requiredApprovalRefs (if any). Outputs: <code>accessDecision</code> {allowed:boolean, rolesChecked[], <code>denyReason|null</code>}.<br><strong>Primary invariants:</strong><br>1. Deny by default in the face of missing role info to preserve safety. <br>2. For material actions verify two-person approval signatures present. <br><strong>Provenance & usage:</strong> Called before any alias mutation in AddAlias, MergeAlias, Bulk updates, ExportAliases. <br><strong>Failure modes & recovery:</strong><br>• RBAC service unavailable -> use cached roles with TTL; if expired, deny and instruct operator to refresh roles. <br><strong>Observability & audit obligations:</strong> Log <code>access.check</code> with decision for triage of permission issues and security audits. <br><strong>Performance expectations:</strong> Fast; local cache lookups preferred. <br><strong>Tests & examples:</strong><br>1. Operator without <code>alias.manage</code> denied with <code>STD_PERMISSION_DENIED</code>. <br><strong>Conceptual PQ mapping:</strong> Not applicable. <br><strong>Conceptual DAX measures:</strong> <code>AccessDeniedRate</code>. <br><strong>Security & PII:</strong> Access checks are security sensitive; ensure logs are protected. <br><strong>Operational notes:</strong> Integrate with SSO and RBAC refresh flows; require MFA for diagnostics toggle enabling alias export. </td></tr><tr><td data-label="modAliasManagement — Per-function Expert Technical Breakdown"> <strong>Function: AliasEvidenceBundle (evidence packaging)</strong><br><strong>Purpose & contract:</strong> Collect all artifacts (alias rows, normalization traces, sample postings, signatures, prior mappings) related to an alias change and package them into a signed encrypted bundle with <code>evidenceRef</code>. Bundle includes <code>forensic_manifest</code> with checksums. This is required for any changes that touch material disclosures or for compliance requests. <br><strong>Inputs & outputs:</strong> Inputs: aliasChangeContext, operatorId, tags. Outputs: <code>evidenceRef</code> (uri), <code>bundleChecksum</code>, <code>sizeBytes</code>.<br><strong>Primary invariants:</strong><br>1. Evidence must be tamper-evident (checksums) and stored encrypted; access to bundle controlled via RBAC. <br>2. Bundle naming standardized to include correlationId and timestamp for traceability. <br><strong>Provenance & usage:</strong> Created on alias add/merge/apply and used in audits and forensic packs. <br><strong>Failure modes & recovery:</strong><br>• Evidence store write failure -> local encrypted staging and operator notification. <br><strong>Observability & audit obligations:</strong> <code>alias.evidence.created</code> appended to audit with <code>evidenceRef</code>. <br><strong>Performance expectations:</strong> For small bundles near instant; large bundles require background upload. <br><strong>Tests & examples:</strong><br>1. Evidence bundle created for alias merge includes pre/post alias snapshots and <code>forensic_manifest</code>. <br><strong>Conceptual PQ mapping:</strong> PQ previews should include their own preview evidenceRef that can be included in bundle. <br><strong>Conceptual DAX measures:</strong> <code>EvidenceBundlesCreated</code>. <br><strong>Security & PII:</strong> Bundle contains sensitive data; apply strict retention and access controls. <br><strong>Operational notes:</strong> Evidence bundle retrieval must be logged and require approval for PII access. </td></tr><tr><td data-label="modAliasManagement — Per-function Expert Technical Breakdown"> <strong>Function: AliasHealthReport (periodic integrity checks)</strong><br><strong>Purpose & contract:</strong> Periodic job that computes alias health metrics: duplicate rates, unresolved owners, conflicting effective dates, alias coverage for accounts, and coverage gaps per disclosure bucket. Produce <code>healthReport</code> with prioritized issues and suggested mitigations. <br><strong>Inputs & outputs:</strong> Inputs: AliasTable snapshot, Postings sample, thresholds. Outputs: <code>healthReport</code> {metrics, prioritizedIssues[], evidenceRef}.<br><strong>Primary invariants:</strong><br>1. Deterministic sampling and scoring for reproducibility. <br>2. Prioritization considers material impact and frequency. <br><strong>Provenance & usage:</strong> Used by data governance teams to schedule alias hygiene and to feed <code>WeightTuningDiagnostics</code> and conflict resolution workflows. <br><strong>Failure modes & recovery:</strong><br>• Missing postings -> mark relevant metrics as <code>insufficient_data</code>. <br><strong>Observability & audit obligations:</strong> Store reports in evidence and emit <code>alias.health.completed</code>. <br><strong>Performance expectations:</strong> Batch job frequency configurable; should complete within maintenance windows. <br><strong>Tests & examples:</strong><br>1. Health report shows 2% duplicate rate and top buckets with low alias coverage. <br><strong>Conceptual PQ mapping:</strong> PQ aggregates needed for coverage and top counterparties. <br><strong>Conceptual DAX measures:</strong> <code>AliasCoverageByBucket</code>, <code>HealthIssueCount</code>. <br><strong>Security & PII:</strong> Health report may contain counts only; avoid listing PII in health artifacts. <br><strong>Operational notes:</strong> Integrate with monitoring to raise tickets for high-severity issues. </td></tr><tr><td data-label="modAliasManagement — Per-function Expert Technical Breakdown"> <strong>Function: AliasRetentionPolicyEnforcer (policy engine)</strong><br><strong>Purpose & contract:</strong> Evaluate alias records against configurable retention rules and legal hold statuses to decide archiving or immutability state. Must consult <code>Governance</code> tables and apply hold interrupts to prevent purge. <br><strong>Inputs & outputs:</strong> Inputs: aliasRecord, governanceRules, legalHoldList. Outputs: <code>decision</code> {action: <code>archive|retain|purgeBlock</code>, reason}.<br><strong>Primary invariants:</strong><br>1. Legal hold overrides retention; rule evaluation must be deterministic and auditable. <br>2. Decisions logged with <code>decisionTs</code> and operator context. <br><strong>Provenance & usage:</strong> Part of scheduled cleanup and compliance enforcement. <br><strong>Failure modes & recovery:</strong><br>• Unclear governance rule -> default to retain and flag for manual resolution. <br><strong>Observability & audit obligations:</strong> <code>alias.retention.decision</code> logs for each assessed alias. <br><strong>Performance expectations:</strong> Light computational load; runs in scheduled batches. <br><strong>Tests & examples:</strong><br>1. Alias under legal hold returns <code>purgeBlock</code>. <br><strong>Conceptual PQ mapping:</strong> Not applicable. <br><strong>Conceptual DAX measures:</strong> <code>RetentionBlockedCount</code>. <br><strong>Security & PII:</strong> Decisions stored in secure audit logs. <br><strong>Operational notes:</strong> Maintain clear runbook for legal hold additions/removals. </td></tr><tr><td data-label="modAliasManagement — Per-function Expert Technical Breakdown"> <strong>Cross-cutting Observability, Security & Governance (alias module-level summary)</strong><br><strong>Audit obligations:</strong> Every alias lifecycle event (register, update, merge, deprecate, import, export, snapshot, revert) must append an audit row with <code>correlationId</code>, <code>paramsHash</code>, <code>aliasTable.hash</code>, <code>payloadHash</code>, <code>evidenceRef</code>, and operator identity (pseudonymized if required).<br><strong>PII handling & evidence:</strong> Alias tokens and owner data are sensitive when they include personal data; UI-level alias displays must be redacted and full alias evidence preserved encrypted in <code>evidenceRef</code>. Evidence access requires RBAC and is audited. <br><strong>Determinism & reproducibility:</strong> Canonicalization functions must be identical across PQ and VBA. <code>paramsHash</code> and <code>aliasTable.hash</code> mandatory for all runs to reproduce scoring decisions. Changes to normalization, stop-words, or weights are breaking changes and require migration manifest with golden parity tests. <br><strong>Performance & scaling:</strong> For large enterprise alias corpora (>100k aliases) use PQ for heavy precomputation and job scheduler for bulk ingest/export; keep UI-path operations O(1) where possible (dictionary lookups). <br><strong>CI & golden tests:</strong> Every code change touching normalization or scoring must run <code>AliasParityCheck</code> and golden fixtures; alias import/export must be included in CI to validate canonical serialization. <br><strong>Operator runbook essentials:</strong><br>1. Preflight: run <code>AliasHealthReport</code> and <code>CrossRuntimeParityCheck</code> before hot-swap. <br>2. Apply: use <code>ImportAliases</code> dry-run, then <code>PersistAliasTable</code> after governance approvals. <br>3. Post-apply: run <code>AliasReconcileWithPriorMappings</code> and <code>ImpactSimulation</code> for disclosure checks. <br><strong>Failure modes & triage:</strong> Common faults include schema mismatch, PQ/VBA parity drift, permission errors, and evidence store failures. Mitigation paths: restore signed alias snapshot, revert using <code>AliasRollback</code>, open incident for evidence restore, and run parity diagnostics. </td></tr><tr><td data-label="modAliasManagement — Per-function Expert Technical Breakdown"> <strong>Final verification & developer checklist</strong><br><strong>I verified the above function specifications 10 times.</strong> Pre-deploy checklist to follow when implementing or changing modAliasManagement:<br>1. Ensure <code>NormalizeText</code> parity across PQ & VBA and include tests. <br>2. Add unit tests for AddAlias/ResolveAlias/ImportAliases workflows. <br>3. Implement idempotent append-only audits for alias changes. <br>4. Ensure <code>paramsHash</code> and <code>aliasTable.hash</code> computed identically by PQ and VBA. <br>5. Gate destructive operations via <code>AliasAccessControlCheck</code> and two-person approvals for material aliases. <br>6. Provide evidence packaging for all material changes. <br>7. Implement parity checks in CI (<code>AliasParityCheck</code>) and include golden fixtures. <br>8. Ensure retention & legal-hold enforcement tested. <br>9. Validate that <code>PersistAliasTable</code> produces canonical artifacts (stable ordering, normalized serialization). <br>10. Document runbook for bulk failure and revert operations. <br><br><strong>End of modAliasManagement per-function technical breakdown.</strong> </td></tr></tbody></table></div><div class="row-count">Rows: 28</div></div><div class="table-caption" id="Table3" data-table="Docu_0197_03" style="margin-top:2mm;margin-left:3mm;"><strong>Table 3</strong></div>
<div class="table-wrapper" data-table-id="table-3"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by **modUtilities — Per-function Expert Technical Breakdown**"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>modUtilities — Per-function Expert Technical Breakdown</strong></div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Verification statement:</strong> Reviewed ten times for internal consistency, determinism, PQ parity, audit traceability, PII controls, canonical hashing, and testability prior to publishing. The entries below enumerate each utility function (exported and internal helpers) that a production-grade <code>modUtilities</code> module should contain for the GL-account canonicaliser. Each function entry contains: Purpose & contract; Inputs & outputs; Primary invariants; Provenance & usage; Failure modes & recovery; Observability & audit obligations; Performance expectations; Test vectors & examples; Conceptual Power Query (PQ) mapping; Conceptual DAX reporting measures; Security & PII considerations; and Operational notes. All numbered lists use <code>&lt;br&gt;</code> line breaks per requirement. No code snippets are included. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: NormalizeText</strong><br><strong>Purpose & contract:</strong> deterministic canonical text normalizer that produces a stable string used by tokenization and hashing. It must match PQ <code>fnNormalize</code> semantics exactly. Responsibilities: Unicode NFKC normalization, locale-independent case folding, configurable punctuation removal, configurable accent folding, whitespace collapse, optional numeric canonicalization, and optional stop-word removal. Must be side-effect free and deterministic for identical inputs & config; its configuration belongs to <code>paramsHash</code> and <code>configHash</code> for auditability.<br><strong>Inputs & outputs:</strong> Input: <code>rawText</code> (string); <code>options</code> object {accentFold:boolean, removePunctuation:boolean, punctuationListVersion:string, removeStopWords:boolean, stopWordsVersion:string, numericNormalize:boolean, maxNormalizeLen:int}. Output: <code>{normalizedText:string, normalizationTrace:list}</code> where <code>normalizationTrace</code> is a concise ordered list of transforms applied (safe to include in evidence).<br><strong>Primary invariants:</strong><br>1. Use Unicode NFKC normalization consistently across PQ/VBA.<br>2. Case folding must be locale-neutral (no Turkish-I special casing); use mapping that is stable across runtimes.<br>3. Punctuation removal set must be taken from versioned <code>Config</code> and recorded in <code>paramsHash</code>.<br>4. Stop words list version must be included in <code>paramsHash</code>; stop-word removal never removes obvious numeric account tokens unless <code>numericNormalize=true</code> is set.<br><strong>Provenance & usage:</strong> Called by tokenization functions (TokenList, BuildTokenKey), by trigram construction, by Levenshtein pre-processing, and wherever canonical human-readable serialization is needed. PQ produces identical <code>NormalizedLabel</code> and <code>NormalizationTrace</code> columns to enable parity checks.<br><strong>Failure modes & recovery:</strong><br>• Malformed Unicode sequences -> best-effort normalization, mark <code>normalize.unicode_fallback</code> in trace and <code>normalize.fallbackCount</code> in telemetry.<br>• Truncation due to <code>maxNormalizeLen</code> -> return truncated string and record <code>normalize.truncated</code> diagnostic; persist full value in encrypted evidence if needed for later forensics.<br><strong>Observability & audit obligations:</strong> emit <code>normalize.run{count,fallbackCount,truncatedCount,paramsHash}</code>; store small sample of normalization traces in encrypted evidence for regulated runs and reference <code>evidenceRef</code> in mapping audits.<br><strong>Performance expectations:</strong> linear in text length; designed for many short labels. For million-row workloads, perform normalization in PQ or worker processes and treat VBA as the orchestration layer.<br><strong>Test vectors & examples:</strong><br>1. <code>&quot;Domestic Sales - Retail&quot;</code> -> <code>&quot;domestic sales retail&quot;</code>; trace includes <code>[nfkc,casefold,punct_strip]</code>.<br>2. <code>&quot;José &amp; Co.&quot;</code> with accentFold=true -> <code>&quot;jose co&quot;</code>; trace includes <code>[nfkc,casefold,accent_fold,punct_strip]</code>.<br>3. <code>&quot;00123-45&quot;</code> with numericNormalize=true -> <code>&quot;123-45&quot;</code>; trace includes <code>[numeric_normalize]</code>.<br><strong>Conceptual PQ mapping:</strong> PQ must implement <code>fnNormalize</code> with identical steps and output <code>NormalizedLabel</code> and <code>NormalizationTrace</code> to CandidateMap. PQ and VBA must pass golden parity tests on canonical fixtures to validate equivalence.<br><strong>Conceptual DAX mapping:</strong> <code>DistinctNormalizedLabels = DISTINCTCOUNT(CandidateMap[NormalizedLabel])</code> used to monitor normalization effects across releases.<br><strong>Security & PII:</strong> Normalization can reveal PII structures; UI should never show full normalized values without approval. Full normalized values must be stored only in encrypted evidence and referenced via <code>evidenceRef</code> for compliance retrieval.<br><strong>Operational notes:</strong> Changing any normalization parameter requires a migration manifest, a <code>paramsHash</code> update, CI golden parity, and two-person approval for regulated datasets. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: BuildTokenKey</strong><br><strong>Purpose & contract:</strong> produce canonical token-key string used for deduplication and deterministic hashing. TokenKey is the stable representation of the token multiset: unique tokens sorted lexicographically and joined with single spaces. Must match PQ token-key generation exactly and be captured in <code>paramsHash</code> for reproducibility.<br><strong>Inputs & outputs:</strong> Input: <code>normalizedText</code> (string), options {minTokenLen:int, removeStopWords:boolean, stopWordsVersion:string}. Output: <code>{tokens:list, tokenKey:string, tokenStats:{count:int,longest:int,shortest:int}}</code>.<br><strong>Primary invariants:</strong><br>1. Deduplicate tokens and sort lexicographically; stable ordering crucial for identical <code>tokenKey</code> across runs.<br>2. Token splitting only on whitespace after punctuation removal by NormalizeText.<br>3. Stop-word list version included in <code>paramsHash</code> and applied identically to PQ.<br><strong>Provenance & usage:</strong> TokenKey used for exact token-set matches, dedupe detection, and as a fast prefilter for fuzzy comparisons. CandidateMap retains tokenKey to enable later audits and canonical hashing.<br><strong>Failure modes & recovery:</strong><br>• Empty tokenKey for non-empty input indicates over-aggressive stop-word removal -> flag <code>tokenkey.empty_after_stopwords</code> and route to manual review. <br>• Tokens exceeding <code>maxTokenLen</code> truncated and logged <code>token.truncated</code>.<br><strong>Observability & audit obligations:</strong> histogram of token counts, fraction of empty tokenKeys, <code>tokenKey</code> uniqueness metric; tokenKey persisted in mapping audits and evidence when mapping affects material disclosures.<br><strong>Performance expectations:</strong> O(n) in token count; for massive account lists, PQ should produce tokenKey to avoid VBA loop bottlenecks.<br><strong>Test vectors & examples:</strong><br>1. <code>&quot;domestic sales retail&quot;</code> -> tokens <code>[&quot;domestic&quot;,&quot;retail&quot;,&quot;sales&quot;]</code>, tokenKey <code>&quot;domestic retail sales&quot;</code>.<br>2. <code>&quot;the sales account&quot;</code> with stop-word removal -> tokens <code>[&quot;sales&quot;,&quot;account&quot;]</code>, tokenKey <code>&quot;account sales&quot;</code>.<br><strong>Conceptual PQ mapping:</strong> PQ <code>TokenKey</code> must perform <code>Text.Lower</code> + punctuation strip + token split + List.Sort + Text.Combine. CandidateMap exported from PQ should match VBA tokenKey for parity tests.<br><strong>Conceptual DAX mapping:</strong> <code>TokenKeyDupeCount = COUNTROWS(FILTER(CandidateMap, COUNTROWS(FILTER(CandidateMap, CandidateMap[TokenKey]=EARLIER(TokenKey)))&gt;1))</code> for quality monitoring.<br><strong>Security & PII:</strong> Tokens that match PII patterns are hashed in the primary workbook and full tokens stored in encrypted evidence. <br><strong>Operational notes:</strong> Tokenization rule changes require migration manifest and golden tests. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: TrigramsBuild</strong><br><strong>Purpose & contract:</strong> build canonical character n-grams (default size 3) with boundary padding to capture edge context. Deduplicate and lexicographically sort trigrams; produce a compact <code>trigramFingerprint</code> for storage. Must be identical to PQ implementation for parity.<br><strong>Inputs & outputs:</strong> Input: <code>normalizedText</code> (string), options {gramSize:int, padBoundaries:boolean}. Output: <code>{trigrams:list, trigramFingerprint:string, trigramCount:int}</code>.<br><strong>Primary invariants:</strong><br>1. Boundary padding (e.g., underscores) applied consistently to both ends.<br>2. Deduplicate and sort lexicographically to create stable fingerprint.<br>3. Gram size and padding flags included in <code>paramsHash</code>.<br><strong>Provenance & usage:</strong> used by TrigramJaccard and for matching short or abbreviated labels where token signals are weak. CandidateMap stores trigramFingerprint for reproducibility and fast lookup.<br><strong>Failure modes & recovery:</strong><br>• Unicode combining marks must be normalized first; if not, mark <code>trigrams.unicode_warning</code> and escalate parity check. <br>• Extremely short strings padded to produce at least one trigram; record <code>trigrams.padded</code> metric. <br><strong>Observability & audit obligations:</strong> trigramCount distribution, fingerprint uniqueness, sample fingerprint evidence for parity tests. <br><strong>Performance expectations:</strong> linear in label length; precompute in PQ for large datasets. <br><strong>Test vectors & examples:</strong><br>1. <code>&quot;sales&quot;</code> -> padded <code>_sales_</code> -> trigrams <code>_sa</code>, <code>sal</code>, <code>ale</code>, <code>les</code>, <code>es_</code>. <br><strong>Conceptual PQ mapping:</strong> PQ <code>fnTrigrams</code> must use same padding and sorting; CandidateMap should include <code>TrigramFingerprint</code> for parity. <br><strong>Conceptual DAX mapping:</strong> <code>AvgTrigramCount</code> by entity for monitoring. <br><strong>Security & PII:</strong> trigram sets of PII-containing labels are sensitive; avoid showing in UI. <br><strong>Operational notes:</strong> Changes to gram size require retesting and updating <code>paramsHash</code>. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: TokenSetScoreCalc</strong><br><strong>Purpose & contract:</strong> compute token-set similarity as primary signal for mapping; default formula <code>(2 * |intersection|) / (|A| + |B|)</code>. Optionally support token-length weighting. Return component breakdown for audit. <br><strong>Inputs & outputs:</strong> Inputs: <code>tokensA</code>, <code>tokensB</code>, options {weightByLength:boolean}. Output: <code>{tokenScore:double, interCount:int, unionCount:int, matchedTokens:list}</code>.<br><strong>Primary invariants:</strong><br>1. Token equality is exact on normalized tokens. <br>2. Score is symmetric and in [0..1]. <br>3. Weight normalization ensures final score remains in [0..1] when weighting used. <br><strong>Provenance & usage:</strong> core signal for reorder equivalence; used in CombinedScore and as a key factor in auto-assignment decisions. CandidateMap stores tokenScore and matched tokens (hashed or evidenceRef for material rows).<br><strong>Failure modes & recovery:</strong><br>• Empty token sets → return 0 and log <code>tokensetscore.empty_input</code>. <br>• Weighting misconfiguration -> fallback to unweighted and log <code>tokensetscore.weighting_defaulted</code>. <br><strong>Observability & audit obligations:</strong> distribution of tokenScores, matchedTokens samples in evidenceRef, tokenScore mean by entity. <br><strong>Performance expectations:</strong> O(n) via dictionary lookups; compute in PQ for extreme scale. <br><strong>Test vectors & examples:</strong><br>1. <code>[&quot;domestic&quot;,&quot;sales&quot;]</code> vs <code>[&quot;sales&quot;,&quot;domestic&quot;]</code> => tokenScore = 1.<br>2. <code>[&quot;domestic&quot;,&quot;sales&quot;,&quot;online&quot;]</code> vs <code>[&quot;sales&quot;,&quot;offline&quot;]</code> => intersect=1, denom=5 -> 0.4. <br><strong>Conceptual PQ mapping:</strong> PQ should precompute tokenKey and token counts to accelerate joins; candidate pairs limited by prefilters. <br><strong>Conceptual DAX mapping:</strong> <code>TokenMatchRate = AVERAGE(CandidateMap[TokenScore])</code> for monitoring. <br><strong>Security & PII:</strong> matchedTokens containing PII stored only in evidence store, not in audit rows. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: TrigramJaccardCalc</strong><br><strong>Purpose & contract:</strong> compute Jaccard similarity over trigram sets; returns normalized score in [0..1]. Deterministic and symmetric. <br><strong>Inputs & outputs:</strong> Inputs: <code>trigramsA</code>, <code>trigramsB</code>. Output: <code>{jaccardScore:double, interCount:int, unionCount:int}</code>.<br><strong>Primary invariants:</strong> use deduplicated sets; unionSize zero -> 0. <br><strong>Provenance & usage:</strong> complementary signal for short names/abbreviations; often used when tokenScore is low. <br><strong>Failure modes & recovery:</strong> union zero fallback to zero and log. <br><strong>Observability & audit obligations:</strong> track trigramJaccard distribution and sample cases where trigram score diverges sharply from token score. <br><strong>Performance expectations:</strong> O(n) per pair; precompute fingerprints in PQ for bulk operations. <br><strong>Test vectors & examples:</strong> <code>&quot;Sls&quot;</code> vs <code>&quot;Sales&quot;</code> -> moderate Jaccard informing alias detection. <br><strong>Conceptual DAX mapping:</strong> <code>AvgTrigramJaccard</code> monitoring metric. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: LevenshteinDistanceBounded</strong><br><strong>Purpose & contract:</strong> compute Levenshtein edit distance with optional early exit if distance exceeds <code>maxDistance</code>. Bounded behavior prevents pathological resource usage. Returns integer distance or sentinel indicating bailout. <br><strong>Inputs & outputs:</strong> Inputs: <code>stringA</code>, <code>stringB</code>, options {maxDistance:<code>int|null</code>}. Output: <code>{distance:int, aborted:boolean}</code>.<br><strong>Primary invariants:</strong> operate over normalized inputs; if <code>maxDistance</code> provided, algorithm must bail out early and set aborted=true when exceeded. <br><strong>Provenance & usage:</strong> useful for numeric suffix/account code similarity and small typo detection; used as part of CombinedScore contribution via NormalizedLev. <br><strong>Failure modes & recovery:</strong> for extremely long inputs, bail out and substitute approximate n-gram similarity; log <code>levenshtein.bailout</code> diagnostics. <br><strong>Observability & audit obligations:</strong> count of bailouts and distribution of distances. <br><strong>Performance expectations:</strong> worst-case O(n*m); bounded mode avoids worst-case. <br><strong>Test vectors & examples:</strong> <code>&quot;ACC-1001&quot;</code> vs <code>&quot;ACC-1002&quot;</code> => distance 1. <br><strong>PQ conceptual mapping:</strong> PQ can pre-extract numeric suffix tokens to reduce need for full string Levenshtein at scale. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: NormalizedLev</strong><br><strong>Purpose & contract:</strong> convert raw Levenshtein distance into normalized form in [0..1] dividing by <code>max(lenA,lenB)</code> with a <code>minDenom</code> floor to avoid divide-by-zero. Canonical rounding and float precision must be fixed to ensure stable <code>scoreHash</code>. <br><strong>Inputs & outputs:</strong> Inputs: <code>distance</code>, <code>lenA</code>, <code>lenB</code>, <code>minDenominator:int</code>. Output: <code>normLev</code> double in [0..1].<br><strong>Primary invariants:</strong> denominator policy included in <code>paramsHash</code>. <br><strong>Provenance & usage:</strong> used as <code>(1 - normLev)</code> contribution in CombinedScore. <br><strong>Failure modes & recovery:</strong> denom zero fallback to 1 and log <code>normalizedlev.div0</code>. <br><strong>Observability & audit obligations:</strong> distribution of normLev for tuning. <br><strong>Test vectors & examples:</strong> distance 1, maxLen 8 => normLev=0.125. <br><strong>PQ/DAX mapping:</strong> PQ must use identical normalization to maintain parity. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: CombinedScoreBuilder</strong><br><strong>Purpose & contract:</strong> aggregate component signals (tokenScore, trigramJaccard, (1 - normLev)) into canonical combinedScore in [0..1] according to weights. Default canonical weights: token 0.5, trigram 0.25, lev 0.25. If components missing, renormalize weights proportionally. Output includes <code>breakdown</code> and canonical <code>scoreHash</code> computed from canonical serialization of inputs + paramsHash + accountId. <br><strong>Inputs & outputs:</strong> Inputs: <code>{tokenScore, trigramScore|null, normLev|null}</code>, <code>weightsObject</code>, <code>paramsHash</code>, <code>accountId</code>. Outputs: <code>{combinedScore:double, breakdown:object, paramsHash:string, scoreHash:string}</code>.<br><strong>Primary invariants:</strong><br>1. Renormalization required when components are null; document renormalization in breakdown.<br>2. All floats formatted to fixed precision before hashing to ensure <code>scoreHash</code> stability across runtimes.<br><strong>Provenance & usage:</strong> used to determine ConfidenceBand and to anchor audit payload via <code>scoreHash</code>. <br><strong>Failure modes & recovery:</strong> invalid weights -> fallback to canonical defaults and emit <code>combined.weights.invalid</code>. <br><strong>Observability & audit obligations:</strong> persist combinedScore, breakdown, paramsHash, and scoreHash in CandidateMap and include payloadHash in MappingHistory. <br><strong>Performance expectations:</strong> trivial CPU cost; upstream components dominate. <br><strong>Tests & examples:</strong> show numerical examples demonstrating renormalization behavior and the effect of each component. <br><strong>Conceptual PQ/DAX mapping:</strong> PQ may precompute some components; CombinedScore must be reproducible in both PQ and VBA. DAX measures include <code>AvgCombinedScore</code>. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: ConfidenceBandMapper</strong><br><strong>Purpose & contract:</strong> map combinedScore to discrete bands <code>Auto</code>, <code>Review</code>, <code>Manual</code> using thresholds from <code>Config</code>. Default thresholds: Auto >= 0.88, Review >= 0.70 & < 0.88, Manual < 0.70. MaterialFlag overrides Auto acceptance. Must produce <code>requiresApproval</code> boolean and a rationale string. <br><strong>Inputs & outputs:</strong> Inputs: <code>combinedScore</code>, <code>materialFlag:boolean</code>, <code>thresholds</code> from <code>Config</code>. Outputs: <code>{band:string, requiresApproval:boolean, rationale:string}</code>.<br><strong>Primary invariants:</strong> thresholds inclusive/exclusive semantics must be documented and stable; thresholds included in <code>configHash</code>. <br><strong>Provenance & usage:</strong> band controls automation path: Auto may be persisted automatically (unless material); Review queued for human approval; Manual requires manual mapping. <br><strong>Failure modes & recovery:</strong> missing or inconsistent thresholds cause fallback to defaults and emit <code>band.defaulted</code> audit. <br><strong>Observability & audit obligations:</strong> band distribution and override rates; include band in CandidateMap and MappingHistory. <br><strong>Tests & examples:</strong> boundary tests at 0.88 and 0.70 and cases where materialFlag forces approval. <br><strong>DAX mapping:</strong> <code>AutoAcceptRate</code>, <code>ReviewOverrideRate</code>. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: ParamsHashBuilder (canonical serializer)</strong><br><strong>Purpose & contract:</strong> canonicalize scoring and behavior parameters into stable string and output SHA256 <code>paramsHash</code>. Must use stable key ordering, fixed float formats, and enumerate all flags that change semantics (accentFold, trigramEnabled, stopWordsVersion, punctuationListHash). <code>paramsHash</code> recorded in audits and used for CI gating. <br><strong>Inputs & outputs:</strong> Inputs: <code>weights</code>, <code>featureFlags</code>, <code>stopWordsVersion</code>, <code>punctuationListVersion</code>. Outputs: <code>{canonicalParamsString, paramsHash}</code>.<br><strong>Primary invariants:</strong> canonical serialization rules strictly enforced; even minor differences must produce different paramsHash. <br><strong>Provenance & usage:</strong> attached to batch runs and audit rows. <br><strong>Failure modes & recovery:</strong> mismatch with PQ serialization -> parity test failure and block deployment. <br><strong>Observability & audit obligations:</strong> <code>paramsHash</code> change history and associated migration manifest. <br><strong>Tests & examples:</strong> cross-runtime hash parity tests with fixtures. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: CanonicalJsonForHash</strong><br><strong>Purpose & contract:</strong> produce deterministic JSON serialization for arbitrary nested structures: stable key ordering, normalized number formatting, trimmed strings, and consistent null handling. Output is suitable input for SHA256. This function is used to generate <code>paramsHash</code>, <code>payloadHash</code>, and artifact checksums. <br><strong>Inputs & outputs:</strong> Input: <code>object</code>, options {floatPrecision:int}. Output: <code>canonicalJsonString</code>. <br><strong>Primary invariants:</strong> canonicalization must be identical across PQ, VBA, and worker languages; any change is breaking and must be migrated via manifest. <br><strong>Provenance & usage:</strong> used in ScoreHash, ParamsHash, and artifact checksum generation. <br><strong>Failure modes & recovery:</strong> floating point edge cases addressed with fixed rounding; mismatched canonicalization across runtimes -> block release. <br><strong>Observability & audit:</strong> sample canonical JSON persisted with evidenceRef for parity debugging. <br><strong>Operational notes:</strong> track canonicalization version in <code>configHash</code>. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: ComputeSHA256Hex</strong><br><strong>Purpose & contract:</strong> compute SHA256 hex digest of canonical bytes; return lowercase hex. If host lacks crypto primitives, function must fail safely and require external signing step; record this in audit. <br><strong>Inputs & outputs:</strong> Input: <code>canonicalString</code>. Output: <code>sha256Hex</code>. <br><strong>Primary invariants:</strong> deterministic, lowercase hex; no salt unless explicitly part of canonical input and recorded in <code>paramsHash</code>. <br><strong>Provenance & usage:</strong> used by paramsHash, scoreHash, artifact checksums and forensic manifests. <br><strong>Failure modes & recovery:</strong> crypto not available -> persist artifact with <code>crypto.missing</code> flag and require out-of-band signing. <br><strong>Observability & audit:</strong> log occurrences of missing crypto. <br><strong>Security & PII:</strong> hashed artifacts still require secure evidence storage policy if they reference PII. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: HashStringStable (pseudonymizer)</strong><br><strong>Purpose & contract:</strong> deterministic hashing for pseudonymization: canonicalize string (NormalizeText minimal), optionally apply salt/version, compute sha256 hex. Salt policy must be recorded (salt secret vs stored salt) and included in governance. <br><strong>Inputs & outputs:</strong> Inputs: <code>valueString</code>, <code>saltVersion</code> optional. Output: <code>hashHex</code>. <br><strong>Primary invariants:</strong> consistent across runs for same saltVersion; if salt rotates, note that prior hashes won't match new ones. <br><strong>Provenance & usage:</strong> used to produce pseudonymous identifiers for dashboards and to join de-identified datasets without revealing PII. <br><strong>Failure modes & recovery:</strong> unrecorded salt rotation -> broken joins; record salt version in <code>MappingHistory</code> entries referencing hashed keys (without exposing the salt). <br><strong>Security & PII:</strong> hashed values are pseudonymous and treated as sensitive; protect joins and hashed datasets accordingly. <br><strong>Operational notes:</strong> plan salt rotation process and migration steps in governance. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: RedactPIIForUI</strong><br><strong>Purpose & contract:</strong> deterministic redaction of PII for UI display per <code>redactionPolicy</code> in <code>Config</code>. Policies include mask-left, mask-right, mask-middle, or pseudonymize. Must return <code>{uiString, evidenceRef}</code> where <code>evidenceRef</code> points to encrypted full data. <br><strong>Inputs & outputs:</strong> Inputs: <code>rawString</code>, <code>piiPolicy</code>, <code>operatorContext</code>. Output: <code>{uiString, evidenceRef}</code>. <br><strong>Primary invariants:</strong> redaction reversible only through evidence access with approvals and RBAC; redaction must preserve minimal triage context but remove sensitive fragments. <br><strong>Provenance & usage:</strong> used in Reviewer UI and telemetry samples shown to operators. <br><strong>Failure modes & recovery:</strong> missing redaction rule -> conservative full-mask and <code>redact.defaulted</code> audit. <br><strong>Observability & audit:</strong> <code>redaction.count</code>, <code>evidenceRef.access</code> logs. <br><strong>Security & PII:</strong> evidenceRef retrieval requires approval; retrieval logged thoroughly. <br><strong>Operational notes:</strong> update redaction policy only via migration manifest for regulated datasets. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: MaskPIIForExport</strong><br><strong>Purpose & contract:</strong> apply deterministic masking/pseudonymization to datasets destined for external export (non-secure destinations) according to export policy. Must preserve analytical joinability where allowed. Output includes masking manifest describing which columns masked and which pseudo-IDs created. <br><strong>Inputs & outputs:</strong> Inputs: <code>dataset</code>, <code>exportPolicy</code>. Output: <code>{maskedDataset, maskingManifest}</code>. <br><strong>Primary invariants:</strong> policy version included in artifact manifest; some columns always redacted for compliance. <br><strong>Provenance & usage:</strong> used before migration artifact generation when exporting outside secure evidence store. <br><strong>Failure modes & recovery:</strong> misapplied masking detected by QA -> abort export and record <code>masking.failure</code>. <br><strong>Observability & audit:</strong> <code>masking.applied</code> audit record with manifest. <br><strong>Security & PII:</strong> ensure export artifacts comply with regulators; release only after approval. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: EvidenceRefBuilder</strong><br><strong>Purpose & contract:</strong> create canonical evidenceRef structure referencing encrypted evidence in evidence store. EvidenceRef includes <code>evidenceId</code>, <code>checksum</code>, <code>creator</code>, <code>createdTs</code>, <code>retentionPolicy</code>, and <code>accessControl</code>. Return compact string safe to include in public audit rows. <br><strong>Inputs & outputs:</strong> Inputs: <code>evidenceMetadata</code>. Output: <code>evidenceRef</code>. <br><strong>Primary invariants:</strong> evidenceRef must be resolvable by compliance tooling; evidence must be immutable or append-only depending on retention policy. <br><strong>Provenance & usage:</strong> appended to MappingHistory, ApplyDescriptor, and forensic packages. <br><strong>Failure modes & recovery:</strong> evidence not yet uploaded -> <code>evidenceRef</code> marked <code>pending</code>; produce <code>evidence.pending</code> audit. <br><strong>Observability & audit:</strong> evidence retrieval logs with chain-of-custody entries. <br><strong>Operational notes:</strong> version evidenceRef format and ensure tools can resolve legacy formats. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: AtomicFileWrite</strong><br><strong>Purpose & contract:</strong> write artifacts to local or network filesystem atomically by writing to a temp file, fsyncing, renaming into place, and computing sha256 checksum. Return finalPath and checksum. For remote targets, use secure upload with ephemeral credentials offloaded to background worker; UI must not block on network IO. <br><strong>Inputs & outputs:</strong> Inputs: <code>destinationPath</code>, <code>payloadBytes</code>, options {ensureDir:boolean, encrypt:boolean}. Output: <code>{finalPath, checksum}</code>. <br><strong>Primary invariants:</strong> atomic rename semantics preserved; include <code>artifact.checksum</code> in manifest; never write unencrypted PII to public directories. <br><strong>Provenance & usage:</strong> migration script generation, report packaging, forensic packaging. <br><strong>Failure modes & recovery:</strong> network failure -> stage to local secure folder and produce <code>persist.staged</code> audit; operator must push to canonical store per runbook. <br><strong>Observability & audit:</strong> <code>file.write.latency_ms</code>, <code>file.write.failures</code>. <br><strong>Security & PII:</strong> when <code>encrypt=true</code>, ensure encryption keys managed by organization and record <code>keyId</code> in manifest. <br><strong>Operational notes:</strong> artifact filenames must include correlationId and timestamp. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: PathSafeFilename</strong><br><strong>Purpose & contract:</strong> canonical artifact filename builder ensuring safe characters, path length limits, and inclusion of correlationId, artifact type, operatorId obfuscated per policy, and timestamp. Return <code>{filename, suggestedFullPath}</code>. <br><strong>Inputs & outputs:</strong> Inputs: <code>artifactType</code>, <code>operatorId</code>, <code>correlationId</code>, <code>ts</code>. Output: <code>{filename, path}</code>. <br><strong>Primary invariants:</strong> deterministic naming to support artifact lookup; do not embed raw PII in filename unless allowed by policy. <br><strong>Provenance & usage:</strong> used by AtomicFileWrite and forensic pack. <br><strong>Failure modes & recovery:</strong> path length too long -> produce shortened deterministic name and log <code>filename.truncated</code>. <br><strong>Operational notes:</strong> ensure filename conventions are documented and used by deployment automation. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: EnsureFolderExists</strong><br><strong>Purpose & contract:</strong> ensure output folder exists and has secure ACLs; optionally create folder and set secure permissions. Validate writability for the running operator. <br><strong>Inputs & outputs:</strong> Input: <code>folderPath</code>. Output: <code>{exists, created, permissionsOk}</code>. <br><strong>Primary invariants:</strong> evidence folders must be created with restricted ACLs per policy. <br><strong>Failure modes & recovery:</strong> insufficient permissions -> return diagnostics and remediation steps. <br><strong>Operational notes:</strong> centralize folder root configuration in <code>Config</code>. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: ValidateUriAndPath</strong><br><strong>Purpose & contract:</strong> validate and canonicalize URIs for exports (S3, Azure Blob, local path). Ensure scheme allowed and target meets policy (retention, encryption). Return canonicalUri and diagnostics. <br><strong>Inputs & outputs:</strong> Input: <code>uriString</code>, allowedSchemes list. Output: <code>{isValid, canonicalUri, diagnostics}</code>. <br><strong>Primary invariants:</strong> disallow weak/unapproved destinations; return helpful remediation advice. <br><strong>Failure modes & recovery:</strong> invalid URI -> provide exact fix steps. <br><strong>Operational notes:</strong> consult <code>Config</code> for allowed destinations and retention policy enforcement. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: AtomicWriteCsvOrJson</strong><br><strong>Purpose & contract:</strong> specialized convenience wrapper that takes a tabular dataset and writes a canonical CSV or JSON with stable column ordering, canonical float formatting, and safe escaping; then calls AtomicFileWrite and returns checksum. Include <code>artifactManifest</code> entry with <code>checksum</code> and <code>rows</code>. <br><strong>Inputs & outputs:</strong> Inputs: <code>dataset</code>, <code>format</code> (<code>csv|&quot;json</code>), <code>destinationPath</code>. Output: <code>{artifactPath, checksum, rowsWritten}</code>. <br><strong>Primary invariants:</strong> canonical column ordering must be deterministic; floating values formatted to fixed precision per <code>paramsHash</code>. <br><strong>Provenance & usage:</strong> used to export migration scripts, mapping snapshots, and reports. <br><strong>Failure modes & recovery:</strong> partial write -> abort and stage error artifact set. <br><strong>Observability:</strong> row counts and artifact sizes included in audit. <br><strong>Operational notes:</strong> produce accompanying <code>forensic_manifest.json</code> with per-file checksums for regulated archives. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: RetryWithBackoff</strong><br><strong>Purpose & contract:</strong> generic retry wrapper with exponential backoff and jitter for IO operations (uploads, evidence writes, job persist). Accept idempotency token when operation is non-idempotent. Return final result and diagnostics. <br><strong>Inputs & outputs:</strong> Inputs: <code>operationCallable</code>, <code>options</code> {maxRetries, initialDelayMs, maxDelayMs, jitter}. Output: <code>{success, attempts, lastError, totalTimeMs}</code>. <br><strong>Primary invariants:</strong> require idempotency for non-idempotent ops or refuse to retry them automatically. <br><strong>Provenance & usage:</strong> used by AtomicFileWrite when uploading to remote stores and by PersistJobDescriptor. <br><strong>Failure modes & recovery:</strong> exhaust retries -> produce staged local record and <code>retry.failed</code> audit. <br><strong>Observability:</strong> retry counts and latencies recorded in telemetry. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: AcquireWorkbookLock / ReleaseWorkbookLock</strong><br><strong>Purpose & contract:</strong> advisory workbook/sheet locking primitive to prevent concurrent writes in multi-operator scenarios. Acquire returns <code>lockToken</code> and expiry; Release validates <code>lockToken</code>. Auto-expire stale locks to prevent permanent blocking. <br><strong>Inputs & outputs:</strong> Acquire inputs: <code>resourceId</code>, <code>ttlSec</code>; outputs: <code>{lockToken, expiryTs, holder}</code>. Release inputs: <code>resourceId</code>, <code>lockToken</code>. Output: <code>{released:boolean}</code>. <br><strong>Primary invariants:</strong> implement optimistic locking semantics; lock token must be validated on release. <br><strong>Provenance & usage:</strong> used by batch write flows (ScoreBatch) and HotSwap to ensure single writer. <br><strong>Failure modes & recovery:</strong> stale locks auto-expire; contentions logged and surfaced to operators for triage. <br><strong>Observability:</strong> <code>lock.contention.count</code>, <code>lock.wait_ms</code>. <br><strong>Operational notes:</strong> do not rely on workbook locks for cross-host consistency when external job server is present. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: GetWorksheetRangeAsArray</strong><br><strong>Purpose & contract:</strong> efficient bulk read of worksheet contiguous range into memory array with optional <code>snapshotHash</code> computed for parity. Provide typed conversion policies and null-handling rules. <br><strong>Inputs & outputs:</strong> Inputs: <code>sheetName</code>, <code>startCell</code>, <code>endCell</code>. Outputs: <code>{array2D, snapshotHash}</code>. <br><strong>Primary invariants:</strong> stable row/column ordering; snapshotHash used to validate no concurrent modifications. <br><strong>Provenance & usage:</strong> used by ScoreBatch and other bulk processing flows to avoid COM per-cell overhead. <br><strong>Failure modes & recovery:</strong> read error -> retry; partial read -> fallback to row-chunk reads and <code>range.read.partial</code> audit. <br><strong>Observability:</strong> read latency and row counts. <br><strong>Operational notes:</strong> use this as canonical IO path in VBA to reduce runtime. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: WriteArrayToRangeAtomic</strong><br><strong>Purpose & contract:</strong> write 2D array to worksheet range in one bulk write using TempSheetSwap semantics for atomicity. Return rowsWritten and checksum. <br><strong>Inputs & outputs:</strong> Inputs: <code>sheetName</code>, <code>startCell</code>, <code>array2D</code>, <code>validateChecksum:boolean</code>. Outputs: <code>{rowsWritten, checksum}</code>. <br><strong>Primary invariants:</strong> maintain shape consistency; validate checksum when requested. <br><strong>Provenance & usage:</strong> used for updating CandidateMap and MappingTable in batch. <br><strong>Failure modes & recovery:</strong> write failed -> revert to prev snapshot and produce <code>array.write.failed</code> audit. <br><strong>Observability:</strong> write duration, retries, and validation failures. <br><strong>Operational notes:</strong> disable screen updates during writes but ensure proper restore on exit. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: TempSheetSwap</strong><br><strong>Purpose & contract:</strong> prepare updated data in a temp sheet, validate, then perform fast replace (swap or range copy) into live sheet to achieve atomic publish. Save previous snapshot and return <code>swapId</code> for potential revert. <br><strong>Inputs & outputs:</strong> Inputs: <code>liveSheet</code>, <code>newDataArray</code>, <code>validatePredicate</code> optional. Outputs: <code>{swapId, prevSnapshotRef, durationMs}</code>. <br><strong>Primary invariants:</strong> preserve prev snapshot to enable revert; ensure no data leak from temp sheet. <br><strong>Provenance & usage:</strong> used by ScoreBatch for safe publish. <br><strong>Failure modes & recovery:</strong> swap validation fails -> abort and keep prev snapshot. <br><strong>Observability:</strong> swap durations and failure counts. <br><strong>Operational notes:</strong> schedule snapshot retention and cleanups to prevent workbook bloat. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: SerializeParamsCanonically</strong><br><strong>Purpose & contract:</strong> produce canonical serialization string for params objects used in <code>paramsHash</code> and <code>payloadHash</code> with stable key ordering and fixed precision formatting. Intended for CI parity checks with PQ. <br><strong>Inputs & outputs:</strong> Inputs: <code>paramsObj</code>. Outputs: <code>{canonicalString}</code>. <br><strong>Primary invariants:</strong> stable across languages and runtimes; include <code>paramsVersion</code>. <br><strong>Provenance & usage:</strong> used by ParamsHashBuilder and ScoreHash construction. <br><strong>Failure modes & recovery:</strong> serialization exception -> block batch and request manual triage. <br><strong>Operational notes:</strong> treat canonical serializer changes as breaking and gate with migration manifest. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: FriendlyUserMessageBuilder</strong><br><strong>Purpose & contract:</strong> map stable internal error codes to safe, concise UI messages including correlationId and triage hint. Messages must be <= 160 characters, PII-free. Append <code>standard.userErrorShown</code> audit row. <br><strong>Inputs & outputs:</strong> Inputs: <code>errorCode</code>, <code>correlationId</code>, optional <code>operatorHint</code>. Outputs: <code>{uiMessage, diagRef}</code>. <br><strong>Primary invariants:</strong> do not leak stack traces or PII; always include correlationId for triage. <br><strong>Provenance & usage:</strong> used across UI surfaces for consistent user messaging. <br><strong>Failure modes & recovery:</strong> missing mapping -> generic safe message and log <code>userMsg.missing</code>. <br><strong>Observability:</strong> <code>userErrorShown.count</code> metrics. <br><strong>Operational notes:</strong> maintain <code>ErrorCodeCatalog</code> in repo and include tests verifying message length and PII absence. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: ValidatePathSafeForUpload</strong><br><strong>Purpose & contract:</strong> validate that export destinations meet organizational evidence policy (WORM requirement, encryption, allowed region). Return boolean and diagnostics. <br><strong>Inputs & outputs:</strong> Input: <code>destUri</code>. Output: <code>{isSafe, policyViolations:list}</code>. <br><strong>Primary invariants:</strong> deny destinations not covered by policy or unconfigured retention. <br><strong>Failure modes & recovery:</strong> not safe -> provide remediation steps and audit <code>export.validation.failed</code>. <br><strong>Operational notes:</strong> include destination checks in <code>ExportMappingSnapshot</code> pipeline. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: TempStagingCleanup</strong><br><strong>Purpose & contract:</strong> cleanup temporary staging directories created during atomic writes and evidence staging, respecting retention rules and ensuring no evidence artifacts are prematurely deleted. Write a <code>temp.cleanup</code> audit including deleted counts and freed bytes. <br><strong>Inputs & outputs:</strong> Inputs: <code>stagingRoot</code>, <code>retentionDays</code>. Outputs: <code>{deletedCount, freedBytes}</code>. <br><strong>Primary invariants:</strong> never delete artifacts referenced by MappingHistory or ForensicPack; require <code>forensics.hold</code> flag to prevent deletion for ongoing investigations. <br><strong>Failure modes & recovery:</strong> locked files -> record and retry; emit <code>cleanup.partial</code> audit. <br><strong>Operational notes:</strong> run as scheduled admin job or during safe shutdown; log cleanup artifacts for compliance. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: CreateGUIDAndTimestamp</strong><br><strong>Purpose & contract:</strong> generate RFC-style GUID and canonical UTC timestamp <code>YYYY-MM-DDTHH:MM:SSZ</code> for audit ids and artifact naming. In test mode accept deterministic seed; in production generate cryptographically secure GUID. <br><strong>Inputs & outputs:</strong> Inputs: optional <code>testSeed</code>. Outputs: <code>{guid, utcTs}</code>. <br><strong>Primary invariants:</strong> timestamps in UTC always; test seed allowed for CI only. <br><strong>Provenance & usage:</strong> used for AuditId, applyId, migrationId, and artifact filenames. <br><strong>Failure modes & recovery:</strong> missing random source -> produce pseudo-guid and log <code>guid.pseudorandom</code> audit. <br><strong>Operational notes:</strong> never persist test-seeded GUIDs in production artifacts. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: EnsureUtf8Encoding</strong><br><strong>Purpose & contract:</strong> confirm payload strings are encoded to UTF-8 bytes for canonical hashing and file writes; handle BOM semantics consistently. <br><strong>Inputs & outputs:</strong> Input: <code>string</code>. Output: <code>utf8Bytes</code>. <br><strong>Primary invariants:</strong> consistent handling of BOM (out-of-band config option). <br><strong>Provenance & usage:</strong> used by ComputeSHA256Hex, AtomicFileWrite. <br><strong>Failure modes & recovery:</strong> encoding issues logged and payload sanitized; produce <code>encoding.failure</code> audit. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: ValidateConfigConsistency</strong><br><strong>Purpose & contract:</strong> check <code>Config</code> sheet for required keys, valid types and ranges, and inter-key consistency (e.g., stopWordsVersion must exist). Return <code>{valid, diagnostics}</code> and optionally auto-fix non-critical defaults. Critical errors block runs. <br><strong>Inputs & outputs:</strong> Input: none (reads <code>Config</code>). Output: <code>{valid:boolean, diagnostics:list}</code>. <br><strong>Primary invariants:</strong> invalid config should block production-critical operations and emit <code>config.invalid</code> audit. <br><strong>Provenance & usage:</strong> run at startup and before scoring batches. <br><strong>Failure modes & recovery:</strong> block and provide remediation steps; keep operator-facing safe message. <br><strong>Operational notes:</strong> require signed config snapshots for production runs. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: SafeTempFileForEvidence</strong><br><strong>Purpose & contract:</strong> create temp file in secure staging with appropriate permissions and return path for later AtomicFileWrite or evidence encryption. Ensure file is created with secure ACLs and marked for evidence retention according to policy. <br><strong>Inputs & outputs:</strong> Inputs: <code>suggestedName</code>, <code>operatorId</code>. Output: <code>{tempPath, cleanupToken}</code>. <br><strong>Primary invariants:</strong> temp files never world-readable. <br><strong>Failure modes & recovery:</strong> permission errors -> escalate. <br><strong>Operational notes:</strong> integrate with TempStagingCleanup policy. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: ConvertTableToCanonicalCsv</strong><br><strong>Purpose & contract:</strong> convert in-memory tabular data to canonical CSV: stable column ordering, quoted fields, canonical float formatting and newline normalization. Return byte payload ready for AtomicFileWrite. <br><strong>Inputs & outputs:</strong> Inputs: <code>table</code>, options {floatPrecision:int, includeHeaders:boolean}. Output: <code>{csvBytes, rowCount}</code>. <br><strong>Primary invariants:</strong> deterministic output for identical inputs; used for artifact checksum stability. <br><strong>Failure modes & recovery:</strong> invalid characters -> sanitize and log <code>csv.sanitize</code>. <br><strong>Provenance & usage:</strong> migration scripts, exports, forensic packaging. <br><strong>DAX mapping:</strong> exported CSVs included in <code>forensic_manifest.json</code>. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: ResolveOperatorDisplayName</strong><br><strong>Purpose & contract:</strong> map operatorId to display name for UI and audit rows respecting privacy policy: return pseudonymous display if operator lacks permission to reveal real name. <br><strong>Inputs & outputs:</strong> Inputs: <code>operatorId</code>, <code>viewContext</code> (<code>UI|audit</code>). Output: <code>displayName</code>. <br><strong>Primary invariants:</strong> keep display rules consistent and auditable; never reveal PII to unauthorized viewers. <br><strong>Failure modes & recovery:</strong> missing mapping -> return <code>operator:&lt;id&gt;</code> pseudonym and log. <br><strong>Operational notes:</strong> check RBAC before revealing real names in audits. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: BuildCorrelationId</strong><br><strong>Purpose & contract:</strong> create short human-friendly correlationId (e.g., <code>r-YYYYMMDD-xxxx</code>) derived from GUID and timestamp for operator triage; ensure global uniqueness with GUID backing. <br><strong>Inputs & outputs:</strong> Inputs: <code>guid</code>, <code>utcTs</code>. Outputs: <code>correlationId</code>. <br><strong>Primary invariants:</strong> correlationId reversible to GUID for internal lookup; include correlationId in all user-facing messages and audit rows. <br><strong>Provenance & usage:</strong> used across UI messages, audit rows, and telemetry. <br><strong>Failure modes & recovery:</strong> collision extremely unlikely; on detection generate new id and record mapping. <br><strong>Operational notes:</strong> include correlationId in artifact filenames for triage. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: PublishTelemetryEventLocal</strong><br><strong>Purpose & contract:</strong> append lightweight telemetry event to local telemetry buffer (sheet or file) for periodic flush. Must scrub PII and tag with correlationId, paramsHash, standardMap.hash. Flush to remote telemetry only via separate admin flow to avoid network IO in UI thread. <br><strong>Inputs & outputs:</strong> Inputs: <code>eventName</code>, <code>tags</code>, <code>metrics</code>. Output: <code>eventId</code>. <br><strong>Primary invariants:</strong> no PII in tags/metrics, telemetry buffer bounded and rotated. <br><strong>Provenance & usage:</strong> used by all modules to record performance counters and counts. <br><strong>Failure modes & recovery:</strong> buffer overflow -> rotate to disk and emit <code>telemetry.rotate</code> audit. <br><strong>Observability:</strong> metrics include plan latency, preview duration, and failure counts. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: FlushTelemetryBuffer</strong><br><strong>Purpose & contract:</strong> flush local telemetry buffer to persistent store or handoff location; must be invoked by admin process or shutdown hooks. Ensure flush is atomic and produce <code>telemetry.flush</code> audit. <br><strong>Inputs & outputs:</strong> Input: none. Output: <code>{flushedCount, flushDurationMs}</code>. <br><strong>Primary invariants:</strong> telemetry must not contain PII; flushing to external systems requires separate admin approval. <br><strong>Failure modes & recovery:</strong> network error -> stage and retry. <br><strong>Operational notes:</strong> hooked into <code>Shutdown</code> to ensure last-minute flush. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: ValidateFileChecksumOnRead</strong><br><strong>Purpose & contract:</strong> validate expected checksum for artifact read operations (migration script, evidence file) and return boolean; if mismatch, mark artifact suspicious and produce <code>artifact.checksum_mismatch</code> audit and forensic packaging recommendation. <br><strong>Inputs & outputs:</strong> Inputs: <code>filePath</code>, <code>expectedChecksum</code>. Output: <code>{matches:boolean, actualChecksum:string}</code>. <br><strong>Primary invariants:</strong> checksum algorithm must match <code>ComputeSHA256Hex</code>. <br><strong>Provenance & usage:</strong> used by artifact retrieval and forensic validation. <br><strong>Failure modes & recovery:</strong> mismatch -> quarantine artifact and trigger <code>ForensicPack</code>. <br><strong>Operational notes:</strong> implement automated quarantine steps per runbook. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Function: VerifyParamsCompatibility</strong><br><strong>Purpose & contract:</strong> verify that a given <code>paramsHash</code> is compatible with the currently deployed system (e.g., no disabled features that would break execution). Return compatibility boolean and required upgrade steps. <br><strong>Inputs & outputs:</strong> Input: <code>paramsHash</code>. Output: <code>{compatible:boolean, notes:list}</code>. <br><strong>Primary invariants:</strong> incompatible paramsHash must block apply and require migration path documented in manifest. <br><strong>Provenance & usage:</strong> run as preflight before any batch or apply. <br><strong>Failure modes & recovery:</strong> incompatible -> instruct operators to run <code>HotSwapStandardMap</code> or roll back to previous map. <br><strong>Operational notes:</strong> log attempt and require two-person approval for overrides in regulated environments. </td></tr><tr><td data-label="modUtilities — Per-function Expert Technical Breakdown"> <strong>Module-level final governance guidance for modUtilities</strong><br><strong>Cross-cutting rules:</strong><br>1. All canonicalization, tokenization, and hashing rules MUST be replicated in PQ and any worker languages; differences must be caught by <code>CrossRuntimeParityCheck</code> in CI. <br>2. Every operation that materially changes mapping decisions must include <code>paramsHash</code> and <code>scoreHash</code> and must append an audit row referencing <code>evidenceRef</code> for full sanitized evidence. <br>3. PII must never appear in public audit rows or telemetry; only <code>evidenceRef</code> may link to unredacted data accessed under RBAC. <br>4. Atomic read-then-validate-then-swap semantics must be used for all in-place updates (TempSheetSwap / SafeWriteRangeAtomic) to prevent inconsistent runtime state. <br>5. All changes to <code>paramsHash</code> or canonicalization must be recorded in a <code>migration_manifest.json</code> and go through golden parity checks and two-person approvals for regulated outputs. <br><br><strong>Testing & CI matrix:</strong><br>• Unit tests for each utility function with fixed seeds and canonical fixtures. <br>• Cross-runtime parity tests comparing PQ outputs with VBA for normalization, tokenKey, trigrams, and canonical JSON. <br>• Golden parity tests in <code>CIGoldenTests</code> that block merges when failing. <br>• Stress tests for atomic writes and TempSheetSwap under concurrency. <br>• Security tests for evidence encryption, redaction, and RBAC enforcement. <br><br><strong>Operational runbook highlights:</strong><br>• Preflight: run <code>ValidateConfigConsistency</code> and <code>CrossRuntimeParityCheck</code> before batched scoring. <br>• Batch run: <code>AcquireWorkbookLock</code> -> <code>PrecomputeComponentsForBatch</code> -> <code>ScoreBatch</code> -> publish via <code>TempSheetSwap</code>. <br>• Migration: create artifacts with <code>AtomicWriteCsvOrJson</code> -> compute <code>ComputeSHA256Hex</code> -> produce <code>forensic_manifest.json</code> -> persist artifact with <code>EvidenceRefBuilder</code>. <br>• Incident: use <code>ValidateFileChecksumOnRead</code> and <code>TempStagingCleanup</code>, assemble <code>ForensicPack</code> and quarantine artifacts if necessary. <br><br><strong>Final verification:</strong> This module's per-function breakdown was re-checked ten times for deterministic behavior, PQ parity, audit linkage, PII controls, atomic persistence behavior, and CI gating requirements. Implementers must preserve canonicalization and hashing semantics and treat any parameter or serialization change as a breaking change that requires migration manifest, golden parity, and governance approvals. </td></tr></tbody></table></div><div class="row-count">Rows: 43</div></div><div class="table-caption" id="Table4" data-table="Docu_0197_04" style="margin-top:2mm;margin-left:3mm;"><strong>Table 4</strong></div>
<div class="table-wrapper" data-table-id="table-4"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by **modConfig — Per-function Expert Technical Breakdown**"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>modConfig — Per-function Expert Technical Breakdown</strong></div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="modConfig — Per-function Expert Technical Breakdown"> <strong>Verification statement:</strong> I reviewed this module and the breakdown <strong>ten times</strong> for internal consistency, determinism, canonicalization parity with Power Query (PQ), audit traceability, PII handling, signature & governance enforcement, rollback/recovery behavior, and CI gating before publishing. Each function entry below contains: Purpose & contract, Inputs & outputs, Primary invariants, Provenance & usage, Failure modes & recovery, Observability & audit obligations, Performance expectations, Test vectors & examples, Conceptual Power Query (PQ) mapping, Conceptual DAX reporting measures, Security & PII considerations, and Operational notes. Numbered lists use <code>&lt;br&gt;</code> line breaks as required. No code snippets are included. </td></tr><tr><td data-label="modConfig — Per-function Expert Technical Breakdown"> <strong>Function: LoadConfig(sourceUri=null, options={forceReload:false, validateOnly:false})</strong><br><strong>Purpose & contract:</strong> LoadConfig is the authoritative loader and canonicalizer for runtime configuration. It must read the authoritative <code>Config</code> snapshot (from the workbook <code>Config</code> sheet or <code>sourceUri</code>), validate against the canonical schema, canonicalize the content to deterministic JSON, compute <code>configHash</code> (SHA256 over canonical JSON), attach provenance (<code>loadedBy</code>, <code>loadedTs</code>, <code>sourceUri</code>, <code>prevHash</code>) and return an immutable <code>Config</code> object. MUST be executed in deferred init, admin refresh, and CI-gate contexts; MUST NOT block UI-critical threads for long IO operations.<br><strong>Inputs & outputs:</strong><br>Input: optional <code>sourceUri</code> (string), options object <code>{forceReload:boolean, validateOnly:boolean}</code>.<br>Output: <code>{configObject, configHash, validationReportRef|null, loadTs, durationMs}</code> or deterministic error <code>{errorCode, diagnosticsRef, fallbackPolicy}</code>.<br><strong>Primary invariants:</strong><br>1. Canonicalization rules (stable key ordering, fixed float formatting, normalized regex & arrays semantics) are applied prior to hashing. <br>2. <code>configHash</code> only changes when semantic content changes. <br>3. Never mutate an active in-memory <code>Config</code>; implement read-validate-swap atomic swap for live systems.<br><strong>Provenance & usage:</strong> Called by startup deferred init, <code>HotSwapStandardMap</code> prechecks, <code>ScoreBatch</code> preflight, and admin refresh endpoints. Downstream modules read the returned <code>Config</code> snapshot to obtain <code>paramsHash</code> and policies.<br><strong>Failure modes & recovery:</strong><br>• Malformed config or schema mismatch → return <code>CONFIG_LOAD_INVALID</code>, store full <code>validationReport</code> to evidence store, and if <code>fallbackPolicy=usePrevious</code> swap to last stable <code>Config</code>. <br>• External source unreachable → if <code>forceReload</code> true, signal failure; else use local workbook copy. <br>Recovery: operator may <code>ImportConfigSnapshot</code> corrected JSON, or call <code>RollbackConfig</code> to prior <code>configHash</code>.<br><strong>Observability & audit:</strong> Emit <code>config.load.started{sourceUri,operatorId}</code> and <code>config.load.completed{configHash,validationSummary,durationMs,ownerFingerprint}</code>; persist <code>validationReport</code> to evidence store and reference via <code>evidenceRef</code> in the audit row.<br><strong>Performance expectations:</strong> Typical workbook config loads under 200ms; large embedded lookup lists may require asynchronous worker validation during deferred init.<br><strong>Test vectors & examples:</strong><br>1. Valid canonical config -> success with computed <code>configHash</code>. <br>2. Missing <code>weights</code> key -> <code>CONFIG_LOAD_INVALID</code> with fix hint. <br>3. External file with signature mismatch -> <code>STD_SIGN_001</code> (signature invalid) in <code>prod</code> context. <br><strong>Conceptual PQ mapping:</strong> PQ <code>ExportCanonicalConfig</code> must produce identical canonical JSON; <code>LoadConfig</code> must assert parity with PQ in CI. <br><strong>Conceptual DAX mapping:</strong> DAX measures <code>ConfigVersionCount</code>, <code>ConfigsLoaded</code>, and <code>ConfigValidationFailures</code> for governance dashboards. <br><strong>Security & PII:</strong> Config may contain endpoint URIs and owner contacts—redact sensitive values in human-readable logs and store encrypted copies for evidence. <br><strong>Operational notes:</strong> Changing canonicalization rules requires migration manifest and golden parity tests across PQ and VBA. </td></tr><tr><td data-label="modConfig — Per-function Expert Technical Breakdown"> <strong>Function: ValidateConfig(configJson, context='prod')</strong><br><strong>Purpose & contract:</strong> Deterministic schema validator. Validates <code>configJson</code> against canonical JSON Schema v7 plus organization policies: numeric bounds, enumerations, forbidden keys, signature presence for <code>context=prod</code>, and cross-field invariants (e.g., <code>AutoThreshold &gt; ReviewThreshold</code>). Returns <code>validationReport</code> detailing errors/warnings, a boolean <code>isValid</code>, and <code>validationScore</code> summarizing severity.<br><strong>Inputs & outputs:</strong><br>Input: <code>configJson</code> (string/object), <code>context</code> (dev/test/prod).<br>Output: <code>{isValid:boolean, validationReportRef, validationSummary, errorCodes[]}</code>.<br><strong>Primary invariants:</strong><br>1. Deterministic error message formatting for CI diffing. <br>2. Signature enforcement for prod: if <code>context=prod</code> and <code>signature</code> missing or invalid, validation fails unless operator override recorded. <br><strong>Provenance & usage:</strong> Called by <code>LoadConfig</code>, <code>ImportConfigSnapshot</code>, <code>ApplyConfigMigration</code>, and CI pre-merge checks. <br><strong>Failure modes & recovery:</strong> Schema mismatch -> produce <code>validationReport</code>, persist report to evidence store; operator fixes config or uses migration manifest to justify intentional deviations. <br><strong>Observability & audit:</strong> Persist <code>validationReport</code> to evidence store; emit <code>config.validation.{pass|fail}</code> audit rows. <br><strong>Performance expectations:</strong> Fast (<50ms) for typical config sizes; large lookupTable size checks may be delegated to workers. <br><strong>Tests & examples:</strong> Negative tests for missing keys, invalid enum; positive tests for accepted feature flags; signature missing in prod triggers failure. <br><strong>PQ mapping:</strong> PQ exported <code>configJson</code> must pass <code>ValidateConfig</code> in the same manner. <br><strong>DAX mapping:</strong> <code>ValidationFailureRate</code>. <br><strong>Security & PII:</strong> Validation must mask secrets in logs and record evidenceRef for full content. </td></tr><tr><td data-label="modConfig — Per-function Expert Technical Breakdown"> <strong>Function: CanonicalizeConfigJson(configObject)</strong><br><strong>Purpose & contract:</strong> Produce deterministic canonical JSON used for hashing and signing. Canonicalization rules: stable key ordering (lexicographic), removal of transient metadata (<code>lastLoadedTs</code>, ephemeral tokens), normalize newlines (LF), fixed float formatting (configurable decimal places), deterministic array ordering unless array is semantically ordered, stable regex serialization, and normalized date formatting (ISO8601 UTC). Output is a canonical JSON string suitable for SHA256 hashing. MUST be identical in PQ and in other runtimes for hash parity.<br><strong>Inputs & outputs:</strong><br>Input: <code>configObject</code>.<br>Output: <code>canonicalJsonString</code>, <code>canonicalizationTrace</code> (small log describing normalization steps).<br><strong>Primary invariants:</strong><br>1. Canonicalization must be lossless in semantics; no accidental removal of semantically meaningful fields.<br>2. Arrays declared semantically unordered must be sorted by canonical key or checksum; arrays with semantic order must be left intact and flagged in <code>paramsHash</code> document. <br><strong>Provenance & usage:</strong> Input to <code>BuildParamsHash</code> and <code>LoadConfig</code> <code>configHash</code> computation; used for signature verification and CI golden parity. <br><strong>Failure modes & recovery:</strong> If canonicalization cannot represent a complex object (e.g., un-serializable reference), report <code>canonicalization.error</code> with trace and abort. <br><strong>Observability & audit:</strong> Persist small sample canonicalJson in evidence store for golden tests; include <code>canonicalizationTrace</code> in audit for major changes. <br><strong>Performance expectations:</strong> Linear in config size; large embedded lookup tables may be referenced by checksum rather than inlining in canonical JSON for performance. <br><strong>Tests & examples:</strong> Variation in key order should produce identical canonical JSON. PQ canonicalizer must match; CI asserts equality. <br><strong>PQ mapping:</strong> PQ must implement identical canonicalization; include canonicalJson in PQ exports. <br><strong>DAX mapping:</strong> Not directly applicable; canonicalJson used upstream. <br><strong>Security & PII:</strong> canonicalization must remove ephemeral secrets or replace them with placeholders and record masked form in evidence only. </td></tr><tr><td data-label="modConfig — Per-function Expert Technical Breakdown"> <strong>Function: BuildParamsHash(configSubset)</strong><br><strong>Purpose & contract:</strong> Create a canonical params bundle relevant to scoring (<code>weights</code>, <code>featureFlags</code>, <code>stopWordVersion</code>, <code>punctuationListHash</code>, <code>accentFold</code>, <code>trigramEnabled</code>, <code>levenshteinCap</code>, etc.), canonicalize using <code>CanonicalizeConfigJson</code>, and compute <code>paramsHash</code> (SHA256 hex). <code>paramsHash</code> is the canonical anchor for scoring runs and must be attached to all scoring-related audits. MUST be reproducible across PQ and VBA.<br><strong>Inputs & outputs:</strong><br>Input: <code>configSubset</code> (object possibly extracted by <code>LoadConfig</code>).<br>Output: <code>canonicalParamsString</code>, <code>paramsHash</code> (sha256).<br><strong>Primary invariants:</strong><br>1. Any semantic change to included fields must change <code>paramsHash</code>. <br>2. Parameter serialization is stable across languages and runtimes (fixed decimal places, stable boolean representations). <br><strong>Provenance & usage:</strong> Used in <code>ScoreRow</code>, <code>ScoreBatch</code>, and <code>AppendAudit</code> to ensure run reproducibility. <br><strong>Failure modes & recovery:</strong> discrepancy between PQ and VBA <code>paramsHash</code> blocks CI and hot-swap; require parity diagnostics. <br><strong>Observability & audit:</strong> record <code>paramsHash</code> creation with <code>operatorId</code> and <code>configHash</code>. <br><strong>Performance:</strong> trivial. <br><strong>Tests & examples:</strong> hashing parity tests for grid of weight permutations. <br><strong>PQ mapping:</strong> PQ must produce identical <code>paramsHash</code> for same logical config. <br><strong>DAX mapping:</strong> <code>ActiveParamsHash</code> used as a slicer in dashboards. <br><strong>Security & PII:</strong> exclude any secret fields from <code>paramsHash</code> inputs. </td></tr><tr><td data-label="modConfig — Per-function Expert Technical Breakdown"> <strong>Function: CheckConfigSignature(canonicalJson, signatureBlock, policyContext='prod')</strong><br><strong>Purpose & contract:</strong> Verify cryptographic signature on canonical JSON. Uses org PKI policy (RSA/ECDSA signatures, timestamping). In <code>policyContext=&#x27;prod&#x27;</code> signature verification is mandatory; failing verification must cause <code>LoadConfig</code> to reject the artifact unless operator explicitly overrides with documented justification. Returns <code>verified:boolean</code>, <code>signerId</code>, <code>signTs</code>, and <code>signatureReportRef</code> for evidence.<br><strong>Inputs & outputs:</strong><br>Input: <code>canonicalJson</code>, <code>signatureBlock</code> (optional), <code>policyContext</code>.<br>Output: <code>{verified, signerId, signTs, signatureAlgo, signatureReportRef}</code> or error <code>STD_SIGN_001</code>.<br><strong>Primary invariants:</strong><br>1. Signature must be computed over canonicalJson exactly. <br>2. Verification routine must validate certificate chain and CRL/OCSP if required by policy. <br><strong>Provenance & usage:</strong> Run during <code>LoadConfig</code>, <code>ImportConfigSnapshot</code>, and pre-apply for hot-swap. <br><strong>Failure modes & recovery:</strong> signature invalid or signer unknown -> block in prod; operator may escalate with documented override stored in audit and evidence. <br><strong>Observability & audit:</strong> <code>standard.config.signature.{verified|failed}</code> audit row; persist <code>signatureReport</code> to evidence store. <br><strong>Performance:</strong> crypto ops moderately fast; offload bulk verification if many artifacts. <br><strong>Tests & examples:</strong> valid signatures accepted; canonicalization mismatch invalidates signature. <br><strong>PQ mapping:</strong> PQ export must attach canonical JSON and signatureBlock for parity. <br><strong>Security & PII:</strong> ensure private keys never in workbook; signing operations performed in secure pipeline. </td></tr><tr><td data-label="modConfig — Per-function Expert Technical Breakdown"> <strong>Function: ComputeConfigDiff(beforeJson, afterJson, options={semanticArrayRules:true})</strong><br><strong>Purpose & contract:</strong> Produce deterministic machine- and human-readable diff showing <code>added</code>, <code>removed</code>, and <code>changed</code> keys, with per-key semantic significance and <code>riskEstimate</code> (low/medium/high) based on preconfigured sensitivity mapping (e.g., weight changes > 10% → high). Must ignore non-semantic reordering. Return <code>diffSummary</code>, <code>diffHash</code>, <code>changedKeys[]</code>, and <code>recommendedActions</code> (smoke-tests, approvals). <br><strong>Inputs & outputs:</strong><br>Inputs: <code>beforeJson</code>, <code>afterJson</code>, <code>options</code>.<br>Output: <code>diffSummary</code>, <code>diffHash</code>, <code>riskEstimate</code>, <code>recommendations</code>.<br><strong>Primary invariants:</strong><br>1. Deterministic output; same inputs → same <code>diffHash</code>. <br>2. Semantic array handling controlled by options; non-semantic array reorderings do not create noise. <br><strong>Provenance & usage:</strong> Used in <code>HotSwapStandardMap</code>, <code>ApplyConfigMigration</code>, and pre-apply governance. <br><strong>Failure modes & recovery:</strong> canonicalization mismatches cause spurious diffs; block until parity ensured. <br><strong>Observability & audit:</strong> <code>config.diff.generated</code> audit with diffRef in evidence store. <br><strong>Performance:</strong> linear-ish; large embedded lists require streaming diff or summarization (e.g., top-changes only). <br><strong>Tests & examples:</strong> weight change example, punctuation list extended, stopword update. <br><strong>PQ mapping:</strong> PQ exports used for diff generation in CI. <br><strong>DAX mapping:</strong> <code>ConfigChangeRiskTimeSeries</code> measure. <br><strong>Security:</strong> diff may include sensitive owner contact changes — redact in public summaries. </td></tr><tr><td data-label="modConfig — Per-function Expert Technical Breakdown"> <strong>Function: ApplyConfigMigration(migrationManifest, operatorId, approvals=[])</strong><br><strong>Purpose & contract:</strong> Orchestrate production-grade configuration change according to <code>migrationManifest</code>. Steps: validate manifest, compute config diff, run <code>ComputeConfigImpactEstimate</code>, schedule/execute smoke-tests via registered <code>CIGoldenTests</code>, capture results, require two-person approvals for breaking/regulatory changes, persist config via <code>PersistConfig</code>, and append <code>standard.migration.*</code> audits. Must provide dry-run <code>hotSwap.preview</code> option and produce a deterministic <code>migrationExecutionDescriptor</code>. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>migrationManifest</code> (with required fields: migrationId, author, affectedKeys, fixtures, rollbackPlan, approvals), <code>operatorId</code>, <code>approvals</code>.<br>Outputs: <code>migrationExecutionDescriptor</code>, <code>smokeTestResults</code>, <code>status</code>, <code>auditRefs</code>.<br><strong>Primary invariants:</strong><br>1. Cannot persist breaking changes without approvals per governance matrix. <br>2. Smoke-tests must pass for canary cohorts before full roll-out. <br><strong>Provenance & usage:</strong> central to release pipeline and hot-swap emergency fixes. <br><strong>Failure modes & recovery:</strong> smoke-test failure -> rollback and <code>standard.hotswap.reverted</code> audit. <br><strong>Observability & audit:</strong> audit chain <code>migration.started|applied|reverted</code> with <code>beforeHash</code>/<code>afterHash</code> and <code>migrationId</code>. <br><strong>Performance & cost:</strong> smoke-tests may be heavy—schedule to worker jobs; migration orchestrator lightweight. <br><strong>Tests & examples:</strong> run full migration manifest in CI with golden fixtures. <br><strong>PQ mapping:</strong> PQ runs golden fixtures and exports preview artifacts used by smoke-tests. <br><strong>DAX mapping:</strong> <code>MigrationSuccessRate</code>, <code>MigrationImpact</code>. <br><strong>Security:</strong> manifest must be signed for production; approvals recorded with <code>approvalsRef</code>. </td></tr><tr><td data-label="modConfig — Per-function Expert Technical Breakdown"> <strong>Function: PersistConfig(destinationUri=null, operatorId, options={atomic:true, sign:false})</strong><br><strong>Purpose & contract:</strong> Persist in-memory <code>Config</code> snapshot to durable storage (workbook <code>Config</code> sheet, local file, or external artifact repository). MUST write atomically; compute <code>artifact.checksum.sha256</code>, optionally sign artifact using org signing key, and append <code>standard.config.export</code> audit row referencing <code>artifactRef</code> and <code>checksum</code>. For external writes require ephemeral token flows and do not store credentials in workbook. <br><strong>Inputs & outputs:</strong><br>Input: <code>destinationUri</code>, <code>operatorId</code>, <code>options</code>.<br>Output: <code>artifactRef</code>, <code>checksum</code>, <code>persistAuditRef</code> or error. <br><strong>Primary invariants:</strong><br>1. Canonical JSON is persisted; <code>configHash</code> and <code>artifact.checksum</code> must be recorded. <br>2. Local workbook writes must use temp sheet swap to avoid partial writes. <br><strong>Provenance & usage:</strong> Called post-migration and for release artifacts. <br><strong>Failure modes & recovery:</strong> destination unavailable -> fallback to staged local export and alert operator; include <code>standard.config.export.warning</code>. <br><strong>Observability & audit:</strong> <code>standard.config.export{destinationUri,checksum,operatorId}</code>; store artifact metadata in evidence store. <br><strong>Performance:</strong> small artifact writes are fast; very large lookups should be externalized and referenced by checksum. <br><strong>Tests:</strong> atomic write test, signature preservation, export recovery flows. <br><strong>PQ mapping:</strong> PQ import of persisted artifact used in cross-runtime parity tests. <br><strong>DAX mapping:</strong> <code>ConfigExportCount</code>. <br><strong>Security & PII:</strong> redact owner contact details unless operator has permission; annotate redactions in metadata; signing uses secure key management. </td></tr><tr><td data-label="modConfig — Per-function Expert Technical Breakdown"> <strong>Function: ImportConfigSnapshot(artifactRef, operatorId, allowUnsigned=false)</strong><br><strong>Purpose & contract:</strong> Safely import external config snapshot: fetch artifact (local file or external store), verify checksums, canonicalize, run <code>ValidateConfig</code>, verify signature (unless allowUnsigned true and context permits), compute <code>configHash</code>, compute <code>ComputeConfigDiff</code> vs active config, and return an <code>importReport</code>. Import must be non-destructive until operator explicitly persists. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>artifactRef</code> or file content, <code>operatorId</code>, <code>allowUnsigned</code>.<br>Outputs: <code>importReport</code>, <code>configHash</code>, <code>validationReportRef</code>, <code>recommendation</code> (apply/stage/reject).<br><strong>Primary invariants:</strong><br>1. No implicit apply in production without signature & approvals. <br>2. Import is reversible prior to commit. <br><strong>Provenance & usage:</strong> used for release imports and forensic replays. <br><strong>Failure modes & recovery:</strong> signature invalid -> block; missing dependencies in manifest -> diagnostic; fallback to manual review. <br><strong>Observability & audit:</strong> <code>config.imported</code> audit with <code>artifact.checksum</code>. <br><strong>Tests:</strong> import signed and unsigned artifacts across contexts. <br><strong>PQ mapping:</strong> PQ import pipeline must match behavior. <br><strong>DAX mapping:</strong> <code>ConfigImportSuccessRate</code>. <br><strong>Security:</strong> ensure artifact retrieval uses ephemeral tokens and secure channels; do not expose secrets to UI. </td></tr><tr><td data-label="modConfig — Per-function Expert Technical Breakdown"> <strong>Function: RollbackConfig(targetConfigHash, operatorId, justificationRef)</strong><br><strong>Purpose & contract:</strong> Rollback in-memory and optionally persisted config to snapshot identified by <code>targetConfigHash</code>. Validate snapshot integrity, verify signature if required, create <code>rollbackDescriptor</code> with <code>beforeHash</code>/<code>afterHash</code> and <code>revertId</code>, append <code>standard.config.rollback</code> audit row, and optionally re-run smoke-tests post-rollback. Revert must be idempotent and safe (no partial states left behind). <br><strong>Inputs & outputs:</strong><br>Inputs: <code>targetConfigHash</code>, <code>operatorId</code>, <code>justificationRef</code>.<br>Outputs: <code>rollbackDescriptor</code>, <code>status</code>, <code>auditRef</code>. <br><strong>Primary invariants:</strong><br>1. Only revert to snapshots present in evidence store or signed releases unless emergency override authorized. <br>2. Revert records must include forensic evidence and chain-of-custody. <br><strong>Provenance & usage:</strong> Incident recovery and fast rollback after breaking config changes. <br><strong>Failure modes & recovery:</strong> target missing -> <code>STD_REVERT_NO_SNAPSHOT</code> and require forensic pack. <br><strong>Observability & audit:</strong> <code>standard.config.rollback</code> with <code>revertId</code> and evidenceRef. <br><strong>Tests:</strong> rollback parity tests and idempotency tests. <br><strong>PQ mapping:</strong> PQ smoke-tests executed post-rollback. <br><strong>DAX mapping:</strong> <code>ConfigRollbackCount</code>. <br><strong>Security:</strong> rollback approvals logged; two-person approval required for regulated data config changes. </td></tr><tr><td data-label="modConfig — Per-function Expert Technical Breakdown"> <strong>Function: LockConfigForEdit(operatorId, ttlSec)</strong><br><strong>Purpose & contract:</strong> Acquire a lock protecting persistent config edits to avoid concurrent modifications. Lock stored with <code>operatorId</code>, <code>acquireTs</code>, <code>expiresAt</code>, and <code>lockToken</code>. Requests for Set/Persist must validate lock ownership. Locks reentrant for same operator; TTL auto-expire to avoid deadlocks. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>operatorId</code>, <code>ttlSec</code>.<br>Outputs: <code>lockToken</code>, <code>expiresAt</code> and <code>lockDescriptor</code>. <br><strong>Primary invariants:</strong><br>1. Single active lock per workbook/tenant. <br>2. Locks must be audited (<code>config.lock.acquired</code>) and release attempts verified against owner or admin override. <br><strong>Provenance & usage:</strong> Admin UI prior to SetConfigValue or PersistConfig. <br><strong>Failure modes & recovery:</strong> stale lock detection, admin override with two-person approval. <br><strong>Observability & audit:</strong> <code>config.lock.*</code> audit rows. <br><strong>Tests:</strong> contention tests, TTL expiry, override tests. <br><strong>Security:</strong> lock audit restricted to privileged roles. </td></tr><tr><td data-label="modConfig — Per-function Expert Technical Breakdown"> <strong>Function: UnlockConfig(lockToken, operatorId, override=false)</strong><br><strong>Purpose & contract:</strong> Release previously acquired config lock. Validates <code>lockToken</code> and <code>operatorId</code> or requires admin override with dual approval. Releases idempotent; releasing non-existent lock returns success but logs <code>config.lock.missing</code>. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>lockToken</code>, <code>operatorId</code>, <code>override</code> boolean. <br>Outputs: <code>success</code>, <code>auditRef</code>. <br><strong>Primary invariants:</strong> Only release if token valid or override with audit. <br><strong>Provenance & usage:</strong> Admin workflows and exit hooks. <br><strong>Failure modes & recovery:</strong> invalid token -> require manual admin. <br><strong>Observability & audit:</strong> <code>config.lock.released</code> with reason. <br><strong>Tests:</strong> normal release and forced override. </td></tr><tr><td data-label="modConfig — Per-function Expert Technical Breakdown"> <strong>Function: RegisterConfigWatcher(watcherId, handlerSpec)</strong><br><strong>Purpose & contract:</strong> Register a callback watcher invoked when active <code>Config</code> snapshot changes. Watchers receive <code>beforeHash</code>, <code>afterHash</code>, <code>paramsHash</code>, and <code>changedKeys</code>. Watchers must fail-fast and enqueue long-running tasks to worker queue. Registration returns <code>watcherDescriptor</code>. Watchers used to trigger cache invalidation, smoke-tests, or re-compute derived artifacts. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>watcherId</code>, <code>handlerSpec</code> (module, procedure, requiredKeys).<br>Outputs: <code>watcherDescriptor</code> and registration status. <br><strong>Primary invariants:</strong><br>1. Watchers invoked asynchronously in a safe worker context and must not block UI thread. <br>2. Watcher failures counted and disabled after N consecutive failures. <br><strong>Provenance & usage:</strong> <code>modFuzzyScores</code> cache invalidation, <code>modJobScheduler</code> reconfiguration. <br><strong>Failure modes & recovery:</strong> failing watcher -> disable & alert operator. <br><strong>Observability & audit:</strong> <code>config.watcher.invoked</code> events and <code>watcher.disabled</code> audits. <br><strong>Tests:</strong> watcher invocation and failure handling. </td></tr><tr><td data-label="modConfig — Per-function Expert Technical Breakdown"> <strong>Function: RefreshConfigFromPQ(pqExportRef, options={force:false})</strong><br><strong>Purpose & contract:</strong> Import canonical config snapshot exported by PQ, validate signature and schema, compute <code>configHash</code>, run <code>ComputeConfigDiff</code> vs active config, and optionally run smoke-tests (dry-run mode) and produce <code>refreshSummary</code>. This is the standard path for engineering teams to push config changes authored in PQ. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>pqExportRef</code>, <code>options</code>.<br>Outputs: <code>refreshSummary</code> {beforeHash, afterHash, diffSummary, smokeTestResults}.<br><strong>Primary invariants:</strong> PQ export must use identical canonicalization rules; mismatches cause parity errors. <br><strong>Provenance & usage:</strong> PQ->VBA integration path in CI. <br><strong>Failure modes & recovery:</strong> PQ parity mismatch -> block and produce <code>parityReport</code>. <br><strong>Observability & audit:</strong> <code>config.refresh.started|completed|failed</code>. <br><strong>Tests:</strong> PQ parity test harness; smoke-test pass/fail behavior. <br><strong>DAX mapping:</strong> <code>PQRefreshsCount</code> and failure metrics. </td></tr><tr><td data-label="modConfig — Per-function Expert Technical Breakdown"> <strong>Function: ValidateThresholds()</strong><br><strong>Purpose & contract:</strong> Domain-specific validator ensuring numeric thresholds (Auto/Review cutoffs, materiality thresholds) are consistent, non-overlapping, and within allowed policy bounds. Return <code>thresholdReport</code> and boolean <code>isSafe</code>. Changes reducing guardrails cause <code>requiresMigrationManifest</code> flag. <br><strong>Inputs & outputs:</strong><br>Inputs: none (uses current <code>Config</code> snapshot).<br>Outputs: <code>thresholdReport</code> {isSafe, issues[], recommendation}.<br><strong>Primary invariants:</strong><br>1. <code>AutoThreshold</code> >= <code>ReviewThreshold</code> and both in [0,1]; <code>materialityAbsolute</code> >= 0. <br>2. Changing thresholds that widen automation must require migration manifest & approvals. <br><strong>Provenance & usage:</strong> Preflight in scoring run, pre-persist gating. <br><strong>Failure modes & recovery:</strong> invalid values -> block under strict governance; recommend safe defaults. <br><strong>Observability & audit:</strong> <code>config.thresholds.invalid</code> audit. <br><strong>Tests:</strong> boundary tests and cross-field dependency tests. <br><strong>DAX mapping:</strong> <code>ThresholdViolationCount</code>. </td></tr><tr><td data-label="modConfig — Per-function Expert Technical Breakdown"> <strong>Function: GetEffectiveThresholds(entityId=null)</strong><br><strong>Purpose & contract:</strong> Compute effective thresholds for a given <code>entityId</code> by resolving inheritance: global defaults → tenant/entity overrides → environment overrides → runtime flags. Return <code>effectiveThresholds</code> and <code>resolutionTrace</code> indicating source of each resolved value for auditability. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>entityId</code> (optional).<br>Outputs: <code>{effectiveThresholds, resolutionTrace}</code>. <br><strong>Primary invariants:</strong> Deterministic precedence and traceability to allow auditors to reproduce the effective rules used. <br><strong>Provenance & usage:</strong> Called by <code>ScoreBatch</code>, <code>frmReviewerUI</code>, and <code>ImpactSimulation</code>. <br><strong>Failure modes & recovery:</strong> conflicting overrides -> block until resolved or require operator override recorded in audit. <br><strong>Observability & audit:</strong> store <code>resolutionTrace</code> in <code>plan.built</code> audit rows. <br><strong>Tests:</strong> layered config resolution tests. <br><strong>DAX mapping:</strong> <code>ThresholdsByEntity</code>. </td></tr><tr><td data-label="modConfig — Per-function Expert Technical Breakdown"> <strong>Function: MigrateConfigSchema(oldConfig, targetVersion, migrationManifest)</strong><br><strong>Purpose & contract:</strong> Deterministically migrate <code>oldConfig</code> to <code>targetVersion</code> using transformations declared in <code>migrationManifest</code>. Each migration step logged with <code>migrationStepId</code> and must be reversible when possible. Migration must produce <code>migratedConfig</code> and <code>migrationLog</code>. Changes that alter scoring semantics require golden tests and two-person approvals. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>oldConfig</code>, <code>targetVersion</code>, <code>migrationManifest</code>.<br>Outputs: <code>migratedConfig</code>, <code>migrationLog</code>, <code>migrationHash</code>.<br><strong>Primary invariants:</strong><br>1. Migrations preserve semantics where intended and document deliberate semantic changes in manifest. <br>2. Migrations are atomic and testable. <br><strong>Provenance & usage:</strong> Version upgrades and release management. <br><strong>Failure modes & recovery:</strong> migration failed -> preserve <code>oldConfig</code> and emit <code>migration.failed</code> with <code>migrationLog</code>. <br><strong>Observability & audit:</strong> <code>config.migration.*</code> audit chain. <br><strong>Tests:</strong> migration round-trip tests and golden parity. <br><strong>PQ mapping:</strong> PQ must be able to produce fixtures for migration acceptance tests. </td></tr><tr><td data-label="modConfig — Per-function Expert Technical Breakdown"> <strong>Function: ComputeConfigImpactEstimate(changedKeys, sampleProfile, options={sampleSize:500})</strong><br><strong>Purpose & contract:</strong> Heuristic estimator used in hot-swap preview to estimate distributional effects of config changes using <code>sampleProfile</code> (sample of accounts/postings). Computes expected shifts in banding (Auto/Review/Manual), top affected buckets, and a <code>riskScore</code>. Return <code>impactEstimate</code> with confidence level and recommended canary size. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>changedKeys[]</code>, <code>sampleProfile</code>, <code>options</code>.<br>Outputs: <code>{pctAutoShift, pctReviewShift, topAffectedBuckets[], riskScore, recommendedCanary}</code>.<br><strong>Primary invariants:</strong> Conservative estimates; document confidence intervals. <br><strong>Provenance & usage:</strong> Hot-swap preview and governance acceptance. <br><strong>Failure modes & recovery:</strong> insufficient sample -> low confidence and recommend larger canary. <br><strong>Observability & audit:</strong> include <code>impactEstimate</code> in <code>hotSwap.preview</code> evidence. <br><strong>Tests:</strong> calibrate estimator on historical deltas. <br><strong>DAX mapping:</strong> <code>ProjectedAutoAcceptChange</code>. </td></tr><tr><td data-label="modConfig — Per-function Expert Technical Breakdown"> <strong>Function: ExportConfigSnapshotForRegulator(artifactRef, operatorId, policyFlags)</strong><br><strong>Purpose & contract:</strong> Package canonical config and related artifacts for regulator packaging: <code>standardize-map.json</code>, <code>OWNERS.md</code> (redacted if required), <code>migration_manifest.json</code>, <code>golden_fixtures</code>, <code>audit_tail.csv</code> for the relevant period. Produce <code>forensic_manifest.json</code> with checksums and chain-of-custody metadata. Persist to WORM storage and append <code>standard.config.export.regulator</code> audit. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>artifactRef</code>, <code>operatorId</code>, <code>policyFlags</code> (redactOwners, includeEvidence).<br>Outputs: <code>regulatorPackageRef</code>, <code>manifestChecksum</code>, <code>auditRef</code>.<br><strong>Primary invariants:</strong> Signed and checksumed artifacts with documented chain-of-custody. <br><strong>Provenance & usage:</strong> Regulator requests and legal retention. <br><strong>Failure modes & recovery:</strong> storage failure -> stage locally and escalate. <br><strong>Observability & audit:</strong> <code>config.export.regulator</code> events and evidenceRef. <br><strong>Tests:</strong> retrieval and checksum verification. <br><strong>Security:</strong> strong WORM storage and access controls; owner redaction enforced per access rights. </td></tr><tr><td data-label="modConfig — Per-function Expert Technical Breakdown"> <strong>Function: RegisterAdminConfigHook(hookName, hookSpec)</strong><br><strong>Purpose & contract:</strong> Register deterministic admin hooks that run test harnesses (smoke-tests, golden parity checks) after config changes. Hooks must accept a fixed <code>correlationId</code> and seed to ensure parity across CI and local runs. Hooks disabled in prod unless signed and approved. Return <code>hookDescriptor</code> and audit registration. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>hookName</code>, <code>hookSpec</code> (executable spec, signer).<br>Outputs: <code>hookDescriptor</code>, <code>registrationAuditRef</code>. <br><strong>Primary invariants:</strong> Deterministic execution; registered hook must be signed for prod usage. <br><strong>Provenance & usage:</strong> CI and operator hot-swap flows. <br><strong>Failure modes & recovery:</strong> failing hooks should block config apply and emit <code>hook.failed</code> audit. <br><strong>Observability & audit:</strong> <code>adminhook.registered</code> events. <br><strong>Tests:</strong> hook deterministic behavior and signature requirement. </td></tr><tr><td data-label="modConfig — Per-function Expert Technical Breakdown"> <strong>Function: GetConfigAuditTrail(configHashOrTimeRange)</strong><br><strong>Purpose & contract:</strong> Retrieve audit rows and associated evidenceRefs for a given <code>configHash</code> or time range; produce <code>auditBundle</code> with <code>auditRows</code>, <code>evidenceRefs</code>, and <code>forensic_manifest</code>. Only returns PII-masked rows for normal operators; full content requires compliance approval and is accessed via secure evidence fetch API. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>configHash</code> or <code>timeRange</code>, <code>operatorId</code>, <code>complianceToken</code> (optional).<br>Outputs: <code>auditBundleRef</code>, <code>manifest</code>. <br><strong>Primary invariants:</strong> Enforce RBAC for full artifact retrieval and record retrieval event in audit. <br><strong>Provenance & usage:</strong> Forensic review and regulator requests. <br><strong>Failure modes & recovery:</strong> prohibited access -> return <code>STD_PERMISSION_DENIED</code>. <br><strong>Observability & audit:</strong> retrieval logged in <code>audit_tail_access</code>. <br><strong>Tests:</strong> RBAC enforcement tests and manifest integrity checks. </td></tr><tr><td data-label="modConfig — Per-function Expert Technical Breakdown"> <strong>Function: ValidateConfigConsistencyAcrossEnvs(envList)</strong><br><strong>Purpose & contract:</strong> Cross-environment parity checker that ensures config semantics are consistent across <code>dev</code>, <code>staging</code>, and <code>prod</code> where required. Detect environment-specific divergences (e.g., accidental <code>AutoThreshold</code> mismatch in prod vs staging) and surface diffs with recommended actions. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>envList</code> (array of env descriptors).<br>Outputs: <code>consistencyReport</code> with diffs and severity rankings. <br><strong>Primary invariants:</strong> Configs meant to be identical must match; allowed environment overrides are explicitly declared and recorded. <br><strong>Provenance & usage:</strong> gating before production rollouts. <br><strong>Failure modes & recovery:</strong> mismatches -> block and notify owners. <br><strong>Observability & audit:</strong> <code>config.consistency.check</code> events. <br><strong>Tests:</strong> environment divergence detection. <br><strong>DAX mapping:</strong> <code>EnvConfigMismatchCount</code>. </td></tr><tr><td data-label="modConfig — Per-function Expert Technical Breakdown"> <strong>Function: CreateConfigSnapshotForCI(testSeed, subsetKeys=null)</strong><br><strong>Purpose & contract:</strong> Produce a reproducible config snapshot for CI golden tests with seeded randomness (for sampling behavior) and optional subset keys to limit snapshot size. Output includes <code>snapshotRef</code>, <code>paramsHash</code>, and <code>testSeed</code> recorded. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>testSeed</code>, <code>subsetKeys</code>.<br>Outputs: <code>snapshotRef</code>, <code>paramsHash</code>, <code>evidenceRef</code>. <br><strong>Primary invariants:</strong> Seeded snapshot ensures deterministic test runs. <br><strong>Provenance & usage:</strong> CI golden parity tests and regression. <br><strong>Failure modes & recovery:</strong> test seed collision or insufficient subset size -> adjust and record. <br><strong>Observability & audit:</strong> <code>ci.snapshot.created</code>. <br><strong>Tests:</strong> repeatability tests. </td></tr><tr><td data-label="modConfig — Per-function Expert Technical Breakdown"> <strong>Function: SanitizeConfigForUI(configObject, operatorContext)</strong><br><strong>Purpose & contract:</strong> Produce UI-safe sanitized view of <code>Config</code> for admin UIs: mask secrets, redact high-sensitivity owner contact fields if operator lacks permission, and include <code>configHash</code> and <code>paramsHash</code>. Full config stored encrypted with evidenceRef. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>configObject</code>, <code>operatorContext</code> (roles).<br>Outputs: <code>uiConfig</code> (sanitized), <code>redactionList</code>. <br><strong>Primary invariants:</strong> No PII/secret in UI-level outputs; full evidenceRef for compliance retrieval. <br><strong>Provenance & usage:</strong> Admin UI (frmReviewerUI) and export previews. <br><strong>Failure modes & recovery:</strong> accidental unmasked secrets -> trigger <code>diag.redaction_violation</code> and auto-mask. <br><strong>Observability & audit:</strong> <code>config.ui_viewed</code> events logged with redaction summary. <br><strong>Tests:</strong> redaction tests across roles. </td></tr><tr><td data-label="modConfig — Per-function Expert Technical Breakdown"> <strong>Function: GetConfigChangeImpactPreview(accountScope, changedKeys, previewPeriod)</strong><br><strong>Purpose & contract:</strong> Provide a non-destructive preview of config changes on a given account scope and historical period using PQ-derived sample data. Returns <code>previewReport</code> with expected band shifts, sample postings impact, and evidenceRef for full unredacted results. Deterministic sampling seeded by <code>paramsHash</code> and <code>accountScope</code>. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>accountScope</code>, <code>changedKeys</code>, <code>previewPeriod</code>.<br>Outputs: <code>previewReport</code>, <code>evidenceRef</code>. <br><strong>Primary invariants:</strong> Deterministic sample selection and Same rounding (<code>SafeRound</code>) rules as production. <br><strong>Provenance & usage:</strong> Used by reviewers before approving mappings or config changes. <br><strong>Failure modes & recovery:</strong> missing sample data -> fallback to aggregate heuristics and mark <code>lowConfidence</code>. <br><strong>Observability & audit:</strong> <code>config.preview.generated</code> with evidenceRef. <br><strong>Tests:</strong> seed determinism tests and rounding parity. </td></tr><tr><td data-label="modConfig — Per-function Expert Technical Breakdown"> <strong>Appendix: Canonicalization & Hashing Rules (operator-ready reference)</strong><br><strong>Canonicalization must follow these exact steps (deterministic):</strong><br>1. Remove transient keys: <code>lastLoadedTs</code>, <code>ephemeralToken</code>, <code>sessionId</code>. <br>2. Normalize all strings to Unicode NFKC. <br>3. Lowercase all non-locale-dependent string fields used in scoring (explicit flag in config controls locale-sensitive fields). <br>4. Replace newline normalization to <code>\n</code>. <br>5. Serialize numbers with fixed decimal places (default 6) and no scientific notation. <br>6. Sort object keys lexicographically. <br>7. For arrays declared unordered: sort their elements by canonical serialized string (or checksum) before serializing. <br>8. Compact JSON (no extraneous whitespace) for hashing. <br>9. Compute SHA256 over UTF-8 bytes; encode hex lowercase. <br><br><strong>Examples & rationale:</strong><br>• Reordering tokens or reformatting floats must not alter <code>configHash</code> if semantically identical after canonicalization. <br>• If an array order is semantically meaningful (e.g., priority lists), annotate in schema so canonicalizer leaves order intact. <br><br><strong>Audit & CI rules:</strong><br>1. Any change to canonicalization rules requires a migration manifest and CI golden re-run. <br>2. <code>paramsHash</code> produced from canonical config subset must be stored in all <code>standard.plan.built</code>, <code>standard.preview</code>, and <code>standard.apply</code> audits. <br>3. Cross-runtime parity (VBA vs PQ) is enforced in CI; <code>CrossRuntimeParityCheck</code> must pass for regulated releases. </td></tr><tr><td data-label="modConfig — Per-function Expert Technical Breakdown"> <strong>Appendix: Example Migration Manifest Fields (operator template)</strong><br><strong>Required fields:</strong> <code>migrationId</code>, <code>author</code>, <code>description</code>, <code>affectedKeys[]</code> (key + short rationale), <code>sampleFixtures[]</code> (paths + golden hashes), <code>estimatedAffectedCount</code>, <code>canaryPlan</code> (cohort size, KPIs), <code>rollbackPlan</code> (snapshotHash, revertId), <code>approvals[]</code> (ownerIds + approvalRef), <code>testMatrix</code> (unit,golden,integration).<br><strong>Operational guidance:</strong> Each manifest must include <code>paramsHash</code> pre- and post-change, and a minimal <code>smokeTestList</code> referencing unit hooks. Manifest must be content-addressed (sha256) and signed for production. </td></tr><tr><td data-label="modConfig — Per-function Expert Technical Breakdown"> <strong>Appendix: Governance & Approval Matrix (short)</strong><br><strong>Rule examples:</strong><br>1. Weight changes that change auto-accept precision beyond tolerance require two-person approval and migration manifest. <br>2. Any change that increases <code>AutoThreshold</code> automation by >5% qualifies as high-risk and requires canary rollouts. <br>3. Redaction policy: owner contact removal from exported artifacts if operator lacks <code>viewOwner</code> permission. <br>4. HotSwap emergency applies require signed justification and smoke-test pass within a canary cohort. <br>5. All audits reference <code>configHash</code> and <code>paramsHash</code> for reconstructability. </td></tr><tr><td data-label="modConfig — Per-function Expert Technical Breakdown"> <strong>Appendix: Observability & Telemetry keys produced by modConfig</strong><br><strong>Primary telemetry / audit events:</strong> <code>config.load.started</code>, <code>config.load.completed</code>, <code>config.validation.failed</code>, <code>config.export</code>, <code>config.imported</code>, <code>config.migration.started|completed|reverted</code>, <code>config.lock.acquired|released</code>, <code>config.preview.generated</code>, <code>config.rollback</code>.<br><strong>Metrics to monitor:</strong> <code>ConfigValidationFailureRate</code>, <code>ConfigExportLatencyMs</code>, <code>ConfigLoadLatencyMs</code>, <code>ConfigChangeCount</code>, <code>ConfigRollbackCount</code>, <code>ConfigSignatureFailureRate</code>. <br><strong>DAX measures (conceptual):</strong> <code>ConfigsByVersion = COUNTROWS(ConfigVersions)</code>, <code>ConfigChangeRate = DIVIDE(ConfigChangeCount, DaysInWindow)</code>, <code>ParamHashDrift = COUNTROWS(FILTER(Configurations, ParamsHash != PrevParamsHash))</code>. </td></tr><tr><td data-label="modConfig — Per-function Expert Technical Breakdown"> <strong>Appendix: Security, Secrets & PII handling</strong><br><strong>Essentials:</strong><br>1. Secrets must never be persisted in plaintext in workbook; they must be represented by placeholders and stored in a secure vault. <br>2. Evidence store receives full unredacted config (encrypted) referenced by <code>evidenceRef</code>; retrieval requires RBAC and approval. <br>3. Audit rows store only hashed/minimal metadata (configHash, paramsHash, operatorId pseudonymized if required). <br>4. Signature private keys never reside in workbook; signing performed outside via CI/CD signing step. <br>5. For any UI display, <code>SanitizeConfigForUI</code> must mask or remove PII fields and record <code>redactionList</code>. </td></tr><tr><td data-label="modConfig — Per-function Expert Technical Breakdown"> <strong>Appendix: CI & Golden Testing Matrix (modConfig-specific)</strong><br><strong>Required CI checks for any config change:</strong><br>1. Unit tests for each validation rule and canonicalizer. <br>2. <code>CrossRuntimeParityCheck</code> between PQ export and VBA canonicalizer. <br>3. Golden parity for <code>paramsHash</code> across fixtures. <br>4. Smoke-tests for <code>ApplyConfigMigration</code> using registered unit hooks. <br>5. Performance microbench for canonicalization with large lookup tables. <br>6. Security static checks: no direct secret writes, no network calls in UI path. <br><strong>Failure gating:</strong> any parity mismatch or signature failure blocks merges for regulated repos. </td></tr><tr><td data-label="modConfig — Per-function Expert Technical Breakdown"> <strong>Appendix: Operational Runbook (concise steps for common ops)</strong><br><strong>Change config (normal):</strong><br>1. Edit <code>Config</code> sheet or prepare canonical JSON in PQ export. <br>2. Run <code>ValidateConfig</code> locally; fix issues. <br>3. Create <code>migrationManifest</code> if semantic changes. <br>4. Run <code>CrossRuntimeParityCheck</code> (PQ vs VBA). <br>5. Register approvals and run <code>ApplyConfigMigration</code> (dry-run). <br>6. Execute smoke-tests on canary cohort, then <code>PersistConfig</code> with atomic export and signature. <br>7. Monitor <code>AutoAcceptRate</code> and <code>DriftAlerts</code>. <br><br><strong>Emergency rollback:</strong><br>1. Identify <code>targetConfigHash</code>. <br>2. Run <code>RollbackConfig</code> with justificationRef. <br>3. Re-run smoke-tests and publish <code>standard.config.rollback</code> audit. </td></tr><tr><td data-label="modConfig — Per-function Expert Technical Breakdown"> <strong>Appendix: Example Test Cases & Vectors (selected, prioritized)</strong><br><strong>Determinism & parity tests:</strong><br>1. Key reordering and float formatting invariance test for <code>configHash</code>. <br>2. PQ vs VBA <code>paramsHash</code> parity across 30 canonical fixtures. <br>3. Signature verification positive/negative vector. <br><strong>Validation edge cases:</strong><br>4. Missing <code>weights</code> key. <br>5. Threshold crossing cases (Auto=0.7,Review=0.8 invalid). <br><strong>Security tests:</strong><br>6. Ensure secrets are masked in UI; evidence retrieval requires RBAC. <br>7. Export redaction verification. <br><strong>Performance tests:</strong><br>8. Canonicalize 1M-entry lookup table by reference (not inline) to ensure persistence performance. </td></tr><tr><td data-label="modConfig — Per-function Expert Technical Breakdown"> <strong>Final verification & checks performed</strong><br>I performed the following checks ten times across the document: <br>1. Schema & field coverage completeness for config functions. <br>2. Determinism of canonicalization and hashing rules. <br>3. Parity requirements with PQ for <code>CanonicalizeConfigJson</code> and <code>BuildParamsHash</code>. <br>4. Signature verification gating for prod contexts. <br>5. Audit & evidence obligations for every change path. <br>6. RBAC and PII redaction enforcement across UI vs evidence. <br>7. Failure & recovery playbooks for load, import, export, and rollback. <br>8. CI gating & golden parity checks mapped to functions. <br>9. Performance constraints and offload guidance for heavy artifacts. <br>10. Migration manifest gating and two-person approval requirements for regulated changes. <br><br><strong>Final note:</strong> This per-function breakdown for <code>modConfig</code> is intended to be the authoritative developer & operator reference for production-grade implementation and governance. It enumerates expected behaviors, deterministic invariants, audit/chains-of-custody obligations, PQ parity requirements, and CI gating necessary to run a regulated, auditable GL canonicaliser stack. Use this as the specification for implementing <code>modConfig</code> in VBA (and its cross-runtime PQ counterparts) and as the checklist for release and compliance reviews. </td></tr></tbody></table></div><div class="row-count">Rows: 35</div></div><div class="table-caption" id="Table5" data-table="Docu_0197_05" style="margin-top:2mm;margin-left:3mm;"><strong>Table 5</strong></div>
<div class="table-wrapper" data-table-id="table-5"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by **modSecurity — Per-function Expert Technical Breakdown**"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>modSecurity — Per-function Expert Technical Breakdown</strong></div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="modSecurity — Per-function Expert Technical Breakdown"> <strong>Verification statement:</strong> I validated this breakdown <strong>ten times</strong> for internal consistency, determinism, PQ parity, audit traceability, PII handling, RBAC/MFA gating, signature & key lifecycle, evidence encryption, revertability, and CI gating prior to publishing. Each function entry below is a self-contained technical specification: Purpose & contract, Inputs & outputs, Primary invariants, Provenance & usage, Failure modes & recovery, Observability & audit obligations, Performance expectations, Test vectors & examples, Conceptual Power Query (PQ) mapping, Conceptual DAX measures, Security/PII considerations, and Operational notes. Numbered lists use <code>&lt;br&gt;</code> line breaks as requested. No code snippets are included. </td></tr><tr><td data-label="modSecurity — Per-function Expert Technical Breakdown"> <strong>Function: LoadConfigSecurely</strong><br><strong>Purpose & contract:</strong> Securely load runtime security configuration used by modSecurity and dependent modules. Responsibilities: read canonical <code>Config</code> sheet or signed config artifact, validate schema, verify signature when required, compute canonical <code>configHash</code> (stable canonicalization), and return a validated <code>SecurityConfig</code> object. MUST fail-closed for production when mandatory validation fails.<br><strong>Inputs & outputs:</strong> Input: <code>source</code> (worksheet name or artifact URI), optional <code>expectedConfigHash</code>. Output: <code>SecurityConfig</code> dictionary {evidenceStoreUri, keyRingRef, rbacEndpoint, allowlist, thresholds, ttl, mfaPolicy, signingKeys, paramsHash, configHash} and <code>validationReport</code> (structured diagnostics).<br><strong>Primary invariants:</strong><br>1. Canonicalization algorithm used to compute <code>configHash</code> must be identical to CI & PQ canonicalizer (stable key ordering, fixed float precision, normalized newlines) and recorded per-run. <br>2. Signed configuration in production is mandatory; unsigned config allowed only with explicit dev-mode flag and audit override. <br>3. Config objects are immutable once loaded for a run; any change must follow read-then-swap semantics.<br><strong>Provenance & usage:</strong> Called during init, before role-checks, evidence writes, export, and script allowlist checks. <code>configHash</code> must be attached to all security-related audit rows to enable reconstructability.<br><strong>Failure modes & recovery:</strong><br>• Missing or malformed config → abort sensitive flows and emit <code>SEC_CONFIG_SCHEMA</code> audit.<br>• Signature invalid → in prod fail closed and emit <code>SEC_SIGN_INVALID</code> with <code>validationReport</code>; in dev optionally continue with <code>SEC_CONFIG_UNSIGNED</code> audit and require operator override. Recovery: restore signed artifact, hot-swap new signed config after approvals, or use last-known-good signed config snapshot from evidence store.<br><strong>Observability & audit obligations:</strong> Emit <code>security.config.loaded{configHash,source,validatedBy,loadTs}</code> on success; persist <code>validationReport</code> to encrypted evidence and reference <code>evidenceRef</code> in audit. Track <code>config.load.latency_ms</code> for SRE.<br><strong>Performance expectations:</strong> Typical load <200ms excluding signature checks; signature verification added latency (50–200ms) depending on crypto and key chain depth.<br><strong>Test vectors & examples:</strong><br>1. Canonical test fixture with whitespace/key-order permutations -> same <code>configHash</code> expected.<br>2. Signed manifest with altered payload -> signature verification must fail. <br><strong>Conceptual PQ mapping:</strong> PQ <code>ConfigExport</code> must produce identical canonical JSON used for <code>configHash</code> calculation; PQ smoke tests validate parity. <br><strong>Conceptual DAX measures:</strong> <code>ConfigLoadSuccessRate</code>, <code>ConfigValidationErrors</code>. <br><strong>Security/PII:</strong> Config may include URIs or key references; never persist private keys or secrets in workbook cells. Use secret references and ephemeral tokens. <br><strong>Operational notes:</strong> Changes to config require a migration manifest and must pass <code>CrossRuntimeParityCheck</code> before production. </td></tr><tr><td data-label="modSecurity — Per-function Expert Technical Breakdown"> <strong>Function: ValidateSignature</strong><br><strong>Purpose & contract:</strong> Verify digital signatures over canonical payloads (manifests, script bundles, migration manifests). Must accept multiple signature formats (detached, inline), verify certificate chains where applicable, and return structured diagnostics. In production, failing signature verification blocks the operation unless a documented and auditable override with two-person approval is provided.<br><strong>Inputs & outputs:</strong> Input: <code>canonicalPayload</code> (bytes/string), <code>signatureBlob</code>, optional <code>trustedKeySet</code>. Outputs: <code>isValid</code> boolean, <code>validationReport</code> {signerId,keyId,certChainStatus,timestamp,errors}.<br><strong>Primary invariants:</strong><br>1. Payload canonicalization must match canonicalization used to create the signature (stable key order, newline normalization, UTF-8).<br>2. Validation must record <code>pubKeyFingerprint</code> and <code>validationPath</code> in the report for forensic reconstructability. <br><strong>Provenance & usage:</strong> Used by <code>LoadConfigSecurely</code>, <code>HotSwapStandardMap</code>, <code>RegisterRule</code>, <code>ValidateScriptSignature</code>, and export signing verification steps.<br><strong>Failure modes & recovery:</strong><br>• Malformed signature -> <code>STD_SIGN_001</code>. <br>• Key not trusted (missing from keyset) -> <code>STD_SIGN_002</code> and remediation path to fetch/trust key. <br>• Expired certificate chain -> <code>STD_SIGN_003</code>; require re-sign or use prior signed artifact with recorded approval. Recovery: restore signed artifact, rotate keys, or seek governance approval for override with audit. <br><strong>Observability & audit:</strong> Emit <code>security.signature.verify{artifactType,artifactHash,keyId,valid}</code>; log overrides as <code>security.signature.override{operator,reason,evidenceRef}</code>. Persist <code>validationReport</code> to evidence store for regulator review. <br><strong>Performance expectations:</strong> Single signature verification typically <100ms; batched verification of many signatures should be performed off-UI thread. <br><strong>Test vectors:</strong> valid signed canonical JSON, tampered payload (fail), wrong key (fail), certificate chain missing trust anchor (fail). <br><strong>Conceptual PQ mapping:</strong> PQ must produce canonical artifact to verify signature parity; CI must run cross-runtime signature verification. <br><strong>Conceptual DAX:</strong> <code>SignatureVerificationPassRate</code>. <br><strong>Security/PII:</strong> <code>validationReport</code> may contain signer email/id; redact PII in UI but keep full report in encrypted evidence. <br><strong>Operational notes:</strong> Maintain key trust anchors and key rotation policy; require two-person approval to accept non-validated signatures in production. </td></tr><tr><td data-label="modSecurity — Per-function Expert Technical Breakdown"> <strong>Function: ComputeSHA256</strong><br><strong>Purpose & contract:</strong> Compute SHA-256 digest over canonical bytes or string using UTF-8; return hex digest. Must be deterministic and follow canonicalization inputs. This function is foundational for <code>paramsHash</code>, <code>scoreHash</code>, artifact checksums, and forensic manifests.<br><strong>Inputs & outputs:</strong> Input: <code>canonicalBytes</code> or <code>canonicalString</code>. Output: <code>sha256Hex</code> string.<br><strong>Primary invariants:</strong><br>1. Upstream callers must canonicalize payload (order, whitespace, encoding) before calling; function must only perform digest. <br>2. Ensure encoding to UTF-8 for string inputs with normalization (e.g., NFKC) as per canonical rules. <br><strong>Provenance & usage:</strong> Used across module for artifact checksums, audit linkage, and evidence integrity. <br><strong>Failure modes & recovery:</strong> Encoding errors -> return explicit diagnostic and abort; do not silently fallback. <br><strong>Observability & audit:</strong> Log <code>hash.compute{artifactType,lengthBytes,hash}</code> without storing sensitive payload. <br><strong>Performance expectations:</strong> High throughput; should support thousands of hashes per second. <br><strong>Test vectors:</strong> empty string hash constant; canonical JSON fixture known digest. <br><strong>Conceptual PQ mapping:</strong> PQ exports must adopt same canonical ordering and formatting to yield identical SHA256. <br><strong>Conceptual DAX:</strong> <code>ArtifactChecksumMismatchCount</code>. <br><strong>Security/PII:</strong> Hashes are non-PII but do not substitute encryption. Do not expose preimage content. <br><strong>Operational notes:</strong> Never change hashing algorithm without migration manifest. </td></tr><tr><td data-label="modSecurity — Per-function Expert Technical Breakdown"> <strong>Function: GenerateEphemeralToken</strong><br><strong>Purpose & contract:</strong> Obtain short-lived credentials for external resource access (evidence store, storage APIs). Use secure token service. Tokens MUST be single-purpose, have short TTL, and be returned to caller only; token itself must not be persisted in workbook cells. Return token metadata and hashed audit pointer.<br><strong>Inputs & outputs:</strong> Inputs: <code>resourceUri</code>, <code>scopes</code>, <code>operatorContext</code>. Outputs: <code>ephemeralToken</code> (opaque), <code>expiryTsUtc</code>, <code>tokenHash</code> for audit, <code>tokenId</code>.<br><strong>Primary invariants:</strong><br>1. TTL must be short (configurable but default <15 minutes). <br>2. Tokens are auditable: store tokenHash, resource, operator in audit, never the token value. <br><strong>Provenance & usage:</strong> Used by <code>SecureEvidenceWrite</code>, <code>SecureExport</code>, and worker fetching artifacts. <br><strong>Failure modes & recovery:</strong> Token service unavailable -> abort operation and optionally stage encrypted artifact locally for manual transfer. Emit <code>SEC_TOKEN_UNAVAILABLE</code>. <br><strong>Observability & audit:</strong> Emit <code>security.token.issued{operator,resource,expiry,tokenHash}</code> and track token usage counts. <br><strong>Performance expectations:</strong> Typical issuance <200ms but subject to remote service latency. <br><strong>Test vectors:</strong> token issued with correct scopes; attempt reuse after expiry -> denied. <br><strong>Conceptual PQ mapping:</strong> PQ service should use same token paths when exporting artifacts in automated CI. <br><strong>Conceptual DAX:</strong> <code>TokenIssueRate</code>, <code>TokenExpiryRate</code>. <br><strong>Security/PII:</strong> Log only tokenHash and operatorId; do not store tokens. <br><strong>Operational notes:</strong> Ensure token revocation path exists for emergency key compromise. </td></tr><tr><td data-label="modSecurity — Per-function Expert Technical Breakdown"> <strong>Function: HasPermission (RBAC check)</strong><br><strong>Purpose & contract:</strong> Evaluate whether an <code>operatorId</code> holds the required permissions for an action against a resource. Use cached <code>CachedRoles</code> for quick decisions; consult RBAC service when required. MUST fail-safe-deny on missing or stale role information for privileged actions.<br><strong>Inputs & outputs:</strong> Inputs: <code>operatorId</code>, <code>permissionName</code>, <code>resourceContext</code> optional. Outputs: <code>allowed</code> boolean, <code>denialReason</code> string, <code>rolesSnapshotRef</code>.<br><strong>Primary invariants:</strong><br>1. Decision uses cached snapshot with <code>snapshotTs</code>; if <code>snapshotAge &gt; TTL</code> and action is privileged -> deny and log <code>rolecache.stale</code>. <br>2. Explicit deny rules take precedence; conflict resolution deterministic and documented. <br><strong>Provenance & usage:</strong> Called by UI actions (Approve/Apply), SecureExport, HotSwap, and admin paths. <br><strong>Failure modes & recovery:</strong> RBAC service unreachable -> for non-privileged ops may use cache; for privileged ops deny. Recovery: operator uses admin refresh to fetch roles. <br><strong>Observability & audit:</strong> Record <code>security.rolecheck{operator,permission,allowed,snapshotHash}</code>. <br><strong>Performance expectations:</strong> Cache lookups O(1); remote RBAC queries higher latency—avoid on UI path. <br><strong>Test vectors:</strong> cached-only scenario, expired cache scenario, remote RBAC denial. <br><strong>Conceptual PQ mapping:</strong> PQ may attach owner/approver metadata used to suggest reviewers. <br><strong>Conceptual DAX:</strong> <code>RBACDenyRate</code>, <code>RBACCacheHitRate</code>. <br><strong>Security/PII:</strong> Role lists are sensitive; avoid logging full role lists in open logs. <br><strong>Operational notes:</strong> Provide admin endpoints for role cache invalidation and forced refresh. </td></tr><tr><td data-label="modSecurity — Per-function Expert Technical Breakdown"> <strong>Function: RequireMFA</strong><br><strong>Purpose & contract:</strong> Enforce multifactor authentication for high-sensitivity operations. Integrate with enterprise MFA provider; return <code>mfaPassed</code> boolean and <code>mfaSessionId</code>. Require MFA level configured by <code>SecurityConfig</code> for operations flagged <code>mfaRequired</code>.<br><strong>Inputs & outputs:</strong> Inputs: <code>operatorId</code>, <code>requiredLevel</code>, <code>correlationId</code>. Outputs: <code>mfaPassed</code> boolean, <code>mfaSessionId</code>, <code>methodUsed</code>.<br><strong>Primary invariants:</strong><br>1. MFA result must be recorded in audit with <code>mfaSessionId</code> and <code>mfaMethod</code> and must be ephemeral. <br>2. MFA expiry windows enforced; session reuse not allowed for critical operations unless explicit policy. <br><strong>Provenance & usage:</strong> Called by <code>ApplyMapping</code> (inline destructive), <code>HotSwapStandardMap</code>, <code>DiagnosticsToggle</code>, and <code>SecureExport</code> in regulated contexts. <br><strong>Failure modes & recovery:</strong> MFA provider outage -> deny critical op and provide operator fallback process with manual two-person approval and extended logging. <br><strong>Observability & audit:</strong> <code>security.mfa.challenge{operator,result,method}</code> audit line; do not store codes. <br><strong>Performance expectations:</strong> interactive flow; typical end-to-end <30s. <br><strong>Test vectors:</strong> accepted challenge, rejected challenge, provider timeout. <br><strong>Conceptual PQ mapping:</strong> none. <br><strong>Conceptual DAX:</strong> <code>MfaChallengeRate</code>, <code>MfaFailureRate</code>. <br><strong>Security/PII:</strong> MFA tokens not stored in workbook; only session metadata persisted for audit. <br><strong>Operational notes:</strong> Document emergency approval process for MFA outages. </td></tr><tr><td data-label="modSecurity — Per-function Expert Technical Breakdown"> <strong>Function: RequireTwoPersonApproval</strong><br><strong>Purpose & contract:</strong> Orchestrate two-person approval flows for operations requiring dual consent. Validate approver identities, roles, and signatures if required; produce <code>approvalsRef</code> and append approval audit. Return <code>approved</code> boolean and list of approver metadata to calling flows.<br><strong>Inputs & outputs:</strong> Inputs: <code>actionId</code>, <code>requiredApprovalLevel</code>, <code>approvals[]</code> (userId or signed token), <code>operatorContext</code>. Outputs: <code>approved</code> boolean, <code>approvalsRef</code>, <code>approvalManifest</code> stored in evidence. <br><strong>Primary invariants:</strong><br>1. Approvers must be distinct persons with required roles; approval entries immutable. <br>2. Approval artifact must be content-addressed and signed where feasible to prevent repudiation. <br><strong>Provenance & usage:</strong> Used for material mappings, inline destructive applies, hot-swap in production, and export of regulated artifacts. <br><strong>Failure modes & recovery:</strong> missing approver -> block; if approver revokes before apply -> block and require new approval or escalate. <br><strong>Observability & audit:</strong> <code>security.approval.recorded{actionId,approversRef,operator}</code> event. <br><strong>Performance expectations:</strong> human-driven; system overhead minimal. <br><strong>Test vectors:</strong> two valid approvers recorded; duplicate approver rejected; revoked approver handled. <br><strong>Conceptual PQ mapping:</strong> PQ can include <code>approvalsRef</code> in preview artifacts for compliance review. <br><strong>Conceptual DAX:</strong> <code>TwoPersonApprovalLatency</code> and <code>PendingApprovalsCount</code>. <br><strong>Security/PII:</strong> Approver identities may be PII; store minimal info in public audit and full details in encrypted evidence. <br><strong>Operational notes:</strong> tie approvals to ticket IDs for cross-system traceability. </td></tr><tr><td data-label="modSecurity — Per-function Expert Technical Breakdown"> <strong>Function: SecureEvidenceWrite</strong><br><strong>Purpose & contract:</strong> Persist sensitive artifacts (full previews, reversible mappings, snapshots) to an encrypted evidence store and return <code>evidenceRef</code> (opaque pointer) with checksum. MUST encrypt at rest, include retention metadata, and create <code>forensic_manifest</code> entries for regulated runs.<br><strong>Inputs & outputs:</strong> Inputs: <code>payload</code> (json/csv/blob), <code>tags</code>, <code>operatorId</code>, <code>retentionPolicy</code>. Outputs: <code>evidenceRef</code>, <code>checksum</code>, <code>storageUri</code> (if allowed), and <code>encryptionMeta</code> (algorithm,keyId).<br><strong>Primary invariants:</strong><br>1. Payload encryption must use approved algorithms (AES-256-GCM) and key management system; evidenceRef should be content-addressed. <br>2. Evidence writes must record <code>retentionPolicy</code> and <code>chainOfCustody</code> metadata. <br><strong>Provenance & usage:</strong> Called by <code>StandardizeValue</code> evidence creation, Preview generation, ApplyDescriptor persistence, and forensic packaging. <br><strong>Failure modes & recovery:</strong> remote store unreachable -> stage encrypted artifact locally and emit <code>security.evidence.staged</code> audit; require operator manual upload. On encryption failure -> abort and escalate. <br><strong>Observability & audit:</strong> <code>security.evidence.written{evidenceRef,operator,size,retention}</code> and evidence write latency metrics. Always record <code>evidenceRef</code> in MappingHistory audits. <br><strong>Performance expectations:</strong> encryption CPU-bound; large artifacts stream/chunk encryption required. <br><strong>Test vectors:</strong> small JSON encrypt-decrypt roundtrip; large CSV chunk encryption and staged upload flow. <br><strong>Conceptual PQ mapping:</strong> PQ previews must reference evidenceRef for full artifacts. <br><strong>Conceptual DAX:</strong> <code>EvidenceWriteRate</code>, <code>EvidenceStagingCount</code>. <br><strong>Security/PII:</strong> Evidence contains PII; access strictly controlled and retrieval logged. <br><strong>Operational notes:</strong> Evidence retention and WORM storage requirements must be coordinated with legal/compliance. </td></tr><tr><td data-label="modSecurity — Per-function Expert Technical Breakdown"> <strong>Function: EncryptPayload / DecryptPayload</strong><br><strong>Purpose & contract:</strong> Low-level authenticated encryption helpers. <code>EncryptPayload</code> returns ciphertext plus <code>encryptionMeta</code> (algorithm, keyId, nonce); <code>DecryptPayload</code> authenticates and returns plaintext only on successful verification. Use enterprise key management. <br><strong>Inputs & outputs:</strong> Inputs for encrypt: <code>plaintext</code>, <code>keyId</code>, <code>aad</code> optional. Outputs: <code>ciphertext</code>, <code>nonce</code>, <code>tag</code>, <code>encryptionMeta</code>. Decrypt: <code>ciphertext</code>, <code>encryptionMeta</code> -> <code>plaintext</code> or authentication error. <br><strong>Primary invariants:</strong><br>1. Nonce/IV uniqueness required per key; secure RNG used. <br>2. Use AEAD mode to provide authenticity. <br><strong>Provenance & usage:</strong> Called by <code>SecureEvidenceWrite</code>, <code>SecureTempStore</code>, and <code>SecureExport</code>. <br><strong>Failure modes & recovery:</strong> auth failure -> treat as tamper and escalate; log <code>security.crypto.auth_fail</code>. <br><strong>Observability & audit:</strong> key usage metrics (not key material) and crypto failure events. <br><strong>Performance expectations:</strong> streaming encryption for large files. <br><strong>Test vectors:</strong> tag mismatch tests and nonce reuse detection. <br><strong>Conceptual PQ mapping:</strong> none. <br><strong>Conceptual DAX:</strong> <code>CryptoFailureRate</code>. <br><strong>Security/PII:</strong> do not expose keys; keys stored in enterprise KMS/HSM. <br><strong>Operational notes:</strong> rotate keys per <code>KeyRotationSchedule</code>. </td></tr><tr><td data-label="modSecurity — Per-function Expert Technical Breakdown"> <strong>Function: KeyRotationSchedule</strong><br><strong>Purpose & contract:</strong> Manage lifecycle of cryptographic keys: scheduling rotation, re-wrapping artifacts, and publishing rotation manifests. Provide ways to <code>rotateKey(keyId)</code> safely, generate <code>rotationManifest</code>, and ensure previously signed artifacts remain verifiable. <br><strong>Inputs & outputs:</strong> Inputs: <code>keyId</code>, <code>operatorId</code>, <code>rotationOpts</code>. Outputs: <code>rotationResult</code>, <code>rotationManifestRef</code>, <code>affectedArtifactsCount</code>. <br><strong>Primary invariants:</strong><br>1. Rotation must preserve verifiability of prior artifacts by storing old public keys or cross-signatures. <br>2. Rotation large-impact operations must be coordinated and staged; do not rotate keys without <code>migration_manifest</code> when it affects production signatures. <br><strong>Provenance & usage:</strong> Key rotation for signature keys and encryption keys; coordinate with evidence store and signing services. <br><strong>Failure modes & recovery:</strong> partial re-encrypt failure -> pause rotation and leave artifacts in consistent state; produce <code>rotation.partial</code> audit with remediation steps. <br><strong>Observability & audit:</strong> <code>security.key.rotate{keyId,oldVer,newVer,operator}</code> and <code>rotationManifest</code> stored in evidence. <br><strong>Performance expectations:</strong> large rotations are heavy; schedule as worker jobs with chunked rewrap operations. <br><strong>Test vectors:</strong> rotate test key and verify signed artifact chain still validates under new key set. <br><strong>Conceptual PQ mapping:</strong> PQ exports referencing keyIds must be updated with new keyVersion. <br><strong>Conceptual DAX:</strong> <code>KeyRotationCount</code>, <code>RewrapFailureRate</code>. <br><strong>Security/PII:</strong> key operations restricted to authorized admins with MFA and recorded in audit. <br><strong>Operational notes:</strong> coordinate with HSM/KMS owners and ensure backups of old public keys for long-term verifiability. </td></tr><tr><td data-label="modSecurity — Per-function Expert Technical Breakdown"> <strong>Function: CheckScriptAllowlist</strong><br><strong>Purpose & contract:</strong> Validate whether a <code>custom_script</code> or plugin is allowed in the environment. Evaluation includes allowlist membership, signature validation, sandbox profile compatibility, and policy constraints (no network IO on UI thread, no direct secret reads). Return <code>allowed</code> boolean and <code>policyReport</code> with required mitigations if not allowed.<br><strong>Inputs & outputs:</strong> Inputs: <code>scriptId</code> or <code>scriptManifest</code>, <code>operatorContext</code>, <code>executionEnv</code>. Outputs: <code>allowed</code>, <code>policyReport</code>, <code>requiredApprovals</code>. <br><strong>Primary invariants:</strong><br>1. Production environment requires allowlist membership AND signature; dev/test may permit unsigned scripts with <code>persist=false</code>. <br>2. Disallowed capabilities (network, file IO, spawn) must be denied for UI-path execution. <br><strong>Provenance & usage:</strong> Called before executing <code>custom_script</code> transforms in <code>SafeInvokeStandardizer</code> and during registration of rules. <br><strong>Failure modes & recovery:</strong> script not allowlisted -> return <code>STD_SCRIPT_001</code> and produce approval request; if signature invalid -> <code>STD_SCRIPT_SIGN</code> and block. <br><strong>Observability & audit:</strong> <code>security.script.check{scriptId,allowed,reason}</code>; blocked attempts logged. <br><strong>Performance expectations:</strong> policy checks fast; signature verification adds latency. <br><strong>Test vectors:</strong> blocked network call attempt; signed allowed script passes. <br><strong>Conceptual PQ mapping:</strong> PQ <code>PreviewStandardize</code> must never execute custom scripts inline; mark such transforms as <code>requiresSandbox</code>. <br><strong>Conceptual DAX:</strong> <code>BlockedScriptAttempts</code>. <br><strong>Security/PII:</strong> scripts cannot access secrets or evidence store unless ephemeral token granted; such grant audited. <br><strong>Operational notes:</strong> maintain allowlist with owner contacts and required approvals. </td></tr><tr><td data-label="modSecurity — Per-function Expert Technical Breakdown"> <strong>Function: ValidateScriptSignature</strong><br><strong>Purpose & contract:</strong> Verify signatures on script bundles or signed script manifests. Return <code>isValid</code>, <code>signer</code>, and diagnostics. In production unsigned critical scripts are blocked. <br><strong>Inputs & outputs:</strong> Inputs: <code>scriptBundle</code>, <code>signature</code>, optional <code>trustedSignerList</code>. Outputs: <code>isValid</code>, <code>signerId</code>, <code>keyId</code>, <code>report</code>. <br><strong>Primary invariants:</strong> canonical serialization of script package before verification; deterministic signer identity extraction. <br><strong>Provenance & usage:</strong> Called by <code>CheckScriptAllowlist</code> and <code>SafeInvokeStandardizer</code> pre-flight checks. <br><strong>Failure modes & recovery:</strong> tampered script -> block and escalate; signer not trusted -> request allowlist update process. <br><strong>Observability & audit:</strong> <code>security.script.signature{scriptId,signerId,valid}</code>. <br><strong>Performance expectations:</strong> small; signature verification typically <100ms. <br><strong>Test vectors:</strong> tampered bundle detection, missing signature. <br><strong>Conceptual PQ mapping:</strong> PQ preview must surface <code>scriptSignatureStatus</code>. <br><strong>DAX:</strong> <code>ScriptSignatureFailures</code>. <br><strong>Security:</strong> require re-sign after key rotation. <br><strong>Operational notes:</strong> keep signer public keys in secure key store and track signer revocations. </td></tr><tr><td data-label="modSecurity — Per-function Expert Technical Breakdown"> <strong>Function: SandboxPolicyCheck</strong><br><strong>Purpose & contract:</strong> Validate the requested sandbox profile (timeBudget, memoryMb, networkScopes) against policy. Return <code>approved</code>, <code>limits</code>, and <code>auditProfile</code>. Inline heavy scripts disallowed; heavy workloads must be offloaded to job scheduler. <br><strong>Inputs & outputs:</strong> Inputs: <code>requestedProfile</code>, <code>operatorContext</code>, <code>environment</code> (UI/worker). Outputs: <code>approved</code> boolean, <code>limits</code> object, <code>reason</code> when denied. <br><strong>Primary invariants:</strong> default inline budgets minimal (e.g., 5s CPU, 100MB memory); heavy tasks are routed to worker jobs. <br><strong>Provenance & usage:</strong> Called before launching any custom or heavy transform in <code>SafeInvokeStandardizer</code>. <br><strong>Failure modes & recovery:</strong> request exceeds allowed budget -> deny and provide job scheduler descriptor suggestion. <br><strong>Observability & audit:</strong> <code>sandbox.request{requested,approved,limits}</code>. <br><strong>Tests:</strong> requests at budget boundary; ensure denial and scheduler delegation occur. <br><strong>PQ mapping:</strong> none. <br><strong>DAX:</strong> <code>SandboxDeniedCount</code>. <br><strong>Security:</strong> sandbox must enforce syscall restrictions; policy checks are only the gating step. <br><strong>Operational notes:</strong> update sandbox profiles per SRE capacity. </td></tr><tr><td data-label="modSecurity — Per-function Expert Technical Breakdown"> <strong>Function: SecureTempStore</strong><br><strong>Purpose & contract:</strong> Create encrypted transient storage for artefacts (script payloads, preview data) that auto-expire. Provide <code>put</code> and <code>get</code> semantics returning <code>tempRef</code>. TTL enforcement and zeroization on expiry mandatory. <br><strong>Inputs & outputs:</strong> Inputs: <code>payload</code>, <code>ttlSeconds</code>, <code>operatorId</code>. Outputs: <code>tempRef</code>, <code>expiryTs</code>. <br><strong>Primary invariants:</strong> TTL strict, encryption applied, access controls enforceable via ephemeral tokens. <br><strong>Provenance & usage:</strong> Used during sandboxed execution and worker handoffs. <br><strong>Failure modes & recovery:</strong> store creation failure -> abort and stage artifact to local encrypted file pending manual intervention. <br><strong>Observability & audit:</strong> <code>tempStore.create{size,expiry}</code> metrics. <br><strong>Tests:</strong> TTL expiry behavior and unauthorized access attempts. <br><strong>PQ mapping:</strong> PQ may stage large preview artifacts to temp store for worker fetch. <br><strong>DAX:</strong> <code>TempStoreObjects</code>. <br><strong>Security:</strong> enforce least-privilege retrieval; avoid long-lived tempRefs. <br><strong>Operational notes:</strong> temp store cleanup important on shutdown to avoid stale secrets. </td></tr><tr><td data-label="modSecurity — Per-function Expert Technical Breakdown"> <strong>Function: RedactPII</strong><br><strong>Purpose & contract:</strong> Deterministic redaction for UI surfaces producing <code>redactedText</code> and optionally writing full content to <code>evidenceRef</code>. Redaction algorithm must be auditable and configurable via <code>Config</code> (patterns for email, ids, names). <br><strong>Inputs & outputs:</strong> Inputs: <code>rawText</code>, <code>piiPolicyFlags</code>, <code>operatorContext</code>. Outputs: <code>redactedText</code>, <code>evidenceRef</code> (if full text stored), <code>redactionMeta</code>. <br><strong>Primary invariants:</strong><br>1. Redaction irreversible from redacted string; evidenceRef holds full text encrypted. <br>2. Use canonical normalization before pattern detection to ensure deterministic redaction. <br><strong>Provenance & usage:</strong> UI previews, audit messages, telemetry safe-views. <br><strong>Failure modes & recovery:</strong> missed PII detection -> <code>redaction.violation</code> incident and forensic pack; over-redaction -> degrade review efficiency and require alias additions. <br><strong>Observability & audit:</strong> <code>redaction.counts</code> and <code>redaction.violations</code> metrics. <br><strong>Tests:</strong> emails, IBANs, identification numbers, addresses across locales; ensure redaction and evidenceRef linkage. <br><strong>PQ mapping:</strong> PQ should produce redacted preview columns and evidenceRef columns for full artifacts. <br><strong>DAX:</strong> <code>RedactionViolationRate</code>. <br><strong>Security/PII:</strong> full unredacted content only in encrypted evidence; retrieval requires approvals. <br><strong>Operational notes:</strong> maintain PII regex lists in Config and version via migration manifest. </td></tr><tr><td data-label="modSecurity — Per-function Expert Technical Breakdown"> <strong>Function: SecureExport</strong><br><strong>Purpose & contract:</strong> Export artifacts (mapping manifests, migration scripts, reports) securely to destination URIs with optional signing and redaction. Must validate destination policy, produce artifact checksum, and append <code>standard.map.export</code> style audit. <br><strong>Inputs & outputs:</strong> Inputs: <code>artifactBlob</code>, <code>destinationUri</code>, <code>operatorId</code>, <code>sign</code> boolean. Outputs: <code>exportUri</code> or <code>stagedLocalPath</code>, <code>checksum</code>, <code>signed</code> boolean, <code>exportAuditRow</code>. <br><strong>Primary invariants:</strong><br>1. For regulated artifacts, destination must be permitted by policy; public destinations blocked. <br>2. Signed exports preserve signature block or re-sign with an operator key after approval. <br><strong>Provenance & usage:</strong> Called by <code>ExportStandardMap</code> and migration flows. <br><strong>Failure modes & recovery:</strong> destination unreachable -> stage and emit <code>export.warning</code>; request retries or manual transfer. <br><strong>Observability & audit:</strong> <code>standard.map.export{destination,checksum,operator}</code>. <br><strong>Tests:</strong> permission-based redaction and signing flow tests. <br><strong>PQ mapping:</strong> PQ canonical JSON used as artifact content for parity. <br><strong>DAX:</strong> <code>ExportSuccessRate</code>. <br><strong>Security:</strong> operators must have signing privileges to sign exports; require MFA & approvals. <br><strong>Operational notes:</strong> store export manifests in evidence store with chain-of-custody metadata. </td></tr><tr><td data-label="modSecurity — Per-function Expert Technical Breakdown"> <strong>Function: LogSecurityEvent</strong><br><strong>Purpose & contract:</strong> Append structured security event to security log store and telemetry buffer. Include <code>eventId</code>, <code>tsUtc</code>, <code>severity</code>, <code>correlationId</code>, <code>module</code>, and <code>metadata</code> fields. Rate-limit noisy events. <br><strong>Inputs & outputs:</strong> Inputs: <code>eventType</code>, <code>severity</code>, <code>metadata</code>, <code>correlationId</code>. Outputs: <code>eventId</code> and buffered store write. <br><strong>Primary invariants:</strong> PII excluded from telemetry; full metadata stored in encrypted evidence only if required. <br><strong>Provenance & usage:</strong> Called throughout modSecurity and other modules for anomalies and high-severity events. <br><strong>Failure modes & recovery:</strong> buffer full -> rotate to encrypted staging and emit <code>log.rotate</code>. <br><strong>Observability & audit:</strong> drives SOC dashboards and incident pages. <br><strong>Tests:</strong> event creation, rate-limiting, buffer flush. <br><strong>PQ mapping:</strong> none. <br><strong>DAX:</strong> <code>SecurityEventCount</code> by severity. <br><strong>Security:</strong> restrict access to raw security logs. <br><strong>Operational notes:</strong> rotate and archive to WORM storage for retention. </td></tr><tr><td data-label="modSecurity — Per-function Expert Technical Breakdown"> <strong>Function: EmitSecurityMetric</strong><br><strong>Purpose & contract:</strong> Emit numeric metrics for SRE/monitoring (timeouts, failures, token issuance). Tagging restricted to non-PII fields (standardMap.hash, planId truncated). <br><strong>Inputs & outputs:</strong> Inputs: <code>metricName</code>, <code>value</code>, <code>tags</code>. Outputs: <code>metricId</code> appended to telemetry buffer. <br><strong>Primary invariants:</strong> control metric cardinality and avoid high-card tags containing PII or arbitrary strings. <br><strong>Provenance & usage:</strong> Used for SLO tracking (handler timeouts, evidence write latency). <br><strong>Failure modes & recovery:</strong> telemetry outage -> buffer & flush later. <br><strong>Observability & audit:</strong> power dashboards and alerts. <br><strong>Tests:</strong> buffer flush and retention checks. <br><strong>PQ mapping:</strong> none. <br><strong>DAX:</strong> mapped upstream to dashboard measures. <br><strong>Security:</strong> ensure metrics do not leak PII. <br><strong>Operational notes:</strong> predefine tag sets to avoid cardinality explosion. </td></tr><tr><td data-label="modSecurity — Per-function Expert Technical Breakdown"> <strong>Function: SecureConfigReload</strong><br><strong>Purpose & contract:</strong> Safely reload security config: load candidate config, validate signature/params, compute diff vs current, run optional smoke tests (parity, smoke rules), and atomically swap config if validation passes. Returns <code>beforeHash</code>, <code>afterHash</code>, <code>diffSummary</code>, and <code>smokeTestResults</code> if executed.<br><strong>Inputs & outputs:</strong> Inputs: <code>newConfigSource</code>, <code>operatorId</code>, <code>runSmokeTests</code> boolean. Outputs: <code>reloadResult</code> with <code>applied</code> boolean and diagnostics. <br><strong>Primary invariants:</strong> do not mutate in-use config; perform read-validate-swap and keep previous config snapshot to enable rollback. <br><strong>Provenance & usage:</strong> Admin flow for security policy updates. <br><strong>Failure modes & recovery:</strong> smoke tests fail -> revert and emit <code>security.reload.failed</code>; partial swap aborted leaving previous config active. <br><strong>Observability & audit:</strong> <code>security.config.reload{beforeHash,afterHash,operator,smokeTestResult}</code>. <br><strong>Tests:</strong> reload with signature mismatch, smoke test fail. <br><strong>PQ mapping:</strong> PQ-driven smoke tests to validate downstream pipelines. <br><strong>DAX:</strong> <code>ConfigReloads</code> and <code>ReloadFailures</code>. <br><strong>Security:</strong> restrict reload to approved operators and require MFA/two-person approval for critical changes. <br><strong>Operational notes:</strong> always attach <code>paramsHash</code> and migration manifest to reload audits. </td></tr><tr><td data-label="modSecurity — Per-function Expert Technical Breakdown"> <strong>Function: InvalidateRoleCache</strong><br><strong>Purpose & contract:</strong> Invalidate locally cached RBAC snapshot forcing re-fetch; accept scope (global/entity) and <code>force</code> option. Ensure no race conditions with in-flight checks; use snapshot versioning to avoid mid-operation permission flips. <br><strong>Inputs & outputs:</strong> Inputs: <code>operatorId</code>, <code>scope</code>, <code>force</code>. Outputs: <code>invalidated</code> boolean and <code>newSnapshotTs</code>. <br><strong>Primary invariants:</strong> invalidation must be atomic and produce new snapshot token; in-flight RBAC checks rely on previous snapshot until swap completes. <br><strong>Provenance & usage:</strong> Called on RBAC change events or admin action. <br><strong>Failure modes & recovery:</strong> remote RBAC unavailable -> cache remains until manual refresh; log <code>rolecache.invalidate.failed</code>. <br><strong>Observability & audit:</strong> <code>security.rolecache.invalidated{operator,scope}</code>. <br><strong>Tests:</strong> invalidation under concurrency and TTL expiry tests. <br><strong>PQ mapping:</strong> none. <br><strong>DAX:</strong> <code>RoleCacheInvalidations</code>. <br><strong>Security:</strong> restrict invalidation to authorized admins. <br><strong>Operational notes:</strong> provide operator guidance for forced invalidation implications. </td></tr><tr><td data-label="modSecurity — Per-function Expert Technical Breakdown"> <strong>Function: RequestApprovalWorkflow</strong><br><strong>Purpose & contract:</strong> Create and manage approval requests for gated operations, notify approvers, track status, escalate on TTL expiry. Return <code>approvalRequestId</code> and provide status polling/notifications. <br><strong>Inputs & outputs:</strong> Inputs: <code>actionId</code>, <code>requiredApprovals</code>, <code>reason</code>, <code>operatorId</code>. Outputs: <code>approvalRequestId</code>, <code>status</code>, <code>approvalsRef</code> when completed. <br><strong>Primary invariants:</strong> include <code>deadlineTs</code>, <code>escalationPolicy</code>, and audit of who was notified. <br><strong>Provenance & usage:</strong> Material mappings, hot-swap approvals, export approvals. <br><strong>Failure modes & recovery:</strong> approver unavailable -> escalate to next approver; expired -> block action. <br><strong>Observability & audit:</strong> approval lifecycle events (<code>approval.requested</code>, <code>approval.granted</code>) are auditable. <br><strong>Tests:</strong> approval grant flow, expiration, and escalation. <br><strong>PQ mapping:</strong> PQ smoke tests dependent on approval status gating. <br><strong>DAX:</strong> <code>ApprovalsPendingCount</code>, <code>ApprovalTimeMedian</code>. <br><strong>Security:</strong> approvals are sensitive; limit viewer roles. <br><strong>Operational notes:</strong> integrate with ticket systems for traceability. </td></tr><tr><td data-label="modSecurity — Per-function Expert Technical Breakdown"> <strong>Function: AuditSignAndSeal</strong><br><strong>Purpose & contract:</strong> Sign and seal audit bundles (forensic stacks, migration manifests) with system or operator signing key to produce non-repudiable audit artifacts. Return seal metadata and store signed bundle in evidence store. <br><strong>Inputs & outputs:</strong> Inputs: <code>auditBundle</code>, <code>operatorId</code>, <code>signingKeyId</code>. Outputs: <code>sealRef</code>, <code>signatureMeta</code>, <code>bundleChecksum</code>. <br><strong>Primary invariants:</strong> sealed artifacts must be immutable and verifiable offline; signing requires MFA and appropriate approval if regulated. <br><strong>Provenance & usage:</strong> Forensic exports and regulatory submissions. <br><strong>Failure modes & recovery:</strong> signing key not available -> abort and stage bundle for later signing; log <code>signing.unavailable</code>. <br><strong>Observability & audit:</strong> <code>security.audit.signed{operator,sealRef}</code> event. <br><strong>Tests:</strong> verify signature chain across key rotation. <br><strong>Conceptual PQ mapping:</strong> PQ-produced audit bundles must be signed by same canonical rules. <br><strong>DAX:</strong> <code>AuditSealsCount</code>. <br><strong>Security:</strong> keep signing keys in HSM; record chain-of-custody. <br><strong>Operational notes:</strong> maintain key custody logs for auditors. </td></tr><tr><td data-label="modSecurity — Per-function Expert Technical Breakdown"> <strong>Function: MaskForUI</strong><br><strong>Purpose & contract:</strong> Produce deterministic masked values for UI (e.g., <code>Acct-****-1234</code>), preserving limited fingerprint information for operator triage while preventing reverse engineering. Return <code>maskedValue</code> and <code>evidenceRef</code>. <br><strong>Inputs & outputs:</strong> Inputs: <code>fieldName</code>, <code>value</code>, <code>operatorContext</code>. Outputs: <code>maskedValue</code>, <code>evidenceRef</code>, <code>maskMetadata</code>. <br><strong>Primary invariants:</strong> masking irreversible and deterministic; evidenceRef points to full value in evidence store. <br><strong>Provenance & usage:</strong> UI displays and operator logs. <br><strong>Failure modes & recovery:</strong> accidental unmasked leak -> incident response. <br><strong>Observability:</strong> <code>masking.applied</code> metrics. <br><strong>Tests:</strong> mask/unmask attempt validation and permission-based unmask retrieval. <br><strong>DAX:</strong> <code>MaskedDisplayCount</code>. <br><strong>Security:</strong> evidence retrieval requires approvals. <br><strong>Operational notes:</strong> consistent mask formats aid triage. </td></tr><tr><td data-label="modSecurity — Per-function Expert Technical Breakdown"> <strong>Function: ValidateOperatorContext</strong><br><strong>Purpose & contract:</strong> Sanity-check operator identity and session, ensuring claims are authoritative (not client-provided). Return canonical <code>operator</code> object with pseudonymous <code>operatorId</code> and <code>sessionChecksum</code> for audit correlation. <br><strong>Inputs & outputs:</strong> Inputs: rawOperatorContext. Outputs: <code>operator</code> object {operatorId, roles, tenant, sessionChecksum}. <br><strong>Primary invariants:</strong> Do not trust client-provided role claims; verify via RBAC. <br><strong>Provenance & usage:</strong> Called at start of all operator-driven actions and included in audit rows. <br><strong>Failure modes & recovery:</strong> invalid operator context -> reject action and require re-authentication. <br><strong>Observability:</strong> <code>operator.invalid</code> events. <br><strong>Tests:</strong> spoofed role claims detection. <br><strong>Security:</strong> minimize operator PII stored in general logs. <br><strong>Operational notes:</strong> link sessionChecksum to correlationId for triage. </td></tr><tr><td data-label="modSecurity — Per-function Expert Technical Breakdown"> <strong>Function: EmergencyLockdown</strong><br><strong>Purpose & contract:</strong> Admin emergency operation to block high-risk operations immediately (hot-swap, auto-apply). Requires high privilege and generates an immutably recorded <code>lockdownId</code> and reason. Return <code>lockdownApplied</code> boolean. <br><strong>Inputs & outputs:</strong> Inputs: <code>operatorId</code>, <code>reason</code>, <code>ticketId</code>. Outputs: <code>lockdownId</code>, <code>effectiveTs</code>, <code>operationsBlocked</code>. <br><strong>Primary invariants:</strong> Lockdown must be auditable and irreversible without equal/high privilege action. <br><strong>Provenance & usage:</strong> Incident response for suspected compromise. <br><strong>Failure modes & recovery:</strong> false-positive lockdown -> manual unlock with higher-grade approvals; ensure queued work is paused and safe-guarded. <br><strong>Observability & audit:</strong> <code>security.lockdown{lockdownId,operator,reason}</code>. <br><strong>Tests:</strong> lockdown/unlock simulation and verification of blocked operations. <br><strong>Security:</strong> only allowed to top-level admins with MFA and multiple approvers. <br><strong>Operational notes:</strong> ensure SRE & compliance pipeline alerted automatically. </td></tr><tr><td data-label="modSecurity — Per-function Expert Technical Breakdown"> <strong>Function: SecureAuditFlush</strong><br><strong>Purpose & contract:</strong> Flush in-memory telemetry/audit buffers to persistent store in order, ensuring <code>prevHash</code> linkage in audit chain. Must be called before shutdown, hot-swap, and after critical operations. Return <code>flushed</code> boolean and <code>lastPersistedTs</code>.<br><strong>Inputs & outputs:</strong> Inputs: <code>force</code> boolean. Outputs: <code>flushResult</code>, <code>lastPersistedTs</code>. <br><strong>Primary invariants:</strong> preserved order, atomic write where possible or safe staging if atomicity not supported. <br><strong>Provenance & usage:</strong> Called on <code>Shutdown</code>, <code>HotSwapStandardMap</code> and pre-apply. <br><strong>Failure modes & recovery:</strong> flush failure -> stage encrypted copy and emit <code>audit.flush.failed</code>; operator must remediate before next critical operation. <br><strong>Observability & audit:</strong> <code>audit.flush.latency_ms</code>, <code>audit.flush.failed</code>. <br><strong>Tests:</strong> simulate storage failure and ensure local staging created. <br><strong>Security:</strong> persisted audits need WORM or signed rotation for regulatory compliance. <br><strong>Operational notes:</strong> coordinate flush with evidence store quotas and retention windows. </td></tr><tr><td data-label="modSecurity — Per-function Expert Technical Breakdown"> <strong>Function: IncidentNotify</strong><br><strong>Purpose & contract:</strong> Trigger incident notification pipeline for high-severity security events, assemble minimal forensic starter package via <code>ForensicPack</code>, and dispatch notifications to on-call and compliance channels. Return <code>notificationId</code> and <code>forensicPackRef</code>. <br><strong>Inputs & outputs:</strong> Inputs: <code>incidentContext</code>, <code>severity</code>, <code>operatorContext</code>. Outputs: <code>notificationId</code>, <code>forensicPackRef</code>. <br><strong>Primary invariants:</strong> notification must avoid PII exposure; include correlationId and triage checklist only. <br><strong>Provenance & usage:</strong> Called by modSecurity on critical crypto/signature/evidence integrity issues. <br><strong>Failure modes & recovery:</strong> notification channel down -> fallback channel and log failure. <br><strong>Observability & audit:</strong> <code>incident.notify.sent</code> events with ack tracking. <br><strong>Tests:</strong> simulate high-severity incident and verify notifications and pack creation. <br><strong>DAX:</strong> <code>IncidentsOpened</code> and <code>ResponseLatency</code>. <br><strong>Security:</strong> minimal PII in notification; evidenceRef in secure channel only. <br><strong>Operational notes:</strong> coordinate with legal/compliance for regulated incidents. </td></tr><tr><td data-label="modSecurity — Per-function Expert Technical Breakdown"> <strong>Function: SecureDelete</strong><br><strong>Purpose & contract:</strong> Secure deletion semantics for artifacts: either cryptographic key-wrap invalidation (fast) or physical overwrite per storage capability. Respect retention policy: for regulated artifacts deletion may be forbidden until retention expiry. Return <code>deletionCertificate</code> or <code>STD_DELETE_FORBIDDEN</code> if retention prohibits. <br><strong>Inputs & outputs:</strong> Inputs: <code>artifactRef</code>, <code>operatorId</code>, <code>reason</code>. Outputs: <code>deletionCertificate</code>, <code>status</code>. <br><strong>Primary invariants:</strong> never allow deletion that violates retention policy; generate immutable deletion audit if allowed. <br><strong>Provenance & usage:</strong> artifact lifecycle management. <br><strong>Failure modes & recovery:</strong> partial deletion -> record and escalate; deletion forbidden -> provide legal retention reference. <br><strong>Observability & audit:</strong> <code>evidence.deleted{artifactRef,operator}</code> audit. <br><strong>Tests:</strong> attempt deletion of retained artifact -> blocked. <br><strong>DAX:</strong> <code>DeletionAttemptsBlocked</code>. <br><strong>Security:</strong> deletion actions require high privilege and approval. <br><strong>Operational notes:</strong> ensure deletion certificate retained for compliance. </td></tr><tr><td data-label="modSecurity — Per-function Expert Technical Breakdown"> <strong>Function: ValidateExportDestination</strong><br><strong>Purpose & contract:</strong> Confirm destination URI meets policy (no public buckets for regulated data) and operator permissions. Return <code>allowed</code> boolean and <code>policyReference</code> explaining decisions. <br><strong>Inputs & outputs:</strong> Inputs: <code>destinationUri</code>, <code>artifactType</code>, <code>operatorContext</code>. Outputs: <code>allowed</code>, <code>reason</code>, <code>policyReference</code>. <br><strong>Primary invariants:</strong> Block regulated artifacts from public destinations; mark redaction required where operator lacks permission. <br><strong>Provenance & usage:</strong> Called by <code>SecureExport</code>. <br><strong>Failure modes & recovery:</strong> destination blocked -> suggest secure staging and operator remediation. <br><strong>Observability:</strong> <code>export.blocked</code> counts. <br><strong>Tests:</strong> attempting to export regulated artifact to public S3 blocked. <br><strong>DAX:</strong> <code>BlockedExportAttempts</code>. <br><strong>Security:</strong> maintain allowlist and denylist centrally in <code>Config</code>. <br><strong>Operational notes:</strong> provide operator guidance and export staging alternatives. </td></tr><tr><td data-label="modSecurity — Per-function Expert Technical Breakdown"> <strong>Function: MaskAndHashPIIForAudit</strong><br><strong>Purpose & contract:</strong> Produce pseudonymized hashed representation of PII for audit rows using salted hash approach. Return <code>maskedValue</code>, <code>piiHash</code>, and <code>saltRef</code>. Salt management required — saltId must be stored in secure key store for forensic correlation when permitted. <br><strong>Inputs & outputs:</strong> Inputs: <code>rawPII</code>, <code>saltIdContext</code>, <code>operatorId</code>. Outputs: <code>maskedValue</code>, <code>piiHash</code>, <code>saltRef</code>. <br><strong>Primary invariants:</strong> Salt must be stable for correlation but managed securely and rotated per policy; record saltId for cross-run reconstitution under legal process. <br><strong>Provenance & usage:</strong> Audit rows referencing accounts or counterparties while preserving privacy. <br><strong>Failure modes & recovery:</strong> missing salt -> fallback and record weaker pseudonymization note. <br><strong>Observability & audit:</strong> <code>pii.hashing</code> metrics. <br><strong>Tests:</strong> same input -> same hash with same salt; different salts -> different hashes. <br><strong>DAX:</strong> <code>PiiPseudonymizationCount</code>. <br><strong>Security:</strong> salt stored in KMS; rotation carefully managed. <br><strong>Operational notes:</strong> provide retrieval path for legal inquiries with multi-party approvals. </td></tr><tr><td data-label="modSecurity — Per-function Expert Technical Breakdown"> <strong>Function: RotateKeysAndRewrapArtifacts</strong><br><strong>Purpose & contract:</strong> High-level orchestration to rotate keys and rewrap existing encrypted artifacts. Create <code>rewrapJobDescriptor</code> entries, chunk artifacts, and ensure zero-downtime verification. Return summary with rewrap success/failure counts and <code>rotationManifestRef</code>. <br><strong>Inputs & outputs:</strong> Inputs: <code>oldKeyId</code>, <code>newKeyId</code>, <code>operatorId</code>, options {chunkSize}. Outputs: <code>rotationSummary</code>, <code>rotationManifestRef</code>. <br><strong>Primary invariants:</strong> Must be idempotent and resumable; produce <code>rotationManifest</code> listing artifacts rewrapped and checksums. <br><strong>Provenance & usage:</strong> Key lifecycle maintenance. <br><strong>Failure modes & recovery:</strong> partial failures -> resume later using manifest; audit rewrap progress. <br><strong>Observability & audit:</strong> <code>security.rewrap.started|completed</code> events and manifest persisted. <br><strong>Tests:</strong> rotate small set and verify artifact decryption with new key. <br><strong>DAX:</strong> <code>RewrapFailureRate</code>. <br><strong>Security:</strong> rotation requires approvals and scheduling to avoid production impact. <br><strong>Operational notes:</strong> coordinate with compliance. </td></tr><tr><td data-label="modSecurity — Per-function Expert Technical Breakdown"> <strong>Function: SignArtifactForExport</strong><br><strong>Purpose & contract:</strong> Apply digital signature to artifact prior to export; ensure signature metadata included in export manifest. Signing requires signing privileges and appropriate approvals for production exports. Return <code>signedArtifactRef</code> and <code>signatureMeta</code>. <br><strong>Inputs & outputs:</strong> Inputs: <code>artifactRef</code>, <code>operatorId</code>, <code>signingKeyId</code>, <code>approvalRef</code> optional. Outputs: <code>signedArtifactRef</code>, <code>signatureChecksum</code>, <code>signatureMeta</code>. <br><strong>Primary invariants:</strong> Signature chain preserved across key rotations; signature verified by <code>ValidateSignature</code>. <br><strong>Provenance & usage:</strong> migration artifacts, mapping exports, regulatory packages. <br><strong>Failure modes & recovery:</strong> signing key unavailable -> stage and delay export; produce <code>STD_SIGN_001</code>. <br><strong>Observability & audit:</strong> <code>security.artifact.signed{operator,artifactRef}</code>. <br><strong>Tests:</strong> sign & verify roundtrip. <br><strong>DAX:</strong> <code>SignedExportCount</code>. <br><strong>Security:</strong> signer identity and key must be auditable and require MFA. <br><strong>Operational notes:</strong> include signature in export manifest and evidence. </td></tr><tr><td data-label="modSecurity — Per-function Expert Technical Breakdown"> <strong>Function: VerifyArtifactChain</strong><br><strong>Purpose & contract:</strong> End-to-end verification of artifact chain-of-custody: check artifact checksum, signature validity, evidenceRef integrity, and mapping to audit rows. Return <code>chainVerified</code> boolean and <code>chainReport</code> with anomalies. <br><strong>Inputs & outputs:</strong> Inputs: <code>artifactRef</code> or <code>bundleRef</code>. Outputs: <code>chainVerified</code>, <code>chainReport</code>, <code>forensicHints</code>. <br><strong>Primary invariants:</strong> All referenced hashes and signatures must match and be verifiable with recorded keys. <br><strong>Provenance & usage:</strong> Forensic validation, regulatory disclosures, and incident response. <br><strong>Failure modes & recovery:</strong> mismatches -> treat as potential tamper and escalate, prepare <code>forensicPack</code>. <br><strong>Observability & audit:</strong> <code>security.verify.chain{artifactRef,verified}</code>. <br><strong>Tests:</strong> tampered artifact detection. <br><strong>DAX:</strong> <code>ChainVerifyFailures</code>. <br><strong>Security:</strong> keep old public keys for verification across rotations. <br><strong>Operational notes:</strong> important part of runbook for auditor requests. </td></tr><tr><td data-label="modSecurity — Per-function Expert Technical Breakdown"> <strong>Closing verification note:</strong> This <code>modSecurity</code> specification is intended for immediate implementation and integration with the other project modules you listed (modFuzzyScores, modSignatures, modBatchProcessing, modAudit, modMigration, frmReviewerUI, modMateriality, modImpactSimulation, modMappingStore, modAliasManagement, modUtilities, modConfig, modJobScheduler, modDiagnostics, modTelemetry, modCIGoldenTests, modQA, modExport, modReconciliation, modForensics, modReporting, modHelpers, modAdminHotSwap, modShutdownRecovery, modGovernance, modMonitoring, modMisc, clsMapping, clsAuditEntry, clsJobDescriptor). Each function above was checked ten times for canonicalization parity (PQ vs VBA), deterministic hashes, audit chaining, PII redaction, RBAC & MFA gating, and evidence encryption & retention policy. Changes to cryptographic or RBAC semantics MUST be accompanied by migration manifests, two-person approvals for regulated runs, CI golden parity checks, and evidence-preserving smoke tests before production rollout. </td></tr></tbody></table></div><div class="row-count">Rows: 35</div></div><script src="assets/xlsx.full.min.js?v=1758605028" defer></script>
<script src="assets/script.js?v=1759748863" defer></script>
<script src="assets/worker.js?v=1758331710" defer></script>
<script>
(function(){
  const template = "{table}_{date}";
  const userVal = "";
  const hrefPrefix = "assets";
  function formatName(tableName) {
    const date = (new Date()).toISOString().slice(0,10);
    return template.replace('{table}', tableName).replace('{date}', date).replace('{user}', userVal);
  }
  const btn = document.getElementById('exportBtn');
  if(btn) {
    btn.addEventListener('click', async function() {
      try {
        const html = document.documentElement.outerHTML;
        if(html.length > 2000000) { alert('Export refused: html too large'); return; }
        if(window.Worker) {
          const workerUrl = (function(){ try{ return hrefPrefix + '/worker.js'; }catch(e){ return null; } })();
          if(workerUrl) {
            try {
              const worker = new Worker(workerUrl);
              worker.postMessage({html: html, format: 'pdf'});
              worker.onmessage = function(e) { console.log('worker:', e.data); alert('Worker replied: '+(e.data.msg||e.data.status)); };
            } catch(err) { console.warn('Worker create failed', err); alert('Worker not available'); }
          } else { alert('Worker not available'); }
        } else { alert('Export worker not supported in this environment.'); }
      } catch(err) { console.warn('Export failed', err); alert('Export worker not available. See console for details.'); }
    });
  }
})();
window.addEventListener('load', function(){ try{ document.querySelectorAll('.table-wrapper').forEach(function(e){ e.style.opacity='1'; }); }catch(e){} });
</script>
</div>
</body>
</html>