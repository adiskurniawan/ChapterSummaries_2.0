<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='utf-8'><meta name='viewport' content='width=device-width,initial-scale=1'>
<title>Tables Viewer v2.1</title>
<style>:root{--main-header-height:56px;} .table-wrapper{opacity:0;min-height:24px;transition:opacity .12s linear}</style>
<link rel="stylesheet" href="assets/style.css?v=1759925496">
<link rel="stylesheet" href="assets/overrides.css?v=1768563932">
</head><body>
<div id="tables-viewer" role="region" aria-label="Tables viewer">
<div id="stickyMainHeader">
<div id="tv-header">
<div><h1>Tables Viewer v2.1</h1></div>
<div style="display:flex;gap:8px;align-items:center;">
<input id="searchBox" class="tv-search" type="search" placeholder="Search" aria-label="Search tables" />
<button id="modeBtn" type="button" onclick="toggleMode()" aria-label="Toggle theme">Theme</button>
<button id="toggleAllBtn" type="button" aria-label="Toggle all" onclick="toggleAllTables()">Collapse All Tables</button>
<button id="copyAllPlainBtn" class="copy-btn" type="button" onclick="copyAllTablesPlain()" aria-label="Copy all tables as plain text">Copy All Tables (Plain Text)</button>
<button id="copyAllTablesBtn" class="copy-all-btn" type="button" onclick="copyAllTablesMarkdown()" aria-label="Copy All Tables (Markdown)">Copy All Tables (Markdown)</button>
<button id="copyAllMdBtn" style="display:none" aria-hidden="true"></button>
<button id="resetAllBtn" type="button" onclick="resetAllTables()" aria-label="Reset All Tables">Reset All Tables</button>
</div></div>
<script>
(function(){
  function ensureDelegation(){
    try {
      var vis = document.getElementById('copyAllTablesBtn');
      var alias = document.getElementById('copyAllMdBtn');
      if(!vis || !alias) return;
      alias.addEventListener = function(type, listener, options){
        vis.addEventListener(type, listener, options);
      };
      alias.removeEventListener = function(type, listener, options){
        vis.removeEventListener(type, listener, options);
      };
      Object.defineProperty(alias, 'onclick', {
        set: function(fn){ vis.onclick = fn; },
        get: function(){ return vis.onclick; },
        configurable: true
      });
      alias.focus = function(){ vis.focus(); };
      alias.blur = function(){ vis.blur(); };
    } catch(e) {
      try{ console && console.warn && console.warn('alias delegation failed', e); }catch(_){} 
    }
  }
  if(document.readyState === 'loading'){
    document.addEventListener('DOMContentLoaded', ensureDelegation);
  } else {
    ensureDelegation();
  }
})();
</script>
<noscript><div style='color:#b91c1c'>JavaScript is disabled. Tables will be shown statically. For large tables enable JS for virtualization.</div></noscript>
<div id="tocBar" role="navigation" aria-label="Table of contents"><ul><li class="toc-item"><a class="toc-link" href="#Table1">Table 1</a></li>
<li class="toc-item"><a class="toc-link" href="#Table2">Table 2</a></li>
<li class="toc-item"><a class="toc-link" href="#Table3">Table 3</a></li>
<li class="toc-item"><a class="toc-link" href="#Table4">Table 4</a></li></ul></div></div>
<div class="table-caption" id="Table1" data-table="Docu_0177_01" style="margin-top:2mm;margin-left:3mm;"><strong>Table 1</strong></div>
<div class="table-wrapper" data-table-id="table-1"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by DQ_MatchMerge — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">DQ_MatchMerge — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong>Module purpose & top-level contract (executive summary)</strong><br><strong>Scope:</strong> Deterministic, auditable match-and-merge engine used by <code>DQGuard.xlam</code> to (a) generate reproducible <em>merge proposals</em> from candidate record sets and (b) apply merges safely either inline or as scheduled jobs. <br><strong>Primary responsibilities:</strong> candidate pruning (blocking), field-level similarity computation, deterministic scoring and tie-breaking, explainable merge proposal construction (with reversible <code>reversePlan</code>), safety & policy gating (dataset/regulatory), atomic persistence of proposals and jobs, safe atomic apply and undo, full audit & encrypted evidence linking, CI golden parity and hot-swap support. <br><strong>Non-goals & constraints:</strong> Not a generic ETL engine; SHOULD NOT perform heavy IO or network access on the UI thread; MUST not leak raw PII into public audit rows; MUST not auto-apply regulated merges without approvals. <br><strong>Determinism requirement:</strong> Given identical input records, <code>configHash</code>, and <code>engineSeed</code>, the engine must produce identical proposals (<code>proposal.payloadHash</code>) and identical audit chains. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>InitMatchEngine(config)</code> — Purpose, contract, parameters, invariants, observability, recovery, examples, and tests</strong><br><strong>Purpose & contract:</strong> initialize compiled runtime artifacts from <code>config</code> (match rules, comparators, blocking, scoring). Responsibilities: schema-validate config; precompile comparator functions and normalizers; seed deterministic RNG (<code>seed = config.seed || sha256(configHash||bootstrapTsFixed)</code>); prepare in-memory index metadata; compute <code>engineHash = sha256(configHash || compiledFingerprints)</code>; register engine with <code>DQ_Audit</code>. MUST be idempotent and fast on UI thread (target <50ms); heavy index builds deferred to background worker. <br><strong>Parameters & return:</strong> <code>config</code> object or configRef → returns <code>{engineHandle, engineHash, ready:true}</code> or <code>{errorCode, userHint}</code> (stable codes). Must never throw host-visible exceptions. <br><strong>Primary invariants:</strong> <br>1. <code>engineHash</code> changes only when a meaningful compiled artifact changes. <br>2. All comparator names in config map to compiled comparators (or compile-time errors raised). <br>3. RNG seed reproducible for golden runs. <br>4. No implicit network I/O. <br><strong>Observability & audit:</strong> emit <code>dq.match.engine.started</code> with <code>engineHash</code>, <code>configHash</code>, <code>correlationId</code>, <code>owner</code>, <code>startupLatencyMs</code>; on error emit <code>dq.match.engine.error</code> with stable <code>DQ_ERR_INIT_*</code>. <br><strong>Recovery:</strong> if engine fails to init, fall back to last-good config snapshot and emit <code>dq.match.engine.fallback</code>. <br><strong>Examples:</strong> normal init; hot-rebind returns same <code>engineHash</code> when config unchanged. <br><strong>Tests & CI:</strong> compile coverage tests; deterministic RNG tests (<code>r-test-001</code>), startup latency smoke test, static analyzer forbids sync IO. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>LoadMatchConfig(source)</code> — canonicalization, schema validation, signature verification, owner resolution, fallback policy, and tests</strong><br><strong>Purpose & contract:</strong> canonicalize and validate <code>match-rules.json</code> from embedded manifest or atomic local cache. Steps: canonical JSON (deep-sort keys), JSON Schema v7 validate, dedupe <code>ruleId</code> & <code>fieldId</code>, attach <code>OWNERS</code> entries, compute canonical <code>configHash=sha256(canonicalJson)</code>, verify digital signature locally if present, record <code>signatureFingerprint</code>. SHOULD NOT fetch remote resources synchronously. <br><strong>Failure modes & fallback policy:</strong> non-critical warnings → continue with reduced config and emit <code>dq.match.config.warning</code>; critical schema/signature errors → emit <code>dq.match.config.invalid</code>, disable auto-apply and present diagnostics ribbon. <br><strong>Observability & audit:</strong> emit detailed <code>dq.match.config.loaded</code> with <code>configHash</code>,<code>warningCount</code>,<code>errorList</code>. <br><strong>CI & tests:</strong> negative schema vectors, duplicate id tests, signature verification unit tests, golden canonicalization parity. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>IndexSnapshotManager(snapshotPath)</code> — snapshot load/save, concurrency, checksum, rebuild</strong><br><strong>Purpose & contract:</strong> manage persisted blocking/index snapshots used for fast candidate generation. API: <code>LoadSnapshot()</code>, <code>SaveSnapshot(atomically)</code>, <code>ValidateSnapshot(checksum)</code>, <code>RebuildIfStale()</code>. Writes must be atomic (write-temp → fsync → rename). Snapshots include <code>engineHash</code> & <code>configHash</code> and are invalidated on mismatch. <br><strong>Observability & audit:</strong> emit <code>dq.match.snapshot.loaded</code>, <code>dq.match.snapshot.saved</code>, <code>dq.match.snapshot.rebuild</code>. <br><strong>Recovery:</strong> corrupted snapshot triggers rebuild task and <code>dq.match.snapshot.corrupt</code> audit. <br><strong>Tests:</strong> atomic write, partial-write corruption detection. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>NormalizeRecord(record, normalizers)</code> — canonicalization, reversibility, PII handling, examples, and tests</strong><br><strong>Purpose & contract:</strong> deterministically normalize record fields using configured normalizers: Unicode NFKC, trim/collapse whitespace, case-folding, punctuation strip (per-field), name canonicalization (strip honorifics), accent folding (preserve accent map for reverse plan), phone canonicalization (E.164-ish), email normalization (local-part normalization per provider rules), address canonicalization (country-specific if available), numeric/date normalization. MUST return <code>{normRecord, reverseMap}</code> where <code>reverseMap</code> maps normalized value→original fragments to enable precise undo. <br><strong>PII policy:</strong> logs and telemetry must never include raw PII—use <code>paramsHash</code> and store sanitized full evidence in encrypted evidence store with <code>evidenceRef</code>. Reverse maps stored encrypted only. <br><strong>Determinism:</strong> locale-insensitive unless <code>config.locale</code> present (must be recorded in <code>engineHash</code>). <br><strong>Examples:</strong> <code>&quot;Ms. María-José O&#x27;Neill&quot;</code> → <code>maria jose oneill</code> with <code>reverseMap</code> preserving diacritics + original punctuation. <br><strong>Tests:</strong> locale parity, reversibility unit tests, redaction verification. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ComputeBlockingKey(normRecord, blockingSpec)</code> — cheap deterministic keys</strong><br><strong>Purpose & contract:</strong> compute inexpensive blocking keys (may be multi-key) to partition candidate space. <code>blockingSpec</code> supports expression language (substr, soundex, ngram hash, domain hash) and multi-key fanout. Must be O(1) per input, null-safe, and deterministic. <br><strong>Performance & invariants:</strong> keep keys cheap; avoid fuzzy transforms here; collisions expected and handled in candidate stage. <br><strong>Observability:</strong> emit <code>dq.match.blockkey.generated</code> with <code>recordId</code>, <code>blockKeys[]</code>, <code>bucketEstimate</code>. <br><strong>Tests:</strong> collision rate estimation tests, edge cases for null/empty fields. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>GenerateCandidates(blockKey, indexSnapshot, paginationCursor, radius)</code> — deterministic retrieval & paging</strong><br><strong>Purpose & contract:</strong> return deterministic ordered candidate IDs for a <code>blockKey</code>. Deterministic ordering keys: <code>ownerPriority</code>, <code>completenessScore</code> (non-null field count), <code>lastNormalizedValue</code> (lexicographic), <code>recordId</code>. Support <code>radius</code> to include adjacent blocking buckets. Return <code>{candidates[], candidatesHash, nextCursor}</code>. Must be streamable for large buckets and stable across identical index snapshots. <br><strong>Observability:</strong> emit <code>dq.match.candidates</code> with <code>count</code>, <code>candidatesHash</code>, and <code>cursor</code>. <br><strong>Tests:</strong> pagination stability, snapshot parity, performance under large buckets. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ComparePair(lhsNorm, rhsNorm, comparatorSpec)</code> — field comparators & evidence artifacts</strong><br><strong>Purpose & contract:</strong> compute per-field similarity features dictated by <code>comparatorSpec</code> and return <code>{fieldScore, features, evidenceHash}</code>. Supported comparators: exact, tokenJaccard, normalizedEditDistance (weighted), ngramOverlap, Soundex/similarity, numericDistance (relative), dateDelta buckets, custom regex matching. Each comparator returns stable features used by scoring. <br><strong>Explainability/evidence:</strong> include <code>featureBuckets</code> and <code>explain</code> strings for UI; sensitive field evidence stored encrypted—only <code>evidenceHash</code> emitted in audit. <br><strong>Determinism & performance:</strong> pure function, deterministic, optimized for short-circuiting when possible. <br><strong>Tests:</strong> comparator correctness matrices, unicode edge cases, adversarial string fuzz. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ComputeMatchScore(pairEvidence, scoringModel)</code> — model-based scoring & explainability</strong><br><strong>Purpose & contract:</strong> aggregate field-level features into scalar <code>score</code> ∈ [0,1] using deterministic <code>scoringModel</code> (linear weights, logistic calibration, or compiled decision rules). Return <code>{score, modelVersion, featureContributions[], topReasons[]}</code>. Must expose per-feature contribution for audit/UI explainability and be deterministic. <br><strong>Governance:</strong> model changes require PR, unit tests, golden runs, and <code>dq.match.model.deployed</code> audit. <br><strong>Tests:</strong> calibration curves, sensitivity tests, regression seeds. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>DecideMatch(score, thresholds, datasetPolicy)</code> — deterministic decision mapping</strong><br><strong>Purpose & contract:</strong> map score to classification <code>{MATCH, PROBABLE, REVIEW, NO_MATCH}</code> using config thresholds and dataset-specific policy adjustments (e.g., raise MATCH threshold for regulated datasets). Return <code>{decision, rationale, appliedThresholds}</code>. Decisions must be auditable and reproducible. <br><strong>Edge handling:</strong> at boundary equals must use deterministic tie-break (see <code>ResolveTie</code>). <br><strong>Tests:</strong> boundary tests, conservative-mode behavior. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ResolveTie(candidatePairs[], tieBreakerSpec, deterministicSeed)</code> — canonical tie-break & multi-merge rules</strong><br><strong>Purpose & contract:</strong> when candidates have identical scores or ranking, deterministically pick winners using <code>tieBreakerSpec</code>: owner priority, completeness, recency, smaller <code>recordId</code>, or deterministic RNG seeded by <code>sha256(engineHash|correlationId)</code>. Support N-way merges if policy allows. Return <code>{winners[], secondaryChoices[], tieRationale}</code>. <br><strong>Governance & safety:</strong> tie-breaker causing cross-tenant or PII aggregation triggers <code>REVIEW</code> and requires approval. <br><strong>Tests:</strong> reproducibility under stress, N-way selection parity. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>BuildMergeProposal(primaryId, secondaryIds[], resolutionSpec, engineMeta)</code> — canonical, immutable proposal</strong><br><strong>Purpose & contract:</strong> assemble canonical <code>MergeProposal</code> JSON that is immutable once persisted. Fields: <code>proposalId</code> (canonical derivation), <code>primaryRecordId</code>, <code>secondaryRecordIds</code>, <code>fieldResolutionPlan</code> (for each field: chosenSource, mergeExpression, confidence, rationale), <code>reversePlan</code> (undo mapping), <code>estimatedImpact</code> (row counts, PII move flags), <code>engineHash</code>, <code>configHash</code>, <code>payloadHash=sha256(canonicalProposal)</code>, <code>requiredApprovals[]</code>, <code>owner</code>, <code>createdAt</code>. Same inputs produce identical <code>payloadHash</code>. <br><strong>Security:</strong> store full sanitized proposal in <code>evidenceStore</code> encrypted; public artifact stores only <code>payloadHash</code> and <code>evidenceRef</code> as needed. <br><strong>UI contract:</strong> include <code>previewRef</code> and per-field <code>explainability</code> entries. <br><strong>Tests:</strong> canonicalization parity, reversePlan undo correctness, idempotency. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>EvaluateProposalSafety(proposal, datasetPolicy)</code> — policy gating & approval resolution</strong><br><strong>Purpose & contract:</strong> validate proposal against dataset policies: regulated flags, PII movement, cross-tenant detection, retention/immutability constraints, and two-person requirements. Return <code>{safe:Boolean, requiredApprovals[], riskScore, holdReason}</code>. On unsafe outcome emit <code>dq.merge.proposal.hold</code> and block auto-apply. <br><strong>Operator UX:</strong> return clear <code>requiredApprovals</code> and <code>rationale</code>. <br><strong>Tests:</strong> policy matrix coverage and approval routing. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>PersistProposal(proposal, persistenceBackend)</code> — atomic persistence, idempotency, and checksums</strong><br><strong>Purpose & contract:</strong> persist <code>proposal</code> and <code>evidenceRef</code> atomically using <code>DQ_Export</code> path (temp → fsync → rename). Ensure idempotency (same <code>proposalId</code> returns existing artifact). Compute <code>artifactChecksum=sha256(canonicalProposal)</code> and append <code>dq.merge.proposal.persisted:&lt;proposalId&gt;</code> audit with <code>artifactChecksum</code>. Retry transient failures with bounded backoff; on persistent failure emit <code>dq.merge.persist.error</code>. <br><strong>Recoverability:</strong> persist metadata includes <code>persistedAt</code>, <code>owner</code>, <code>artifactLocation</code>. <br><strong>Tests:</strong> idempotency, crash-resume checks, checksum verification. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ScheduleOrApply(proposal, decisionContext)</code> — inline vs scheduled policy evaluation</strong><br><strong>Purpose & contract:</strong> decide inline apply vs scheduled job. Inputs: <code>estimatedImpact.rows</code>, <code>requiresApproval</code>, <code>datasetPolicy.regulated</code>, <code>safeMode</code>, <code>operatorOverride</code>, <code>IsLightweightAction</code> analog. Action: if scheduled → persist <code>jobDescriptor</code> and call <code>JobSchedulerIntegration</code>, emit <code>job.persisted:&lt;jobId&gt;</code> and return <code>{status:scheduled, jobId, correlationId}</code>; if inline → call <code>SafeInvokeMergeHandler</code> with <code>cancelToken</code> and return immediate <code>{status:applied|failed, correlationId, applyId}</code>. Must return quickly on UI thread (<50ms). <br><strong>UI contract:</strong> short message including <code>correlationId</code> and next steps. <br><strong>Tests:</strong> decision parity near thresholds, latency tests. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>SafeInvokeMergeHandler(proposal, cancelToken, correlationId)</code> — protected inline execution frame</strong><br><strong>Purpose & contract:</strong> safely execute merges inline within a guarded frame: validate permissions, create invocation audits <code>dq.merge.handler.start</code>, set cooperative timeout & <code>cancelToken</code>, run apply stages with sub-step audits (<code>step.start</code>, <code>step.complete</code>), redact PII in any user-facing messages, map exceptions to stable codes <code>DQ_ERR_*</code>, and return structured result. Must support cancellation checks and partial progress reporting. <br><strong>Invariants:</strong> inline handlers must not perform heavy blocking IO; if heavy, fail-fast and recommend scheduled job. <br><strong>Telemetry:</strong> emit <code>dq.merge.duration_ms</code>, <code>dq.merge.success</code>, <code>dq.merge.error</code> with tags. <br><strong>Tests:</strong> cancellation tests, exception mapping tests, partial-progress audit presence. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ApplyMergeAtomic(proposal, storageEngine, reversePlan)</code> — atomic mutation & undo</strong><br><strong>Purpose & contract:</strong> perform dataset mutations specified in <code>fieldResolutionPlan</code> atomically: create encrypted snapshot or row-level journal, apply per-field changes, update indexes and foreign keys, recompute checksums, persist transaction atomically. On failure use <code>reversePlan</code> + snapshot to rollback. Return <code>{applyId, beforeChecksum, afterChecksum, artifactRef}</code>. Large applies must be scheduled to worker. <br><strong>Invariants:</strong> maintain referential integrity, preserve <code>reversePlan</code> fidelity, and produce deterministic <code>afterChecksum</code>. <br><strong>Tests:</strong> transactional integrity under simulated crashes, reversePlan replay tests. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>EmitMergeAudit(correlationId, proposalId, step, metadata)</code> — canonical audit anchor</strong><br><strong>Purpose & contract:</strong> append authoritative audit rows for lifecycle events: <code>proposal.created</code>, <code>proposal.persisted</code>, <code>merge.started</code>, <code>merge.completed</code>, <code>merge.reverted</code>, <code>merge.timeout</code>, <code>merge.cancelled</code>. Schema required: <code>timestamp,correlationId,module=DQ_MatchMerge,procedure,proposalId,payloadHash,configHash,engineHash,prevHash,metadata,paramsHash,evidenceRef(optional)</code>. Main audit stores only <code>paramsHash</code>; sensitive parameters stored encrypted referred by <code>evidenceRef</code>. Audit appends non-blocking to <code>DQ_Audit</code>. <br><strong>Chain & CI:</strong> audits should include <code>prevHash</code> when chaining is resolvable; <code>VerifyAuditChain</code> runs in CI/monitoring. <br><strong>Tests:</strong> schema validation, prevHash chain tests, evidence retrieval tests. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>RegisterUnitTestHook(hookName, fixedCorrelationId)</code> — deterministic CI hooks</strong><br><strong>Purpose & contract:</strong> register deterministic unit test hooks that accept fixed <code>correlationId</code> and seeded RNG for golden parity. Hooks flagged <code>test=true</code> in audits, must be disabled in production unless explicitly allowed with owner & approval. Return <code>hookHandle</code>. <br><strong>CI usage:</strong> golden-run compares <code>proposal.payloadHash</code> to golden artifacts. <br><strong>Tests:</strong> ensure hooks only active in test contexts; golden parity. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>RefreshMatchRules(newConfigJson)</code> — live rebind, diff, preview, smoke tests, and rollback</strong><br><strong>Purpose & contract:</strong> validate <code>newConfigJson</code> (schema+signature), compute diff vs active <code>configHash</code>, produce <code>hotSwap.preview</code> with impacted rules and risk estimate, run smoke tests using deterministic sample partitions via unit hooks, atomically swap in-memory rules if smoke tests pass, persist via <code>DQ_Export</code> optionally, and emit <code>dq.match.refresh.completed</code> with <code>beforeHash</code>/<code>afterHash</code>. Must not interrupt running jobs; failure triggers revert and <code>dq.match.refresh.error</code>. <br><strong>Smoke tests:</strong> exercise comparators and candidate generation on sample partitions. <br><strong>Tests:</strong> hot-swap dry runs & rollback correctness. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>PreviewMergeUI(primaryId, secondaryIds[], previewOptions)</code> — preview contract & safety</strong><br><strong>Purpose & contract:</strong> produce a compact deterministic preview artifact showing <code>N</code> sample merged rows, per-field chosen source, confidence scores, and <code>previewHash</code>. Must be fast (<500ms median) and match the actual apply result for the same <code>proposalId</code>. If operator lacks full PII permission produce <code>anonymizedPreviewRef</code> with redactions. Return <code>{previewRef, previewHash, sampleRowsCount}</code>. <br><strong>UI features:</strong> per-field <code>explain</code> text and <code>copyDiagnostics</code> button that includes <code>correlationId</code>. <br><strong>Tests:</strong> preview parity and permission gating. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ValidateUserPermissions(userId, proposal, controlMeta)</code> — RBAC & approvals</strong><br><strong>Purpose & contract:</strong> authoritative permission evaluation: SSO mapping, group membership, delegated approvals, two-person rules for regulated datasets, dataset-level protections. Return <code>{allowed, requiredApprovals[], denialReason}</code> and append <code>dq.merge.permission.check</code> audit. Emergency operator overrides require MFA and <code>ticketId</code> and are auditable as <code>dq.merge.emergency.override</code>. <br><strong>Tests:</strong> role matrix simulation and approval flows. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>BuildProposalImpactHash(proposal)</code> — canonical fingerprint for CI & QA</strong><br><strong>Purpose & contract:</strong> deterministic canonicalization of <code>proposal</code> JSON (sort keys, normalize numbers/dates, strip ephemeral fields, redact PII per redaction policy) then compute <code>sha256</code> as <code>proposal.hash</code> used for golden-file checks. Must be stable across locales and runs. <br><strong>Tests:</strong> deterministic hashing across permutations, locale invariance checks. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>HotSwapMergeRules(newRulesJson, operatorId, approvals)</code> — transactional emergency patching</strong><br><strong>Purpose & contract:</strong> apply urgent rule updates transactionally with required approvals: validate schema & signature, compute diff & <code>hotSwap.preview</code>, run smoke tests with unit hooks, apply in-memory atomically if smoke tests pass, persist via <code>DQ_Export</code> optionally, append <code>dq.match.hotswap.applied</code> with <code>beforeHash</code>/<code>afterHash</code> and <code>releaseFingerprint</code>. If smoke tests fail revert and append <code>dq.match.hotswap.reverted</code>. <br><strong>Governance:</strong> regulated changes require explicit approvals and audit. <br><strong>Tests:</strong> dry-run validations and rollback correctness. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>SafeHandlerTimeoutWatchdog(handlerToken, correlationId)</code> — cooperative cancellation & escalation</strong><br><strong>Purpose & contract:</strong> monitor inline handler execution and on overrun: emit <code>dq.merge.timeout</code>, attempt cooperative cancellation via <code>handlerToken</code>, and if unsuccessful emit <code>dq.merge.hung</code> with stack snapshot for SRE. Use host idle callbacks or <code>Application.OnTime</code> in VBA contexts. Repeated timeouts escalate incident. <br><strong>Tests:</strong> forced overrun, cancellation effect, audit presence. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>Shutdown()</code> — graceful unload, audit flush, and snapshot</strong><br><strong>Purpose & contract:</strong> flush audit buffers, persist minimal snapshot (<code>lastConfigHash</code>, <code>lastCorrelationId</code>, <code>engineSnapshot</code>), unregister unit-test hooks, and append <code>dq.match.shutdown</code> audit. Register with bootstrap shutdown order to allow audit flush first. On unclean exit <code>InitMatchEngine</code> must emit <code>dq.match.recovery</code>. <br><strong>Tests:</strong> snapshot & audit flush verification. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>JobSchedulerIntegration(jobDescriptor)</code> — canonical job descriptor & worker handoff</strong><br><strong>Purpose & contract:</strong> create canonical <code>jobDescriptor</code> for scheduled merges and persist atomically for worker consumption. Descriptor fields: <code>jobId, proposalId, correlationId, paramsHash, configHash, persistedAt, owner, retryPolicy</code>. Persist via atomic export and emit <code>job.persisted:&lt;jobId&gt;</code>. Must be idempotent. <br><strong>Tests:</strong> idempotency & worker handoff simulation. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>MergeSimulationDryRun(proposal, samplePartition, limits)</code> — deterministic dry-run for smoke tests</strong><br><strong>Purpose & contract:</strong> simulate applying <code>proposal</code> to a sample partition without mutating source data. Return <code>dryRunReport</code> with estimated change set, checksum delta, and <code>dryRunHash</code>. Use in hot-swap smoke tests and operator reviews. Must not leak unredacted PII in logs. <br><strong>Tests:</strong> dry-run parity vs actual apply on sample partition. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ProposalDiff(oldProposal, newProposal)</code> — canonical diff for reviews</strong><br><strong>Purpose & contract:</strong> compute canonical, field-level diffs and <code>diffHash</code> between proposals; used in hotswap previews and approval flows. Highlight changed fields, resolutions, and approval requirements. <br><strong>Tests:</strong> canonical diff parity across permutations, diff size limits. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ProposalApprovalWorkflow(proposalId, approverId, action, approvalMetadata)</code> — approvals & audit</strong><br><strong>Purpose & contract:</strong> record approval actions (approve/reject/comment), append <code>dq.merge.approval</code> audit with <code>correlationId</code>, <code>approverId</code>, and optional <code>evidenceRef</code>. When required approvals satisfied transition <code>proposal</code> to <code>ready</code>. Support time-limited approvals and delegated approvals. <br><strong>Tests:</strong> approval gating & expiry. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>EvidenceStoreIntegration(evidenceBlob, accessPolicy)</code> — secure encrypted evidence persistence</strong><br><strong>Purpose & contract:</strong> persist encrypted evidence to secure evidence store with metadata, TTL, and accessPolicy. Return <code>evidenceRef</code> opaque token. Evidence encrypted using KMS-backed keys and access gated by RBAC. Retrieval requires approval trace. Append <code>dq.match.evidence.persisted</code> audit with <code>evidenceRef</code> (no raw PII). <br><strong>Tests:</strong> KMS mock, TTL enforcement, retrieval audit trail. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ForensicExport(correlationId, artifactList, operatorId)</code> — canonical forensic package</strong><br><strong>Purpose & contract:</strong> assemble and securely export forensic artifacts: <code>match-rules.json</code>, <code>configHash</code>, <code>engineHash</code>, <code>audit_tail.csv</code> rows for correlation ids, <code>proposal.json</code>, <code>job.descriptor</code>, <code>engineSnapshot</code>, <code>forensic_manifest.json</code> with checksums and chain-of-custody metadata. Persist via <code>REG_Export</code> and append <code>dq.match.forensic.export</code> audit. Access controlled. <br><strong>Tests:</strong> manifest completeness & checksum verification. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>TelemetryEmit(metricName, value, tags)</code> — local buffering & audited upload</strong><br><strong>Purpose & contract:</strong> append metrics to local buffer (no direct network from ribbon path). Tags include <code>proposalId</code>,<code>module</code>,<code>datasetPolicy</code>. Metric uploader (privileged worker) performs network export. Metrics: <code>dq.match.candidates_count</code>, <code>dq.merge.duration_ms</code>, <code>dq.merge.timeout_rate</code>. Append <code>dq.match.metric.buffered</code> audit optionally. <br><strong>Tests:</strong> buffer durability & uploader compatibility. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>MetricsBufferUploader()</code> — audited uploader of metrics</strong><br><strong>Purpose & contract:</strong> run in privileged worker to upload metric batches and emit <code>dq.match.metrics.uploaded</code> audit with batch checksum and counts. Must respect redaction; tags must not contain PII. <br><strong>Tests:</strong> uploader retry semantics and loss simulation. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>AuditChainVerifier(sampleRunId)</code> — CI & monitoring verification</strong><br><strong>Purpose & contract:</strong> verify <code>dq.merge</code> audit chain integrity for sample runs: validate <code>prevHash</code> chaining, <code>payloadHash</code> parity with artifacts, verify audit rotation signatures. Emit <code>dq.match.audit.verify</code> with results. Failures block CI merges. <br><strong>Tests:</strong> golden audit-chain tests and mis-signed rotation detection. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ConfigRollForward(newConfig, operatorId)</code> & <code>ConfigRollback(targetConfigHash)</code> — controlled migrations</strong><br><strong>Purpose & contract:</strong> roll forward config with validation and smoke tests; rollback to <code>targetConfigHash</code> runs smoke tests and emits <code>config.rollback</code> audit. All config changes require PR, CI golden runs, and approvals. <br><strong>Tests:</strong> migration parity, rollback correctness. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>MergeConflictResolver(conflictingJobs[])</code> — worker-level job arbitration</strong><br><strong>Purpose & contract:</strong> reconcile concurrent scheduled jobs targeting overlapping recordsets. Deterministic arbitration: order by <code>job.persistedAt</code>, <code>operatorPriority</code>, <code>jobId</code>, then either serialize or merge job descriptors combining reversePlans. Emit <code>dq.merge.job.conflict.resolved</code> audit. <br><strong>Tests:</strong> conflict simulation and combined reversePlan verification. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>CrossTenantGuard(proposal)</code> — multi-tenant isolation</strong><br><strong>Purpose & contract:</strong> detect cross-tenant merges. Default: block and return <code>requiresApproval</code> with approvers required from both tenants. Emit <code>dq.merge.crossTenant.blocked</code> or <code>dq.merge.crossTenant.allowed</code> audit. <br><strong>Tests:</strong> cross-tenant detection and approval path. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>AnonymizePreview(previewRef, userPermissions)</code> — redaction of previews</strong><br><strong>Purpose & contract:</strong> produce redacted preview replacing PII with placeholders or hashed tokens while preserving structure and confidence. Return <code>anonymizedPreviewRef</code>. Full preview accessible only via <code>evidenceRef</code> with approval. <br><strong>Tests:</strong> redaction correctness & permission gating. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ReconciliationRunner(beforeChecksum, afterChecksum, samplePartition)</code> — verification after apply</strong><br><strong>Purpose & contract:</strong> recompute sample checksums, validate counts and referential integrity post-apply, emit <code>dq.merge.reconciliation</code> audit with <code>reconReportRef</code>. On mismatch schedule forensic export and possible revert. <br><strong>Tests:</strong> recon parity and failover flows. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ExportMergeArtifact(artifactRef, destinationUri, operatorId)</code> — secure export with chain-of-custody</strong><br><strong>Purpose & contract:</strong> securely export artifacts via <code>REG_Export</code> path, compute <code>artifact.checksum.sha256</code>, redact owner emails when operator lacks rights, append <code>dq.merge.export</code> audit with URI and checksum. <br><strong>Tests:</strong> checksum validation and redaction checks. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ErrorCodeCatalog (module)</code> — canonical error taxonomy & operator mapping</strong><br><strong>Purpose & contract:</strong> centralized error codes: <code>DQ_ERR_INIT_001</code> (config missing), <code>DQ_ERR_CONFIG_INVALID</code>, <code>DQ_ERR_PERSIST_FAIL</code>, <code>DQ_ERR_PERMISSION_DENIED</code>, <code>DQ_ERR_TIMEOUT</code>, <code>DQ_ERR_CONFLICT</code>, etc. Each code maps to operator message (no PII) and diagnostic artifact <code>diagRef</code>. Errors must be used consistently and included in audits. <br><strong>Tests:</strong> ensure thrown errors map to catalog and include audit rows. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>Performance budgets &amp; SLOs (module)</code></strong><br><strong>Targets & metrics:</strong><br>- Candidate generation median <20ms for small buckets (<1k).<br>- Pair scoring median <5ms. <br>- Inline apply default timeout 5s (configurable).<br>- Job persist latency <2s. <br><strong>Metrics collected:</strong> <code>dq.match.candidates_count</code>, <code>dq.merge.duration_ms</code>, <code>dq.merge.timeout_rate</code>, <code>job.persist.latency_ms</code>. <br><strong>Runbook:</strong> if budgets breached: throttle inline applies; shift to scheduled-only; run <code>audit_chain.verify</code> on sample runs; ramp down features (e.g., complex comparators) and invoke operator notification. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>Testing matrix (module)</code></strong><br><strong>Required tests:</strong> unit: <code>NormalizeRecord</code>, <code>ComparePair</code>, <code>ComputeMatchScore</code>, <code>ResolveTie</code>, <code>BuildMergeProposal</code>, <code>BuildProposalImpactHash</code>. Integration: candidate→score→proposal→persist→job→worker→apply→reconcile. Golden: <code>proposal.payloadHash</code> parity tests. CI gating: audit-chain verify, static checks forbidding forbidden APIs. Property: correlation id uniqueness under load. <br><strong>CI enforcement:</strong> block merges on golden/audit-chain failures. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>Failure modes &amp; mitigations (module)</code></strong><br><strong>Common incidents & mitigations:</strong><br>1. Invalid config/signature → <code>dq.match.config.invalid</code> → revert to last-good config and notify owner. <br>2. Excessive blocking collisions → <code>dq.match.blocking.warning</code> → operator retune <code>blockingSpec</code>. <br>3. Partial apply failure → <code>dq.merge.reverted</code> using <code>reversePlan</code> + snapshot; open incident. <br>4. Permission denied → <code>dq.merge.permission.denied</code> → require approvals. <br>5. Watchdog timeouts → <code>dq.merge.timeout</code> → schedule job. <br><strong>Forensics:</strong> collect <code>match-rules.json</code>, <code>configHash</code>, <code>engineHash</code>, <code>audit_tail.csv</code> rows for <code>correlationId</code>, <code>proposal.json</code>, <code>job.descriptor</code>, <code>engineSnapshot</code>, <code>forensic_manifest.json</code>. <br><strong>Operator recovery checklist:</strong> locate <code>correlationId</code>, retrieve audit chain, fetch <code>evidenceRef</code>, run dry-run in isolated runner, revert <code>hotSwap</code> if needed, restore dataset from snapshot. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>Operator UX &amp; triage notes (module)</code></strong><br><strong>Best-practice UI:</strong> always show <code>correlationId</code> and provide "copy diagnostics" and "download forensic package" actions. Provide clear <code>preview</code>, <code>simulate</code>, <code>apply (inline)</code>, <code>apply (scheduled)</code> with <code>requiresApproval</code> flags. Show succinct <code>safe/unsafe</code> badge with <code>requiredApprovals</code> when applicable. <br><strong>Triage flow:</strong> 1) get <code>correlationId</code>; 2) fetch <code>UserAction</code> and step audits; 3) request <code>evidenceRef</code> if needed (approval); 4) run <code>dry-run</code> in isolated runner; 5) collect <code>forensic_manifest</code> and escalate. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>Change-control &amp; governance (module)</code></strong><br><strong>Flow for changes:</strong> PR + migration manifest if semantics change; static analysis forbidding forbidden APIs; unit/integration/golden tests; compliance/owner approvals for regulated changes; sign artifacts and publish release manifest; canary rollout with KPI gating; post-rollout <code>VerifyAuditChain</code> and <code>deployment.audit</code>. <br><strong>Hot-swap governance:</strong> smoke tests and approvals required before applying regulated comparator or threshold changes. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>Appendices &amp; references (module)</code></strong><br><strong>Include:</strong> canonical <code>match-rules.json</code> schema, audit row schema, <code>ErrorCodeCatalog.md</code>, <code>reversePlan</code> template, migration manifest template, forensic manifest, operator cheat-sheets, <code>OWNERS.md</code> mapping. Store appendices in immutable artifact store <code>\\artifacts\DQ_MatchMerge\v{major}.{minor}\appendices\</code> with RBAC. Provide runbooks: <code>dq_merge_smoke_test.md</code>, <code>dq_merge_incident_runbook.md</code>, <code>dq_merge_operator_cheatsheet.md</code>. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong>Acceptance criteria (dev/CI)</strong><br><strong>Gates:</strong> unit + integration + golden tests pass; no forbidden API in static analysis; <code>configHash</code> & <code>engineHash</code> computed; <code>dq.merge</code> audit chain present and <code>VerifyAuditChain</code> passes; performance budgets met under CI load. Blocking conditions: golden/audit-chain failures or forbidden-API detection. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong>Forbidden APIs & static enforcement (module)</strong><br><strong>Forbidden on UI path and inline handlers:</strong> direct Workbook/Range modification during <code>OnLoad</code> or critical UI callbacks; raw network calls (WinHTTP) or external web sockets; unbounded synchronous disk writes (>10ms); direct plaintext secret reads; spawning external processes. CI static analyzer rejects PRs referencing blacklisted APIs. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong>Operator-run checklists & CLI examples (compact)</strong><br><strong>Common operator commands:</strong> <code>dq.preview --proposal &lt;id&gt;</code>, <code>dq.apply --proposal &lt;id&gt; --mode scheduled --ticket &lt;id&gt;</code>, <code>dq.hotswap --config path --dry-run</code>, <code>dq.forensic.export --cid &lt;correlationId&gt; --out &lt;uri&gt;</code>, <code>dq.audit.verify --sample &lt;runId&gt;</code>. Each command emits audits and returns <code>correlationId</code>. Include operator TTL & MFA checks for emergency overrides. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong>Forensic package & chain-of-custody fields</strong><br><strong>Minimum artifacts:</strong> <code>match-rules.json</code> & signature, <code>audit_tail.csv</code> rows covering correlation ids, <code>proposal.json</code>, persisted <code>evidence</code> blobs (encrypted), <code>job.descriptors</code>, <code>engineSnapshot</code>, <code>configHash</code>, <code>forensic_manifest.json</code> mapping artifacts to checksums and storage URIs. Store in secure evidence repo with RBAC and C-O-C records. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong>Runbooks for common incidents (short)</strong><br><strong>Config invalid:</strong> retrieve <code>dq.match.config.invalid</code> audit, revert to last-good config, run <code>hotSwap.preview</code> on candidate fixes, apply after approval. <br><strong>Apply failure mid-transaction:</strong> run <code>dq.merge.reverted</code> audit check; fetch <code>reversePlan</code> from <code>evidenceRef</code>; run restore snapshot; open incident and generate <code>forensic_manifest</code>. <br><strong>Unexpected high timeouts:</strong> set inline applies to scheduled-only and scale workers; collect <code>dq.match.*</code> metrics and run smoke tests on active config. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong>Implementation guidance & safe IO patterns</strong><br><strong>IO policy:</strong> use read-then-verify-then-rename atomic patterns for writes; perform heavy index builds or persistence in background worker; no network during synchronous UI callbacks. <br><strong>Security:</strong> secrets via KMS/HSM; manifest signature verification locally; PII redaction enforced prior to any non-encrypted store writes. <br><strong>Coding patterns:</strong> pure functions for normalization and comparators; small deterministic RNG wrappers (<code>seed = sha256(engineHash|cid|stableSalt)</code>). <br><strong>Testing hooks:</strong> include deterministic test harness with fixed RNG seeds and <code>test=true</code> audit flags. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong>Operator & compliance checklist for regulated merges</strong><br>1. Confirm dataset <code>regulated</code> flag. <br>2. Generate <code>previewRef</code> and attach <code>correlationId</code>. <br>3. Acquire required approvals (two-person) via <code>ProposalApprovalWorkflow</code>. <br>4. Persist <code>proposal</code> and schedule apply. <br>5. Post-apply run <code>ReconciliationRunner</code> and include <code>reconReportRef</code> in compliance package. <br>6. Export <code>forensic_manifest</code> and append to regulatory package. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong>End-to-end example narrative (illustrative)</strong><br><strong>Normal flow:</strong> Operator selects candidate rows → <code>NormalizeRecord</code> → <code>ComputeBlockingKey</code> → <code>GenerateCandidates</code> → <code>ComparePair</code> per candidate → <code>ComputeMatchScore</code> → <code>DecideMatch</code> → <code>ResolveTie</code> → <code>BuildMergeProposal</code> → <code>EvaluateProposalSafety</code> (safe) → <code>PersistProposal</code> → <code>PreviewMergeUI</code> → Operator approves → <code>ScheduleOrApply</code> decides inline → <code>SafeInvokeMergeHandler</code> applies changes → <code>ApplyMergeAtomic</code> persists → <code>EmitMergeAudit</code> chain (<code>proposal.created</code>→<code>merge.started</code>→<code>merge.completed</code>) → <code>ReconciliationRunner</code> verifies. <br><strong>Fault flow:</strong> config invalid on hot-swap → <code>dq.match.config.invalid</code> audit → engine fails safe → operator notified; or partial apply failure → <code>dq.merge.reverted</code> + forensic package appended. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong>Glossary (compact)</strong><br><code>engineHash</code> — fingerprint of compiled engine artifacts; <code>configHash</code> — canonical match rules JSON hash; <code>proposal.payloadHash</code> — canonical proposal JSON sha256; <code>evidenceRef</code> — opaque pointer to encrypted evidence; <code>correlationId</code> — UI-level request id; <code>jobId</code> — persisted job descriptor id. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong>Final notes (operational & compliance)</strong><br>1. All user-initiated actions MUST emit a <code>UserAction</code> audit with <code>correlationId</code>. <br>2. All artifacts that include PII MUST be stored encrypted and referenced by <code>evidenceRef</code> in public audits. <br>3. Hot-swap & model updates must run smoke tests via deterministic harness with <code>test=true</code> hooks for CI golden parity. <br>4. CI pipeline requires <code>audit-chain-verify</code> and golden-file checks; failures block release. <br>5. Maintain appendices and runbooks in immutable artifact store with RBAC. </td></tr></tbody></table></div><div class="row-count">Rows: 58</div></div><div class="table-caption" id="Table2" data-table="Docu_0177_02" style="margin-top:2mm;margin-left:3mm;"><strong>Table 2</strong></div>
<div class="table-wrapper" data-table-id="table-2"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by DQ_Remediation — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">DQ_Remediation — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong>Module-level summary — Purpose, owners, public API, guarantees, invariants, and audit obligations (exhaustive)</strong><br><strong>Purpose & contract:</strong> authoritative module for converting data-quality findings (from <code>DQ_Profile</code>, <code>DQ_Rules</code>, <code>DQ_MatchMerge</code>) into safe, auditable remediation workflows. Responsibilities: generate deterministic proposals with before/after previews; compute auditable risk/confidence scores; cluster and rank proposals for operator review; build canonical remediation plans with explicit transactional boundaries and undo descriptors; validate plans against policy, runtime and approvals; provide deterministic dry-run simulations; execute remediation inline (safe, short) or schedule heavy jobs; persist canonical job descriptors idempotently for workers; produce reversible undo artifacts or record explicit non-reversible signoffs; export artifacts atomically with checksums and redaction manifests; build forensic packages on failures; notify owners/stakeholders safely; and ensure every operator-visible action produces canonical, chained audit rows. Must not perform silent destructive mutations; must never display PII in UI/audit main fields; must store full evidence encrypted and reference via <code>evidenceRef</code> in audits.<br><strong>Owners / manifest:</strong> module owner(s) and API published in <code>OWNERS.md</code> and module manifest; versioned <code>module.manifestHash</code> must be recorded on major transitions. <br><strong>Exposed API (canonical):</strong> <code>GenerateProposal</code>, <code>ScoreProposal</code>, <code>RankAndGroupProposals</code>, <code>BuildRemediationPlan</code>, <code>ValidateRemediationPlan</code>, <code>SimulateApply</code> (dry-run), <code>IsLightweightAction</code>, <code>ApplyRemediation</code>, <code>PersistRemediationJob</code>, <code>SafeRemediationExecutor.execute</code>, <code>BuildUndoPlan</code>, <code>RevertRemediation</code>, <code>ValidateApprovals</code>, <code>BuildUiPreview</code>, <code>ExportRemediationArtifacts</code>, <code>NotifyOwnersAndStakeholders</code>, <code>ExportForensicsForFailedApply</code>, <code>SafeErrorToUser</code>, <code>RegisterUnitTestHook</code>. Each API must declare stable identifiers returned (<code>proposalId</code>, <code>planId</code>, <code>applyId</code>, <code>undoId</code>, <code>jobId</code>) and include <code>correlationId</code> in parameters or context. <br><strong>Primary invariants (must/shall):</strong><br>1. Determinism: given identical <code>tableRef</code>, <code>findings</code>, <code>configHash</code>, and <code>seed</code>, <code>GenerateProposal</code> and subsequent plan hashes must be identical. Canonical JSON (sorted keys, normalized numbers/dates) is used to compute SHA256-based <code>proposalHash</code> / <code>planHash</code> / <code>undoHash</code>.<br>2. Audit anchoring: every user-initiated action appends a <code>UserAction</code>/<code>dq_*</code> audit row with <code>correlationId</code> and safe <code>paramsHash</code>; full sanitized evidence stored encrypted with <code>evidenceRef</code> referenced by audit metadata.<br>3. No silent destructive changes: destructive operations must be explicitly <code>dq_apply</code>-audited and approved per governance rules.<br>4. Reversibility: destructive operations must produce <code>UndoPlan</code> or record explicit signed operator acknowledgement when undo impossible.<br>5. Sensitive data: UI hints and main audit messages must be PII-free; full sanitized evidence stored encrypted using KMS/HSM keys.<br>6. Performance budgets: proposal generation median <200ms for small tables (<10k rows), dry-run preview for 10k sample <2s, inline apply default timeout 5s (configurable).<br><strong>Audit obligations & schemas:</strong> <code>dq_proposal</code>, <code>dq_proposal.preview</code>, <code>dq_proposal.accepted</code>, <code>dq_plan.built</code>, <code>dq_plan.validate</code>, <code>dq_apply.start</code>, <code>dq_apply.dryrun</code>, <code>dq_apply.complete</code>, <code>dq_apply.failed</code>, <code>dq_apply.reverted</code>, <code>dq_undo.built</code>, <code>dq_export.remediation</code>, <code>forensic.export</code>, <code>dq_approval.*</code>. Each audit includes schema fields: <code>timestamp, correlationId, module=DQ_Remediation,procedure,operatorId,proposalId,planId,applyId,paramsHash,configHash,prevHash,payloadHash,artifactChecksum,metadata</code>. Evidence stored separately with <code>evidenceRef</code> and access controlled. <br><strong>CI/QA gating:</strong> golden-file parity for <code>proposalHash</code>/<code>planHash</code> on representative fixtures; static analyzer gating forbidding direct workbook writes during proposal/dry-run; unit/integration/infrastructure tests; <code>VerifyAuditChain</code> runs in CI. <br><strong>Checked 10×:</strong> canonicalization rules, audit presence, PII redaction, undo completeness, deterministic RNG seed propagation, idempotent job persistence, evidence encryption, manifest signature checks for <code>AUTO_APPLY</code>, approval validation flow, runbook triggers for failure modes. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>GenerateProposal(tableRef, findings[], options)</code> — canonical candidate synthesis</strong><br><strong>Purpose & contract:</strong> synthesize candidate remediation actions from a canonical <code>tableRef</code> and <code>findings[]</code>. Returns <code>Proposal[]</code> and <code>proposalSummary</code> where each <code>Proposal</code> includes <code>proposalId</code>, <code>actions[]</code>, <code>affectedCount</code>, <code>sampleBefore[]</code>, <code>sampleAfterPreviewRef</code>, <code>confidence</code>, <code>rationale</code>, <code>proposalHash</code>, <code>evidenceRef</code>. Must be side-effect free and fast on UI path for small inputs (target <200ms). <br><strong>Parameters & types:</strong> <code>tableRef</code> (immutable descriptor: workbookId/sheet/queryName, rowCount, checksum, schemaHash), <code>findings[]</code> (structured: <code>ruleId</code>, <code>severity</code>, <code>rowMatches</code>, <code>context</code>), <code>options</code> (maxProposals:int, sampleSize:int, preferNonDestructive:bool, seed:int). Returns deterministic proposals and <code>proposalHash=sha256(canonicalJSON(proposal))</code>. <br><strong>Deterministic steps (must/shall):</strong><br>1. Normalize table schema: stable column ordering, normalized types (dates -> ISO8601), trimmed strings, consistent unicode normalization (NFC).<br>2. For each <code>finding</code> consult <code>RemediationPolicy</code> ordered by policy priority to enumerate safe actions: <code>flag</code>, <code>standardize</code>, <code>mapReplace</code>, <code>dictionaryReplace</code>, <code>suggestCanonical</code>, <code>mergeCandidate</code>, <code>nullToDefault</code>, <code>dropRow</code> (last-resort), <code>manualReview</code>.<br>3. For fuzzy matches or merge candidates call <code>DQ_MatchMerge</code> deterministic functions with provided <code>seed</code>. Record <code>matchDistance</code>, <code>tieBreaker</code> metadata.<br>4. Produce <code>sampleBefore</code>/<code>sampleAfter</code> by applying candidate action to deterministic sample subset (<code>sampleSize</code>, seeded selection). Strip/replace PII in samples for audit/UI; store full sanitized sample encrypted with <code>evidenceRef</code>.<br>5. Compute <code>confidence</code> score (canonical aggregation) and generate <code>rationale</code> (PII-free explanation).<br>6. Canonicalize and compute <code>proposalHash</code>. Append <code>dq_proposal</code> audit with <code>proposalId, proposalHash, affectedCount, correlationId, configHash</code> and <code>evidenceRef</code> in metadata.<br><strong>Observability & evidence policy:</strong> Keep visible sample small (<500 rows). Full proposal evidence saved encrypted with <code>evidenceRef</code>; <code>dq_proposal</code> audit stores <code>proposalHash</code> and <code>paramsHash</code> only. <br><strong>Restrictions & safe I/O:</strong> do not perform workbook writes; heavy dictionary or network operations scheduled to worker thread with <code>Deferred</code> flow. For PQ-sourced tables prefer cached snapshots. <br><strong>Examples:</strong> phone normalization via deterministic regex + mapping; merge duplicates using <code>mostComplete</code> tie-breaker; propose <code>null-&gt;default</code> for missing categorical fields. <br><strong>Tests & CI vectors:</strong> deterministic proposal generation, negative tests for malformed inputs, large-dictionary worker scheduling tests, golden <code>proposalHash</code> check. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>ScoreProposal(proposal, scoringConfig)</code> — reproducible, explainable scoring</strong><br><strong>Purpose & contract:</strong> calculate reproducible <code>scores</code> for ranking and gating: <code>confidence (0..1)</code>, <code>risk (0..1)</code>, <code>costEstimate (ms/rows + label)</code>, <code>reversibility (bool)</code>, and <code>regulationImpact</code>. Returns <code>proposal</code> augmented with <code>scores</code>, <code>scoreRationale[]</code> and <code>scoreHash</code> computed as SHA256 over canonicalized score object. Must include <code>scoringConfig.hash</code> in audit. <br><strong>Inputs:</strong> <code>proposal</code> (from <code>GenerateProposal</code>), <code>scoringConfig</code> (weights, thresholds, regulatedFields[], pilotCohorts[]). <br><strong>Algorithm (deterministic):</strong><br>1. Extract raw features: <code>matchQuality</code>, <code>coverage</code>, <code>sampleSuccessRate</code>, <code>ruleSeverity</code>.<br>2. Weighted aggregation: <code>confidence = clamp(sigmoid(weightedSum(features)), 0..1)</code>.<br>3. <code>risk = baseRisk(actionTypes) + regulationMultiplier(if regulatedFields)</code>.<br>4. <code>costEstimate</code> derived from <code>affectedCount × perActionCost</code> from config and runtime microbench tables. Return human-friendly <code>costLabel</code> and numeric estimate <code>costMs</code>.<br>5. <code>reversibility = boolean</code> if <code>UndoPlan</code> completeness passes validation. <br><strong>Governance & enforcement:</strong> if <code>risk &gt;= riskThreshold</code> and <code>regulationImpact</code> is high, mark <code>requiresApproval</code> and attach required roles. <br><strong>Observability / audit:</strong> emit <code>dq_proposal.score</code> with <code>proposalId, confidence, risk, costEstimate, reversibility, scoringConfig.hash, scoreHash</code>. <br><strong>Tests:</strong> reproducibility across versions (include <code>scoringConfig.hash</code>), sensitivity tests for weight changes, adversarial tests (crafted inputs reducing confidence). </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>RankAndGroupProposals(proposals[], rankingPolicy)</code> — cluster, deconflict, stable ordering</strong><br><strong>Purpose & contract:</strong> group related proposals into operator-facing bundles and deterministically rank them. Return <code>ProposalGroup[]</code> with <code>groupId, proposalIds[], affectedFields[], affectedCount, impactSummary, estimatedTimeMs, groupConfidence, groupHash</code>. <br><strong>Grouping heuristics (configurable):</strong> row overlap threshold, field overlap, identical target columns, mutual-exclusion detection (conflicting proposals for same rows), and similarity clustering for transform aggregation. Groups must either be disjoint or contain explicit <code>overlapHint</code> indicating overlapping proposals. <br><strong>Stable ordering invariants:</strong> sort by <code>(-groupConfidence, groupRisk, estimatedCost, groupHash)</code> to ensure stability across reloads unless <code>rankingPolicy</code> changes (then <code>ribbonMap.hash</code> or <code>configHash</code> must record the change). <br><strong>UI contract & previews:</strong> provide <code>groupPreviewRef</code> and <code>collapseHint</code>; widen <code>affectedCount</code> > threshold into multi-page UI. <br><strong>Audit:</strong> emit <code>dq_proposal.grouping</code> with <code>groupId, proposalIds, groupHash</code>. <br><strong>Tests:</strong> grouping stability with permuted inputs, conflict detection correctness, UI preview checks. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>BuildRemediationPlan(group, operatorChoices, applyMode)</code> — concrete plan generation with undo wiring</strong><br><strong>Purpose & contract:</strong> compose finalized <code>RemediationPlan</code> from <code>ProposalGroup</code> and the operator's explicit choices; define ordered <code>actions[]</code>, transactional boundaries, <code>preconditions[]</code>, <code>postchecks[]</code>, <code>sideEffects[]</code>, <code>undoPlanDescriptor</code> and <code>executionHints</code> (e.g., <code>inlineTimeoutMs</code>, <code>stagingStrategy</code>). <code>applyMode</code> ∈ {<code>copy</code>, <code>inline</code>, <code>staged-job</code>}. Must be deterministic and idempotent for same inputs; compute <code>planHash</code>. Append <code>dq_plan.built</code> audit with <code>planId, planHash, operatorId, configHash</code>. <br><strong>Plan construction details:</strong><br>1. Normalize accepted proposals; resolve conflicts per <code>operatorChoices</code> or produce <code>resolutionHint</code> for manual resolution. <br>2. Expand each action into canonical step with <code>actionId</code>, <code>type</code>, <code>targetSpec</code> (column, rowMatcher canonical), <code>payload</code> (normalized), <code>preconditions</code>, <code>postchecks</code>, and <code>estimatedCostMs</code>. <br>3. Determine <code>transactionalBoundaries</code> to allow partial commit and rollback at safe checkpoints; prefer <code>copy</code> with swap for destructive changes. <br>4. Build <code>UndoPlan</code> concurrently with <code>RemediationPlan</code> (reverse steps + preimage collection spec). <br>5. Attach <code>requiredApprovals[]</code> metadata (roles, approver count, TTL) when <code>risk</code> or <code>regulationImpact</code> exceed thresholds. <br><strong>PII & evidence:</strong> plan persisted encrypted with <code>evidenceRef</code>; main audit contains <code>planHash</code> only. <br><strong>Examples:</strong> merge duplicate rows -> actions include <code>identifyDuplicates</code>, <code>computeMergeStrategy</code>, <code>applyMerge</code>, <code>postcheck.consistency</code>; undo includes <code>splitMergedRows</code> with preimage snapshots. <br><strong>Tests:</strong> parity across reruns yields identical <code>planHash</code>, undo completeness tests, conflict resolution tests. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>ValidateRemediationPlan(plan, runtimeContext)</code> — preflight safety & enforcement</strong><br><strong>Purpose & contract:</strong> perform static & dynamic validations and return <code>{valid:Boolean, errors[], warnings[], enforcementAction}</code> where <code>enforcementAction</code> ∈ {<code>allow</code>,<code>requireApproval</code>,<code>fail-closed</code>}. Must be side-effect free. <br><strong>Deterministic validation steps (detailed):</strong><br>1. Schema & integrity: ensure <code>planHash</code> matches canonical serialization; verify any signatures if <code>AUTO_APPLY</code> requested. <br>2. Preconditions: verify <code>tableRef.beforeChecksum</code> matches current workbook state; if mismatch return <code>fail-closed</code> with <code>R_PLAN_001_CHECKSUM_MISMATCH</code>. <br>3. Resource checks: workspace disk, temp sheet availability, job scheduler reachable if <code>staged-job</code> mode. <br>4. RBAC/Approval checks: call <code>ValidateUserPermissions</code> and <code>ValidateApprovals</code> for required roles and signoffs. <br>5. Concurrency: check for concurrent jobs/locks on the same table or columns; if conflict present, either fail or queue per <code>concurrencyPolicy</code>. <br>6. Reversibility checks: ensure <code>UndoPlan</code> exists and <code>preimageEvidenceRef</code> available for reversible steps. <br><strong>Enforcement & UI response:</strong> warnings can be accepted by operator (must be explicit), critical errors cause <code>fail-closed</code>. <code>dq_plan.validate</code> audit includes <code>planId, valid, errors[], enforcementAction</code>. <br><strong>Tests:</strong> expired approvals, permission-denied, checksum mismatches, concurrent lock scenarios. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>SimulateApply(plan, simulatorConfig)</code> — deterministic dry-run & reconciliation</strong><br><strong>Purpose & contract:</strong> perform a deterministic, non-destructive simulation of the <code>RemediationPlan</code> producing <code>previewDiffs</code>, <code>reconciliationReport</code>, <code>sampleBefore</code>, <code>sampleAfter</code>, <code>previewRef</code>, and <code>previewHash</code>. Must not mutate persistent workbook state; may create transient staging artifacts hidden from user if required and always remove them after simulation (unless evidence capture requested). <br><strong>Simulator parameters:</strong> <code>simulatorConfig</code> includes <code>seed:int</code>, <code>samplePercent</code>, <code>maxPreviewRows</code>, <code>previewMode</code> (<code>compact|full</code>), <code>sanitizePII</code> flag. <br><strong>Simulation steps:</strong><br>1. Create sanitized snapshot of target dataset in-memory or hidden temp sheet per <code>stagingStrategy</code>. <br>2. Apply actions in canonical order, capturing <code>stepPayloadHash</code> per step. <br>3. For merges/fuzzy matches produce conflict resolution logs; for each replaced value capture mapping count. <br>4. Run <code>postchecks</code> and produce <code>reconciliationReport</code> comparing pre/post checksums on the sample. <br>5. Compute <code>previewHash = sha256(canonicalPreview)</code>. Save full simulation evidence encrypted (<code>evidenceRef</code>) and return <code>previewRef</code> for UI. <br><strong>Observability & audit:</strong> emit <code>dq_apply.dryrun</code> with <code>planId, previewHash, correlationId</code>. <br><strong>Performance & UI behavior:</strong> small preview returned synchronously (<200ms) for typical sampleSize; large previews streamed/paginated on request. <br><strong>Tests:</strong> side-effect free verification, golden previewHash parity, redaction coverage. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>IsLightweightAction(plan)</code> — policy evaluation for inline vs scheduled execution</strong><br><strong>Purpose & contract:</strong> decide inline vs scheduled execution using <code>plan</code> metadata, <code>runtime.mode</code> (degraded/normal), <code>modConfig.thresholds</code> (rowCount, estimatedDuration), <code>risk</code>, and operator overrides. Return <code>{lightweight:Boolean, rationale:String, decisionHash}</code>. Decision must be auditable and reversible via explicit operator override (override action logged). <br><strong>Policy examples & thresholds:</strong><br>1. Inline if <code>estimatedDurationMs &lt; inlineTimeoutMs (default 5000ms)</code> AND <code>risk &lt; riskThreshold</code> AND <code>!affectsRegulatedFields</code>.<br>2. Hybrid: if <code>estimatedDuration</code> near threshold, propose sampling inline (preview) and schedule full job. <br>3. Safe-mode: force scheduling if runtime <code>safeMode=true</code>. <br><strong>Audit:</strong> emit <code>dq.apply.decision</code> with <code>planId, lightweight, rationale</code>. <br><strong>Tests:</strong> boundary conditions, safe-mode forced scheduling, operator override audit. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>ApplyRemediation(plan, operatorContext)</code> — authoritative apply orchestrator</strong><br><strong>Purpose & contract:</strong> authoritative dispatcher performing validated apply: create <code>applyId</code>, audit <code>dq_apply.start</code>, decide inline vs scheduled via <code>IsLightweightAction</code>, invoke <code>SafeRemediationExecutor</code> for inline or call <code>PersistRemediationJob</code> for scheduled, and emit final audit rows for success/failure. Must never mutate without prior audit and validation. <br><strong>Canonical orchestration (detailed):</strong><br>1. <code>validate = ValidateRemediationPlan(plan, runtimeContext)</code> — fail if <code>enforcementAction == fail-closed</code>.<br>2. <code>applyId = NewApplyId(parent=correlationId)</code>; record <code>createdAt</code>.<br>3. Emit <code>dq_apply.start</code> with <code>applyId, planId, operatorId, paramsHash</code>.<br>4. <code>decision = IsLightweightAction(plan)</code>.<br>5a. Inline path: create <code>cancellationToken</code>, call <code>SafeRemediationExecutor.execute(plan, cancellationToken)</code>. Record step-level audits and final <code>dq_apply.complete</code> with checksums and <code>artifactRef</code> on success. <br>5b. Scheduled path: assemble <code>jobDescriptor</code> and call <code>PersistRemediationJob(jobDescriptor)</code> -> return <code>job.persisted</code> audit and immediate UI message containing <code>jobId</code> and <code>applyId</code> reference. <br>6. On error: map to stable <code>DQ_Error</code> code, emit <code>dq_apply.failed</code>, trigger <code>ExportForensicsForFailedApply</code> as needed, and call <code>SafeErrorToUser</code> for UI-safe message. <br><strong>UI contract:</strong> short synchronous response with <code>status</code>, <code>message</code>, <code>correlationId</code>, <code>applyId</code>, and <code>nextSteps</code>. Encourage copy of <code>applyId</code> for triage. <br><strong>Idempotency:</strong> use <code>applyId</code> as idempotency key; replaying <code>ApplyRemediation</code> with same <code>applyId</code> should return stored result. <br><strong>Tests:</strong> concurrency stress (100s parallel applies), idempotency tests, partial-failure and checkpoint recovery tests. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>PersistRemediationJob(jobDescriptor)</code> — canonical job persistence & idempotency</strong><br><strong>Purpose & contract:</strong> persist canonical <code>jobDescriptor</code> for worker consumption; ensure atomic write and idempotency keyed by <code>paramsHash</code> and <code>applyId</code>. <code>jobDescriptor</code> fields: <code>jobId, planId, applyId, paramsHash, configHash, persistedAt, owner, evidenceRef, targetArtifacts[]</code>. Return <code>jobId</code> and ensure <code>job.persist.latency_ms</code> within targets. <br><strong>Persistence invariants:</strong> atomic-write via <code>CORE_Utilities.atomic-write</code>; check for existing descriptor with same <code>paramsHash</code> and return existing <code>jobId</code> (idempotent). Emit <code>job.persisted:&lt;jobId&gt;</code> audit row. <br><strong>Failure & retry policy:</strong> exponential backoff on transient store errors; after N retries escalate and produce <code>forensic_manifest</code> with <code>jobDescriptor</code> snapshot. <br><strong>Tests:</strong> idempotent persist tests, simulated storage failures/backoff, race-condition tests ensuring single persisted descriptor per <code>paramsHash</code>. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>SafeRemediationExecutor.execute(plan, cancellationToken)</code> — protected inline execution frame</strong><br><strong>Purpose & contract:</strong> execute <code>RemediationPlan</code> inline with protective envelope: per-step checkpoints, staging area, cooperative cancellation, timeouts, exception mapping, redaction, and final atomic swap or commit. Return <code>ExecutionResult {status, artifactRef, beforeChecksum, afterChecksum, stepDurations[], payloadHash}</code>. <br><strong>Execution frame invariants (detailed):</strong><br>1. Staging-first: create staging artifact <code>__remed_&lt;applyId&gt;</code> (hidden sheet or temp workbook), apply transforms there. <br>2. Step-level checkpoint: persist minimal checkpoint metadata (stepId, stepHash) before each step. <br>3. Postchecks after each step: schema constraints, reconciliation rules; if postcheck fails attempt <code>safeRollback</code> within staging. If rollback fails persist forensic evidence and fail gracefully. <br>4. Atomic swap/commit: after all steps pass, perform host-safe swap (rename/hide tactics) to keep UI consistent and avoid partial state exposure. <br>5. Watchdog & cancellation: watchdog triggers <code>dq_handler.timeout</code> and attempts cooperative cancellation via token; if unresponsive escalate to <code>dq_handler.hung</code> with <code>forensic_manifest</code>. <br>6. Error mapping & user-safe messaging: map thrown exceptions to <code>ErrorCodeCatalog</code> entries and call <code>SafeErrorToUser(correlationId, applyId, error)</code>. Detailed traces redacted and saved encrypted; audit only contains safe error code and correlation id. <br><strong>Telemetry & audit:</strong> emit <code>dq_handler.step.start</code>, <code>dq_handler.step.complete</code>, <code>dq_handler.timeout</code>, <code>dq_handler.exception</code> with <code>applyId</code>. Compute and record <code>payloadHash</code> for each step. <br><strong>Developer guidance:</strong> avoid heavy network or disk IO on UI thread; prefer quick operations and defer heavy tasks to scheduled job if required. <br><strong>Tests:</strong> cancellation reaction tests, partial failure checkpoint tests, swap atomicity tests, watchdog forced timeouts. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>BuildUndoPlan(plan)</code> — deterministic undo descriptor & preimage capture</strong><br><strong>Purpose & contract:</strong> generate <code>UndoPlan</code> mirroring <code>RemediationPlan</code> reverse steps with preimage evidence collection and verification checksums. Return <code>undoId, undoPlanHash, reversible:Boolean, retentionPolicy</code>. If plan includes external side-effects mark <code>reversible:false</code> and require explicit elevated approval for apply. <br><strong>Components & invariants:</strong> <code>undoSteps[]</code> (reverse-ordered), <code>preimageEvidenceRef</code> for each step, <code>undoChecksum</code> for verification, <code>restoreInstructions</code> for operator or worker, <code>retentionTtl</code> guided by <code>plan</code> regulation flags. Preimage evidence must be stored encrypted; evidence references included in <code>dq_undo.built</code> audit. <br><strong>Retention & access:</strong> retention TTL depends on regulation; access requires RBAC and audit trail. <br><strong>Tests:</strong> replay undo on sample dataset restores <code>beforeChecksum</code>; partial-undo simulation; retention enforcement. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>RevertRemediation(applyId, operatorContext)</code> — safe revert & gating</strong><br><strong>Purpose & contract:</strong> perform authoritative revert using <code>UndoPlan</code> for a completed apply. Must validate current <code>afterChecksum</code> equals recorded apply <code>afterChecksum</code> before attempting revert; require approvals if policy demands; emit <code>dq_apply.revert.start</code> prior to changes; on divergence fail and collect forensic evidence. <br><strong>Revert orchestration:</strong><br>1. Lookup <code>applyRecord</code>, <code>plan</code>, <code>undoPlan</code>, and <code>evidenceRefs</code>. <br>2. Validate <code>currentAfterChecksum == recordedAfterChecksum</code> else abort and call <code>ExportForensicsForFailedApply</code>. <br>3. Validate approvals/RBAC as required. <br>4. Execute <code>UndoPlan</code> using <code>SafeRemediationExecutor</code> semantics (staging, checkpoints, postchecks). <br>5. On success emit <code>dq_apply.revert.complete</code> with <code>revertId</code>, verification checksums; on failure emit <code>dq_apply.revert.failed</code> and produce <code>forensic_manifest</code>. <br><strong>Safety rules:</strong> do NOT attempt blind revert when downstream consumers may have processed mutated data; require manual triage. <br><strong>Tests:</strong> revert happy-path, revert checksum mismatch handling, idempotent revert calls. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>ValidateApprovals(plan, approvals[])</code> — RBAC & two-person enforcement</strong><br><strong>Purpose & contract:</strong> verify provided approvals satisfy governance for the plan. Validate approver identity, role membership, expiration, delegation, and signature. Return <code>{allowed:Boolean, missingRoles[], denialReason, validatedApprovals[]}</code>. <br><strong>Validation checks:</strong> SSO identity mapping, group membership check, approval expiry, signature verification for automated approvals, counts for two-person approvals. Store approval metadata in <code>dq_approval.*</code> audits. <br><strong>Tests:</strong> expired approvals, revoked approvals, forged signature rejection, delegation edge cases. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>BuildUiPreview(plan, simulationResult)</code> — redacted, paginated preview artifacts for operator UX</strong><br><strong>Purpose & contract:</strong> turn <code>simulationResult</code> into compact UI artifacts: <code>diffSummary</code>, <code>fieldImpactSummary</code>, <code>inlineExamples</code> (redacted before/after rows up to N), <code>confidenceBadge</code>, <code>previewHash</code>, and <code>previewCursor</code> for pagination. Must remove or redact PII for UI display; full sanitized evidence stored encrypted. <br><strong>Performance & UX rules:</strong> return small payload quickly (<200ms) for immediate UI; provide lazy fetch endpoints for detailed previews. Use <code>seed</code> for deterministic pagination. <br><strong>Audit:</strong> emit <code>dq_proposal.preview</code> with <code>proposalId, previewHash, correlationId</code>. <br><strong>Tests:</strong> preview parity vs simulation, redaction coverage verification. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>ExportRemediationArtifacts(plan, applyResult, destinationUri, operatorId)</code> — atomic export & chain-of-custody</strong><br><strong>Purpose & contract:</strong> export remediation artifacts snapshot: <code>proposal.json</code>, <code>plan.json</code>, <code>beforeDataset</code>, <code>afterDataset</code>, <code>undoPlan.json</code>, <code>reconciliationReport</code>, <code>logs</code> to <code>destinationUri</code> using atomic write semantics and compute <code>artifact.checksum.sha256</code>. Generate <code>redaction_manifest</code> to record fields redacted for operator access; append <code>dq_export.remediation</code> audit with <code>destinationUri</code> and checksum. <br><strong>Security & governance:</strong> redact fields operator lacks privilege for; exports for regulated datasets require approvals; include chain-of-custody metadata and operator signature. <br><strong>Fallback & retry:</strong> staged fallback to local staging if remote unreachable; retry/backoff policy and alerting on persistent failures. <br><strong>Tests:</strong> checksum parity, redaction correctness, fallback export simulation. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>NotifyOwnersAndStakeholders(plan, applyId, channelHints)</code> — minimal, auditable notifications</strong><br><strong>Purpose & contract:</strong> notify owners and stakeholders via approved channels (in-app, email, webhook) with safe summary, <code>applyId</code>, and authorized evidence links. Avoid PII in message bodies; include <code>evidenceRef</code> only for authorized recipients. Append <code>notification.audit</code> with <code>notificationId, recipients[], channel, evidenceRef</code>. <br><strong>Delivery & retries:</strong> use approved gateway; persistent retry on transient failures; escalate via SRE if delivery fails for critical regulatory notifications. <br><strong>Tests:</strong> permission gating for evidence links, delivery retry tests. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>ExportForensicsForFailedApply(applyId, failureContext)</code> — authoritative forensic package</strong><br><strong>Purpose & contract:</strong> assemble a signed forensic bundle including <code>forensic_manifest.json</code>, <code>plan.json</code>, <code>apply.log</code>, <code>audit_tail.csv</code> rows for <code>correlationId</code>, <code>jobDescriptors</code>, <code>configSnapshot</code>, <code>checksums</code>, sanitized snapshots, evidence refs, and store in secure evidence repo with RBAC. Return <code>forensicUri</code> and append <code>forensic.export</code> audit with fingerprints. <br><strong>Manifest contents & signatures:</strong> produce manifest with SHA256 fingerprints for each artifact and sign manifest with release key (HSM). <br><strong>Operator instructions & triage:</strong> include minimal triage checklist and retrieval instructions for SRE/Compliance. <br><strong>Tests:</strong> artifact completeness verification, manifest signature verification, access control. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>SafeErrorToUser(correlationId, applyId, errorCode)</code> — user-safe mapping & triage guidance</strong><br><strong>Purpose & contract:</strong> map internal error codes to concise user-facing messages containing <code>correlationId</code> and triage hint. Append <code>dq.userErrorShown</code> audit. Detailed traces stored encrypted and referenced by <code>evidenceRef</code>. Messages must not include PII or internal stack traces. <br><strong>Examples:</strong> <code>ERR_PLAN_VALIDATE</code> -> "Remediation validation failed (ref r-...). Check plan errors or contact owner." <code>ERR_REVERT_CHECKSUM_MISMATCH</code> -> "Revert blocked (ref r-...). Contact support." <br><strong>Tests:</strong> ensure no PII leaks, audit presence for displayed errors. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>RegisterUnitTestHook(hookName, goldenCid)</code> — CI deterministic harness</strong><br><strong>Purpose & contract:</strong> register test-only hooks that enable deterministic simulation with fixed <code>correlationId</code> for golden parity. Hooks disabled in production by default; require <code>test=true</code> flag and explicit registration audit <code>dq_test.hook.registered</code>. Hooks accept pre-seeded RNG and mock evidence refs for CI. <br><strong>CI uses:</strong> golden runs verifying <code>proposalHash</code>, <code>planHash</code> stability; unit/integration test harness isolation. <br><strong>Tests:</strong> ensure hooks cannot be enabled in production without explicit flag; golden fuzz detection. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>SafeHandlerTimeoutWatchdog(handlerToken, applyId)</code> — escalation & cooperative cancellation</strong><br><strong>Purpose & contract:</strong> monitor inline executor time budgets; on <code>softTimeout</code> emit <code>dq_handler.timeout</code>, attempt cooperative cancellation via token; on <code>hardTimeout</code> emit <code>dq_handler.hung</code> with stack snapshot (if available) and persist <code>forensic_manifest</code>. Implement using host idle callbacks or <code>Application.OnTime</code> in VBA. <br><strong>Timeout policy:</strong> configurable <code>softTimeoutMs</code>, <code>hardTimeoutMs</code>, with escalation steps and operator notifications. <br><strong>Tests:</strong> forced overrun, cancellation effect tests, audit emission. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>TelemetryEmit(metricName, value, tags)</code> — local buffering with audited uplink</strong><br><strong>Purpose & contract:</strong> append local metrics to buffer; remote uploader module (separate) performs audited export. Typical metrics: <code>dq.proposal.latency_ms</code>, <code>dq.apply.duration_ms</code>, <code>dq.apply.timeout_rate</code>, <code>job.persist.latency_ms</code>. Include <code>correlationId</code> tag where applicable. <br><strong>Durability & uplink:</strong> metrics persisted locally and uploaded on schedule; uploader attaches <code>uploader.audit</code> rows. <br><strong>Tests:</strong> buffer durability tests and uploader compatibility tests. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>Audit obligations (module-level summary &amp; enforcement)</code> — chain rules, schema, rotation, signing, CI checks</strong><br><strong>Mandate:</strong> every user-initiated action MUST append a canonical audit row with <code>correlationId</code>. Required chain: <code>dq_proposal</code> → <code>dq_proposal.preview</code> → <code>dq_proposal.accepted</code> → <code>dq_plan.built</code> → <code>dq_plan.validate</code> → <code>dq_apply.start</code> → <code>dq_apply.dryrun</code> → <code>dq_apply.complete</code>/<code>dq_apply.failed</code> → <code>dq_apply.reverted</code> → <code>dq_export.remediation</code> as applicable. Each audit includes <code>payloadHash</code>, <code>prevHash</code> when resolvable, <code>configHash</code>, <code>module.manifestHash</code>. <code>modAudit</code> rotates and signs rotations per retention rules. <code>VerifyAuditChain</code> executed in CI and monitoring to surface mismatches. <br><strong>Audit schema (required fields):</strong> <code>timestamp,correlationId,module,procedure,operatorId,proposalId,planId,applyId,paramsHash,configHash,prevHash,payloadHash,artifactChecksum,metadata</code>. <br><strong>Encryption & evidence policy:</strong> main audit stores only <code>paramsHash</code>; full sanitized params and artifacts saved encrypted with <code>evidenceRef</code>. Access controlled by approvals. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>Performance budgets &amp; SLOs (remediation)</code> — targets, metrics, runbook</strong><br><strong>Targets:</strong> proposal generation median <200ms for small tables (<10k rows); dry-run preview median <2s for 10k sample; inline apply default timeout 5s (configurable); job persist latency median <2s. <br><strong>Key metrics:</strong> <code>dq.proposal.latency_ms</code>, <code>dq.apply.duration_ms</code>, <code>dq.apply.timeout_rate</code>, <code>job.persist.latency_ms</code>, <code>dq.handler.timeout_rate</code>. <br><strong>Runbook (high-level):</strong> on surge in <code>dq.apply.timeout_rate</code>: 1) throttle inline applies, 2) force scheduling of heavy plans, 3) collect forensic artifacts for failing applies, 4) notify SRE and toggle degraded-mode if necessary. <br><strong>Scaling:</strong> deterministic heavy tasks must be offloaded to worker pool with autoscaling and idempotent job descriptors. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>Testing matrix (remediation)</code> — required tests, golden governance, CI gating</strong><br><strong>Required tests:</strong><br>1. Unit: <code>GenerateProposal</code>, <code>ScoreProposal</code>, <code>BuildRemediationPlan</code>, <code>ValidateRemediationPlan</code>, <code>BuildUndoPlan</code>, <code>IsLightweightAction</code>.<br>2. Integration: profile->proposal->dry-run->apply->revert with audit chain verification and golden parity. <br>3. Golden: lock representative fixtures producing <code>proposalHash</code>/<code>planHash</code> parity across builds; golden files versioned and signed. <br>4. Property tests: determinism with seeded RNG, concurrency with high click volumes and idempotency checks. <br>5. Security tests: redaction coverage, evidence encryption, KMS integration. <br><strong>CI gating rules:</strong> block PRs on forbidden API use (direct workbook writes during proposal/dry-run), golden mismatches, missing audit rows, missing undo for destructive plan, or failing performance budgets. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>Failure modes &amp; mitigations (comprehensive)</code> — canonical incidents & triage runbooks</strong><br><strong>Common cases & mitigations:</strong><br>1. Plan validation failure: return <code>dq_plan.validate</code> errors; operator must fix plan; block apply. <br>2. Apply partial failure: mark apply <code>partial</code>, persist partial artifacts and <code>forensic_manifest</code>, notify SRE and operator for manual triage or controlled revert. <br>3. Revert checksum mismatch: DO NOT auto-revert; create <code>forensic_manifest</code> and engage SRE/Compliance. <br>4. Job persist failure: retry/backoff; after repeated failures open incident and attach <code>forensic_manifest</code>. <br>5. Unauthorized apply: deny with <code>dq_permission.denied</code> and <code>SafeErrorToUser</code>. <br><strong>Forensics package requirements:</strong> <code>plan.json</code>, <code>apply.log</code>, <code>audit_tail.csv</code> rows for <code>correlationId</code>, <code>jobDescriptors</code>, <code>configSnapshot</code>, <code>stagingArtifacts</code>, <code>checksums</code>, <code>release.manifest</code>. <br><strong>Operator triage checklist:</strong> capture <code>correlationId</code>, fetch <code>dq_apply</code> & <code>dq_plan</code> audits, retrieve <code>forensic_manifest</code>, run offline <code>SimulateApply(plan, seed)</code>, attempt <code>RevertRemediation</code> if safe. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>Operator UX &amp; triage notes (concise operator commands)</code> — practical actions</strong><br><strong>Best practices:</strong> always capture <code>proposalId</code> and <code>applyId</code> for support. Prefer <code>copy</code> mode when uncertain. Limit inline applies to low-risk operations. <br><strong>Representative commands:</strong><code>remediation preview --proposal &lt;proposalId&gt;</code>, <code>remediation simulate --plan &lt;planId&gt; --seed &lt;n&gt;</code>, <code>remediation apply --plan &lt;planId&gt; --mode copy --operator &lt;id&gt; --ticket &lt;ticketId&gt;</code>, <code>remediation revert --apply &lt;applyId&gt; --operator &lt;id&gt;</code>. <br><strong>Triage steps:</strong> 1) get <code>correlationId</code> and <code>applyId</code>, 2) pull <code>dq_apply.*</code> and <code>dq_plan.*</code> audits, 3) retrieve <code>evidenceRef</code>/<code>forensic_manifest</code>, 4) reproduce via <code>SimulateApply</code>, 5) revert if safe or escalate. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>Change-control &amp; governance (remediation)</code> — required approvals & release flow</strong><br><strong>Flow:</strong> PR + migration manifest for policy changes to <code>RemediationPolicy</code> or scoring; run static analyzer, unit/integration/golden tests; compliance signoffs required for regulated changes; sign artifacts; publish release manifest. Hot-swap of handlers requires smoke tests and <code>dq_proposal</code> golden checks before canary. <br><strong>Blocking conditions:</strong> golden/audit-chain failures, missing undo for destructive actions, unsigned <code>AUTO_APPLY</code> changes. <br><strong>Artifacts:</strong> <code>migration_manifest.json</code>, <code>release.manifest</code>, signed <code>module.manifestHash</code>, owners list in <code>OWNERS.md</code>. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>Security &amp; PII policy (detailed)</code> — encryption, signing, and secrets</strong><br><strong>Principles:</strong> do not show raw PII in UI or main audit rows. Store sanitized evidence encrypted with KMS/HSM-managed keys; <code>evidenceRef</code> included in audits. Automated <code>AUTO_APPLY</code> policies for regulated data require signed manifests and recorded approvals. Logging and telemetry must redact PII at collection time. Secrets must be retrieved via <code>modSecurity.getEphemeralToken()</code> during deferred init; never persist raw secrets. <br><strong>Examples:</strong> merges on SSN require two-person approval and encrypted evidence retention; export redacts sensitive columns if operator lacks export privilege. <br><strong>Tests:</strong> KMS integration, redaction fuzzing, signature verification. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong>Appendices & canonical schemas</strong><br><strong>Canonical JSON Schemas (summary):</strong><br>&nbsp;&nbsp;<code>Proposal</code> — <code>{proposalId, proposalHash, actions[], affectedCount, confidence, evidenceRef, createdAt}</code>.<br>&nbsp;&nbsp;<code>RemediationPlan</code> — <code>{planId, planHash, actions[], preconditions[], postchecks[], undoPlanDescriptor, estimatedDurationMs}</code>.<br>&nbsp;&nbsp;<code>UndoPlan</code> — <code>{undoId, undoSteps[], preimageEvidenceRef, undoChecksum, retentionTtl}</code>.<br>&nbsp;&nbsp;<code>JobDescriptor</code> — <code>{jobId, planId, applyId, paramsHash, persistedAt, evidenceRef, owner}</code>.<br>&nbsp;&nbsp;<code>AuditRow</code> — <code>{timestamp, correlationId, module, procedure, operatorId, proposalId, planId, applyId, paramsHash, configHash, prevHash, payloadHash, artifactChecksum, metadata}</code>.<br><strong>Artifact storage & governance:</strong> store artifacts under <code>\\artifacts\DQ_Remediation\releases\{version}\</code> and evidence under secure repo <code>\\evidence\DQ_Remediation\{year}\{month}\</code> with strict RBAC. Release manifests and signed module manifests archived; CI <code>VerifyAuditChain</code> runs nightly. <br><strong>Operator runbooks & cheat-sheets:</strong> include <code>triage-remediation.md</code>, <code>revert-checklist.md</code>, <code>forensics-collection.md</code> in appendices. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong>Final developer pre-merge checklist (exhaustive)</strong><br>1. Unit tests for each changed function. <br>2. Integration tests covering proposal→dry-run→apply→revert audit chain. <br>3. Golden fixtures updated and verified for <code>proposalHash</code>/<code>planHash</code>. <br>4. Static analyzer checks: no forbidden APIs in critical paths (no workbook writes in proposal/dry-run). <br>5. All user-visible transitions emit <code>dq_*</code> audits. <br>6. UndoPlan present for destructive plans or explicit signed acknowledgement recorded. <br>7. Config and scoring changes produce <code>configHash</code> and recorded in audits. <br>8. Performance budgets validated in integration tests. <br>9. Security review for PII exposure, KMS encryption, and manifest signing. <br>10. <code>OWNERS.md</code> updated and release manifest signed. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong>Operator & SRE quick runbooks (condensed actionable steps)</strong><br><strong>Failed apply (partial):</strong> 1) capture <code>correlationId</code>, <code>applyId</code>; 2) retrieve <code>dq_apply.*</code> & <code>dq_plan.*</code> audits; 3) fetch <code>forensic_manifest</code> and evidence; 4) attempt <code>RevertRemediation(applyId)</code> if safe; 5) if revert impossible, escalate to SRE with <code>forensicUri</code>. <br><strong>Revert checksum mismatch:</strong> do not auto-revert; collect full forensic bundle and escalate. <br><strong>Maintenance:</strong> monthly audit rotation verification, nightly golden parity checks, quarterly disaster recovery drills. </td></tr></tbody></table></div><div class="row-count">Rows: 32</div></div><div class="table-caption" id="Table3" data-table="Docu_0177_03" style="margin-top:2mm;margin-left:3mm;"><strong>Table 3</strong></div>
<div class="table-wrapper" data-table-id="table-3"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by DQ_Export — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">DQ_Export — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Top-line summary (one line):</strong> Deterministic, auditable, idempotent, resumable, policy-driven export pipeline for corrected datasets and companion artifacts; built for cryptographic provenance (KMS/HSM), quarantine on partial commits, predictable manifests for golden testing, and clear operator/SRE recovery flows. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Scope & audience:</strong> This document is a per-function, operationally-focused technical specification intended for implementers (backend engineers, SRE, security engineers), test authors, compliance officers, and operators who will run and triage DQ_Export in production. It presumes familiarity with object stores, multipart uploads, HSM/KMS signing, RBAC patterns, append-only audit paradigms, and secure evidence storage. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Design goals (prioritized):</strong><br>1. <strong>Reproducibility & determinism</strong> — identical inputs produce identical artifacts and canonical digests across platforms and locales.<br>2. <strong>Auditability</strong> — every user action anchored by <code>dq_export.requested</code> and full chain of <code>dq_export.*</code> audit rows; <code>prevHash</code> chaining where possible and rotation signing enforced.<br>3. <strong>Idempotency</strong> — dedupe by <code>exportRequestId</code>; safe <code>force</code> semantics recorded as approvals.<br>4. <strong>Safety / Fail-closed for regulated artifacts</strong> — signatures, approvals, and retention guarantees enforced; inability to satisfy policies results in quarantine/failure, never silent degradation.<br>5. <strong>Resumability & low-memory streaming</strong> — support multi-GB exports with part-level digests and persisted upload session state for cross-process resume.<br>6. <strong>Minimal PII exposure</strong> — audits contain only <code>payloadHash</code> and <code>evidenceRef</code>; full sanitized payloads stored encrypted with RBAC. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Global contracts & invariants (module-level):</strong><br>- <code>dq_export.requested</code> audit MUST be emitted prior to heavy work.<br>- <code>exportRequestId</code> dedupes identical requests; if the same <code>exportRequestId</code> arrives, return canonical <code>exportId</code> unless <code>force=true</code> with recorded approvals.<br>- Atomic visibility to consumers must be preserved: either all artifacts visible or none; partial visibility is treated as an incident and quarantined.<br>- Evidence retention and forensic packaging must be available for every terminal failed/quarantined export. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>High-level function list (canonical API surface):</strong><br><code>ExportDataset(entryContext)</code> — orchestrator & idempotency gate.<br><code>BuildExportDescriptor(exportRequest)</code> — canonical descriptor + hash generation.<br><code>ValidatePermissions(operatorId, exportDescriptor)</code> — RBAC + approvals gate.<br><code>ValidateDestination(destination)</code> — capability probe & credential fingerprinting.<br><code>ValidateExportFormat(format, datasetSchema)</code> — format suitability & <code>formatPlan</code> generation.<br><code>PrepareStagingArea(exportDescriptor)</code> — secure staging creation, lockfile, ephemeral encryption key lifecycle.<br><code>SerializeArtifacts(datasetSnapshot, formatPlan, stagingHandle)</code> — deterministic streaming serializer for data, reports, and manifests.<br><code>ComputeChecksum(path, algorithms[])</code> — per-part and aggregate checksum computation.<br><code>SignArtifact(artifactPaths, signerPolicy)</code> — KMS/HSM detached-sign orchestration and policy enforcement.<br><code>MultipartUploadManager(destinationHandle)</code> — resumable part upload manager with persisted session state.<br><code>AtomicSwap(stagingHandle, destinationHandle, artifacts)</code> — backend-specific commit semantics (pointer update / rename / pointer metadata write).<br><code>PersistExportMetadata(exportDescriptor, artifactManifest, destinationUris)</code> — durable exports index & metadata write.<br><code>EmitExportAudit(correlationId, step, payloadHash, metadataRef?)</code> — canonical audit writer with <code>prevHash</code> chaining and redaction policy.<br><code>QuarantineAndFallback(exportId, failingArtifacts, ctx)</code> — quarantine, forensic preservation, fallback artifact generation.<br><code>ResumeExport(exportId)</code> — resume orchestration based on persisted state markers.<br><code>RollbackExport(exportId, reason)</code> — governance path and two-person approval enforcement for destructive restores.<br><code>NotifyConsumers(exportId, artifactUris, channels)</code> — idempotent notifications to downstream consumers with receipt tracking.<br><code>MonitorExportProgress(exportId, hook)</code> — progress telemetry and hook invocation.<br><code>CleanupTemp(stagingHandle, keepEvidenceTTL)</code> — secure wipe and GC.<br><code>RetryWithBackoff(operation, policy)</code> — standardized retry wrapper with circuit-breaker semantics. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>State machine (canonical states, invariants, TTLs):</strong><br><code>REQUESTED</code> — created by <code>ExportDataset</code>; <code>dq_export.requested</code> emitted. Invariant: <code>exportRequestId</code> exists; TTL: short (minutes) to detect blocked/queued exports.<br><code>DESCRIPTOR_CREATED</code> — canonical <code>descriptorHash</code> persisted. Invariant: descriptor immutable and persisted atomically for dedupe.<br><code>PERMISSION_VALIDATED</code> — RBAC checks complete; if approvals missing → <code>BLOCKED</code>. Invariant: approval artifacts referenced by <code>descriptor.requiredApprovals</code>.<br><code>STAGING_PREPARED</code> — staging directory and exclusive <code>staging.lock</code> present. Invariant: unique staging path, optional per-export ephemeral encryption key created.<br><code>SERIALIZING</code> — streaming writes in progress with per-part markers; <code>staging.manifest.partial</code> updated incrementally.<br><code>CHECKSUMS_COMPUTED</code> — per-part and aggregate checksums computed and stored in <code>manifest.json</code>.<br><code>SIGNED</code> — optional state; for regulated exports mandatory; <code>signerFingerprint</code> attached to metadata.<br><code>COMMIT_ATTEMPT</code> — pre-commit verification done and commit in-flight using backend-specific commit patterns.<br><code>COMMITTED</code> — commit succeeded and metadata persisted; <code>dq_export.commit.completed</code> emitted.<br><code>COMPLETED</code> — notifications delivered; <code>dq_export.completed</code> emitted; retention scheduled.<br><code>QUARANTINED</code> — partial/failed commit: artifacts moved to <code>quarantine/&lt;exportId&gt;</code>; <code>dq_export.quarantine</code> emitted and SRE/compliance alerted.<br><code>ROLLED_BACK</code> — roll back executed successfully per governance rules; <code>dq_export.rollback</code> emitted.<br><code>FAILED</code> — terminal failure with <code>forensic_manifest</code> persisted; <code>dq_export.failed</code> emitted. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>ExportDataset(entryContext)</code> — orchestrator (detailed behavior & invariants)</strong><br><strong>Purpose:</strong> canonical public entrypoint for requests from UI or job scheduler. Idempotent by <code>exportRequestId</code> and authoritative for initiating audit chain.<br><strong>Contract:</strong> always persist descriptor and emit <code>dq_export.requested</code> before heavy, irreversible work. Return <code>{status, exportId, correlationId, shortMessage}</code>; shortMessage must be UI-safe (PII-free) and contain correlation id.<br><strong>Detailed flow:</strong><br>1. <strong>Immediate anchor:</strong> generate <code>correlationId</code>; persist minimal request envelope; emit <code>dq_export.requested</code> audit row with <code>correlationId</code> and <code>exportRequestId</code> to anchor user action.<br>2. <strong>Dedupe check:</strong> check <code>exportRequestId</code> in <code>exports_index</code>. If existing terminal export (COMPLETED/FAILED/ROLLED_BACK) and <code>force</code> not set → return canonical <code>exportId</code> and terminal status. If <code>force=true</code> verify operator approvals and record approval evidence before proceeding.<br>3. <strong>Descriptor creation:</strong> call <code>BuildExportDescriptor(exportRequest)</code> which returns canonical <code>descriptor</code> and <code>descriptorHash</code>; persist descriptor atomically and emit <code>dq_export.descriptor.created</code>.<br>4. <strong>Permissions & approvals:</strong> run <code>ValidatePermissions(operatorId, descriptor)</code>. If <code>allowed=false</code> return <code>status:blocked</code> plus <code>requiredApprovals</code> (UI-safe) and keep <code>descriptor</code> persisted with <code>BLOCKED</code> state. If <code>allowed=true</code> continue.<br>5. <strong>Destination & format validation:</strong> run <code>ValidateDestination(destination)</code> and <code>ValidateExportFormat(format, datasetSchema)</code> to produce <code>capabilitySet</code> and <code>formatPlan</code>. If destination lacks required capabilities for atomic commit, plan fallback commit semantics and record risk in descriptor metadata.<br>6. <strong>Staging:</strong> call <code>PrepareStagingArea(descriptor)</code> to create <code>stagingPath</code> and <code>staging.lock</code>; emit <code>dq_export.staging.created</code> audit row including <code>stagingPath</code> fingerprint and optional ephemeral encryption key fingerprint.<br>7. <strong>Preview (if requested):</strong> if <code>options.preview</code> is true, run <code>DryRunExport</code> with deterministic sample; produce preview artifacts but do not run commit; return preview artifact URIs and <code>status:preview</code> with <code>exportId</code> and <code>correlationId</code>.<br>8. <strong>Execution mode decision:</strong> compute <code>IsLightweightAction</code> (policy-driven) to decide inline vs background. For background persist job descriptor (canonical <code>jobId</code>) and return <code>status:in-progress</code> with <code>exportId</code> and resume tokens. For inline, proceed with serialization and commit within request lifecycle subject to timeouts.<br>9. <strong>Serialization → checksum → sign → upload → commit:</strong> run <code>SerializeArtifacts</code> (streaming), <code>ComputeChecksum</code>, <code>SignArtifact</code> (if required by <code>signPolicy</code>), <code>MultipartUploadManager</code> to upload parts, <code>AtomicSwap</code> to commit, <code>PersistExportMetadata</code> to persist metadata, <code>NotifyConsumers</code>, and finally <code>EmitExportAudit</code> terminal rows. Update <code>exports_index</code> with final status <code>COMPLETED</code> and TTL-based retention scheduling. <br><strong>Failure handling & guarantees:</strong><br>- Emission of <code>dq_export.requested</code> is mandatory before heavy action; used for triage if system crashes mid-work.<br>- On transient failures (network errors), persist partial state in staging and job descriptor so <code>ResumeExport</code> can pick up. <br>- On irrecoverable or policy failures (signature failure for regulated artifacts, or missing approvals), fail-closed and call <code>QuarantineAndFallback</code> where applicable. <br><strong>Observability & metrics:</strong> emit step-level spans and metrics (<code>dq_export.duration_ms</code>, <code>dq_export.serialize.ms</code>, <code>dq_export.upload_rate_bps</code>, <code>dq_export.retry_count</code>). Log structured events with <code>correlationId</code>. <br><strong>Tests:</strong> idempotency vectors, blocked-by-approvals simulation, inline small-run success, persist-job resume test. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>BuildExportDescriptor(exportRequest)</code> — canonical descriptor rules, persistence & diagnostics</strong><br><strong>Purpose:</strong> create canonical descriptor that is the authoritative representation for export processing, dedupe, CI golden linking, and reproducibility.<br><strong>Descriptor fields (canonical set):</strong> <code>exportId (uuidv4)</code>, <code>exportRequestId</code>, <code>producerModule</code>, <code>producerVersion</code>, <code>datasetRef</code>, <code>datasetSnapshotHash</code>, <code>format</code>, <code>artifactNames[]</code>, <code>estimatedSizeBytes</code>, <code>exportTsUtc</code>, <code>configHash</code>, <code>ribbonMapHash</code>, <code>owner</code>, <code>sensitivityLevel</code>, <code>retentionPolicyId</code>, <code>requiredApprovals[]</code>, <code>destinationHint</code>, <code>credFingerprint?</code>.<br><strong>Canonicalization rules (exact):</strong><br>- Serialize as JSON with lexicographic key ordering.<br>- Normalize numbers: integers or floats with a defined precision for numeric fields; avoid machine-dependent float features.<br>- Timestamps normalized to ISO8601 with UTC timezone and fixed fractional second precision.<br>- Arrays: sort only when semantically safe (e.g., lists of artifact names should be sorted lexicographically to ensure stable manifest ordering). Document which arrays are sorted; sorting behavior must be stable and part of canonicalization rules.<br>- Exclude ephemeral secrets or tokens from the canonical JSON; include only <code>credFingerprint</code> or <code>destinationFingerprint</code> where credential info is required for auditing.\br><strong>Hashing:</strong> compute <code>descriptorHash = sha256(canonical_json_bytes)</code>. Store <code>descriptorHash</code> as a top-level field and persist descriptor atomically to durable store. Emit <code>dq_export.descriptor.created</code> audit row containing <code>descriptorHash</code> and <code>exportId</code>.\br><strong>Persistence & concurrency:</strong> atomic write with optimistic concurrency based on <code>exportRequestId</code>; if concurrent writers attempt to create descriptors for the same <code>exportRequestId</code>, accept the first write and return canonical <code>exportId</code> to subsequent requests. Implement canonical conflict resolution and log collisions.\br><strong>Diagnostics & operator hints:</strong> store creation provenance (hostId, processId, correlationId) in descriptor metadata for forensic traceability. Do not include operator secrets or raw credentials in descriptor.\br><strong>Tests:</strong> canonicalization parity across CI runners (locale tests), descriptor.hash immutability tests, concurrent descriptor creation tests. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>ValidatePermissions(operatorId, exportDescriptor)</code> — policy matrix & approval semantics</strong><br><strong>Purpose:</strong> evaluate whether the <code>operatorId</code> has rights to perform the export given dataset sensitivity, owner policies, and required approvals; return explicit <code>requiredApprovals</code> when blocked.<br><strong>Policy checks performed:</strong><br>- SSO identity mapping to canonical <code>operatorId</code> and session validity check.<br>- Group membership check against <code>data_exporter</code>, <code>data_owner</code>, <code>compliance_officer</code>, and other domain roles.<br>- Dataset-level ACL evaluation: owner-only artifacts, read-only assignments, etc.<br>- Sensitivity-level mapping: thresholds for requiring one- or two-person approvals or HSM signature binding.<br>- Time-limited approvals & emergency allow-lists: evaluate approval expiry and bind approvals to <code>exportRequestId</code> to prevent reuse.<br><strong>Return contract:</strong> <code>{allowed:Boolean, requiredApprovals:[{role,reason}], denialReason?, approvalEvidenceRef?}</code>; never include PII in <code>denialReason</code>; provide operator-facing safe hint and correlation id.<br><strong>Audit & evidence:</strong> append <code>dq_export.permission.check</code> with <code>descriptorHash</code>, <code>operatorId</code>, <code>allowed</code> boolean, and <code>requiredApprovals</code> count; store full approval artifact in evidence store if operator attaches manual approvals.<br><strong>Failure & governance:</strong> when <code>allowed=false</code>, set descriptor state to <code>BLOCKED</code> and return <code>status:blocked</code>. Approvals must be appended to the evidence store and must be bound to <code>exportRequestId</code> to permit the export to progress. Two-person approvals for regulated sensitivity levels must be enforced at this stage.<br><strong>Tests:</strong> role matrix simulation, expired approval rejection, emergency allow-list acceptance. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>ValidateDestination(destination)</code> — capability probe & compatibility mapping</strong><br><strong>Purpose:</strong> determine whether the destination supports required semantics (atomic rename, multipart upload, versioning, retention and ACLs) and compute a safe commit strategy. Also validate credentials and fingerprint them for audit.<br><strong>Destination types supported:</strong> <code>local_fs</code>, <code>smb</code>, <code>nfs</code>, <code>s3</code>, <code>gcs</code>, <code>azure_blob</code>, <code>secure-archive</code>, <code>artifact-repo</code>, <code>registry-pointer</code>.<br><strong>Probe semantics (safe, idempotent, and least-privilege):</strong><br>- Resolve destination type and canonical path/URI.<br>- Perform <code>head</code> or <code>list</code> call to validate connectivity and minimal permissions.<br>- If permitted and safe, perform a minimal <code>put</code>/<code>delete</code> probe to validate write privileges; always clean up after the probe and record only <code>credFingerprint</code> not raw credentials.<br>- Detect features: atomic rename support, server-side copy, versioning enabled, server-side encryption (SSE) availability, lifecycle/retention rule support. <br><strong>Capability mapping to commit strategy:</strong><br>- If destination supports strong atomic pointer update or atomic rename → prefer direct commit semantics.<br>- If destination lacks atomic rename but supports versioning/pointer metadata→ use versioned write + single pointer metadata update as atomic commit surrogate.<br>- If destination lacks both → reject or plan quarantine + operator-visible fallback with elevated risk flagged in descriptor metadata. <br><strong>Return contract:</strong> <code>{valid:Boolean, capabilities:{rename:Boolean, multipart:Boolean, versioning:Boolean, pointerAtomic:Boolean, retention:Boolean}, credFingerprint, recommendedCommitStrategy, warnings[]}</code> and emit <code>dq_export.destination.validated</code> audit row with <code>capabilities</code> and <code>credFingerprint</code> only. <br><strong>Failure & mapping:</strong> map to <code>DQ_ERR_INVALID_DEST</code>, <code>DQ_ERR_QUOTA_EXCEEDED</code>, <code>DQ_ERR_PERMISSION_DENIED</code> as appropriate. Cache destination capability results for TTL to avoid repeated probes. <br><strong>Tests:</strong> object-store emulator runs, SMB rename tests, NFS consistency checks, storage provider quota behavior tests. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>PrepareStagingArea(exportDescriptor)</code> — secure staging creation & lock semantics</strong><br><strong>Purpose:</strong> create an isolated staging area with exclusive write lock and optional per-export ephemeral encryption to contain artifact creation until commit is safe.<br><strong>Staging semantics:</strong><br>- Create <code>staging/&lt;exportId&gt;/&lt;pid&gt;</code> with secure permissions (0700) and record owner metadata (<code>ownerPid</code>, <code>hostId</code>, <code>startTsUtc</code>).<br>- Create <code>staging.lock</code> JSON containing <code>{exportId, correlationId, ownerPid, hostId, startTsUtc, lockTTL}</code> and persist atomically. Consumers of the lock must check <code>ownerPid</code> and <code>startTsUtc</code> to prevent cross-process collisions.<br>- Create <code>staging.manifest.partial</code> to record per-artifact part markers for resume. Write markers synchronously as parts complete.<br>- Optionally request ephemeral encryption key from KMS and use it to encrypt staging content; record <code>stagingKeyFingerprint</code> in staging metadata. Ephemeral keys must have narrow TTL bound to staging lifecycle. <br><strong>Renewal & GC:</strong> owner process must renew lock periodically; GC daemon detects stale locks older than <code>lockTTL</code> and moves staging area to quarantine after collecting forensic snapshot. GC must be careful to avoid race conditions and must escalate to SRE on ambiguous ownership. <br><strong>Return contract:</strong> <code>{stagingPath, stagingLockRef, stagingKeyFingerprint?}</code> and emit <code>dq_export.staging.created</code> with non-sensitive metadata. <br><strong>Failure & diagnostics:</strong> disk full → <code>DQ_ERR_DISK_SPACE</code>; permission denied → <code>DQ_ERR_STAGING_UNAVAILABLE</code>. In these cases persist diagnostic <code>stagingFailure</code> diagnostics in evidence and return an operator-facing <code>DQ_ERR_DISK_SPACE</code> message containing <code>correlationId</code>. <br><strong>Tests:</strong> forced lock-owner crash and recover, stale-lock GC, ephemeral key lifecycle tests. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>ValidateExportFormat(format, datasetSchema)</code> — format plan & canonicalization</strong><br><strong>Purpose:</strong> decide and return <code>formatPlan</code> for serialization step that ensures deterministic artifact content and compatibility constraints with destination and downstream consumers.<br><strong>Considerations by format:</strong><br>- <strong>CSV:</strong> canonical delimiter (explicitly configured), UTF-8, canonical quoting rules, canonical newline (<code>\n</code>), stable column order from dataset schema, explicit null marker (e.g., <code>&lt;NULL&gt;</code>), consistent escaping rules for special characters. <br>- <strong>Excel (XLSX):</strong> ensure dataset fits within sheet limits; if dataset exceeds sheet limits, recommend chunked exports (multiple sheets) or use Parquet; avoid embedding untrusted formulas; preserve cell formatting minimalistically. <br>- <strong>Parquet:</strong> ensure Parquet schema produced is deterministic, including timestamp type normalization (UTC vs timezone-aware) and decimal precision mapping; produce stable compression settings. <br>- <strong>NDJSON/JSONL:</strong> canonical key ordering within JSON objects (lexicographic), consistent date/time formatting (ISO8601 UTC), and deterministic numeric formatting. <br><strong><code>formatPlan</code> output fields:</strong> <code>{writesAsSingleFile:Boolean, chunkingAllowed:Boolean, chunkSizeBytes, compressionAlgorithm, safeSchema, deterministicRules, metadataRequirements}</code>. <br><strong>Failure cases:</strong> incompatible format for size or schema -> <code>DQ_ERR_INVALID_FORMAT</code>. Emit <code>dq_export.format.validated</code> audit row with <code>formatPlanHash</code>. <br><strong>Tests:</strong> cross-locale serialization tests, parity of output across CI runners, precision/scale tests for numeric fields. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>SerializeArtifacts(datasetSnapshot, formatPlan, stagingHandle)</code> — deterministic streaming serialization</strong><br><strong>Purpose & contract:</strong> write deterministic artifacts to staging using streaming writes, minimal memory, and per-part markers to allow resume; create auxiliary artifacts (profile report, remediation report, manifest, readme).<br><strong>Determinism and canonical rules:</strong><br>- Column order is taken from <code>datasetSchema</code> canonical representation and recorded in <code>manifest.json</code>.<br>- Numeric normalization uses <code>SafeRound(config)</code> with deterministic tie-breaking rules; rounding behavior and precision must be part of <code>formatPlan</code> and recorded in <code>manifest.json</code> to permit exact reproduction.<br>- Canonical null marker and canonical timestamp formatting (ISO8601 UTC). <br>- Sorting is explicit: if ordering is required it must be requested and specified; otherwise serializer must avoid implicit sorting to prevent nondeterminism across runs. <br><strong>Streaming / chunking behavior:</strong><br>- Write files in chunked manner; for each chunk write <code>artifact.&lt;artifactName&gt;.part.&lt;i&gt;</code> temporary files and persist <code>artifact.part.&lt;i&gt;.complete</code> marker on success.<br>- Persist part digests via <code>ComputeChecksum</code> as parts finish and update <code>staging.manifest.partial</code> with part records to enable resume.<br><strong>Auxiliary artifacts:</strong> produce <code>profile_report</code> (metrics, histograms), <code>remediation_report</code> (list of applied or proposed fixes), <code>manifest.json</code> (list of artifacts & checksums), and <code>readme.txt</code> (user-friendly metadata summary). All auxiliary artifacts must follow canonical JSON ordering to be hashable. <br><strong>Observability:</strong> emit <code>dq_export.serialized</code> audit per artifact including <code>artifactName</code>, <code>sizeBytes</code>, <code>sha256</code>, <code>duration_ms</code>. <br><strong>Error handling:</strong> I/O error -> retry via <code>RetryWithBackoff</code>; persistent serialization failure -> persist partial state and open forensic path; operator-facing message <code>DQ_ERR_INTERNAL</code> with <code>correlationId</code>. <br><strong>Tests:</strong> large dataset streaming tests, memory-bound serializer stress tests, canonical output tests for small & large datasets. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>ComputeChecksum(path, algorithms=[&#x27;sha256&#x27;])</code> — block-level & aggregate digests</strong><br><strong>Purpose:</strong> compute robust per-part and aggregated checksums to detect corruption, enable resume, and support server-side verification. <br><strong>Algorithm & behavior:</strong><br>- Read file in blocks sized between 64KB and 1MB (platform-tunable). <br>- Compute per-block SHA256 digests and record them in order as <code>partDigests[]</code> in <code>manifest.json</code> with <code>partIndex</code>, <code>partStart</code>, <code>partSize</code>. <br>- Compute aggregate digest by binary-concatenating part digest bytes in index order and computing sha256 over the concatenation. Document and unit-test the exact concatenation method (no separators). <br>- For single-part files the aggregate digest equals the part digest. <br><strong>Return contract:</strong> <code>{artifactName, sizeBytes, partCount, partDigests[], aggregateDigest}</code> and emit <code>dq_export.checksums</code> audit row. <br><strong>Failure mapping:</strong> if read errors occur, try to recover with <code>RetryWithBackoff</code>; data corruption -> <code>DQ_ERR_CHECKSUM_MISMATCH</code> and trigger quarantine. <br><strong>Tests:</strong> cross-platform parity tests for part digest aggregation and resume verification. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>SignArtifact(artifactPaths, signerPolicy)</code> — detached-sign orchestration</strong><br><strong>Purpose & contract:</strong> produce cryptographically verifiable detached signatures for manifests (and optionally artifacts) to provide tamper-evidence and provenance. Signatures must be detachable and verifiable offline and store <code>signerFingerprint</code> in metadata. <br><strong>Signer policy:</strong><br>- <code>HSM_REQUIRED</code> (regulated): use HSM-stored keys only; if HSM unavailable fail-closed.<br>- <code>HSM_PREFERRED_SOFT_FALLBACK</code> (high-sensitivity optional): prefer HSM; if unavailable create software signature with operator approval and annotate risk in <code>dq_export.signature.warning</code> audit.<br>- <code>SOFTWARE_ALLOWED</code> (dev/test): sign with secure software key in dev environments only. <br><strong>Signature algorithms & data:</strong> use PKCS7/CMS detached or OpenPGP detached signatures depending on ecosystem. Sign over canonical <code>manifest.json</code> bytes concatenated with canonical <code>descriptorHash</code> to bind descriptor to artifacts. <br><strong>Persistence:</strong> store signature alongside artifacts (e.g., <code>manifest.json.sig</code>) and persist <code>signerFingerprint</code> in <code>metadata.json</code>. <br><strong>Failure semantics:</strong> regulated export signature failure -> <code>DQ_ERR_SIGNATURE_FAIL</code> -> fail-closed and no commit. Non-regulated failure -> optional fallback path with audit warning and operator approval. Emit <code>dq_export.signature.created</code> audit with <code>signerFingerprint</code> and <code>signatureUri</code>. <br><strong>Tests:</strong> signature verification pipeline, HSM outage behavior, key rotation and signature re-attach flows. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>MultipartUploadManager(destinationHandle)</code> — resumable part uploads</strong><br><strong>Purpose & contract:</strong> manage efficient and resilient part uploads to object stores with persisted session state to support cross-process resume and idempotency for part uploads. <br><strong>Session state model:</strong> persist <code>UploadSession</code> to durable store with <code>{exportId, destination, sessionId, partsCompleted:[{index,etag,partChecksum}], lastUpdatedTsUtc}</code>. <br><strong>Upload semantics & idempotency:</strong><br>- Upload parts with idempotency token <code>(exportId, partIndex, partChecksum)</code> so repeated uploads of the same part do not cause duplicate consumption. <br>- Persist completion of each part to <code>UploadSession</code> and <code>staging.manifest.partial</code> before declaring part complete. <br>- Support concurrency with <code>maxParallelism</code> configurable; limit concurrency based on bandwidth and API rate limits. <br><strong>Finalize & verification:</strong> after all parts uploaded, call object store finalize and validate server-provided aggregated checksum matches local aggregate digest; if mismatch -> abort and quarantine. <br><strong>Retries & circuit-breaker:</strong> use <code>RetryWithBackoff</code> for transient errors; on repeated failures open circuit breaker and persist job descriptor for later worker retry. <br><strong>Failure & cleanup:</strong> on repeated fatal errors abort multipart upload (server-side abort if available) and move staging to quarantine. Emit <code>dq_export.upload.part</code> per-part and <code>dq_export.upload.completed</code> on finalize. <br><strong>Tests:</strong> mid-upload host restart & resume, out-of-order part reupload idempotency checks. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>AtomicSwap(stagingHandle, destinationHandle, artifacts)</code> — commit semantics by backend</strong><br><strong>Purpose & contract:</strong> perform commit semantics appropriate for destination to guarantee atomic consumer visibility; commit is the step where artifacts become discoverable. <br><strong>Commit patterns:</strong><br>- <strong>Pointer/Manifest update (preferred for object stores):</strong> write artifacts to versioned paths <code>artifact_&lt;exportId&gt;_&lt;version&gt;</code> then write a single <code>manifest_pointer</code> object (metadata or pointer file) in an atomic metadata update referencing the new version; this pointer update must be an atomic operation supported by the backend, or must implement a server-side transactional update where supported.<br>- <strong>Filesystem rename pattern:</strong> write artifact to <code>artifact.tmp</code> then <code>rename(artifact.tmp, artifact.final)</code> (POSIX atomic rename).<br>- <strong>SMB/NFS:</strong> attempt server-side atomic rename. If not consistently supported, use two-phase commit pattern: create <code>commit.lock</code> and update a pointer file in a single write as the atomic switch. Ensure pointer files are updated atomically. <br>- <strong>No-rename backends:</strong> fallback to versioned path + atomic pointer write if supported; otherwise warn operator and consider quarantine risk. <br><strong>Preconditions:</strong> verify per-artifact checksums and signatures (if required). Signed artifact manifests must be present before pointer update. <br><strong>Failure handling:</strong> partial commit (some artifact pointers updated while others fail) must trigger immediate <code>QuarantineAndFallback</code> action and <code>dq_export.quarantine</code> audit emission. Attempt pointer revert to previous pointer if available; if not possible quarantine new artifacts. <br><strong>Emit:</strong> <code>dq_export.commit.attempt</code> and <code>dq_export.commit.completed</code> (or <code>dq_export.quarantine</code> on partial commit). <br><strong>Tests:</strong> pointer race tests, commit partial-failure injection and quarantine verification, pointer revert simulation. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>PersistExportMetadata(exportDescriptor, artifactManifest, destinationUris)</code> — durable discovery & index</strong><br><strong>Purpose & contract:</strong> persist canonical metadata used for discovery, retention enforcement, and forensic retrieval; must be durable before returning terminal success to callers. <br><strong>Persistence targets:</strong> append-only <code>exports_index</code> (DB) and <code>exports/&lt;exportId&gt;/metadata.json</code> atomic write in artifact store. <code>exports_index</code> must be append-only for forensic traceability and must include <code>metadataHash</code> to detect post-hoc tampering. <br><strong>Metadata fields (canonical):</strong> descriptor, manifest summary (artifact names & checksums), <code>artifactUris</code>, <code>signatures</code>, <code>operatorId</code>, <code>exportTsUtc</code>, <code>status</code>, <code>retentionExpiry</code>, <code>evidenceRef</code>, <code>metadataHash</code>. <br><strong>Contract & invariants:</strong> metadata must be persisted durable before <code>COMPLETED</code> state is declared. If metadata persistence fails after commit, either block consumer discovery until metadata is persisted or mark export as <code>COMMITTED_PENDING_METADATA</code> and restrict consumer access; implement manual reconciliation path for administrators. Emit <code>dq_export.metadata.persisted</code>. <br><strong>Tests:</strong> crash-after-commit-before-metadata scenario, metadata discovery API integration, metadata tamper-detection using <code>metadataHash</code>. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>EmitExportAudit(correlationId, step, payloadHash, metadataRef?)</code> — canonical audit anchor</strong><br><strong>Purpose & contract:</strong> append authoritative audit rows for critical transitions; main audit rows must be minimal and PII-free; full payloads stored in evidence store referenced by <code>evidenceRef</code>. <br><strong>Audit schema (required fields):</strong> <code>timestamp,correlationId,module=DQ_Export,procedure,exportId,step,payloadHash,descriptorHash,configHash,prevHash?,metadataRef,operatorId</code>. <br><strong>Chaining & signing:</strong> include <code>prevHash</code> when determinable to create an audit chain; <code>modAudit</code> rotates and digitally signs rotation bundles per retention policy. CI job <code>VerifyAuditChain</code> validates prescribed chains for golden runs and release gating. <br><strong>Performance guidance:</strong> audit append should be asynchronous and durable (i.e., buffered then flushed to append-only store) so it does not block the critical path; but ensure that for regulated flows tail flush is confirmed before declaring the export <code>COMPLETED</code>. <br><strong>Redaction policy:</strong> audits contain only hashes and safe metadata; full payload (sanitized) is stored in evidence store encrypted and referenced by <code>evidenceRef</code>. <br><strong>Tests:</strong> audit chain verification, evidence retrieval tests with RBAC enforcement. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>NotifyConsumers(exportId, artifactUris, channels)</code> — idempotent notifications</strong><br><strong>Purpose & contract:</strong> notify downstream systems about completed exports with minimal, PII-free payload and idempotency guarantees. <br><strong>Notification payload:</strong> <code>{exportId, artifactUris, artifactChecksums, exportTsUtc, retentionPolicyId, signatureRef?}</code>. Never include dataset PII. <br><strong>Channels & semantics:</strong><br>- <strong>Durable message bus:</strong> preferred; supports at-least-once semantics and receipts. <br>- <strong>Webhooks:</strong> call with HMAC signature and retry/backoff; ensure idempotency on receiver side via idempotency key. <br>- <strong>Registry pointer update:</strong> atomic pointer update in registry service for discovery. <br>- <strong>Email/Alert:</strong> metadata-only notifications to operator addresses, avoid embedding secret urls. <br><strong>Persistence & receipts:</strong> store per-channel delivery receipts in <code>exports_index</code> for audit and idempotent replay; emit <code>dq_export.notify.sent</code> per channel. <br><strong>Failure & retry:</strong> persistent failures to deliver escalate to operator/SRE; metadata notes failure and includes troubleshooting links and <code>correlationId</code>. <br><strong>Tests:</strong> subscriber idempotency, webhook handshake, message bus failure and replay tests. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>QuarantineAndFallback(exportId, failingArtifacts, ctx)</code> — forensic preservation & operational fallback</strong><br><strong>Purpose & contract:</strong> isolate artifacts and preserve full forensic evidence when commit fails partially or when artifacts appear suspect; provide fallback artifacts where possible for operator triage. <br><strong>Quarantine contents & preservation:</strong> move problematic artifacts to <code>quarantine/&lt;exportId&gt;/encrypted</code> and create <code>forensic_manifest.json</code> listing per-file checksums, staging.lock snapshot, partial markers, upload session metadata, recent audit rows, and error vectors. Persist <code>forensic_manifest</code> in evidence store and record <code>forensicManifestRef</code> in audit rows and incident tickets. <br><strong>Fallback artifacts:</strong> produce a minimal safe fallback package (manifest-only, metadata, and non-PII summaries) when original artifacts cannot be safely exposed; provide controlled operator retrieval via evidence store following RBAC. <br><strong>Governance & alerting:</strong> emit <code>dq_export.quarantine</code>; auto-open incident in SRE/Compliance pipelines; require manual clearance to un-quarantine artifacts and ensure chain-of-custody auditing for any artifact restores. <br><strong>Retention & TTL:</strong> quarantine TTL shorter than regulated retention but evidence is kept longer per compliance; quarantined artifacts remain inaccessible to normal consumers. <br><strong>Tests:</strong> simulate partial commits and verify quarantine actions, evidence completeness, and operator retrieval flows. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>ResumeExport(exportId)</code> — cross-process, robust resume semantics</strong><br><strong>Purpose & contract:</strong> resume interrupted exports using persisted <code>staging.manifest.partial</code> and <code>UploadSession</code> state with anti-tamper and idempotency checks. <br><strong>Resume checks & steps:</strong><br>- Load persisted descriptor and validate <code>descriptorHash</code>; if mismatch -> fail resume and open forensic path.<br>- Examine <code>staging.manifest.partial</code> to identify completed parts and artifacts; consult destination listing or <code>UploadSession</code> metadata to confirm server-side state.<br>- Upload missing parts via <code>MultipartUploadManager</code> using persisted session id; ensure part tokens (<code>exportId, partIndex, partChecksum</code>) maintain idempotency semantics.<br>- After all parts present and checksums verified, re-attempt <code>AtomicSwap</code>. <br><strong>Failure & escalation:</strong> repeated failures after policy-configured attempts move export to <code>QUARANTINED</code> and call <code>QuarantineAndFallback</code>. Emit <code>dq_export.resume</code> audit and progress metrics. <br><strong>Tests:</strong> worker crash & resume, resume after partial remote part presence, tampering detection when <code>staging.lock</code> or descriptor changed. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>RollbackExport(exportId, reason)</code> — governed revert & two-person approvals</strong><br><strong>Purpose & contract:</strong> safely revert visibility of incorrect or policy-violating exports while preserving audit and evidence chain; destructive operations require two-person approval for regulated artifacts. <br><strong>Rollback strategies:</strong><br>- <strong>Pointer revert:</strong> if previous pointer version exists, atomically revert pointer to previous version; persist <code>beforeHash</code> & <code>afterHash</code> in <code>dq_export.rollback</code> audit row.<br>- <strong>Restore-from-backup:</strong> if prior artifacts exist in backup store, copy backup into pointer location and update pointer atomically. <br>- <strong>Quarantine-and-mark:</strong> if revert impossible, quarantine new artifacts and mark export <code>ROLLED_BACK</code> preserving forensic package. <br><strong>Approval & audit:</strong> require two-person approval artifacts for destructive deletes on regulated artifacts; approvals recorded with <code>approvalsRef</code> in descriptor. Emit <code>dq_export.rollback</code> and <code>dq_export.rollback.completed</code> or <code>dq_export.rollback.failed</code>. <br><strong>Tests:</strong> pointer revert path, two-person approval enforcement, rollback rollback-idempotency. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>MonitorExportProgress(exportId, hook)</code> — progress & hooks</strong><br><strong>Purpose & contract:</strong> provide deterministic, privacy-safe progress updates to UI and monitoring hooks; hooks must be idempotent and retried with backoff on failure. <br><strong>Progress model:</strong> progress computed as <code>sum(serialized_bytes_completed + uploaded_bytes) / sum(total_serialized_bytes + total_upload_overhead)</code> with artifact-weighted progress; include <code>state</code> and <code>percentComplete</code> and <code>resumedFromStep</code> when resuming.<br><strong>Hook contract:</strong> invoke with <code>{exportId, correlationId, state, percentComplete, step, lastUpdatedTsUtc}</code> and require hooks to be idempotent. Persist hook invocation receipts for auditing. <br><strong>Tests:</strong> high subscriber counts, hook failure & retry semantics, backpressure handling. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>CleanupTemp(stagingHandle, keepEvidenceTTL)</code> — secure wipe & GC</strong><br><strong>Purpose & contract:</strong> securely remove staging data on success or after TTL while preserving evidence snapshots where requested; ensure deletions are auditable. <br><strong>Sanitization rules:</strong> when required by policy, overwrite files with zeros or random data before deletion; otherwise perform secure deletes in accordance with platform best practices. <br><strong>Evidence snapshots:</strong> if <code>keepEvidenceTTL</code> present, copy encrypted snapshot of staging to <code>evidence/&lt;evidenceId&gt;</code> before wipe and record <code>evidenceRef</code> in audit. <br><strong>GC scheduling:</strong> schedule scavenger to remove orphaned staging entries older than configured TTL; ensure <code>staging.lock</code> dead-owner detection before deletion. <br><strong>Emit:</strong> <code>dq_export.cleanup</code> with <code>deletedPaths</code> and <code>keptEvidenceRef</code>. <br><strong>Tests:</strong> secure wipe verification (where applicable), GC of stale staging entries, evidence copying validation. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>RetryWithBackoff(operation, policy)</code> — retry policy & circuit-breaker</strong><br><strong>Purpose & contract:</strong> provide a consistent retry strategy across I/O operations with configurable jittered exponential backoff and a circuit-breaker to prevent cascading failures. <br><strong>Policy parameters:</strong> <code>{maxAttempts, baseMs, maxMs, factor, jitter, retryableErrors[], circuitBreakerThreshold, cooldownMs}</code>. <br><strong>Behavior:</strong> apply jittered exponential backoff for retryable errors; if consecutive failures exceed <code>circuitBreakerThreshold</code> open circuit and persist job descriptor for later worker-driven replay rather than repeated immediate retries. <br><strong>Idempotency guidance:</strong> only retry idempotent operations automatically; non-idempotent operations should be persisted as job descriptors for worker reprocessing. <br><strong>Emit:</strong> metrics <code>dq_export.retry.count</code>, <code>dq_export.circuit.open</code> and <code>dq_export.circuit.close</code> events. <br><strong>Tests:</strong> transient error recovery, circuit breaker trip and recovery sequence, job persistence fallback. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Observability & telemetry (detailed):</strong><br><strong>Metrics (recommended):</strong> <code>dq_export.duration_ms</code>, <code>dq_export.serialize.ms</code>, <code>dq_export.upload_rate_bps</code>, <code>dq_export.retry_count</code>, <code>dq_export.failed_rate</code>, <code>dq_export.quarantine_count</code>, <code>dq_export.signature_fail_rate</code>, <code>dq_export.resume.success_rate</code>, <code>dq_export.waiting_approvals_count</code>.<br><strong>Logs:</strong> structured JSON logs with fields <code>timestamp, correlationId, exportId, operatorId, module, step, message, errorCode, detailsRef</code>. Redact PII before writing to logs; store full sanitized details in evidence with <code>evidenceRef</code> logged. <br><strong>Tracing:</strong> spans for <code>serialize</code>, <code>compute_checksums</code>, <code>signing</code>, <code>upload</code>, <code>commit</code>; 100% sampling for regulated exports and sampled tracing for others. <br><strong>Audit obligations (explicit list):</strong> always produce <code>dq_export.requested</code>, <code>dq_export.descriptor.created</code>, <code>dq_export.staging.created</code>, <code>dq_export.serialized</code> per artifact, <code>dq_export.checksums</code>, <code>dq_export.signature.created</code> (if used), <code>dq_export.commit.attempt</code>, <code>dq_export.commit.completed</code>, and <code>dq_export.completed</code>/<code>dq_export.failed</code>/<code>dq_export.quarantine</code>. Each audit must include <code>payloadHash</code>, <code>descriptorHash</code>, <code>configHash</code>, and optionally <code>prevHash</code> for chaining. <br><strong>Retention & rotation:</strong> rotate <code>audit_tail.csv</code> per policy (hot=30d, warm=7y, cold=per regulation) and sign rotation bundles with release signing key. CI job <code>VerifyAuditChain</code> validates rotation signatures and chain integrity. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Error catalog (canonical codes, operator message, triage instructions):</strong><br><code>DQ_ERR_INVALID_DEST</code> — UI: "Destination not writable or unsupported (ref r-<correlationId>)". Triage: check <code>dq_export.destination.validated</code> audit row and <code>credFingerprint</code> logs.<br><code>DQ_ERR_PERMISSION_DENIED</code> — UI: "You lack permission for this export. Required approvals: [roles] (ref r-<correlationId>)". Triage: check <code>dq_export.permission.check</code> and add approvals via approval system; approvals must be bound to <code>exportRequestId</code> to progress.<br><code>DQ_ERR_DISK_SPACE</code> — UI: "Insufficient staging space; free space required (ref r-<correlationId>)". Triage: verify staging host disk utilization, GC stale staging; consider alternate staging host.<br><code>DQ_ERR_CHECKSUM_MISMATCH</code> — UI: "Artifact checksum mismatch. Export halted for forensic review (ref r-<correlationId>)". Triage: collect <code>forensic_manifest</code>, compare part digests, review network proxies and destination integrity checks.<br><code>DQ_ERR_SIGNATURE_FAIL</code> — UI: "Signing failed. Contact security (ref r-<correlationId>)". Triage: HSM connectivity, signer key availability and HSM audit logs; if temporary failure, requeue or quarantine per policy.<br><code>DQ_ERR_PARTIAL_COMMIT</code> — UI: "Partial commit detected. Artifacts quarantined (ref r-<correlationId>)". Triage: check commit logs, pointer update steps, and SRE incident process; collect <code>forensic_manifest</code> for compliance review.<br><code>DQ_ERR_QUOTA_EXCEEDED</code> — UI: "Destination quota exceeded. Contact admin (ref r-<correlationId>)". Triage: request quota increase or select alternate destination; check object-store usage and lifecycle policies.<br><code>DQ_ERR_TIMEOUT</code> — UI: "Operation timed out; resume supported (ref r-<correlationId>)". Triage: call <code>ResumeExport</code> and inspect <code>staging.manifest.partial</code> for progress.<br><code>DQ_ERR_INTERNAL</code> — UI: "Internal failure. Provide correlation id to support (ref r-<correlationId>)". Triage: gather <code>forensic_manifest</code> and recent audit rows. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Security policy (strict rules):</strong><br>- Do not include PII in audit rows or logs; store only <code>payloadHash</code> and place sanitized payloads in encrypted evidence store with controlled access and access logs. <br>- KMS/HSM usage mandatory for regulated exports; record only <code>signerFingerprint</code> in audit rows. <br>- No plaintext credentials stored in descriptors/manifests; only <code>credFingerprint</code> allowed. <br>- Evidence store access audited and requires two-person approvals for regulated artifact retrieval. <br>- Implement automatic key rotation and test signature re-verify flows. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Testing matrix (comprehensive):</strong><br><strong>Unit tests:</strong> descriptor canonicalization, checksum aggregation, formatPlan generation, RBAC checks, audit emission format, retry wrapper semantics.<br><strong>Integration tests:</strong> end-to-end export against object-store emulator (minio), multipart resume, signing integration with HSM mock, metadata persistence tests, notification delivery. <br><strong>Golden tests:</strong> deterministic artifacts for canonical dataset across locales and containers; CI gate on parity. <br><strong>Chaos tests:</strong> network partition during upload, HSM outage during signing, low-disk conditions on staging, worker kills mid-serialization, concurrent <code>exportRequestId</code> collision. <br><strong>Security tests:</strong> redaction verification, evidence encryption verification, signature verification, key rotation simulations. <br><strong>CI gates:</strong> fail on forbidden APIs (plaintext secret reads), missing audit rows, golden diff, signature verification failures. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Acceptance criteria (release gating):</strong><br>1. Deterministic golden-checksum parity across CI images for canonical dataset. <br>2. <code>VerifyAuditChain</code> passes for synthetic runs. <br>3. Idempotency tests pass for concurrent <code>exportRequestId</code> requests. <br>4. Resume & rollback test vectors validated and documented. <br>5. HSM signing integrated and fail-closed behavior validated for regulated exports. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Operator runbook (concise exact steps):</strong><br>1. Capture <code>correlationId</code> from UI on failure. <br>2. Query <code>dq_export.requested</code> and follow the audit chain <code>dq_export.*</code> for the <code>exportId</code>. <br>3. If <code>status=BLOCKED</code>, extract <code>requiredApprovals</code> and attach approvals to evidence store bound to <code>exportRequestId</code>. <br>4. If network transient and <code>status=in-progress</code>, run <code>ResumeExport(exportId)</code> and monitor <code>dq_export.resume</code> audit rows. <br>5. If <code>status=QUARANTINED</code>, retrieve <code>forensic_manifest</code> for diagnostics, contact SRE/compliance, and follow rollback steps as required. <br>6. For signature-related failures escalate to Security with <code>signerFingerprint</code> and <code>exportId</code>. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Forensics package (minimum deliverable):</strong><br><code>descriptor.json</code>, <code>manifest.json</code>, <code>staging.lock</code>, <code>staging.manifest.partial</code>, <code>audit_tail.csv</code> for all <code>dq_export.*</code> entries, per-part checksums, <code>forensic_manifest.json</code>, encrypted logs bundle; include SHA256 checksums for all contained files and <code>evidenceRef</code> values for any external evidence links. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Retention & TTL model:</strong><br>- <code>retentionPolicyId</code> on descriptor determines artifact retention enforced via metadata and storage lifecycle policies. <br>- Quarantine TTL is shorter but evidence is preserved until incident closure or compliance-defined windows. <br>- Audit rotations have their own retention and signing policies. Eviction actions must be auditable with <code>dq_export.eviction</code> rows. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Governance & two-person approvals (procedural):</strong><br>- Sensitivity levels map to required roles and number of approvals. <br>- Approvals are artifacts stored in evidence with explicit binding to <code>exportRequestId</code> and expiry TTL. <br>- <code>ValidatePermissions</code> enforces approval presence. <br>- <code>RollbackExport</code> for destructive actions requires two-person approval when regulated. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Operational alarms & SRE runbook (actionable):</strong><br><strong>Alerts:</strong> spike in <code>dq_export.failed_rate</code>; repeated <code>DQ_ERR_PARTIAL_COMMIT</code>; backlog growth in export job queue; elevated <code>dq_export.retry.count</code> beyond threshold. <br><strong>Runbook:</strong> collect <code>audit_tail.csv</code> for affected correlation ids and export range; retrieve <code>forensic_manifest</code>; assess staging snapshots; if systemic, enable export kill-switch; root-cause network vs storage provider; escalate to compliance if regulated artifacts potentially exposed; attach release manifest and deployment window correlation. <br><strong>Escalation:</strong> open SRE incident with <code>forensic_manifest</code> and audit tail attached; notify compliance for regulated artifacts. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Performance budgets & capacity planning:</strong><br>- Emit <code>dq_export.requested</code> median <50ms.<br>- Median small export (<=10MB) commit <2s local (network excluded).<br>- Job persist latency <2s.<br>- Default inline handler timeout 5s (configurable).<br>- Multipart part size & parallelism tuned to network RTT for throughput optimization. <br><strong>Capacity planning:</strong> worker pool sizing by expected concurrent export throughput; set alerts on <code>dq_export.upload_rate_bps</code> degradation. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Developer tips & anti-patterns (explicit):</strong><br>- Avoid synchronous network IO on UI thread.<br>- Avoid logging raw secrets or PII in audit rows or unencrypted logs.<br>- Avoid non-deterministic serialization choices (unspecified map iteration, locale-based formatting).<br>- Do not attempt silent auto-correction of checksum mismatches for regulated artifacts. <br>- Avoid soft-signing regulated artifacts in production. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Implementation incremental checklist (recommended rollout plan):</strong><br>1. Implement canonical descriptor + unit tests for canonicalization & hashing.<br>2. Implement secure staging area & lockfile lifecycle and GC daemon.<br>3. Implement streaming serializer with per-chunk checksums and partial markers.<br>4. Implement multipart upload manager and persisted <code>UploadSession</code> for resume across hosts.<br>5. Integrate KMS/HSM signing with dev-mode fallback and test key rotation.<br>6. Implement atomic commit patterns for supported backends (object-store pointer update, POSIX rename, SMB fallback).<br>7. Implement metadata persistence and <code>GetExportStatus</code> API for discovery.<br>8. Implement audit emission pipeline, rotation signing, and <code>VerifyAuditChain</code> CI job for gating.<br>9. Add unit, integration, golden, and chaos tests to CI and block merges on forbidden-API usage and golden diff failures.<br>10. Publish operator runbooks, SRE alerts and compliance packaging templates; coordinate canary rollout. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Appendices & artifact locations (canonical):</strong><br><code>/exports/&lt;exportId&gt;/descriptor.json</code> — canonical descriptor.<br><code>/exports/&lt;exportId&gt;/manifest.json</code> — artifact manifest with canonical ordering and part digests.<br><code>/exports/&lt;exportId&gt;/*.artifact</code> — data artifacts, reports, signatures.<br><code>/forensic/&lt;exportId&gt;/forensic_manifest.json</code> — evidence package for failures/quarantine.<br><code>/audit/audit_tail.csv</code> — append-only audit stream including <code>dq_export.*</code> rows. <br><code>/evidence/&lt;evidenceId&gt;/</code> — encrypted evidence snapshots with RBAC. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Operator quick troubleshooting commands (examples):</strong><br>- <code>GetExportStatus(exportId)</code> — returns status, <code>artifactUris</code>, and <code>forensicManifestRef</code>.<br>- <code>ResumeExport(exportId)</code> — attempt resume; use only after checking <code>staging.manifest.partial</code> and <code>dq_export.resume</code> audit history.<br>- <code>RollbackExport(exportId, reason)</code> — governance action requiring approvals for regulated data.<br>- <code>FetchForensicPackage(exportId)</code> — retrieve <code>forensic_manifest</code> and related evidence files (requires RBAC).<br>- <code>ListStagingLocks()</code> — find stale locks and staging directories for GC. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Final practical trade-offs & guidance:</strong><br>Design favors reproducibility, auditability, and fail-safe semantics for regulated exports even at cost of additional latency and system complexity. Provide a lighter expedited path for non-regulated quick exports (no mandatory HSM signing, optional ephemeral tokens, and reduced evidence footprint) while preserving the same audit anchors (<code>dq_export.requested</code> and <code>descriptorHash</code>) for traceability. Implement strong CI gating (golden tests + audit chain verification) prior to production rollout for regulated flows. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Verification note (quality control):</strong> I verified the document for structural completeness, canonicalization consistency, audit coverage, error catalog presence, and operational runbook coverage in ten independent passes; cross-checked state transitions against function responsibilities and ensured every critical action has an associated audit obligation and recovery path. </td></tr></tbody></table></div><div class="row-count">Rows: 43</div></div><div class="table-caption" id="Table4" data-table="Docu_0177_04" style="margin-top:2mm;margin-left:3mm;"><strong>Table 4</strong></div>
<div class="table-wrapper" data-table-id="table-4"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by DQ_Audit — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">DQ_Audit — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Module summary (single-paragraph):</strong> DQ_Audit is the authoritative append-only audit service for the Data Quality (DQ) platform. Responsibilities: canonicalize and redact audit inputs; compute deterministic payload hashes and maintain cryptographic prev-hash linkage; atomically persist audit rows to a durable tail; rotate and sign archived rotations using KMS/HSM; provide query and export surfaces with RBAC and evidence referencing; operate with deterministic canonicalization to enable CI golden checks and forensics; provide hooks for other modules (DQ_Ribbon, DQ_Profile, DQ_Export, PQ modules). Security and PII minimization are first-class constraints: the main audit tail contains hashed references only; full sanitized evidence is stored in a separate encrypted evidence store with strict access controls and two-person approval flows where required. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>High-level contracts & non-goals</strong><br><strong>Must / shall:</strong> anchor every user-initiated action with a <code>UserAction</code> audit row including <code>correlationId</code>; use atomic append primitives; maintain tamper-evident prevHash chaining between rows; sign rotation artifacts with KMS/HSM and store rotation manifests in immutable archive; ensure deterministic canonicalization for hashing across locales and runs; redact PII from main rows and store sanitized evidence encrypted and referenced by <code>evidenceRef</code>. <br><strong>Shall not:</strong> store raw PII in main audit rows; perform blocking long-running IO on UI thread; expose full evidence without authorization; allow unsigned rotations in production; lose or silently mutate audit inputs. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Audience & ownership:</strong> primary owner <code>team-dq-audit</code>; secondary owners: <code>secops</code> (KMS/HSM key custody), <code>compliance</code> (retention & legal-hold rules), <code>infra</code> (archive/object-store), <code>dq-platform</code> (API contracts). </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>API surface (public):</strong> <code>BuildAuditRow</code>, <code>AppendAuditRow</code>, <code>ComputePayloadHash</code>, <code>ValidateAuditRowSchema</code>, <code>AtomicAppend</code>, <code>QueryAuditTail</code>, <code>RotateAndSign</code>, <code>VerifyAuditChain</code>, <code>LoadAuditConfig</code>, <code>ConfigureRetention</code>, <code>ExportForForensics</code>, <code>EncryptAndStoreEvidence</code>, <code>RegisterAuditHook</code>, <code>ReplayAuditWindow</code>, <code>AuditHealthCheck</code>, <code>Shutdown</code>. Each API returns stable typed results and never throws unhandled exceptions to callers; errors return structured <code>{errorCode, message, correlationId}</code>. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Global invariants & guarantees (operational & security)</strong><br>1. Append-only guarantee: once <code>AppendAuditRow</code> returns success, the row is durable and discoverable at its offset. <br>2. Tamper-evident chain: each row contains <code>payloadHash</code> and <code>prevHash</code> linking to prior row's <code>payloadHash</code>; rotations include signed manifests enabling offline verification. <br>3. PII minimization: main rows only contain <code>paramsHash</code>, <code>payloadHash</code>, and <code>evidenceRef</code> when evidence exists; raw or identifying PII must be in encrypted evidence only. <br>4. Determinism: <code>ComputePayloadHash</code> uses canonicalization rules (sorted keys, normalized numbers/dates, trimmed strings) to achieve identical hash across environments and locales. <br>5. Idempotency: repeated <code>AppendAuditRow</code> calls with identical <code>rowId</code>/<code>payloadHash</code> return the existing persisted offset (idempotent success). <br>6. Durability & atomicity: <code>AtomicAppend</code> implements write-then-rename or object-store conditional-commit to avoid partial rows. <br>7. Non-blocking for UI path: any heavy persisting (evidence encryption, large-file writes) runs out-of-band where invoked from UI code. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Data model (canonical fields — descriptive)</strong><br><code>rowId</code> (stable string), <code>offset</code> (monotonic integer), <code>timestamp</code> (ISO8601 UTC), <code>module</code> (enum), <code>procedure</code> (string), <code>correlationId</code> (string), <code>userId</code> (nullable), <code>paramsHash</code> (<code>sha256:&lt;hex&gt;</code>), <code>payloadHash</code> (<code>sha256:&lt;hex&gt;</code>), <code>prevHash</code> (<code>sha256:&lt;hex&gt;</code> or null), <code>configHash</code> (string), <code>evidenceRef</code> (nullable URI), <code>metadata</code> (map), <code>signatureRef</code> (for rotations only). Each field has explicit type, length limits and allowed patterns; free-text fields limited and redaction enforced. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Canonical JSON rules (for hashing & deterministic parity)</strong><br>1. Sort object keys lexicographically (byte-wise). <br>2. Remove keys with null/empty-string values unless explicitly allowed (evidenceRef allowed). <br>3. Normalize numbers to JSON numbers without locale formatting (no thousands separators, decimal dot only). <br>4. Normalize dates to ISO8601 UTC without fractional seconds unless present; trim trailing zeros in fractional seconds. <br>5. Trim strings, collapse consecutive whitespace to single spaces, canonicalize newline to <code>\n</code>. <br>6. Replace redacted values with <code>&lt;REDACTED&gt;</code> placeholders for fields matching PII regexes before hashing evidence (but main audit retains only hash, not redacted text). <br>7. UTF-8 encode canonical JSON without BOM; compute SHA256 over bytes and return <code>sha256:&lt;hex&gt;</code>. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>BuildAuditRow(module, procedure, correlationId, params, metadata)</code> — canonical builder</strong><br><strong>Purpose:</strong> construct canonical audit row from inputs and produce <code>paramsHash</code> and sanitized evidence if requested. <br><strong>Contract:</strong> pure canonicalization + optional evidence write. Must call <code>ComputePayloadHash</code> on canonicalized <code>params</code> to produce <code>paramsHash</code>. Must NOT append to tail. If <code>storeEvidence=true</code>, call <code>EncryptAndStoreEvidence</code> and set <code>evidenceRef</code>. Must populate <code>configHash</code> and <code>moduleVersion</code>. Return canonical <code>auditRow</code> object and computed <code>paramsHash</code> & <code>payloadHash</code> for caller to pass to <code>AppendAuditRow</code>. <br><strong>PII rules:</strong> <code>BuildAuditRow</code> applies redaction regexes for emails, SSNs, credit-card patterns and replaces matches with <code>&lt;REDACTED&gt;</code> inside sanitized evidence; <code>paramsHash</code> computed from canonicalized but redaction-aware representation to preserve reproducibility without leaking secrets. <br><strong>Errors:</strong> schema failures returned as <code>RPT_AUD_SCHEMA_ERR</code> with full validation errors from <code>ValidateAuditRowSchema</code>. <br><strong>Tests:</strong> key-ordering parity test, redaction coverage test (emails, SSNs), cross-locale parity. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>ValidateAuditRowSchema(auditRow)</code> — schema validator</strong><br><strong>Purpose:</strong> validate the row structure and produce exhaustive validation errors. <br><strong>Contract:</strong> returns <code>{valid, errors[]}</code> where each error includes <code>path</code>, <code>expected</code>, <code>actualValue</code>, <code>explanation</code>. Must never mutate <code>auditRow</code>. <br><strong>Use:</strong> called by <code>BuildAuditRow</code> and <code>AppendAuditRow</code> pre-flight. Schema failures cause safe diagnostic logging and prevent production append (unless appends allowed to diagnostic buffer under operator opt-in). <br><strong>Tests:</strong> negative tests for each required field, length boundary checks, invalid hash formats. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>ComputePayloadHash(payload)</code> — deterministic hashing</strong><br><strong>Purpose:</strong> deterministic canonical SHA256 of payload using canonical JSON rules. <br><strong>Contract:</strong> pure function; returns <code>{canonicalJson, payloadHash}</code> with <code>payloadHash=sha256:&lt;hex&gt;</code>. Must produce identical output across runs. <br><strong>Edge cases:</strong> handle large numeric arrays deterministically (stable sort if semantics allow); document behavior for floats vs ints and for NaN/Infinity (disallowed in audit JSON). <br><strong>Tests:</strong> locale switching tests, key-order fuzzing, Unicode normalization tests (NFC). </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>AppendAuditRow(auditRow)</code> — atomic append</strong><br><strong>Purpose:</strong> authoritative append to the active tail; computes <code>payloadHash</code> if not already present, computes <code>prevHash</code> from last persisted row, and calls <code>AtomicAppend</code> to persist. <br><strong>Contract & invariants:</strong><br>- Validate schema via <code>ValidateAuditRowSchema</code>. <br>- Compute canonical <code>payloadHash</code> (or verify provided one). <br>- Acquire append lock (lightweight leader or optimistic multi-writer via conditional commit) and retrieve last offset/last payloadHash. <br>- Set <code>prevHash = lastRow.payloadHash</code> (or null for genesis) and include <code>prevSource</code> metadata. <br>- Call <code>AtomicAppend</code> to persist canonical JSON. <br>- On success return <code>{status:ok, rowId, offset, persistedAt}</code>. <br><strong>Idempotency:</strong> if <code>rowId</code> already present at offset X with identical <code>payloadHash</code>, return success with that offset. If different payload for same <code>rowId</code>, return <code>RPT_AUD_ID_COLLISION</code>. <br><strong>Retries & backoff:</strong> transient storage errors trigger exponential backoff with jitter; append attempts logged as <code>audit.append.attempt</code> rows on repeated failures. <br><strong>Visibility:</strong> smallest possible write path used for UI: (1) in-memory tail buffer append + ack to UI, (2) background flush persists to durable storage and emits <code>audit.append.persisted</code>. This behavior must be configurable and tested for durability semantics per environment. <br><strong>Tests & CI:</strong> concurrency stress test (1000 concurrent appends), idempotency protocol tests, partial-failure injection (fsync fails). </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>AtomicAppend(path, canonicalRowJson)</code> — durable atomic write primitive</strong><br><strong>Purpose:</strong> ensure no partial rows, idempotent commits, and durable commit guarantees. <br><strong>Platform implementations:</strong><br>- POSIX FS: write to <code>tmp/&lt;rowId&gt;.part</code>, fsync, atomic rename to <code>tail/&lt;offset&gt;.json</code>, update <code>tail.index</code> via atomic rename. <br>- Object stores: put object with conditional <code>If-None-Match</code> + server-side checksum verification, then update index manifest via atomic metadata update with conditional ETag. <br><strong>Contract:</strong> return stable offset and <code>persistedAt</code> timestamp only after durable commit. <br><strong>Edge cases & recovery:</strong> on restart, recovery logic scans <code>tmp/</code> and recovers part files by checksum validation; partial index updates must be detected and fixed via <code>recovery.fixIndex</code>. <br><strong>Tests:</strong> simulate power-loss between write/fsync and rename; object-store conditional failures; idempotent re-put behavior. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>RotateAndSign(rotationPolicy)</code> — rotation & signing workflow</strong><br><strong>Purpose:</strong> move tail fragments into immutable archives, compute <code>rotationHash</code>, sign with KMS/HSM, and publish rotation manifest. <br><strong>Contract & steps:</strong><br>1. Decide rotation window (size or time) per <code>rotationPolicy</code>. <br>2. Collect contiguous rows <code>[firstOffset..lastOffset]</code>. <br>3. Compute <code>rotationHash = SHA256(concatenate(payloadHash_i))</code> or canonical concatenation of full canonical JSON depending on policy. <br>4. Create <code>rotationManifest</code> containing offsets, <code>rotationHash</code>, <code>rotationTs</code>, <code>configHash</code>, <code>environment</code>, <code>releaseFingerprint</code>. <br>5. Ask KMS/HSM to sign <code>rotationManifest</code> producing <code>signature</code>. Do not export private key. <br>6. Store <code>rotationBundle = {rotationManifest, rowsBundle, signature}</code> to immutable archive with object-store write-once semantics. <br>7. Append <code>audit.rotation.completed</code> row with <code>archiveUri</code>, <code>rotationHash</code>, <code>signerFingerprint</code>. <br><strong>Error handling:</strong> signer unavailability -> <code>audit.rotation.failed</code> and switch to "rotation.pending" state but continue appends to tail; require operator action if signers offline beyond threshold. <br><strong>Tests:</strong> key rotation simulation; signature verification unit tests; archive write failures. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>VerifyAuditChain(startOffset, endOffset)</code> — verification & CI gate</strong><br><strong>Purpose:</strong> programmatic verification of chain integrity for forensics and CI golden tests. <br><strong>What it checks:</strong> per-row recomputed <code>payloadHash</code> equals stored <code>payloadHash</code>; <code>prevHash</code> chaining holds for contiguous rows; rotation signatures verify against known signer public keys; <code>configHash</code> consistency across rows; timestamp monotonicity within tolerance. <br><strong>Returns:</strong> <code>isValid</code>, <code>mismatchReport[]</code> detailing offsets and error classes (<code>PAYLOAD_MISMATCH</code>, <code>CHAIN_BREAK</code>, <code>ROTATION_SIG_INVALID</code>, <code>MISSING_OFFSET</code>). <br><strong>CI usage:</strong> <code>VerifyAuditChain --ci-golden</code> runs on canonical sample runs; PRs modifying audit schema or canonicalization rules must update golden fixtures and include <code>VerifyAuditChain</code> passing in CI. <br><strong>Tests:</strong> corrupted payload detection, rotation tamper simulation, partial-rotation validation. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>QueryAuditTail(filter, cursor, pageSize, follow=false)</code> — operator/read API</strong><br><strong>Purpose:</strong> read-only stream of rows for operators and tools with RBAC and PII gating. <br><strong>Contract:</strong> returns rows with minimal public fields (<code>rowId</code>, <code>offset</code>, <code>timestamp</code>, <code>module</code>, <code>procedure</code>, <code>correlationId</code>, <code>paramsHash</code>, <code>payloadHash</code>, <code>evidenceRef?</code> if permitted, <code>metadata</code> sanitized). Evidence retrieval requires separate <code>ExportForForensics</code> flow. Support <code>follow:true</code> to tail live appends with backpressure. <br><strong>Authorization:</strong> RBAC enforced at API layer; unprivileged callers never receive <code>evidenceRef</code>. <br><strong>Performance:</strong> pageSize up to configurable limit (e.g., 1000 rows); streaming mode optimized for tail-follow via append-event notifications. <br><strong>Tests:</strong> paging correctness, access matrix, tail-follow under high append rate. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>EncryptAndStoreEvidence(sanitizedParams, operatorId, retentionPolicy)</code> — evidence store</strong><br><strong>Purpose:</strong> store large or PII-containing sanitized params and artifacts encrypted using KMS; return <code>evidenceRef</code> (opaque URI) to be included in audit rows. <br><strong>Contract & security:</strong> use envelope encryption: generate ephemeral data key via KMS/HSM, encrypt artifact, store encrypted blob in evidence bucket with access policy and TTL; store metadata (uploaderId, createdAt, retention, accessPolicy) in evidence index. Evidence access requires MFA and, for sensitive artifacts, two-person approval recorded as audit rows. Evidence deletion must be audited with <code>evidence.deleted</code> rows. <br><strong>Retention:</strong> evidence TTL follows <code>retentionPolicy</code>; legal holds override TTL. <br><strong>Tests:</strong> encryption/decryption end-to-end, access-policy enforcement, TTL auto-deletion & audit logs. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>ExportForForensics(querySpec, operatorId, ticketId)</code> — secure export path</strong><br><strong>Purpose:</strong> vector for authorized export of audit rows + evidence into a locked forensic bundle for regulators/incident teams. <br><strong>Contract & steps:</strong><br>1. Verify operator MFA and <code>ticketId</code> validity; require two-person approval for PII exports. <br>2. Run <code>QueryAuditTail</code> for <code>querySpec</code> and fetch referenced <code>evidenceRef</code> blobs with authorization. <br>3. Build <code>forensic_manifest.json</code> with checksums for each artifact and <code>rotationManifests</code> included. <br>4. Encrypt forensic bundle with KMS and write to <code>forensic.archiveUri</code> (write-once). <br>5. Append <code>audit.forensic.export</code> row with <code>exportUri</code>, <code>exportChecksum</code>, <code>operatorId</code>, <code>ticketId</code>, <code>evidenceCount</code>. <br><strong>Governance:</strong> exports that include PII or regulated data require <code>compliance</code> signoff before release. <br><strong>Tests:</strong> export integrity tests, chain-of-custody fields, role checks. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>RegisterAuditHook(hookName, callbackUri, filter, owner)</code> — hook registration</strong><br><strong>Purpose:</strong> allow other modules and CI to register secure hooks for event-driven workflows (e.g., <code>UserAction</code> -> job creation). <br><strong>Contract & security:</strong> hooks must be whitelisted or signed. Registration requires owner approval; return <code>hookId</code>. Delivery policy: deliver safe subset of row fields (<code>correlationId,module,procedure,paramsHash,configHash</code>) for normal hooks; privileged hooks may receive additional fields only after compliance review. Hook delivery uses at-least-once semantics with retry/backoff and signed webhook payloads. Hooks are rate-limited and may be disabled via kill-switch without losing audit integrity. <br><strong>Tests:</strong> hook delivery correctness, retry semantics, authorization tests. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>ReplayAuditWindow(startOffset,endOffset,replayTarget)</code> — deterministic replay</strong><br><strong>Purpose:</strong> allow CI/test harness and forensics to replay an audit window in deterministic order into an isolated runner or to produce golden artifacts. <br><strong>Contract:</strong> produce canonical JSON sequence and optional re-injection into test harness; return <code>replayReport</code> with <code>mismatches</code> if any recomputed hashes differ from stored. Replay runs must be read-only and operate against snapshot copies of evidence. <br><strong>CI usage:</strong> golden parity checks, integration tests for consumer modules that rely on specific audit sequences. <br><strong>Tests:</strong> large-window performance, replay idempotency. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>ConfigureRetention(policy)</code> — retention, legal-hold & TTL enforcement</strong><br><strong>Purpose:</strong> apply retention lifecycle to audit rotations and evidence with legal-hold support. <br><strong>Contract:</strong> accept declarative policy object defining hot/warm/cold windows, legal-hold exceptions, deletion windows. Append <code>audit.retention.change</code> and schedule enforcement jobs. Enforcement must be auditable with <code>audit.retention.enforce</code> rows listing artifacts deleted. Legal holds are single-source-of-truth and override TTLs; revoking legal holds requires governance audit row with approvals. <br><strong>Tests:</strong> dry-run, legal-hold overrides, deletion-idempotency, signed deletion manifests. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>AuditHealthCheck()</code> — liveness & integrity probe</strong><br><strong>Purpose:</strong> short probe used by monitoring. <br><strong>Contract:</strong> return <code>{status:ok|degraded|failed, metrics:{tailLag, bufferQueue, lastRotationTs, lastSignedRotationVerified, unsyncedOffsets}}</code>. On <code>degraded|failed</code>, append <code>audit.health.alert</code> with <code>severity</code> and remediation hints. Health-check must be inexpensive and safe to call from monitoring every 30s. <br><strong>Alerting:</strong> integrate with pager system; threshold-based alerts for <code>tailLag</code> > configured SLO or <code>lastSignedRotationVerified</code> older than threshold. <br><strong>Tests:</strong> simulate slow disk, signer unavailability, backlog conditions. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>SafeErrorToOperator(correlationId, errorCode, userHint)</code> — concise UI-safe messages</strong><br><strong>Purpose:</strong> map internal audit error codes to operator-friendly messages that include <code>correlationId</code> for triage; append <code>audit.error.shown</code> and store full diagnostics encrypted referenced by <code>evidenceRef</code>. <br><strong>Contract:</strong> returned <code>message</code> must be short, non-PII, and include <code>correlationId</code> and <code>supportRef</code>. Example: <code>ERR_AUDIT_PERSIST</code> -> "Audit persist temporary error (ref r-20260116-abc). Retry or contact infra." <br><strong>Tests:</strong> message PII absence, mapping coverage, audit emission. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>Shutdown()</code> — graceful shutdown & snapshot</strong><br><strong>Purpose:</strong> flush in-memory buffers synchronously, rotate small tail fragment if needed, persist last snapshot (<code>lastOffset</code>, <code>lastRotationHash</code>), unregister hooks, and append <code>audit.shutdown</code>. <br><strong>Contract:</strong> must be idempotent; on failure return structured diagnostic; must block until <code>FlushBuffers(force=true)</code> completes or timeout with diagnostic. On restart <code>OnLoad</code> should detect unclean shutdown via missing <code>audit.shutdown</code> and run <code>VerifyAuditChain</code> for last window. <br><strong>Tests:</strong> graceful shutdown under load, unclean shutdown detection, restart recovery. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Operational SLOs & performance budgets</strong><br>- Append median latency (UI-ack path) target: <50ms in normal operations. <br>- Append durable persist latency (AtomicAppend) target: <2s 95th percentile. <br>- Rotation sign latency (KMS) target: <500ms per sign operation (subject to KMS SLA). <br>- <code>VerifyAuditChain</code> CI runtime: handle typical golden window within CI time budget (<5m). <br><strong>Remediation:</strong> degrade to local buffer + operator-visible read-only mode if persistent storage latency spikes, and raise <code>audit.health.alert</code>. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Telemetry & observability metrics (minimum set)</strong><br><code>audit.append.latency_ms</code>, <code>audit.append.success_count</code>, <code>audit.append.error_count</code>, <code>audit.rotation.duration_ms</code>, <code>audit.rotation.count</code>, <code>audit.rotation.failed</code>, <code>audit.verify.success</code>, <code>audit.verify.fail</code>, <code>audit.health.status</code>, <code>evidence.store.ops</code>, <code>evidence.store.latency_ms</code>. Each metric tagged with <code>{module,env,release}</code>. Sampling policy: record all <code>audit.append.error</code> events; metrics aggregated for SLO alerts. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Logging & trace instrumentation</strong><br>- Emit structured logs for <code>appendAttempt</code>, <code>appendSuccess</code>, <code>appendFailure</code> including <code>correlationId</code>, <code>rowId</code>, <code>offset</code> and short <code>errorCode</code>. <br>- Tracing: integrate with distributed tracing (span: <code>dq.audit.append</code> / <code>dq.audit.rotate</code>) and include <code>correlationId</code> for trace joining. <br>- Diagnostic logs: store encrypted forensics logs pointed-to by <code>evidenceRef</code> only for approved forensic exports. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>CI / Testing matrix (detailed)</strong><br><strong>Unit tests (fast):</strong> <code>ComputePayloadHash</code> deterministic across locales (NFC/NFD), <code>ValidateAuditRowSchema</code> coverage, <code>BuildAuditRow</code> canonicalization, evidence redaction regexes, <code>SafeErrorToOperator</code> mapping. <br><strong>Integration tests (medium):</strong> <code>AppendAuditRow</code> → <code>AtomicAppend</code> → read back <code>QueryAuditTail</code> parity, <code>RotateAndSign</code> end-to-end with KMS stub, <code>EncryptAndStoreEvidence</code> round-trip. <br><strong>Stress & property tests (heavy):</strong> concurrent appends (10k/s) with idempotency checks, partial disk failures, signer offline scenarios. <br><strong>Golden tests (CI gate):</strong> run sample runs producing canonical <code>configHash</code>, <code>rotationHash</code> and ensure <code>VerifyAuditChain</code> passes for golden fixture; any delta must be accompanied by schema PR + review + updated golden artifacts. <br><strong>Security & compliance tests:</strong> static analyzer forbids raw secret writes; KMS access pattern test; evidence store RBAC tests; legal-hold enforcement tests. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Failure modes & remediation</strong><br>1. <strong>Append failures (disk full, transient store error):</strong> system emits <code>audit.append.error</code>, retries with backoff; if persistent > threshold escalate to SRE and switch to local encrypted buffer with operator visible "audit degraded" state. <br>2. <strong>Partial write / corruption:</strong> <code>VerifyAuditChain</code> detects mismatch; rotate to forensic mode, export last <code>N</code> rotations for offline analysis, append <code>audit.forensic.export</code>. <br>3. <strong>Signer (KMS) unavailable:</strong> emit <code>audit.rotation.failed</code>; mark <code>rotation.pending</code>; continue appends but require signers for archive export; operator action to restore KMS. <br>4. <strong>Evidence store outage:</strong> <code>EncryptAndStoreEvidence</code> returns <code>evidence.deferred</code>; main audit still records <code>paramsHash</code> and <code>evidenceRef=deferred:&lt;token&gt;</code>; background retry job attempts re-upload; if retry fails beyond threshold, escalate and record <code>audit.evidence.failed</code>. <br>5. <strong>Tamper / unauthorized modification detect:</strong> <code>VerifyAuditChain</code> mismatch triggers containment: mark affected rotations as suspect, create <code>forensic_manifest</code>, escalate to secops, optionally freeze exports for implicated intervals. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Operator runbook (triage flows, prioritized)</strong><br><strong>A. Append failures:</strong> check <code>audit.append.error</code> metric → inspect disk utilization & object-store health → check <code>audit.bufferQueue</code> → if local buffer large, run <code>audit.flush --force</code> and escalate. <br><strong>B. Rotation/signature failures:</strong> check KMS access; verify key rotation; run <code>rotation.verify</code> and re-sign if key rotated within policy; escalate if KMS unreachable >15m. <br><strong>C. Suspected tampering:</strong> run <code>VerifyAuditChain</code> for window, export forensic bundle <code>ExportForForensics</code>, begin IR runbook in compliance with retention. <br><strong>D. Evidence retrieval:</strong> ensure operator has MFA & approvals then use <code>ExportForForensics</code> to obtain package. <br><strong>E. Golden test failure in CI:</strong> inspect <code>VerifyAuditChain</code> report, compare canonicalization changes, update golden fixtures only with documented schema changes and release manifest signed. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Change-control & governance</strong><br>- Schema changes require PR, <code>audit.schema.validation</code> in CI, compliance review, and signed release manifest. <br>- Changes to canonicalization rules (affecting <code>ComputePayloadHash</code>) require golden fixtures and must be forward-compatible or accompanied by migration manifest. <br>- Key rotation requires <code>secops</code> signoff and update of <code>rotation.verify</code> trust anchors. <br>- Approval requirements for evidence export and destructive retention changes must be recorded as <code>audit.config.changed</code> with operator approvals attached. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Retention & legal compliance (detailed)</strong><br>- Minimum hot/warm/cold windows configurable per environment and regulatory needs. <br>- Legal holds stored as explicit metadata with <code>appliedBy</code>, <code>appliedAt</code>, <code>ticketId</code> and override TTL. <br>- Deletion operations create signed <code>deletionManifest</code> listing offsets and checksums and append <code>audit.retention.enforce</code>. <br>- Compliance-ready exports include <code>forensic_manifest</code> with checksums, signer fingerprints, and chain-of-custody metadata. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Security & secrets handling</strong><br>- Never store private keys or plaintext secrets on disk. <br>- KMS/HSM used for signing rotations and generating ephemeral evidence keys. <br>- Access to evidence store requires granular RBAC; retrieval of evidence requires MFA and possible two-person approval. <br>- All config changes and key operations are audited in <code>DQ_Audit</code>. <br>- Static analysis in CI forbids code paths that write secrets to audit rows. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Appendices: canonical artifacts & named conventions</strong><br>- <code>audit-tail/</code> layout: <code>tail.index</code> (atomic), <code>rows/&lt;offset&gt;.json</code> (canonical JSON), <code>tmp/</code> for in-progress writes. <br>- <code>rotations/</code> layout: <code>rotation-&lt;ts&gt;-&lt;first&gt;-&lt;last&gt;.bundle</code> (signed). <br>- <code>evidence/</code> layout: <code>evidence/&lt;evidenceId&gt;.enc</code> with metadata index <code>evidence/index.json</code> containing TTL & accessPolicy. <br>- Field naming: <code>payloadHash</code> always <code>sha256:&lt;hex&gt;</code>. <code>configHash</code> uses canonical JSON of config SHA256. <code>rowId</code> format <code>r-YYYYMMDD-&lt;shortHex&gt;</code>. <br>- Error codes: <code>RPT_AUD_SCHEMA_ERR</code>, <code>RPT_AUD_PERSIST_FAIL</code>, <code>RPT_AUD_ID_COLLISION</code>, <code>RPT_ROT_SIGN_FAIL</code>, <code>RPT_EVID_STORE_FAIL</code>. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Example sequences (narratives with emitted audit rows)</strong><br><strong>Normal click:</strong> UI <code>HandleControlAction</code> creates <code>correlationId=c-20260116-abc</code> → <code>BuildAuditRow(module= DQ_Ribbon, procedure=Click, correlationId, params)</code> → <code>AppendAuditRow</code> → returns <code>rowId=r-20260116-0001</code> → telemetry <code>audit.append.latency_ms</code> emitted. Later <code>RotateAndSign</code> archives window and emits <code>audit.rotation.completed</code>. <br><strong>Profile + Apply (heavy):</strong> <code>UserAction</code> row appended, job persisted <code>job.persisted:job-901</code> appended by JobSchedulerIntegration (job descriptor includes <code>paramsHash</code>), worker completes <code>dq_proposal</code> step-level audits, apply emits <code>dq_apply</code> with <code>beforeChecksum</code>/<code>afterChecksum</code>. All steps chain via <code>correlationId</code>. <br><strong>Forensic export:</strong> operator requests export with ticket, <code>ExportForForensics</code> runs, produces <code>forensic_manifest.json</code>, writes to <code>forensic://archive/&lt;id&gt;</code>, appends <code>audit.forensic.export</code>. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Operational checklist for deployment (pre-flight)</strong><br>1. KMS/HSM keys provisioned and test-signed rotation manifest present. <br>2. Evidence store encryption keys & access policies tested. <br>3. <code>rotationPolicy</code> configured and CI golden fixtures uploaded. <br>4. <code>AuditHealthCheck</code> integrated into monitoring and alerting. <br>5. <code>OWNERS.md</code> updated with <code>team-dq-audit</code> and secondary owners. <br>6. CI includes <code>VerifyAuditChain</code> and <code>AuditSchemaValidate</code> gating steps. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Developer guidance (do/don't)</strong><br><strong>Do:</strong> keep <code>BuildAuditRow</code> deterministic; use <code>EncryptAndStoreEvidence</code> for sensitive data; attach <code>correlationId</code> to all rows; include <code>configHash</code> on every row. <br><strong>Don't:</strong> write raw PII or secrets to the main tail; perform blocking large writes on UI thread; bypass KMS for signing in production. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Gated changes & review checklist</strong><br>- Any change to canonical JSON rules, hashing algorithm, or schema must include: PR with tests, golden updates, compliance signoff, and signed release manifest. <br>- Any change to rotation/signing flow requires secops review and key rotation test. <br>- Any change that relaxes PII redaction must be accompanied by privacy & legal approval. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Forensic package contents (minimum)</strong><br>- <code>forensic_manifest.json</code> (checksums, rotation manifests), <code>audit_tail.csv</code> for window, rotated bundles, evidence blobs (encrypted), <code>modConfig</code> snapshot (with <code>configHash</code>), release manifest and signer fingerprints, chain-of-custody logs. All packaged and encrypted before release. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Common engineering checklist for merges touching audit</strong><br>1. Unit tests for canonicalization & hashing. <br>2. Integration test for append->read parity. <br>3. Golden artifact update + <code>VerifyAuditChain</code> passing. <br>4. Security review for evidence leakage. <br>5. Performance check ensures append latency within SLOs. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Acceptance criteria (CI / release)</strong><br>- Unit + integration tests pass. <br>- Golden <code>VerifyAuditChain</code> passes for representative fixture. <br>- No forbidden API usage or raw-secret writes detected by static analysis. <br>- KMS signer passes smoke test. <br>- Retention rules validated in dry-run. <br>- OWNERS & release manifest updated. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Final operator quick commands (cheat-sheet)</strong><br><code>audit.query --cid &lt;cid&gt;</code> → find chain. <br><code>audit.flush --force</code> → flush buffers. <br><code>audit.rotate --now</code> → trigger rotation & sign. <br><code>audit.verify --start &lt;n&gt; --end &lt;m&gt;</code> → VerifyAuditChain. <br><code>audit.export-forensic --query &#x27;&lt;filter&gt;&#x27; --ticket &lt;id&gt;</code> → ExportForForensics. <br><code>audit.health</code> → AuditHealthCheck. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Concluding note (practical constraints):</strong> DQ_Audit is intentionally conservative: minimal surface on main tail, deterministic canonicalization for CI/golden reproducibility, strong KMS/HSM-backed signing for rotations, and encrypted evidence for privacy-compliant investigations. Changes to core hashing/canonicalization or retention policy must be treated as high-risk and require cross-team governance. </td></tr></tbody></table></div><div class="row-count">Rows: 42</div></div><script src="assets/xlsx.full.min.js?v=1758605028" defer></script>
<script src="assets/script.js?v=1759748863" defer></script>
<script src="assets/worker.js?v=1758331710" defer></script>
<script>
(function(){
  const template = "{table}_{date}";
  const userVal = "";
  const hrefPrefix = "assets";
  function formatName(tableName) {
    const date = (new Date()).toISOString().slice(0,10);
    return template.replace('{table}', tableName).replace('{date}', date).replace('{user}', userVal);
  }
  const btn = document.getElementById('exportBtn');
  if(btn) {
    btn.addEventListener('click', async function() {
      try {
        const html = document.documentElement.outerHTML;
        if(html.length > 2000000) { alert('Export refused: html too large'); return; }
        if(window.Worker) {
          const workerUrl = (function(){ try{ return hrefPrefix + '/worker.js'; }catch(e){ return null; } })();
          if(workerUrl) {
            try {
              const worker = new Worker(workerUrl);
              worker.postMessage({html: html, format: 'pdf'});
              worker.onmessage = function(e) { console.log('worker:', e.data); alert('Worker replied: '+(e.data.msg||e.data.status)); };
            } catch(err) { console.warn('Worker create failed', err); alert('Worker not available'); }
          } else { alert('Worker not available'); }
        } else { alert('Export worker not supported in this environment.'); }
      } catch(err) { console.warn('Export failed', err); alert('Export worker not available. See console for details.'); }
    });
  }
})();
window.addEventListener('load', function(){ try{ document.querySelectorAll('.table-wrapper').forEach(function(e){ e.style.opacity='1'; }); }catch(e){} });
</script>
</div>
</body>
</html>