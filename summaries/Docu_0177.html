<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='utf-8'><meta name='viewport' content='width=device-width,initial-scale=1'>
<title>Tables Viewer v2.1</title>
<style>:root{--main-header-height:56px;} .table-wrapper{opacity:0;min-height:24px;transition:opacity .12s linear}</style>
<link rel="stylesheet" href="assets/style.css?v=1759925496">
<link rel="stylesheet" href="assets/overrides.css?v=1768656556">
</head><body>
<div id="tables-viewer" role="region" aria-label="Tables viewer">
<div id="stickyMainHeader">
<div id="tv-header">
<div><h1>Tables Viewer v2.1</h1></div>
<div style="display:flex;gap:8px;align-items:center;">
<input id="searchBox" class="tv-search" type="search" placeholder="Search" aria-label="Search tables" />
<button id="modeBtn" type="button" onclick="toggleMode()" aria-label="Toggle theme">Theme</button>
<button id="toggleAllBtn" type="button" aria-label="Toggle all" onclick="toggleAllTables()">Collapse All Tables</button>
<button id="copyAllPlainBtn" class="copy-btn" type="button" onclick="copyAllTablesPlain()" aria-label="Copy all tables as plain text">Copy All Tables (Plain Text)</button>
<button id="copyAllTablesBtn" class="copy-all-btn" type="button" onclick="copyAllTablesMarkdown()" aria-label="Copy All Tables (Markdown)">Copy All Tables (Markdown)</button>
<button id="copyAllMdBtn" style="display:none" aria-hidden="true"></button>
<button id="resetAllBtn" type="button" onclick="resetAllTables()" aria-label="Reset All Tables">Reset All Tables</button>
</div></div>
<script>
(function(){
  function ensureDelegation(){
    try {
      var vis = document.getElementById('copyAllTablesBtn');
      var alias = document.getElementById('copyAllMdBtn');
      if(!vis || !alias) return;
      alias.addEventListener = function(type, listener, options){
        vis.addEventListener(type, listener, options);
      };
      alias.removeEventListener = function(type, listener, options){
        vis.removeEventListener(type, listener, options);
      };
      Object.defineProperty(alias, 'onclick', {
        set: function(fn){ vis.onclick = fn; },
        get: function(){ return vis.onclick; },
        configurable: true
      });
      alias.focus = function(){ vis.focus(); };
      alias.blur = function(){ vis.blur(); };
    } catch(e) {
      try{ console && console.warn && console.warn('alias delegation failed', e); }catch(_){} 
    }
  }
  if(document.readyState === 'loading'){
    document.addEventListener('DOMContentLoaded', ensureDelegation);
  } else {
    ensureDelegation();
  }
})();
</script>
<noscript><div style='color:#b91c1c'>JavaScript is disabled. Tables will be shown statically. For large tables enable JS for virtualization.</div></noscript>
<div id="tocBar" role="navigation" aria-label="Table of contents"><ul><li class="toc-item"><a class="toc-link" href="#Table1">Table 1</a></li>
<li class="toc-item"><a class="toc-link" href="#Table2">Table 2</a></li>
<li class="toc-item"><a class="toc-link" href="#Table3">Table 3</a></li>
<li class="toc-item"><a class="toc-link" href="#Table4">Table 4</a></li>
<li class="toc-item"><a class="toc-link" href="#Table5">Table 5</a></li>
<li class="toc-item"><a class="toc-link" href="#Table6">Table 6</a></li>
<li class="toc-item"><a class="toc-link" href="#Table7">Table 7</a></li>
<li class="toc-item"><a class="toc-link" href="#Table8">Table 8</a></li></ul></div></div>
<div class="table-caption" id="Table1" data-table="Docu_0177_01" style="margin-top:2mm;margin-left:3mm;"><strong>Table 1</strong></div>
<div class="table-wrapper" data-table-id="table-1"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by DQ_MatchMerge — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">DQ_MatchMerge — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong>Module purpose & top-level contract (executive summary)</strong><br><strong>Scope:</strong> Deterministic, auditable match-and-merge engine used by <code>DQGuard.xlam</code> to (a) generate reproducible <em>merge proposals</em> from candidate record sets and (b) apply merges safely either inline or as scheduled jobs. <br><strong>Primary responsibilities:</strong> candidate pruning (blocking), field-level similarity computation, deterministic scoring and tie-breaking, explainable merge proposal construction (with reversible <code>reversePlan</code>), safety & policy gating (dataset/regulatory), atomic persistence of proposals and jobs, safe atomic apply and undo, full audit & encrypted evidence linking, CI golden parity and hot-swap support. <br><strong>Non-goals & constraints:</strong> Not a generic ETL engine; SHOULD NOT perform heavy IO or network access on the UI thread; MUST not leak raw PII into public audit rows; MUST not auto-apply regulated merges without approvals. <br><strong>Determinism requirement:</strong> Given identical input records, <code>configHash</code>, and <code>engineSeed</code>, the engine must produce identical proposals (<code>proposal.payloadHash</code>) and identical audit chains. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>InitMatchEngine(config)</code> — Purpose, contract, parameters, invariants, observability, recovery, examples, and tests</strong><br><strong>Purpose & contract:</strong> initialize compiled runtime artifacts from <code>config</code> (match rules, comparators, blocking, scoring). Responsibilities: schema-validate config; precompile comparator functions and normalizers; seed deterministic RNG (<code>seed = config.seed || sha256(configHash||bootstrapTsFixed)</code>); prepare in-memory index metadata; compute <code>engineHash = sha256(configHash || compiledFingerprints)</code>; register engine with <code>DQ_Audit</code>. MUST be idempotent and fast on UI thread (target <50ms); heavy index builds deferred to background worker. <br><strong>Parameters & return:</strong> <code>config</code> object or configRef → returns <code>{engineHandle, engineHash, ready:true}</code> or <code>{errorCode, userHint}</code> (stable codes). Must never throw host-visible exceptions. <br><strong>Primary invariants:</strong> <br>1. <code>engineHash</code> changes only when a meaningful compiled artifact changes. <br>2. All comparator names in config map to compiled comparators (or compile-time errors raised). <br>3. RNG seed reproducible for golden runs. <br>4. No implicit network I/O. <br><strong>Observability & audit:</strong> emit <code>dq.match.engine.started</code> with <code>engineHash</code>, <code>configHash</code>, <code>correlationId</code>, <code>owner</code>, <code>startupLatencyMs</code>; on error emit <code>dq.match.engine.error</code> with stable <code>DQ_ERR_INIT_*</code>. <br><strong>Recovery:</strong> if engine fails to init, fall back to last-good config snapshot and emit <code>dq.match.engine.fallback</code>. <br><strong>Examples:</strong> normal init; hot-rebind returns same <code>engineHash</code> when config unchanged. <br><strong>Tests & CI:</strong> compile coverage tests; deterministic RNG tests (<code>r-test-001</code>), startup latency smoke test, static analyzer forbids sync IO. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>LoadMatchConfig(source)</code> — canonicalization, schema validation, signature verification, owner resolution, fallback policy, and tests</strong><br><strong>Purpose & contract:</strong> canonicalize and validate <code>match-rules.json</code> from embedded manifest or atomic local cache. Steps: canonical JSON (deep-sort keys), JSON Schema v7 validate, dedupe <code>ruleId</code> & <code>fieldId</code>, attach <code>OWNERS</code> entries, compute canonical <code>configHash=sha256(canonicalJson)</code>, verify digital signature locally if present, record <code>signatureFingerprint</code>. SHOULD NOT fetch remote resources synchronously. <br><strong>Failure modes & fallback policy:</strong> non-critical warnings → continue with reduced config and emit <code>dq.match.config.warning</code>; critical schema/signature errors → emit <code>dq.match.config.invalid</code>, disable auto-apply and present diagnostics ribbon. <br><strong>Observability & audit:</strong> emit detailed <code>dq.match.config.loaded</code> with <code>configHash</code>,<code>warningCount</code>,<code>errorList</code>. <br><strong>CI & tests:</strong> negative schema vectors, duplicate id tests, signature verification unit tests, golden canonicalization parity. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>IndexSnapshotManager(snapshotPath)</code> — snapshot load/save, concurrency, checksum, rebuild</strong><br><strong>Purpose & contract:</strong> manage persisted blocking/index snapshots used for fast candidate generation. API: <code>LoadSnapshot()</code>, <code>SaveSnapshot(atomically)</code>, <code>ValidateSnapshot(checksum)</code>, <code>RebuildIfStale()</code>. Writes must be atomic (write-temp → fsync → rename). Snapshots include <code>engineHash</code> & <code>configHash</code> and are invalidated on mismatch. <br><strong>Observability & audit:</strong> emit <code>dq.match.snapshot.loaded</code>, <code>dq.match.snapshot.saved</code>, <code>dq.match.snapshot.rebuild</code>. <br><strong>Recovery:</strong> corrupted snapshot triggers rebuild task and <code>dq.match.snapshot.corrupt</code> audit. <br><strong>Tests:</strong> atomic write, partial-write corruption detection. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>NormalizeRecord(record, normalizers)</code> — canonicalization, reversibility, PII handling, examples, and tests</strong><br><strong>Purpose & contract:</strong> deterministically normalize record fields using configured normalizers: Unicode NFKC, trim/collapse whitespace, case-folding, punctuation strip (per-field), name canonicalization (strip honorifics), accent folding (preserve accent map for reverse plan), phone canonicalization (E.164-ish), email normalization (local-part normalization per provider rules), address canonicalization (country-specific if available), numeric/date normalization. MUST return <code>{normRecord, reverseMap}</code> where <code>reverseMap</code> maps normalized value→original fragments to enable precise undo. <br><strong>PII policy:</strong> logs and telemetry must never include raw PII—use <code>paramsHash</code> and store sanitized full evidence in encrypted evidence store with <code>evidenceRef</code>. Reverse maps stored encrypted only. <br><strong>Determinism:</strong> locale-insensitive unless <code>config.locale</code> present (must be recorded in <code>engineHash</code>). <br><strong>Examples:</strong> <code>&quot;Ms. María-José O&#x27;Neill&quot;</code> → <code>maria jose oneill</code> with <code>reverseMap</code> preserving diacritics + original punctuation. <br><strong>Tests:</strong> locale parity, reversibility unit tests, redaction verification. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ComputeBlockingKey(normRecord, blockingSpec)</code> — cheap deterministic keys</strong><br><strong>Purpose & contract:</strong> compute inexpensive blocking keys (may be multi-key) to partition candidate space. <code>blockingSpec</code> supports expression language (substr, soundex, ngram hash, domain hash) and multi-key fanout. Must be O(1) per input, null-safe, and deterministic. <br><strong>Performance & invariants:</strong> keep keys cheap; avoid fuzzy transforms here; collisions expected and handled in candidate stage. <br><strong>Observability:</strong> emit <code>dq.match.blockkey.generated</code> with <code>recordId</code>, <code>blockKeys[]</code>, <code>bucketEstimate</code>. <br><strong>Tests:</strong> collision rate estimation tests, edge cases for null/empty fields. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>GenerateCandidates(blockKey, indexSnapshot, paginationCursor, radius)</code> — deterministic retrieval & paging</strong><br><strong>Purpose & contract:</strong> return deterministic ordered candidate IDs for a <code>blockKey</code>. Deterministic ordering keys: <code>ownerPriority</code>, <code>completenessScore</code> (non-null field count), <code>lastNormalizedValue</code> (lexicographic), <code>recordId</code>. Support <code>radius</code> to include adjacent blocking buckets. Return <code>{candidates[], candidatesHash, nextCursor}</code>. Must be streamable for large buckets and stable across identical index snapshots. <br><strong>Observability:</strong> emit <code>dq.match.candidates</code> with <code>count</code>, <code>candidatesHash</code>, and <code>cursor</code>. <br><strong>Tests:</strong> pagination stability, snapshot parity, performance under large buckets. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ComparePair(lhsNorm, rhsNorm, comparatorSpec)</code> — field comparators & evidence artifacts</strong><br><strong>Purpose & contract:</strong> compute per-field similarity features dictated by <code>comparatorSpec</code> and return <code>{fieldScore, features, evidenceHash}</code>. Supported comparators: exact, tokenJaccard, normalizedEditDistance (weighted), ngramOverlap, Soundex/similarity, numericDistance (relative), dateDelta buckets, custom regex matching. Each comparator returns stable features used by scoring. <br><strong>Explainability/evidence:</strong> include <code>featureBuckets</code> and <code>explain</code> strings for UI; sensitive field evidence stored encrypted—only <code>evidenceHash</code> emitted in audit. <br><strong>Determinism & performance:</strong> pure function, deterministic, optimized for short-circuiting when possible. <br><strong>Tests:</strong> comparator correctness matrices, unicode edge cases, adversarial string fuzz. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ComputeMatchScore(pairEvidence, scoringModel)</code> — model-based scoring & explainability</strong><br><strong>Purpose & contract:</strong> aggregate field-level features into scalar <code>score</code> ∈ [0,1] using deterministic <code>scoringModel</code> (linear weights, logistic calibration, or compiled decision rules). Return <code>{score, modelVersion, featureContributions[], topReasons[]}</code>. Must expose per-feature contribution for audit/UI explainability and be deterministic. <br><strong>Governance:</strong> model changes require PR, unit tests, golden runs, and <code>dq.match.model.deployed</code> audit. <br><strong>Tests:</strong> calibration curves, sensitivity tests, regression seeds. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>DecideMatch(score, thresholds, datasetPolicy)</code> — deterministic decision mapping</strong><br><strong>Purpose & contract:</strong> map score to classification <code>{MATCH, PROBABLE, REVIEW, NO_MATCH}</code> using config thresholds and dataset-specific policy adjustments (e.g., raise MATCH threshold for regulated datasets). Return <code>{decision, rationale, appliedThresholds}</code>. Decisions must be auditable and reproducible. <br><strong>Edge handling:</strong> at boundary equals must use deterministic tie-break (see <code>ResolveTie</code>). <br><strong>Tests:</strong> boundary tests, conservative-mode behavior. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ResolveTie(candidatePairs[], tieBreakerSpec, deterministicSeed)</code> — canonical tie-break & multi-merge rules</strong><br><strong>Purpose & contract:</strong> when candidates have identical scores or ranking, deterministically pick winners using <code>tieBreakerSpec</code>: owner priority, completeness, recency, smaller <code>recordId</code>, or deterministic RNG seeded by <code>sha256(engineHash|correlationId)</code>. Support N-way merges if policy allows. Return <code>{winners[], secondaryChoices[], tieRationale}</code>. <br><strong>Governance & safety:</strong> tie-breaker causing cross-tenant or PII aggregation triggers <code>REVIEW</code> and requires approval. <br><strong>Tests:</strong> reproducibility under stress, N-way selection parity. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>BuildMergeProposal(primaryId, secondaryIds[], resolutionSpec, engineMeta)</code> — canonical, immutable proposal</strong><br><strong>Purpose & contract:</strong> assemble canonical <code>MergeProposal</code> JSON that is immutable once persisted. Fields: <code>proposalId</code> (canonical derivation), <code>primaryRecordId</code>, <code>secondaryRecordIds</code>, <code>fieldResolutionPlan</code> (for each field: chosenSource, mergeExpression, confidence, rationale), <code>reversePlan</code> (undo mapping), <code>estimatedImpact</code> (row counts, PII move flags), <code>engineHash</code>, <code>configHash</code>, <code>payloadHash=sha256(canonicalProposal)</code>, <code>requiredApprovals[]</code>, <code>owner</code>, <code>createdAt</code>. Same inputs produce identical <code>payloadHash</code>. <br><strong>Security:</strong> store full sanitized proposal in <code>evidenceStore</code> encrypted; public artifact stores only <code>payloadHash</code> and <code>evidenceRef</code> as needed. <br><strong>UI contract:</strong> include <code>previewRef</code> and per-field <code>explainability</code> entries. <br><strong>Tests:</strong> canonicalization parity, reversePlan undo correctness, idempotency. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>EvaluateProposalSafety(proposal, datasetPolicy)</code> — policy gating & approval resolution</strong><br><strong>Purpose & contract:</strong> validate proposal against dataset policies: regulated flags, PII movement, cross-tenant detection, retention/immutability constraints, and two-person requirements. Return <code>{safe:Boolean, requiredApprovals[], riskScore, holdReason}</code>. On unsafe outcome emit <code>dq.merge.proposal.hold</code> and block auto-apply. <br><strong>Operator UX:</strong> return clear <code>requiredApprovals</code> and <code>rationale</code>. <br><strong>Tests:</strong> policy matrix coverage and approval routing. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>PersistProposal(proposal, persistenceBackend)</code> — atomic persistence, idempotency, and checksums</strong><br><strong>Purpose & contract:</strong> persist <code>proposal</code> and <code>evidenceRef</code> atomically using <code>DQ_Export</code> path (temp → fsync → rename). Ensure idempotency (same <code>proposalId</code> returns existing artifact). Compute <code>artifactChecksum=sha256(canonicalProposal)</code> and append <code>dq.merge.proposal.persisted:&lt;proposalId&gt;</code> audit with <code>artifactChecksum</code>. Retry transient failures with bounded backoff; on persistent failure emit <code>dq.merge.persist.error</code>. <br><strong>Recoverability:</strong> persist metadata includes <code>persistedAt</code>, <code>owner</code>, <code>artifactLocation</code>. <br><strong>Tests:</strong> idempotency, crash-resume checks, checksum verification. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ScheduleOrApply(proposal, decisionContext)</code> — inline vs scheduled policy evaluation</strong><br><strong>Purpose & contract:</strong> decide inline apply vs scheduled job. Inputs: <code>estimatedImpact.rows</code>, <code>requiresApproval</code>, <code>datasetPolicy.regulated</code>, <code>safeMode</code>, <code>operatorOverride</code>, <code>IsLightweightAction</code> analog. Action: if scheduled → persist <code>jobDescriptor</code> and call <code>JobSchedulerIntegration</code>, emit <code>job.persisted:&lt;jobId&gt;</code> and return <code>{status:scheduled, jobId, correlationId}</code>; if inline → call <code>SafeInvokeMergeHandler</code> with <code>cancelToken</code> and return immediate <code>{status:applied|failed, correlationId, applyId}</code>. Must return quickly on UI thread (<50ms). <br><strong>UI contract:</strong> short message including <code>correlationId</code> and next steps. <br><strong>Tests:</strong> decision parity near thresholds, latency tests. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>SafeInvokeMergeHandler(proposal, cancelToken, correlationId)</code> — protected inline execution frame</strong><br><strong>Purpose & contract:</strong> safely execute merges inline within a guarded frame: validate permissions, create invocation audits <code>dq.merge.handler.start</code>, set cooperative timeout & <code>cancelToken</code>, run apply stages with sub-step audits (<code>step.start</code>, <code>step.complete</code>), redact PII in any user-facing messages, map exceptions to stable codes <code>DQ_ERR_*</code>, and return structured result. Must support cancellation checks and partial progress reporting. <br><strong>Invariants:</strong> inline handlers must not perform heavy blocking IO; if heavy, fail-fast and recommend scheduled job. <br><strong>Telemetry:</strong> emit <code>dq.merge.duration_ms</code>, <code>dq.merge.success</code>, <code>dq.merge.error</code> with tags. <br><strong>Tests:</strong> cancellation tests, exception mapping tests, partial-progress audit presence. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ApplyMergeAtomic(proposal, storageEngine, reversePlan)</code> — atomic mutation & undo</strong><br><strong>Purpose & contract:</strong> perform dataset mutations specified in <code>fieldResolutionPlan</code> atomically: create encrypted snapshot or row-level journal, apply per-field changes, update indexes and foreign keys, recompute checksums, persist transaction atomically. On failure use <code>reversePlan</code> + snapshot to rollback. Return <code>{applyId, beforeChecksum, afterChecksum, artifactRef}</code>. Large applies must be scheduled to worker. <br><strong>Invariants:</strong> maintain referential integrity, preserve <code>reversePlan</code> fidelity, and produce deterministic <code>afterChecksum</code>. <br><strong>Tests:</strong> transactional integrity under simulated crashes, reversePlan replay tests. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>EmitMergeAudit(correlationId, proposalId, step, metadata)</code> — canonical audit anchor</strong><br><strong>Purpose & contract:</strong> append authoritative audit rows for lifecycle events: <code>proposal.created</code>, <code>proposal.persisted</code>, <code>merge.started</code>, <code>merge.completed</code>, <code>merge.reverted</code>, <code>merge.timeout</code>, <code>merge.cancelled</code>. Schema required: <code>timestamp,correlationId,module=DQ_MatchMerge,procedure,proposalId,payloadHash,configHash,engineHash,prevHash,metadata,paramsHash,evidenceRef(optional)</code>. Main audit stores only <code>paramsHash</code>; sensitive parameters stored encrypted referred by <code>evidenceRef</code>. Audit appends non-blocking to <code>DQ_Audit</code>. <br><strong>Chain & CI:</strong> audits should include <code>prevHash</code> when chaining is resolvable; <code>VerifyAuditChain</code> runs in CI/monitoring. <br><strong>Tests:</strong> schema validation, prevHash chain tests, evidence retrieval tests. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>RegisterUnitTestHook(hookName, fixedCorrelationId)</code> — deterministic CI hooks</strong><br><strong>Purpose & contract:</strong> register deterministic unit test hooks that accept fixed <code>correlationId</code> and seeded RNG for golden parity. Hooks flagged <code>test=true</code> in audits, must be disabled in production unless explicitly allowed with owner & approval. Return <code>hookHandle</code>. <br><strong>CI usage:</strong> golden-run compares <code>proposal.payloadHash</code> to golden artifacts. <br><strong>Tests:</strong> ensure hooks only active in test contexts; golden parity. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>RefreshMatchRules(newConfigJson)</code> — live rebind, diff, preview, smoke tests, and rollback</strong><br><strong>Purpose & contract:</strong> validate <code>newConfigJson</code> (schema+signature), compute diff vs active <code>configHash</code>, produce <code>hotSwap.preview</code> with impacted rules and risk estimate, run smoke tests using deterministic sample partitions via unit hooks, atomically swap in-memory rules if smoke tests pass, persist via <code>DQ_Export</code> optionally, and emit <code>dq.match.refresh.completed</code> with <code>beforeHash</code>/<code>afterHash</code>. Must not interrupt running jobs; failure triggers revert and <code>dq.match.refresh.error</code>. <br><strong>Smoke tests:</strong> exercise comparators and candidate generation on sample partitions. <br><strong>Tests:</strong> hot-swap dry runs & rollback correctness. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>PreviewMergeUI(primaryId, secondaryIds[], previewOptions)</code> — preview contract & safety</strong><br><strong>Purpose & contract:</strong> produce a compact deterministic preview artifact showing <code>N</code> sample merged rows, per-field chosen source, confidence scores, and <code>previewHash</code>. Must be fast (<500ms median) and match the actual apply result for the same <code>proposalId</code>. If operator lacks full PII permission produce <code>anonymizedPreviewRef</code> with redactions. Return <code>{previewRef, previewHash, sampleRowsCount}</code>. <br><strong>UI features:</strong> per-field <code>explain</code> text and <code>copyDiagnostics</code> button that includes <code>correlationId</code>. <br><strong>Tests:</strong> preview parity and permission gating. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ValidateUserPermissions(userId, proposal, controlMeta)</code> — RBAC & approvals</strong><br><strong>Purpose & contract:</strong> authoritative permission evaluation: SSO mapping, group membership, delegated approvals, two-person rules for regulated datasets, dataset-level protections. Return <code>{allowed, requiredApprovals[], denialReason}</code> and append <code>dq.merge.permission.check</code> audit. Emergency operator overrides require MFA and <code>ticketId</code> and are auditable as <code>dq.merge.emergency.override</code>. <br><strong>Tests:</strong> role matrix simulation and approval flows. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>BuildProposalImpactHash(proposal)</code> — canonical fingerprint for CI & QA</strong><br><strong>Purpose & contract:</strong> deterministic canonicalization of <code>proposal</code> JSON (sort keys, normalize numbers/dates, strip ephemeral fields, redact PII per redaction policy) then compute <code>sha256</code> as <code>proposal.hash</code> used for golden-file checks. Must be stable across locales and runs. <br><strong>Tests:</strong> deterministic hashing across permutations, locale invariance checks. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>HotSwapMergeRules(newRulesJson, operatorId, approvals)</code> — transactional emergency patching</strong><br><strong>Purpose & contract:</strong> apply urgent rule updates transactionally with required approvals: validate schema & signature, compute diff & <code>hotSwap.preview</code>, run smoke tests with unit hooks, apply in-memory atomically if smoke tests pass, persist via <code>DQ_Export</code> optionally, append <code>dq.match.hotswap.applied</code> with <code>beforeHash</code>/<code>afterHash</code> and <code>releaseFingerprint</code>. If smoke tests fail revert and append <code>dq.match.hotswap.reverted</code>. <br><strong>Governance:</strong> regulated changes require explicit approvals and audit. <br><strong>Tests:</strong> dry-run validations and rollback correctness. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>SafeHandlerTimeoutWatchdog(handlerToken, correlationId)</code> — cooperative cancellation & escalation</strong><br><strong>Purpose & contract:</strong> monitor inline handler execution and on overrun: emit <code>dq.merge.timeout</code>, attempt cooperative cancellation via <code>handlerToken</code>, and if unsuccessful emit <code>dq.merge.hung</code> with stack snapshot for SRE. Use host idle callbacks or <code>Application.OnTime</code> in VBA contexts. Repeated timeouts escalate incident. <br><strong>Tests:</strong> forced overrun, cancellation effect, audit presence. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>Shutdown()</code> — graceful unload, audit flush, and snapshot</strong><br><strong>Purpose & contract:</strong> flush audit buffers, persist minimal snapshot (<code>lastConfigHash</code>, <code>lastCorrelationId</code>, <code>engineSnapshot</code>), unregister unit-test hooks, and append <code>dq.match.shutdown</code> audit. Register with bootstrap shutdown order to allow audit flush first. On unclean exit <code>InitMatchEngine</code> must emit <code>dq.match.recovery</code>. <br><strong>Tests:</strong> snapshot & audit flush verification. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>JobSchedulerIntegration(jobDescriptor)</code> — canonical job descriptor & worker handoff</strong><br><strong>Purpose & contract:</strong> create canonical <code>jobDescriptor</code> for scheduled merges and persist atomically for worker consumption. Descriptor fields: <code>jobId, proposalId, correlationId, paramsHash, configHash, persistedAt, owner, retryPolicy</code>. Persist via atomic export and emit <code>job.persisted:&lt;jobId&gt;</code>. Must be idempotent. <br><strong>Tests:</strong> idempotency & worker handoff simulation. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>MergeSimulationDryRun(proposal, samplePartition, limits)</code> — deterministic dry-run for smoke tests</strong><br><strong>Purpose & contract:</strong> simulate applying <code>proposal</code> to a sample partition without mutating source data. Return <code>dryRunReport</code> with estimated change set, checksum delta, and <code>dryRunHash</code>. Use in hot-swap smoke tests and operator reviews. Must not leak unredacted PII in logs. <br><strong>Tests:</strong> dry-run parity vs actual apply on sample partition. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ProposalDiff(oldProposal, newProposal)</code> — canonical diff for reviews</strong><br><strong>Purpose & contract:</strong> compute canonical, field-level diffs and <code>diffHash</code> between proposals; used in hotswap previews and approval flows. Highlight changed fields, resolutions, and approval requirements. <br><strong>Tests:</strong> canonical diff parity across permutations, diff size limits. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ProposalApprovalWorkflow(proposalId, approverId, action, approvalMetadata)</code> — approvals & audit</strong><br><strong>Purpose & contract:</strong> record approval actions (approve/reject/comment), append <code>dq.merge.approval</code> audit with <code>correlationId</code>, <code>approverId</code>, and optional <code>evidenceRef</code>. When required approvals satisfied transition <code>proposal</code> to <code>ready</code>. Support time-limited approvals and delegated approvals. <br><strong>Tests:</strong> approval gating & expiry. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>EvidenceStoreIntegration(evidenceBlob, accessPolicy)</code> — secure encrypted evidence persistence</strong><br><strong>Purpose & contract:</strong> persist encrypted evidence to secure evidence store with metadata, TTL, and accessPolicy. Return <code>evidenceRef</code> opaque token. Evidence encrypted using KMS-backed keys and access gated by RBAC. Retrieval requires approval trace. Append <code>dq.match.evidence.persisted</code> audit with <code>evidenceRef</code> (no raw PII). <br><strong>Tests:</strong> KMS mock, TTL enforcement, retrieval audit trail. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ForensicExport(correlationId, artifactList, operatorId)</code> — canonical forensic package</strong><br><strong>Purpose & contract:</strong> assemble and securely export forensic artifacts: <code>match-rules.json</code>, <code>configHash</code>, <code>engineHash</code>, <code>audit_tail.csv</code> rows for correlation ids, <code>proposal.json</code>, <code>job.descriptor</code>, <code>engineSnapshot</code>, <code>forensic_manifest.json</code> with checksums and chain-of-custody metadata. Persist via <code>REG_Export</code> and append <code>dq.match.forensic.export</code> audit. Access controlled. <br><strong>Tests:</strong> manifest completeness & checksum verification. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>TelemetryEmit(metricName, value, tags)</code> — local buffering & audited upload</strong><br><strong>Purpose & contract:</strong> append metrics to local buffer (no direct network from ribbon path). Tags include <code>proposalId</code>,<code>module</code>,<code>datasetPolicy</code>. Metric uploader (privileged worker) performs network export. Metrics: <code>dq.match.candidates_count</code>, <code>dq.merge.duration_ms</code>, <code>dq.merge.timeout_rate</code>. Append <code>dq.match.metric.buffered</code> audit optionally. <br><strong>Tests:</strong> buffer durability & uploader compatibility. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>MetricsBufferUploader()</code> — audited uploader of metrics</strong><br><strong>Purpose & contract:</strong> run in privileged worker to upload metric batches and emit <code>dq.match.metrics.uploaded</code> audit with batch checksum and counts. Must respect redaction; tags must not contain PII. <br><strong>Tests:</strong> uploader retry semantics and loss simulation. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>AuditChainVerifier(sampleRunId)</code> — CI & monitoring verification</strong><br><strong>Purpose & contract:</strong> verify <code>dq.merge</code> audit chain integrity for sample runs: validate <code>prevHash</code> chaining, <code>payloadHash</code> parity with artifacts, verify audit rotation signatures. Emit <code>dq.match.audit.verify</code> with results. Failures block CI merges. <br><strong>Tests:</strong> golden audit-chain tests and mis-signed rotation detection. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ConfigRollForward(newConfig, operatorId)</code> & <code>ConfigRollback(targetConfigHash)</code> — controlled migrations</strong><br><strong>Purpose & contract:</strong> roll forward config with validation and smoke tests; rollback to <code>targetConfigHash</code> runs smoke tests and emits <code>config.rollback</code> audit. All config changes require PR, CI golden runs, and approvals. <br><strong>Tests:</strong> migration parity, rollback correctness. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>MergeConflictResolver(conflictingJobs[])</code> — worker-level job arbitration</strong><br><strong>Purpose & contract:</strong> reconcile concurrent scheduled jobs targeting overlapping recordsets. Deterministic arbitration: order by <code>job.persistedAt</code>, <code>operatorPriority</code>, <code>jobId</code>, then either serialize or merge job descriptors combining reversePlans. Emit <code>dq.merge.job.conflict.resolved</code> audit. <br><strong>Tests:</strong> conflict simulation and combined reversePlan verification. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>CrossTenantGuard(proposal)</code> — multi-tenant isolation</strong><br><strong>Purpose & contract:</strong> detect cross-tenant merges. Default: block and return <code>requiresApproval</code> with approvers required from both tenants. Emit <code>dq.merge.crossTenant.blocked</code> or <code>dq.merge.crossTenant.allowed</code> audit. <br><strong>Tests:</strong> cross-tenant detection and approval path. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>AnonymizePreview(previewRef, userPermissions)</code> — redaction of previews</strong><br><strong>Purpose & contract:</strong> produce redacted preview replacing PII with placeholders or hashed tokens while preserving structure and confidence. Return <code>anonymizedPreviewRef</code>. Full preview accessible only via <code>evidenceRef</code> with approval. <br><strong>Tests:</strong> redaction correctness & permission gating. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ReconciliationRunner(beforeChecksum, afterChecksum, samplePartition)</code> — verification after apply</strong><br><strong>Purpose & contract:</strong> recompute sample checksums, validate counts and referential integrity post-apply, emit <code>dq.merge.reconciliation</code> audit with <code>reconReportRef</code>. On mismatch schedule forensic export and possible revert. <br><strong>Tests:</strong> recon parity and failover flows. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ExportMergeArtifact(artifactRef, destinationUri, operatorId)</code> — secure export with chain-of-custody</strong><br><strong>Purpose & contract:</strong> securely export artifacts via <code>REG_Export</code> path, compute <code>artifact.checksum.sha256</code>, redact owner emails when operator lacks rights, append <code>dq.merge.export</code> audit with URI and checksum. <br><strong>Tests:</strong> checksum validation and redaction checks. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ErrorCodeCatalog (module)</code> — canonical error taxonomy & operator mapping</strong><br><strong>Purpose & contract:</strong> centralized error codes: <code>DQ_ERR_INIT_001</code> (config missing), <code>DQ_ERR_CONFIG_INVALID</code>, <code>DQ_ERR_PERSIST_FAIL</code>, <code>DQ_ERR_PERMISSION_DENIED</code>, <code>DQ_ERR_TIMEOUT</code>, <code>DQ_ERR_CONFLICT</code>, etc. Each code maps to operator message (no PII) and diagnostic artifact <code>diagRef</code>. Errors must be used consistently and included in audits. <br><strong>Tests:</strong> ensure thrown errors map to catalog and include audit rows. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>Performance budgets &amp; SLOs (module)</code></strong><br><strong>Targets & metrics:</strong><br>- Candidate generation median <20ms for small buckets (<1k).<br>- Pair scoring median <5ms. <br>- Inline apply default timeout 5s (configurable).<br>- Job persist latency <2s. <br><strong>Metrics collected:</strong> <code>dq.match.candidates_count</code>, <code>dq.merge.duration_ms</code>, <code>dq.merge.timeout_rate</code>, <code>job.persist.latency_ms</code>. <br><strong>Runbook:</strong> if budgets breached: throttle inline applies; shift to scheduled-only; run <code>audit_chain.verify</code> on sample runs; ramp down features (e.g., complex comparators) and invoke operator notification. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>Testing matrix (module)</code></strong><br><strong>Required tests:</strong> unit: <code>NormalizeRecord</code>, <code>ComparePair</code>, <code>ComputeMatchScore</code>, <code>ResolveTie</code>, <code>BuildMergeProposal</code>, <code>BuildProposalImpactHash</code>. Integration: candidate→score→proposal→persist→job→worker→apply→reconcile. Golden: <code>proposal.payloadHash</code> parity tests. CI gating: audit-chain verify, static checks forbidding forbidden APIs. Property: correlation id uniqueness under load. <br><strong>CI enforcement:</strong> block merges on golden/audit-chain failures. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>Failure modes &amp; mitigations (module)</code></strong><br><strong>Common incidents & mitigations:</strong><br>1. Invalid config/signature → <code>dq.match.config.invalid</code> → revert to last-good config and notify owner. <br>2. Excessive blocking collisions → <code>dq.match.blocking.warning</code> → operator retune <code>blockingSpec</code>. <br>3. Partial apply failure → <code>dq.merge.reverted</code> using <code>reversePlan</code> + snapshot; open incident. <br>4. Permission denied → <code>dq.merge.permission.denied</code> → require approvals. <br>5. Watchdog timeouts → <code>dq.merge.timeout</code> → schedule job. <br><strong>Forensics:</strong> collect <code>match-rules.json</code>, <code>configHash</code>, <code>engineHash</code>, <code>audit_tail.csv</code> rows for <code>correlationId</code>, <code>proposal.json</code>, <code>job.descriptor</code>, <code>engineSnapshot</code>, <code>forensic_manifest.json</code>. <br><strong>Operator recovery checklist:</strong> locate <code>correlationId</code>, retrieve audit chain, fetch <code>evidenceRef</code>, run dry-run in isolated runner, revert <code>hotSwap</code> if needed, restore dataset from snapshot. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>Operator UX &amp; triage notes (module)</code></strong><br><strong>Best-practice UI:</strong> always show <code>correlationId</code> and provide "copy diagnostics" and "download forensic package" actions. Provide clear <code>preview</code>, <code>simulate</code>, <code>apply (inline)</code>, <code>apply (scheduled)</code> with <code>requiresApproval</code> flags. Show succinct <code>safe/unsafe</code> badge with <code>requiredApprovals</code> when applicable. <br><strong>Triage flow:</strong> 1) get <code>correlationId</code>; 2) fetch <code>UserAction</code> and step audits; 3) request <code>evidenceRef</code> if needed (approval); 4) run <code>dry-run</code> in isolated runner; 5) collect <code>forensic_manifest</code> and escalate. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>Change-control &amp; governance (module)</code></strong><br><strong>Flow for changes:</strong> PR + migration manifest if semantics change; static analysis forbidding forbidden APIs; unit/integration/golden tests; compliance/owner approvals for regulated changes; sign artifacts and publish release manifest; canary rollout with KPI gating; post-rollout <code>VerifyAuditChain</code> and <code>deployment.audit</code>. <br><strong>Hot-swap governance:</strong> smoke tests and approvals required before applying regulated comparator or threshold changes. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>Appendices &amp; references (module)</code></strong><br><strong>Include:</strong> canonical <code>match-rules.json</code> schema, audit row schema, <code>ErrorCodeCatalog.md</code>, <code>reversePlan</code> template, migration manifest template, forensic manifest, operator cheat-sheets, <code>OWNERS.md</code> mapping. Store appendices in immutable artifact store <code>\\artifacts\DQ_MatchMerge\v{major}.{minor}\appendices\</code> with RBAC. Provide runbooks: <code>dq_merge_smoke_test.md</code>, <code>dq_merge_incident_runbook.md</code>, <code>dq_merge_operator_cheatsheet.md</code>. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong>Acceptance criteria (dev/CI)</strong><br><strong>Gates:</strong> unit + integration + golden tests pass; no forbidden API in static analysis; <code>configHash</code> & <code>engineHash</code> computed; <code>dq.merge</code> audit chain present and <code>VerifyAuditChain</code> passes; performance budgets met under CI load. Blocking conditions: golden/audit-chain failures or forbidden-API detection. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong>Forbidden APIs & static enforcement (module)</strong><br><strong>Forbidden on UI path and inline handlers:</strong> direct Workbook/Range modification during <code>OnLoad</code> or critical UI callbacks; raw network calls (WinHTTP) or external web sockets; unbounded synchronous disk writes (>10ms); direct plaintext secret reads; spawning external processes. CI static analyzer rejects PRs referencing blacklisted APIs. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong>Operator-run checklists & CLI examples (compact)</strong><br><strong>Common operator commands:</strong> <code>dq.preview --proposal &lt;id&gt;</code>, <code>dq.apply --proposal &lt;id&gt; --mode scheduled --ticket &lt;id&gt;</code>, <code>dq.hotswap --config path --dry-run</code>, <code>dq.forensic.export --cid &lt;correlationId&gt; --out &lt;uri&gt;</code>, <code>dq.audit.verify --sample &lt;runId&gt;</code>. Each command emits audits and returns <code>correlationId</code>. Include operator TTL & MFA checks for emergency overrides. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong>Forensic package & chain-of-custody fields</strong><br><strong>Minimum artifacts:</strong> <code>match-rules.json</code> & signature, <code>audit_tail.csv</code> rows covering correlation ids, <code>proposal.json</code>, persisted <code>evidence</code> blobs (encrypted), <code>job.descriptors</code>, <code>engineSnapshot</code>, <code>configHash</code>, <code>forensic_manifest.json</code> mapping artifacts to checksums and storage URIs. Store in secure evidence repo with RBAC and C-O-C records. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong>Runbooks for common incidents (short)</strong><br><strong>Config invalid:</strong> retrieve <code>dq.match.config.invalid</code> audit, revert to last-good config, run <code>hotSwap.preview</code> on candidate fixes, apply after approval. <br><strong>Apply failure mid-transaction:</strong> run <code>dq.merge.reverted</code> audit check; fetch <code>reversePlan</code> from <code>evidenceRef</code>; run restore snapshot; open incident and generate <code>forensic_manifest</code>. <br><strong>Unexpected high timeouts:</strong> set inline applies to scheduled-only and scale workers; collect <code>dq.match.*</code> metrics and run smoke tests on active config. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong>Implementation guidance & safe IO patterns</strong><br><strong>IO policy:</strong> use read-then-verify-then-rename atomic patterns for writes; perform heavy index builds or persistence in background worker; no network during synchronous UI callbacks. <br><strong>Security:</strong> secrets via KMS/HSM; manifest signature verification locally; PII redaction enforced prior to any non-encrypted store writes. <br><strong>Coding patterns:</strong> pure functions for normalization and comparators; small deterministic RNG wrappers (<code>seed = sha256(engineHash|cid|stableSalt)</code>). <br><strong>Testing hooks:</strong> include deterministic test harness with fixed RNG seeds and <code>test=true</code> audit flags. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong>Operator & compliance checklist for regulated merges</strong><br>1. Confirm dataset <code>regulated</code> flag. <br>2. Generate <code>previewRef</code> and attach <code>correlationId</code>. <br>3. Acquire required approvals (two-person) via <code>ProposalApprovalWorkflow</code>. <br>4. Persist <code>proposal</code> and schedule apply. <br>5. Post-apply run <code>ReconciliationRunner</code> and include <code>reconReportRef</code> in compliance package. <br>6. Export <code>forensic_manifest</code> and append to regulatory package. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong>End-to-end example narrative (illustrative)</strong><br><strong>Normal flow:</strong> Operator selects candidate rows → <code>NormalizeRecord</code> → <code>ComputeBlockingKey</code> → <code>GenerateCandidates</code> → <code>ComparePair</code> per candidate → <code>ComputeMatchScore</code> → <code>DecideMatch</code> → <code>ResolveTie</code> → <code>BuildMergeProposal</code> → <code>EvaluateProposalSafety</code> (safe) → <code>PersistProposal</code> → <code>PreviewMergeUI</code> → Operator approves → <code>ScheduleOrApply</code> decides inline → <code>SafeInvokeMergeHandler</code> applies changes → <code>ApplyMergeAtomic</code> persists → <code>EmitMergeAudit</code> chain (<code>proposal.created</code>→<code>merge.started</code>→<code>merge.completed</code>) → <code>ReconciliationRunner</code> verifies. <br><strong>Fault flow:</strong> config invalid on hot-swap → <code>dq.match.config.invalid</code> audit → engine fails safe → operator notified; or partial apply failure → <code>dq.merge.reverted</code> + forensic package appended. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong>Glossary (compact)</strong><br><code>engineHash</code> — fingerprint of compiled engine artifacts; <code>configHash</code> — canonical match rules JSON hash; <code>proposal.payloadHash</code> — canonical proposal JSON sha256; <code>evidenceRef</code> — opaque pointer to encrypted evidence; <code>correlationId</code> — UI-level request id; <code>jobId</code> — persisted job descriptor id. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong>Final notes (operational & compliance)</strong><br>1. All user-initiated actions MUST emit a <code>UserAction</code> audit with <code>correlationId</code>. <br>2. All artifacts that include PII MUST be stored encrypted and referenced by <code>evidenceRef</code> in public audits. <br>3. Hot-swap & model updates must run smoke tests via deterministic harness with <code>test=true</code> hooks for CI golden parity. <br>4. CI pipeline requires <code>audit-chain-verify</code> and golden-file checks; failures block release. <br>5. Maintain appendices and runbooks in immutable artifact store with RBAC. </td></tr></tbody></table></div><div class="row-count">Rows: 58</div></div><div class="table-caption" id="Table2" data-table="Docu_0177_02" style="margin-top:2mm;margin-left:3mm;"><strong>Table 2</strong></div>
<div class="table-wrapper" data-table-id="table-2"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by DQ_Remediation — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">DQ_Remediation — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong>Module-level summary — Purpose, owners, public API, guarantees, invariants, and audit obligations (exhaustive)</strong><br><strong>Purpose & contract:</strong> authoritative module for converting data-quality findings (from <code>DQ_Profile</code>, <code>DQ_Rules</code>, <code>DQ_MatchMerge</code>) into safe, auditable remediation workflows. Responsibilities: generate deterministic proposals with before/after previews; compute auditable risk/confidence scores; cluster and rank proposals for operator review; build canonical remediation plans with explicit transactional boundaries and undo descriptors; validate plans against policy, runtime and approvals; provide deterministic dry-run simulations; execute remediation inline (safe, short) or schedule heavy jobs; persist canonical job descriptors idempotently for workers; produce reversible undo artifacts or record explicit non-reversible signoffs; export artifacts atomically with checksums and redaction manifests; build forensic packages on failures; notify owners/stakeholders safely; and ensure every operator-visible action produces canonical, chained audit rows. Must not perform silent destructive mutations; must never display PII in UI/audit main fields; must store full evidence encrypted and reference via <code>evidenceRef</code> in audits.<br><strong>Owners / manifest:</strong> module owner(s) and API published in <code>OWNERS.md</code> and module manifest; versioned <code>module.manifestHash</code> must be recorded on major transitions. <br><strong>Exposed API (canonical):</strong> <code>GenerateProposal</code>, <code>ScoreProposal</code>, <code>RankAndGroupProposals</code>, <code>BuildRemediationPlan</code>, <code>ValidateRemediationPlan</code>, <code>SimulateApply</code> (dry-run), <code>IsLightweightAction</code>, <code>ApplyRemediation</code>, <code>PersistRemediationJob</code>, <code>SafeRemediationExecutor.execute</code>, <code>BuildUndoPlan</code>, <code>RevertRemediation</code>, <code>ValidateApprovals</code>, <code>BuildUiPreview</code>, <code>ExportRemediationArtifacts</code>, <code>NotifyOwnersAndStakeholders</code>, <code>ExportForensicsForFailedApply</code>, <code>SafeErrorToUser</code>, <code>RegisterUnitTestHook</code>. Each API must declare stable identifiers returned (<code>proposalId</code>, <code>planId</code>, <code>applyId</code>, <code>undoId</code>, <code>jobId</code>) and include <code>correlationId</code> in parameters or context. <br><strong>Primary invariants (must/shall):</strong><br>1. Determinism: given identical <code>tableRef</code>, <code>findings</code>, <code>configHash</code>, and <code>seed</code>, <code>GenerateProposal</code> and subsequent plan hashes must be identical. Canonical JSON (sorted keys, normalized numbers/dates) is used to compute SHA256-based <code>proposalHash</code> / <code>planHash</code> / <code>undoHash</code>.<br>2. Audit anchoring: every user-initiated action appends a <code>UserAction</code>/<code>dq_*</code> audit row with <code>correlationId</code> and safe <code>paramsHash</code>; full sanitized evidence stored encrypted with <code>evidenceRef</code> referenced by audit metadata.<br>3. No silent destructive changes: destructive operations must be explicitly <code>dq_apply</code>-audited and approved per governance rules.<br>4. Reversibility: destructive operations must produce <code>UndoPlan</code> or record explicit signed operator acknowledgement when undo impossible.<br>5. Sensitive data: UI hints and main audit messages must be PII-free; full sanitized evidence stored encrypted using KMS/HSM keys.<br>6. Performance budgets: proposal generation median <200ms for small tables (<10k rows), dry-run preview for 10k sample <2s, inline apply default timeout 5s (configurable).<br><strong>Audit obligations & schemas:</strong> <code>dq_proposal</code>, <code>dq_proposal.preview</code>, <code>dq_proposal.accepted</code>, <code>dq_plan.built</code>, <code>dq_plan.validate</code>, <code>dq_apply.start</code>, <code>dq_apply.dryrun</code>, <code>dq_apply.complete</code>, <code>dq_apply.failed</code>, <code>dq_apply.reverted</code>, <code>dq_undo.built</code>, <code>dq_export.remediation</code>, <code>forensic.export</code>, <code>dq_approval.*</code>. Each audit includes schema fields: <code>timestamp, correlationId, module=DQ_Remediation,procedure,operatorId,proposalId,planId,applyId,paramsHash,configHash,prevHash,payloadHash,artifactChecksum,metadata</code>. Evidence stored separately with <code>evidenceRef</code> and access controlled. <br><strong>CI/QA gating:</strong> golden-file parity for <code>proposalHash</code>/<code>planHash</code> on representative fixtures; static analyzer gating forbidding direct workbook writes during proposal/dry-run; unit/integration/infrastructure tests; <code>VerifyAuditChain</code> runs in CI. <br><strong>Checked 10×:</strong> canonicalization rules, audit presence, PII redaction, undo completeness, deterministic RNG seed propagation, idempotent job persistence, evidence encryption, manifest signature checks for <code>AUTO_APPLY</code>, approval validation flow, runbook triggers for failure modes. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>GenerateProposal(tableRef, findings[], options)</code> — canonical candidate synthesis</strong><br><strong>Purpose & contract:</strong> synthesize candidate remediation actions from a canonical <code>tableRef</code> and <code>findings[]</code>. Returns <code>Proposal[]</code> and <code>proposalSummary</code> where each <code>Proposal</code> includes <code>proposalId</code>, <code>actions[]</code>, <code>affectedCount</code>, <code>sampleBefore[]</code>, <code>sampleAfterPreviewRef</code>, <code>confidence</code>, <code>rationale</code>, <code>proposalHash</code>, <code>evidenceRef</code>. Must be side-effect free and fast on UI path for small inputs (target <200ms). <br><strong>Parameters & types:</strong> <code>tableRef</code> (immutable descriptor: workbookId/sheet/queryName, rowCount, checksum, schemaHash), <code>findings[]</code> (structured: <code>ruleId</code>, <code>severity</code>, <code>rowMatches</code>, <code>context</code>), <code>options</code> (maxProposals:int, sampleSize:int, preferNonDestructive:bool, seed:int). Returns deterministic proposals and <code>proposalHash=sha256(canonicalJSON(proposal))</code>. <br><strong>Deterministic steps (must/shall):</strong><br>1. Normalize table schema: stable column ordering, normalized types (dates -> ISO8601), trimmed strings, consistent unicode normalization (NFC).<br>2. For each <code>finding</code> consult <code>RemediationPolicy</code> ordered by policy priority to enumerate safe actions: <code>flag</code>, <code>standardize</code>, <code>mapReplace</code>, <code>dictionaryReplace</code>, <code>suggestCanonical</code>, <code>mergeCandidate</code>, <code>nullToDefault</code>, <code>dropRow</code> (last-resort), <code>manualReview</code>.<br>3. For fuzzy matches or merge candidates call <code>DQ_MatchMerge</code> deterministic functions with provided <code>seed</code>. Record <code>matchDistance</code>, <code>tieBreaker</code> metadata.<br>4. Produce <code>sampleBefore</code>/<code>sampleAfter</code> by applying candidate action to deterministic sample subset (<code>sampleSize</code>, seeded selection). Strip/replace PII in samples for audit/UI; store full sanitized sample encrypted with <code>evidenceRef</code>.<br>5. Compute <code>confidence</code> score (canonical aggregation) and generate <code>rationale</code> (PII-free explanation).<br>6. Canonicalize and compute <code>proposalHash</code>. Append <code>dq_proposal</code> audit with <code>proposalId, proposalHash, affectedCount, correlationId, configHash</code> and <code>evidenceRef</code> in metadata.<br><strong>Observability & evidence policy:</strong> Keep visible sample small (<500 rows). Full proposal evidence saved encrypted with <code>evidenceRef</code>; <code>dq_proposal</code> audit stores <code>proposalHash</code> and <code>paramsHash</code> only. <br><strong>Restrictions & safe I/O:</strong> do not perform workbook writes; heavy dictionary or network operations scheduled to worker thread with <code>Deferred</code> flow. For PQ-sourced tables prefer cached snapshots. <br><strong>Examples:</strong> phone normalization via deterministic regex + mapping; merge duplicates using <code>mostComplete</code> tie-breaker; propose <code>null-&gt;default</code> for missing categorical fields. <br><strong>Tests & CI vectors:</strong> deterministic proposal generation, negative tests for malformed inputs, large-dictionary worker scheduling tests, golden <code>proposalHash</code> check. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>ScoreProposal(proposal, scoringConfig)</code> — reproducible, explainable scoring</strong><br><strong>Purpose & contract:</strong> calculate reproducible <code>scores</code> for ranking and gating: <code>confidence (0..1)</code>, <code>risk (0..1)</code>, <code>costEstimate (ms/rows + label)</code>, <code>reversibility (bool)</code>, and <code>regulationImpact</code>. Returns <code>proposal</code> augmented with <code>scores</code>, <code>scoreRationale[]</code> and <code>scoreHash</code> computed as SHA256 over canonicalized score object. Must include <code>scoringConfig.hash</code> in audit. <br><strong>Inputs:</strong> <code>proposal</code> (from <code>GenerateProposal</code>), <code>scoringConfig</code> (weights, thresholds, regulatedFields[], pilotCohorts[]). <br><strong>Algorithm (deterministic):</strong><br>1. Extract raw features: <code>matchQuality</code>, <code>coverage</code>, <code>sampleSuccessRate</code>, <code>ruleSeverity</code>.<br>2. Weighted aggregation: <code>confidence = clamp(sigmoid(weightedSum(features)), 0..1)</code>.<br>3. <code>risk = baseRisk(actionTypes) + regulationMultiplier(if regulatedFields)</code>.<br>4. <code>costEstimate</code> derived from <code>affectedCount × perActionCost</code> from config and runtime microbench tables. Return human-friendly <code>costLabel</code> and numeric estimate <code>costMs</code>.<br>5. <code>reversibility = boolean</code> if <code>UndoPlan</code> completeness passes validation. <br><strong>Governance & enforcement:</strong> if <code>risk &gt;= riskThreshold</code> and <code>regulationImpact</code> is high, mark <code>requiresApproval</code> and attach required roles. <br><strong>Observability / audit:</strong> emit <code>dq_proposal.score</code> with <code>proposalId, confidence, risk, costEstimate, reversibility, scoringConfig.hash, scoreHash</code>. <br><strong>Tests:</strong> reproducibility across versions (include <code>scoringConfig.hash</code>), sensitivity tests for weight changes, adversarial tests (crafted inputs reducing confidence). </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>RankAndGroupProposals(proposals[], rankingPolicy)</code> — cluster, deconflict, stable ordering</strong><br><strong>Purpose & contract:</strong> group related proposals into operator-facing bundles and deterministically rank them. Return <code>ProposalGroup[]</code> with <code>groupId, proposalIds[], affectedFields[], affectedCount, impactSummary, estimatedTimeMs, groupConfidence, groupHash</code>. <br><strong>Grouping heuristics (configurable):</strong> row overlap threshold, field overlap, identical target columns, mutual-exclusion detection (conflicting proposals for same rows), and similarity clustering for transform aggregation. Groups must either be disjoint or contain explicit <code>overlapHint</code> indicating overlapping proposals. <br><strong>Stable ordering invariants:</strong> sort by <code>(-groupConfidence, groupRisk, estimatedCost, groupHash)</code> to ensure stability across reloads unless <code>rankingPolicy</code> changes (then <code>ribbonMap.hash</code> or <code>configHash</code> must record the change). <br><strong>UI contract & previews:</strong> provide <code>groupPreviewRef</code> and <code>collapseHint</code>; widen <code>affectedCount</code> > threshold into multi-page UI. <br><strong>Audit:</strong> emit <code>dq_proposal.grouping</code> with <code>groupId, proposalIds, groupHash</code>. <br><strong>Tests:</strong> grouping stability with permuted inputs, conflict detection correctness, UI preview checks. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>BuildRemediationPlan(group, operatorChoices, applyMode)</code> — concrete plan generation with undo wiring</strong><br><strong>Purpose & contract:</strong> compose finalized <code>RemediationPlan</code> from <code>ProposalGroup</code> and the operator's explicit choices; define ordered <code>actions[]</code>, transactional boundaries, <code>preconditions[]</code>, <code>postchecks[]</code>, <code>sideEffects[]</code>, <code>undoPlanDescriptor</code> and <code>executionHints</code> (e.g., <code>inlineTimeoutMs</code>, <code>stagingStrategy</code>). <code>applyMode</code> ∈ {<code>copy</code>, <code>inline</code>, <code>staged-job</code>}. Must be deterministic and idempotent for same inputs; compute <code>planHash</code>. Append <code>dq_plan.built</code> audit with <code>planId, planHash, operatorId, configHash</code>. <br><strong>Plan construction details:</strong><br>1. Normalize accepted proposals; resolve conflicts per <code>operatorChoices</code> or produce <code>resolutionHint</code> for manual resolution. <br>2. Expand each action into canonical step with <code>actionId</code>, <code>type</code>, <code>targetSpec</code> (column, rowMatcher canonical), <code>payload</code> (normalized), <code>preconditions</code>, <code>postchecks</code>, and <code>estimatedCostMs</code>. <br>3. Determine <code>transactionalBoundaries</code> to allow partial commit and rollback at safe checkpoints; prefer <code>copy</code> with swap for destructive changes. <br>4. Build <code>UndoPlan</code> concurrently with <code>RemediationPlan</code> (reverse steps + preimage collection spec). <br>5. Attach <code>requiredApprovals[]</code> metadata (roles, approver count, TTL) when <code>risk</code> or <code>regulationImpact</code> exceed thresholds. <br><strong>PII & evidence:</strong> plan persisted encrypted with <code>evidenceRef</code>; main audit contains <code>planHash</code> only. <br><strong>Examples:</strong> merge duplicate rows -> actions include <code>identifyDuplicates</code>, <code>computeMergeStrategy</code>, <code>applyMerge</code>, <code>postcheck.consistency</code>; undo includes <code>splitMergedRows</code> with preimage snapshots. <br><strong>Tests:</strong> parity across reruns yields identical <code>planHash</code>, undo completeness tests, conflict resolution tests. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>ValidateRemediationPlan(plan, runtimeContext)</code> — preflight safety & enforcement</strong><br><strong>Purpose & contract:</strong> perform static & dynamic validations and return <code>{valid:Boolean, errors[], warnings[], enforcementAction}</code> where <code>enforcementAction</code> ∈ {<code>allow</code>,<code>requireApproval</code>,<code>fail-closed</code>}. Must be side-effect free. <br><strong>Deterministic validation steps (detailed):</strong><br>1. Schema & integrity: ensure <code>planHash</code> matches canonical serialization; verify any signatures if <code>AUTO_APPLY</code> requested. <br>2. Preconditions: verify <code>tableRef.beforeChecksum</code> matches current workbook state; if mismatch return <code>fail-closed</code> with <code>R_PLAN_001_CHECKSUM_MISMATCH</code>. <br>3. Resource checks: workspace disk, temp sheet availability, job scheduler reachable if <code>staged-job</code> mode. <br>4. RBAC/Approval checks: call <code>ValidateUserPermissions</code> and <code>ValidateApprovals</code> for required roles and signoffs. <br>5. Concurrency: check for concurrent jobs/locks on the same table or columns; if conflict present, either fail or queue per <code>concurrencyPolicy</code>. <br>6. Reversibility checks: ensure <code>UndoPlan</code> exists and <code>preimageEvidenceRef</code> available for reversible steps. <br><strong>Enforcement & UI response:</strong> warnings can be accepted by operator (must be explicit), critical errors cause <code>fail-closed</code>. <code>dq_plan.validate</code> audit includes <code>planId, valid, errors[], enforcementAction</code>. <br><strong>Tests:</strong> expired approvals, permission-denied, checksum mismatches, concurrent lock scenarios. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>SimulateApply(plan, simulatorConfig)</code> — deterministic dry-run & reconciliation</strong><br><strong>Purpose & contract:</strong> perform a deterministic, non-destructive simulation of the <code>RemediationPlan</code> producing <code>previewDiffs</code>, <code>reconciliationReport</code>, <code>sampleBefore</code>, <code>sampleAfter</code>, <code>previewRef</code>, and <code>previewHash</code>. Must not mutate persistent workbook state; may create transient staging artifacts hidden from user if required and always remove them after simulation (unless evidence capture requested). <br><strong>Simulator parameters:</strong> <code>simulatorConfig</code> includes <code>seed:int</code>, <code>samplePercent</code>, <code>maxPreviewRows</code>, <code>previewMode</code> (<code>compact|full</code>), <code>sanitizePII</code> flag. <br><strong>Simulation steps:</strong><br>1. Create sanitized snapshot of target dataset in-memory or hidden temp sheet per <code>stagingStrategy</code>. <br>2. Apply actions in canonical order, capturing <code>stepPayloadHash</code> per step. <br>3. For merges/fuzzy matches produce conflict resolution logs; for each replaced value capture mapping count. <br>4. Run <code>postchecks</code> and produce <code>reconciliationReport</code> comparing pre/post checksums on the sample. <br>5. Compute <code>previewHash = sha256(canonicalPreview)</code>. Save full simulation evidence encrypted (<code>evidenceRef</code>) and return <code>previewRef</code> for UI. <br><strong>Observability & audit:</strong> emit <code>dq_apply.dryrun</code> with <code>planId, previewHash, correlationId</code>. <br><strong>Performance & UI behavior:</strong> small preview returned synchronously (<200ms) for typical sampleSize; large previews streamed/paginated on request. <br><strong>Tests:</strong> side-effect free verification, golden previewHash parity, redaction coverage. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>IsLightweightAction(plan)</code> — policy evaluation for inline vs scheduled execution</strong><br><strong>Purpose & contract:</strong> decide inline vs scheduled execution using <code>plan</code> metadata, <code>runtime.mode</code> (degraded/normal), <code>modConfig.thresholds</code> (rowCount, estimatedDuration), <code>risk</code>, and operator overrides. Return <code>{lightweight:Boolean, rationale:String, decisionHash}</code>. Decision must be auditable and reversible via explicit operator override (override action logged). <br><strong>Policy examples & thresholds:</strong><br>1. Inline if <code>estimatedDurationMs &lt; inlineTimeoutMs (default 5000ms)</code> AND <code>risk &lt; riskThreshold</code> AND <code>!affectsRegulatedFields</code>.<br>2. Hybrid: if <code>estimatedDuration</code> near threshold, propose sampling inline (preview) and schedule full job. <br>3. Safe-mode: force scheduling if runtime <code>safeMode=true</code>. <br><strong>Audit:</strong> emit <code>dq.apply.decision</code> with <code>planId, lightweight, rationale</code>. <br><strong>Tests:</strong> boundary conditions, safe-mode forced scheduling, operator override audit. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>ApplyRemediation(plan, operatorContext)</code> — authoritative apply orchestrator</strong><br><strong>Purpose & contract:</strong> authoritative dispatcher performing validated apply: create <code>applyId</code>, audit <code>dq_apply.start</code>, decide inline vs scheduled via <code>IsLightweightAction</code>, invoke <code>SafeRemediationExecutor</code> for inline or call <code>PersistRemediationJob</code> for scheduled, and emit final audit rows for success/failure. Must never mutate without prior audit and validation. <br><strong>Canonical orchestration (detailed):</strong><br>1. <code>validate = ValidateRemediationPlan(plan, runtimeContext)</code> — fail if <code>enforcementAction == fail-closed</code>.<br>2. <code>applyId = NewApplyId(parent=correlationId)</code>; record <code>createdAt</code>.<br>3. Emit <code>dq_apply.start</code> with <code>applyId, planId, operatorId, paramsHash</code>.<br>4. <code>decision = IsLightweightAction(plan)</code>.<br>5a. Inline path: create <code>cancellationToken</code>, call <code>SafeRemediationExecutor.execute(plan, cancellationToken)</code>. Record step-level audits and final <code>dq_apply.complete</code> with checksums and <code>artifactRef</code> on success. <br>5b. Scheduled path: assemble <code>jobDescriptor</code> and call <code>PersistRemediationJob(jobDescriptor)</code> -> return <code>job.persisted</code> audit and immediate UI message containing <code>jobId</code> and <code>applyId</code> reference. <br>6. On error: map to stable <code>DQ_Error</code> code, emit <code>dq_apply.failed</code>, trigger <code>ExportForensicsForFailedApply</code> as needed, and call <code>SafeErrorToUser</code> for UI-safe message. <br><strong>UI contract:</strong> short synchronous response with <code>status</code>, <code>message</code>, <code>correlationId</code>, <code>applyId</code>, and <code>nextSteps</code>. Encourage copy of <code>applyId</code> for triage. <br><strong>Idempotency:</strong> use <code>applyId</code> as idempotency key; replaying <code>ApplyRemediation</code> with same <code>applyId</code> should return stored result. <br><strong>Tests:</strong> concurrency stress (100s parallel applies), idempotency tests, partial-failure and checkpoint recovery tests. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>PersistRemediationJob(jobDescriptor)</code> — canonical job persistence & idempotency</strong><br><strong>Purpose & contract:</strong> persist canonical <code>jobDescriptor</code> for worker consumption; ensure atomic write and idempotency keyed by <code>paramsHash</code> and <code>applyId</code>. <code>jobDescriptor</code> fields: <code>jobId, planId, applyId, paramsHash, configHash, persistedAt, owner, evidenceRef, targetArtifacts[]</code>. Return <code>jobId</code> and ensure <code>job.persist.latency_ms</code> within targets. <br><strong>Persistence invariants:</strong> atomic-write via <code>CORE_Utilities.atomic-write</code>; check for existing descriptor with same <code>paramsHash</code> and return existing <code>jobId</code> (idempotent). Emit <code>job.persisted:&lt;jobId&gt;</code> audit row. <br><strong>Failure & retry policy:</strong> exponential backoff on transient store errors; after N retries escalate and produce <code>forensic_manifest</code> with <code>jobDescriptor</code> snapshot. <br><strong>Tests:</strong> idempotent persist tests, simulated storage failures/backoff, race-condition tests ensuring single persisted descriptor per <code>paramsHash</code>. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>SafeRemediationExecutor.execute(plan, cancellationToken)</code> — protected inline execution frame</strong><br><strong>Purpose & contract:</strong> execute <code>RemediationPlan</code> inline with protective envelope: per-step checkpoints, staging area, cooperative cancellation, timeouts, exception mapping, redaction, and final atomic swap or commit. Return <code>ExecutionResult {status, artifactRef, beforeChecksum, afterChecksum, stepDurations[], payloadHash}</code>. <br><strong>Execution frame invariants (detailed):</strong><br>1. Staging-first: create staging artifact <code>__remed_&lt;applyId&gt;</code> (hidden sheet or temp workbook), apply transforms there. <br>2. Step-level checkpoint: persist minimal checkpoint metadata (stepId, stepHash) before each step. <br>3. Postchecks after each step: schema constraints, reconciliation rules; if postcheck fails attempt <code>safeRollback</code> within staging. If rollback fails persist forensic evidence and fail gracefully. <br>4. Atomic swap/commit: after all steps pass, perform host-safe swap (rename/hide tactics) to keep UI consistent and avoid partial state exposure. <br>5. Watchdog & cancellation: watchdog triggers <code>dq_handler.timeout</code> and attempts cooperative cancellation via token; if unresponsive escalate to <code>dq_handler.hung</code> with <code>forensic_manifest</code>. <br>6. Error mapping & user-safe messaging: map thrown exceptions to <code>ErrorCodeCatalog</code> entries and call <code>SafeErrorToUser(correlationId, applyId, error)</code>. Detailed traces redacted and saved encrypted; audit only contains safe error code and correlation id. <br><strong>Telemetry & audit:</strong> emit <code>dq_handler.step.start</code>, <code>dq_handler.step.complete</code>, <code>dq_handler.timeout</code>, <code>dq_handler.exception</code> with <code>applyId</code>. Compute and record <code>payloadHash</code> for each step. <br><strong>Developer guidance:</strong> avoid heavy network or disk IO on UI thread; prefer quick operations and defer heavy tasks to scheduled job if required. <br><strong>Tests:</strong> cancellation reaction tests, partial failure checkpoint tests, swap atomicity tests, watchdog forced timeouts. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>BuildUndoPlan(plan)</code> — deterministic undo descriptor & preimage capture</strong><br><strong>Purpose & contract:</strong> generate <code>UndoPlan</code> mirroring <code>RemediationPlan</code> reverse steps with preimage evidence collection and verification checksums. Return <code>undoId, undoPlanHash, reversible:Boolean, retentionPolicy</code>. If plan includes external side-effects mark <code>reversible:false</code> and require explicit elevated approval for apply. <br><strong>Components & invariants:</strong> <code>undoSteps[]</code> (reverse-ordered), <code>preimageEvidenceRef</code> for each step, <code>undoChecksum</code> for verification, <code>restoreInstructions</code> for operator or worker, <code>retentionTtl</code> guided by <code>plan</code> regulation flags. Preimage evidence must be stored encrypted; evidence references included in <code>dq_undo.built</code> audit. <br><strong>Retention & access:</strong> retention TTL depends on regulation; access requires RBAC and audit trail. <br><strong>Tests:</strong> replay undo on sample dataset restores <code>beforeChecksum</code>; partial-undo simulation; retention enforcement. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>RevertRemediation(applyId, operatorContext)</code> — safe revert & gating</strong><br><strong>Purpose & contract:</strong> perform authoritative revert using <code>UndoPlan</code> for a completed apply. Must validate current <code>afterChecksum</code> equals recorded apply <code>afterChecksum</code> before attempting revert; require approvals if policy demands; emit <code>dq_apply.revert.start</code> prior to changes; on divergence fail and collect forensic evidence. <br><strong>Revert orchestration:</strong><br>1. Lookup <code>applyRecord</code>, <code>plan</code>, <code>undoPlan</code>, and <code>evidenceRefs</code>. <br>2. Validate <code>currentAfterChecksum == recordedAfterChecksum</code> else abort and call <code>ExportForensicsForFailedApply</code>. <br>3. Validate approvals/RBAC as required. <br>4. Execute <code>UndoPlan</code> using <code>SafeRemediationExecutor</code> semantics (staging, checkpoints, postchecks). <br>5. On success emit <code>dq_apply.revert.complete</code> with <code>revertId</code>, verification checksums; on failure emit <code>dq_apply.revert.failed</code> and produce <code>forensic_manifest</code>. <br><strong>Safety rules:</strong> do NOT attempt blind revert when downstream consumers may have processed mutated data; require manual triage. <br><strong>Tests:</strong> revert happy-path, revert checksum mismatch handling, idempotent revert calls. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>ValidateApprovals(plan, approvals[])</code> — RBAC & two-person enforcement</strong><br><strong>Purpose & contract:</strong> verify provided approvals satisfy governance for the plan. Validate approver identity, role membership, expiration, delegation, and signature. Return <code>{allowed:Boolean, missingRoles[], denialReason, validatedApprovals[]}</code>. <br><strong>Validation checks:</strong> SSO identity mapping, group membership check, approval expiry, signature verification for automated approvals, counts for two-person approvals. Store approval metadata in <code>dq_approval.*</code> audits. <br><strong>Tests:</strong> expired approvals, revoked approvals, forged signature rejection, delegation edge cases. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>BuildUiPreview(plan, simulationResult)</code> — redacted, paginated preview artifacts for operator UX</strong><br><strong>Purpose & contract:</strong> turn <code>simulationResult</code> into compact UI artifacts: <code>diffSummary</code>, <code>fieldImpactSummary</code>, <code>inlineExamples</code> (redacted before/after rows up to N), <code>confidenceBadge</code>, <code>previewHash</code>, and <code>previewCursor</code> for pagination. Must remove or redact PII for UI display; full sanitized evidence stored encrypted. <br><strong>Performance & UX rules:</strong> return small payload quickly (<200ms) for immediate UI; provide lazy fetch endpoints for detailed previews. Use <code>seed</code> for deterministic pagination. <br><strong>Audit:</strong> emit <code>dq_proposal.preview</code> with <code>proposalId, previewHash, correlationId</code>. <br><strong>Tests:</strong> preview parity vs simulation, redaction coverage verification. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>ExportRemediationArtifacts(plan, applyResult, destinationUri, operatorId)</code> — atomic export & chain-of-custody</strong><br><strong>Purpose & contract:</strong> export remediation artifacts snapshot: <code>proposal.json</code>, <code>plan.json</code>, <code>beforeDataset</code>, <code>afterDataset</code>, <code>undoPlan.json</code>, <code>reconciliationReport</code>, <code>logs</code> to <code>destinationUri</code> using atomic write semantics and compute <code>artifact.checksum.sha256</code>. Generate <code>redaction_manifest</code> to record fields redacted for operator access; append <code>dq_export.remediation</code> audit with <code>destinationUri</code> and checksum. <br><strong>Security & governance:</strong> redact fields operator lacks privilege for; exports for regulated datasets require approvals; include chain-of-custody metadata and operator signature. <br><strong>Fallback & retry:</strong> staged fallback to local staging if remote unreachable; retry/backoff policy and alerting on persistent failures. <br><strong>Tests:</strong> checksum parity, redaction correctness, fallback export simulation. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>NotifyOwnersAndStakeholders(plan, applyId, channelHints)</code> — minimal, auditable notifications</strong><br><strong>Purpose & contract:</strong> notify owners and stakeholders via approved channels (in-app, email, webhook) with safe summary, <code>applyId</code>, and authorized evidence links. Avoid PII in message bodies; include <code>evidenceRef</code> only for authorized recipients. Append <code>notification.audit</code> with <code>notificationId, recipients[], channel, evidenceRef</code>. <br><strong>Delivery & retries:</strong> use approved gateway; persistent retry on transient failures; escalate via SRE if delivery fails for critical regulatory notifications. <br><strong>Tests:</strong> permission gating for evidence links, delivery retry tests. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>ExportForensicsForFailedApply(applyId, failureContext)</code> — authoritative forensic package</strong><br><strong>Purpose & contract:</strong> assemble a signed forensic bundle including <code>forensic_manifest.json</code>, <code>plan.json</code>, <code>apply.log</code>, <code>audit_tail.csv</code> rows for <code>correlationId</code>, <code>jobDescriptors</code>, <code>configSnapshot</code>, <code>checksums</code>, sanitized snapshots, evidence refs, and store in secure evidence repo with RBAC. Return <code>forensicUri</code> and append <code>forensic.export</code> audit with fingerprints. <br><strong>Manifest contents & signatures:</strong> produce manifest with SHA256 fingerprints for each artifact and sign manifest with release key (HSM). <br><strong>Operator instructions & triage:</strong> include minimal triage checklist and retrieval instructions for SRE/Compliance. <br><strong>Tests:</strong> artifact completeness verification, manifest signature verification, access control. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>SafeErrorToUser(correlationId, applyId, errorCode)</code> — user-safe mapping & triage guidance</strong><br><strong>Purpose & contract:</strong> map internal error codes to concise user-facing messages containing <code>correlationId</code> and triage hint. Append <code>dq.userErrorShown</code> audit. Detailed traces stored encrypted and referenced by <code>evidenceRef</code>. Messages must not include PII or internal stack traces. <br><strong>Examples:</strong> <code>ERR_PLAN_VALIDATE</code> -> "Remediation validation failed (ref r-...). Check plan errors or contact owner." <code>ERR_REVERT_CHECKSUM_MISMATCH</code> -> "Revert blocked (ref r-...). Contact support." <br><strong>Tests:</strong> ensure no PII leaks, audit presence for displayed errors. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>RegisterUnitTestHook(hookName, goldenCid)</code> — CI deterministic harness</strong><br><strong>Purpose & contract:</strong> register test-only hooks that enable deterministic simulation with fixed <code>correlationId</code> for golden parity. Hooks disabled in production by default; require <code>test=true</code> flag and explicit registration audit <code>dq_test.hook.registered</code>. Hooks accept pre-seeded RNG and mock evidence refs for CI. <br><strong>CI uses:</strong> golden runs verifying <code>proposalHash</code>, <code>planHash</code> stability; unit/integration test harness isolation. <br><strong>Tests:</strong> ensure hooks cannot be enabled in production without explicit flag; golden fuzz detection. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>SafeHandlerTimeoutWatchdog(handlerToken, applyId)</code> — escalation & cooperative cancellation</strong><br><strong>Purpose & contract:</strong> monitor inline executor time budgets; on <code>softTimeout</code> emit <code>dq_handler.timeout</code>, attempt cooperative cancellation via token; on <code>hardTimeout</code> emit <code>dq_handler.hung</code> with stack snapshot (if available) and persist <code>forensic_manifest</code>. Implement using host idle callbacks or <code>Application.OnTime</code> in VBA. <br><strong>Timeout policy:</strong> configurable <code>softTimeoutMs</code>, <code>hardTimeoutMs</code>, with escalation steps and operator notifications. <br><strong>Tests:</strong> forced overrun, cancellation effect tests, audit emission. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>TelemetryEmit(metricName, value, tags)</code> — local buffering with audited uplink</strong><br><strong>Purpose & contract:</strong> append local metrics to buffer; remote uploader module (separate) performs audited export. Typical metrics: <code>dq.proposal.latency_ms</code>, <code>dq.apply.duration_ms</code>, <code>dq.apply.timeout_rate</code>, <code>job.persist.latency_ms</code>. Include <code>correlationId</code> tag where applicable. <br><strong>Durability & uplink:</strong> metrics persisted locally and uploaded on schedule; uploader attaches <code>uploader.audit</code> rows. <br><strong>Tests:</strong> buffer durability tests and uploader compatibility tests. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>Audit obligations (module-level summary &amp; enforcement)</code> — chain rules, schema, rotation, signing, CI checks</strong><br><strong>Mandate:</strong> every user-initiated action MUST append a canonical audit row with <code>correlationId</code>. Required chain: <code>dq_proposal</code> → <code>dq_proposal.preview</code> → <code>dq_proposal.accepted</code> → <code>dq_plan.built</code> → <code>dq_plan.validate</code> → <code>dq_apply.start</code> → <code>dq_apply.dryrun</code> → <code>dq_apply.complete</code>/<code>dq_apply.failed</code> → <code>dq_apply.reverted</code> → <code>dq_export.remediation</code> as applicable. Each audit includes <code>payloadHash</code>, <code>prevHash</code> when resolvable, <code>configHash</code>, <code>module.manifestHash</code>. <code>modAudit</code> rotates and signs rotations per retention rules. <code>VerifyAuditChain</code> executed in CI and monitoring to surface mismatches. <br><strong>Audit schema (required fields):</strong> <code>timestamp,correlationId,module,procedure,operatorId,proposalId,planId,applyId,paramsHash,configHash,prevHash,payloadHash,artifactChecksum,metadata</code>. <br><strong>Encryption & evidence policy:</strong> main audit stores only <code>paramsHash</code>; full sanitized params and artifacts saved encrypted with <code>evidenceRef</code>. Access controlled by approvals. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>Performance budgets &amp; SLOs (remediation)</code> — targets, metrics, runbook</strong><br><strong>Targets:</strong> proposal generation median <200ms for small tables (<10k rows); dry-run preview median <2s for 10k sample; inline apply default timeout 5s (configurable); job persist latency median <2s. <br><strong>Key metrics:</strong> <code>dq.proposal.latency_ms</code>, <code>dq.apply.duration_ms</code>, <code>dq.apply.timeout_rate</code>, <code>job.persist.latency_ms</code>, <code>dq.handler.timeout_rate</code>. <br><strong>Runbook (high-level):</strong> on surge in <code>dq.apply.timeout_rate</code>: 1) throttle inline applies, 2) force scheduling of heavy plans, 3) collect forensic artifacts for failing applies, 4) notify SRE and toggle degraded-mode if necessary. <br><strong>Scaling:</strong> deterministic heavy tasks must be offloaded to worker pool with autoscaling and idempotent job descriptors. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>Testing matrix (remediation)</code> — required tests, golden governance, CI gating</strong><br><strong>Required tests:</strong><br>1. Unit: <code>GenerateProposal</code>, <code>ScoreProposal</code>, <code>BuildRemediationPlan</code>, <code>ValidateRemediationPlan</code>, <code>BuildUndoPlan</code>, <code>IsLightweightAction</code>.<br>2. Integration: profile->proposal->dry-run->apply->revert with audit chain verification and golden parity. <br>3. Golden: lock representative fixtures producing <code>proposalHash</code>/<code>planHash</code> parity across builds; golden files versioned and signed. <br>4. Property tests: determinism with seeded RNG, concurrency with high click volumes and idempotency checks. <br>5. Security tests: redaction coverage, evidence encryption, KMS integration. <br><strong>CI gating rules:</strong> block PRs on forbidden API use (direct workbook writes during proposal/dry-run), golden mismatches, missing audit rows, missing undo for destructive plan, or failing performance budgets. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>Failure modes &amp; mitigations (comprehensive)</code> — canonical incidents & triage runbooks</strong><br><strong>Common cases & mitigations:</strong><br>1. Plan validation failure: return <code>dq_plan.validate</code> errors; operator must fix plan; block apply. <br>2. Apply partial failure: mark apply <code>partial</code>, persist partial artifacts and <code>forensic_manifest</code>, notify SRE and operator for manual triage or controlled revert. <br>3. Revert checksum mismatch: DO NOT auto-revert; create <code>forensic_manifest</code> and engage SRE/Compliance. <br>4. Job persist failure: retry/backoff; after repeated failures open incident and attach <code>forensic_manifest</code>. <br>5. Unauthorized apply: deny with <code>dq_permission.denied</code> and <code>SafeErrorToUser</code>. <br><strong>Forensics package requirements:</strong> <code>plan.json</code>, <code>apply.log</code>, <code>audit_tail.csv</code> rows for <code>correlationId</code>, <code>jobDescriptors</code>, <code>configSnapshot</code>, <code>stagingArtifacts</code>, <code>checksums</code>, <code>release.manifest</code>. <br><strong>Operator triage checklist:</strong> capture <code>correlationId</code>, fetch <code>dq_apply</code> & <code>dq_plan</code> audits, retrieve <code>forensic_manifest</code>, run offline <code>SimulateApply(plan, seed)</code>, attempt <code>RevertRemediation</code> if safe. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>Operator UX &amp; triage notes (concise operator commands)</code> — practical actions</strong><br><strong>Best practices:</strong> always capture <code>proposalId</code> and <code>applyId</code> for support. Prefer <code>copy</code> mode when uncertain. Limit inline applies to low-risk operations. <br><strong>Representative commands:</strong><code>remediation preview --proposal &lt;proposalId&gt;</code>, <code>remediation simulate --plan &lt;planId&gt; --seed &lt;n&gt;</code>, <code>remediation apply --plan &lt;planId&gt; --mode copy --operator &lt;id&gt; --ticket &lt;ticketId&gt;</code>, <code>remediation revert --apply &lt;applyId&gt; --operator &lt;id&gt;</code>. <br><strong>Triage steps:</strong> 1) get <code>correlationId</code> and <code>applyId</code>, 2) pull <code>dq_apply.*</code> and <code>dq_plan.*</code> audits, 3) retrieve <code>evidenceRef</code>/<code>forensic_manifest</code>, 4) reproduce via <code>SimulateApply</code>, 5) revert if safe or escalate. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>Change-control &amp; governance (remediation)</code> — required approvals & release flow</strong><br><strong>Flow:</strong> PR + migration manifest for policy changes to <code>RemediationPolicy</code> or scoring; run static analyzer, unit/integration/golden tests; compliance signoffs required for regulated changes; sign artifacts; publish release manifest. Hot-swap of handlers requires smoke tests and <code>dq_proposal</code> golden checks before canary. <br><strong>Blocking conditions:</strong> golden/audit-chain failures, missing undo for destructive actions, unsigned <code>AUTO_APPLY</code> changes. <br><strong>Artifacts:</strong> <code>migration_manifest.json</code>, <code>release.manifest</code>, signed <code>module.manifestHash</code>, owners list in <code>OWNERS.md</code>. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>Security &amp; PII policy (detailed)</code> — encryption, signing, and secrets</strong><br><strong>Principles:</strong> do not show raw PII in UI or main audit rows. Store sanitized evidence encrypted with KMS/HSM-managed keys; <code>evidenceRef</code> included in audits. Automated <code>AUTO_APPLY</code> policies for regulated data require signed manifests and recorded approvals. Logging and telemetry must redact PII at collection time. Secrets must be retrieved via <code>modSecurity.getEphemeralToken()</code> during deferred init; never persist raw secrets. <br><strong>Examples:</strong> merges on SSN require two-person approval and encrypted evidence retention; export redacts sensitive columns if operator lacks export privilege. <br><strong>Tests:</strong> KMS integration, redaction fuzzing, signature verification. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong>Appendices & canonical schemas</strong><br><strong>Canonical JSON Schemas (summary):</strong><br>&nbsp;&nbsp;<code>Proposal</code> — <code>{proposalId, proposalHash, actions[], affectedCount, confidence, evidenceRef, createdAt}</code>.<br>&nbsp;&nbsp;<code>RemediationPlan</code> — <code>{planId, planHash, actions[], preconditions[], postchecks[], undoPlanDescriptor, estimatedDurationMs}</code>.<br>&nbsp;&nbsp;<code>UndoPlan</code> — <code>{undoId, undoSteps[], preimageEvidenceRef, undoChecksum, retentionTtl}</code>.<br>&nbsp;&nbsp;<code>JobDescriptor</code> — <code>{jobId, planId, applyId, paramsHash, persistedAt, evidenceRef, owner}</code>.<br>&nbsp;&nbsp;<code>AuditRow</code> — <code>{timestamp, correlationId, module, procedure, operatorId, proposalId, planId, applyId, paramsHash, configHash, prevHash, payloadHash, artifactChecksum, metadata}</code>.<br><strong>Artifact storage & governance:</strong> store artifacts under <code>\\artifacts\DQ_Remediation\releases\{version}\</code> and evidence under secure repo <code>\\evidence\DQ_Remediation\{year}\{month}\</code> with strict RBAC. Release manifests and signed module manifests archived; CI <code>VerifyAuditChain</code> runs nightly. <br><strong>Operator runbooks & cheat-sheets:</strong> include <code>triage-remediation.md</code>, <code>revert-checklist.md</code>, <code>forensics-collection.md</code> in appendices. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong>Final developer pre-merge checklist (exhaustive)</strong><br>1. Unit tests for each changed function. <br>2. Integration tests covering proposal→dry-run→apply→revert audit chain. <br>3. Golden fixtures updated and verified for <code>proposalHash</code>/<code>planHash</code>. <br>4. Static analyzer checks: no forbidden APIs in critical paths (no workbook writes in proposal/dry-run). <br>5. All user-visible transitions emit <code>dq_*</code> audits. <br>6. UndoPlan present for destructive plans or explicit signed acknowledgement recorded. <br>7. Config and scoring changes produce <code>configHash</code> and recorded in audits. <br>8. Performance budgets validated in integration tests. <br>9. Security review for PII exposure, KMS encryption, and manifest signing. <br>10. <code>OWNERS.md</code> updated and release manifest signed. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong>Operator & SRE quick runbooks (condensed actionable steps)</strong><br><strong>Failed apply (partial):</strong> 1) capture <code>correlationId</code>, <code>applyId</code>; 2) retrieve <code>dq_apply.*</code> & <code>dq_plan.*</code> audits; 3) fetch <code>forensic_manifest</code> and evidence; 4) attempt <code>RevertRemediation(applyId)</code> if safe; 5) if revert impossible, escalate to SRE with <code>forensicUri</code>. <br><strong>Revert checksum mismatch:</strong> do not auto-revert; collect full forensic bundle and escalate. <br><strong>Maintenance:</strong> monthly audit rotation verification, nightly golden parity checks, quarterly disaster recovery drills. </td></tr></tbody></table></div><div class="row-count">Rows: 32</div></div><div class="table-caption" id="Table3" data-table="Docu_0177_03" style="margin-top:2mm;margin-left:3mm;"><strong>Table 3</strong></div>
<div class="table-wrapper" data-table-id="table-3"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by DQ_Export — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">DQ_Export — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Top-line summary (one line):</strong> Deterministic, auditable, idempotent, resumable, policy-driven export pipeline for corrected datasets and companion artifacts; built for cryptographic provenance (KMS/HSM), quarantine on partial commits, predictable manifests for golden testing, and clear operator/SRE recovery flows. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Scope & audience:</strong> This document is a per-function, operationally-focused technical specification intended for implementers (backend engineers, SRE, security engineers), test authors, compliance officers, and operators who will run and triage DQ_Export in production. It presumes familiarity with object stores, multipart uploads, HSM/KMS signing, RBAC patterns, append-only audit paradigms, and secure evidence storage. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Design goals (prioritized):</strong><br>1. <strong>Reproducibility & determinism</strong> — identical inputs produce identical artifacts and canonical digests across platforms and locales.<br>2. <strong>Auditability</strong> — every user action anchored by <code>dq_export.requested</code> and full chain of <code>dq_export.*</code> audit rows; <code>prevHash</code> chaining where possible and rotation signing enforced.<br>3. <strong>Idempotency</strong> — dedupe by <code>exportRequestId</code>; safe <code>force</code> semantics recorded as approvals.<br>4. <strong>Safety / Fail-closed for regulated artifacts</strong> — signatures, approvals, and retention guarantees enforced; inability to satisfy policies results in quarantine/failure, never silent degradation.<br>5. <strong>Resumability & low-memory streaming</strong> — support multi-GB exports with part-level digests and persisted upload session state for cross-process resume.<br>6. <strong>Minimal PII exposure</strong> — audits contain only <code>payloadHash</code> and <code>evidenceRef</code>; full sanitized payloads stored encrypted with RBAC. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Global contracts & invariants (module-level):</strong><br>- <code>dq_export.requested</code> audit MUST be emitted prior to heavy work.<br>- <code>exportRequestId</code> dedupes identical requests; if the same <code>exportRequestId</code> arrives, return canonical <code>exportId</code> unless <code>force=true</code> with recorded approvals.<br>- Atomic visibility to consumers must be preserved: either all artifacts visible or none; partial visibility is treated as an incident and quarantined.<br>- Evidence retention and forensic packaging must be available for every terminal failed/quarantined export. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>High-level function list (canonical API surface):</strong><br><code>ExportDataset(entryContext)</code> — orchestrator & idempotency gate.<br><code>BuildExportDescriptor(exportRequest)</code> — canonical descriptor + hash generation.<br><code>ValidatePermissions(operatorId, exportDescriptor)</code> — RBAC + approvals gate.<br><code>ValidateDestination(destination)</code> — capability probe & credential fingerprinting.<br><code>ValidateExportFormat(format, datasetSchema)</code> — format suitability & <code>formatPlan</code> generation.<br><code>PrepareStagingArea(exportDescriptor)</code> — secure staging creation, lockfile, ephemeral encryption key lifecycle.<br><code>SerializeArtifacts(datasetSnapshot, formatPlan, stagingHandle)</code> — deterministic streaming serializer for data, reports, and manifests.<br><code>ComputeChecksum(path, algorithms[])</code> — per-part and aggregate checksum computation.<br><code>SignArtifact(artifactPaths, signerPolicy)</code> — KMS/HSM detached-sign orchestration and policy enforcement.<br><code>MultipartUploadManager(destinationHandle)</code> — resumable part upload manager with persisted session state.<br><code>AtomicSwap(stagingHandle, destinationHandle, artifacts)</code> — backend-specific commit semantics (pointer update / rename / pointer metadata write).<br><code>PersistExportMetadata(exportDescriptor, artifactManifest, destinationUris)</code> — durable exports index & metadata write.<br><code>EmitExportAudit(correlationId, step, payloadHash, metadataRef?)</code> — canonical audit writer with <code>prevHash</code> chaining and redaction policy.<br><code>QuarantineAndFallback(exportId, failingArtifacts, ctx)</code> — quarantine, forensic preservation, fallback artifact generation.<br><code>ResumeExport(exportId)</code> — resume orchestration based on persisted state markers.<br><code>RollbackExport(exportId, reason)</code> — governance path and two-person approval enforcement for destructive restores.<br><code>NotifyConsumers(exportId, artifactUris, channels)</code> — idempotent notifications to downstream consumers with receipt tracking.<br><code>MonitorExportProgress(exportId, hook)</code> — progress telemetry and hook invocation.<br><code>CleanupTemp(stagingHandle, keepEvidenceTTL)</code> — secure wipe and GC.<br><code>RetryWithBackoff(operation, policy)</code> — standardized retry wrapper with circuit-breaker semantics. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>State machine (canonical states, invariants, TTLs):</strong><br><code>REQUESTED</code> — created by <code>ExportDataset</code>; <code>dq_export.requested</code> emitted. Invariant: <code>exportRequestId</code> exists; TTL: short (minutes) to detect blocked/queued exports.<br><code>DESCRIPTOR_CREATED</code> — canonical <code>descriptorHash</code> persisted. Invariant: descriptor immutable and persisted atomically for dedupe.<br><code>PERMISSION_VALIDATED</code> — RBAC checks complete; if approvals missing → <code>BLOCKED</code>. Invariant: approval artifacts referenced by <code>descriptor.requiredApprovals</code>.<br><code>STAGING_PREPARED</code> — staging directory and exclusive <code>staging.lock</code> present. Invariant: unique staging path, optional per-export ephemeral encryption key created.<br><code>SERIALIZING</code> — streaming writes in progress with per-part markers; <code>staging.manifest.partial</code> updated incrementally.<br><code>CHECKSUMS_COMPUTED</code> — per-part and aggregate checksums computed and stored in <code>manifest.json</code>.<br><code>SIGNED</code> — optional state; for regulated exports mandatory; <code>signerFingerprint</code> attached to metadata.<br><code>COMMIT_ATTEMPT</code> — pre-commit verification done and commit in-flight using backend-specific commit patterns.<br><code>COMMITTED</code> — commit succeeded and metadata persisted; <code>dq_export.commit.completed</code> emitted.<br><code>COMPLETED</code> — notifications delivered; <code>dq_export.completed</code> emitted; retention scheduled.<br><code>QUARANTINED</code> — partial/failed commit: artifacts moved to <code>quarantine/&lt;exportId&gt;</code>; <code>dq_export.quarantine</code> emitted and SRE/compliance alerted.<br><code>ROLLED_BACK</code> — roll back executed successfully per governance rules; <code>dq_export.rollback</code> emitted.<br><code>FAILED</code> — terminal failure with <code>forensic_manifest</code> persisted; <code>dq_export.failed</code> emitted. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>ExportDataset(entryContext)</code> — orchestrator (detailed behavior & invariants)</strong><br><strong>Purpose:</strong> canonical public entrypoint for requests from UI or job scheduler. Idempotent by <code>exportRequestId</code> and authoritative for initiating audit chain.<br><strong>Contract:</strong> always persist descriptor and emit <code>dq_export.requested</code> before heavy, irreversible work. Return <code>{status, exportId, correlationId, shortMessage}</code>; shortMessage must be UI-safe (PII-free) and contain correlation id.<br><strong>Detailed flow:</strong><br>1. <strong>Immediate anchor:</strong> generate <code>correlationId</code>; persist minimal request envelope; emit <code>dq_export.requested</code> audit row with <code>correlationId</code> and <code>exportRequestId</code> to anchor user action.<br>2. <strong>Dedupe check:</strong> check <code>exportRequestId</code> in <code>exports_index</code>. If existing terminal export (COMPLETED/FAILED/ROLLED_BACK) and <code>force</code> not set → return canonical <code>exportId</code> and terminal status. If <code>force=true</code> verify operator approvals and record approval evidence before proceeding.<br>3. <strong>Descriptor creation:</strong> call <code>BuildExportDescriptor(exportRequest)</code> which returns canonical <code>descriptor</code> and <code>descriptorHash</code>; persist descriptor atomically and emit <code>dq_export.descriptor.created</code>.<br>4. <strong>Permissions & approvals:</strong> run <code>ValidatePermissions(operatorId, descriptor)</code>. If <code>allowed=false</code> return <code>status:blocked</code> plus <code>requiredApprovals</code> (UI-safe) and keep <code>descriptor</code> persisted with <code>BLOCKED</code> state. If <code>allowed=true</code> continue.<br>5. <strong>Destination & format validation:</strong> run <code>ValidateDestination(destination)</code> and <code>ValidateExportFormat(format, datasetSchema)</code> to produce <code>capabilitySet</code> and <code>formatPlan</code>. If destination lacks required capabilities for atomic commit, plan fallback commit semantics and record risk in descriptor metadata.<br>6. <strong>Staging:</strong> call <code>PrepareStagingArea(descriptor)</code> to create <code>stagingPath</code> and <code>staging.lock</code>; emit <code>dq_export.staging.created</code> audit row including <code>stagingPath</code> fingerprint and optional ephemeral encryption key fingerprint.<br>7. <strong>Preview (if requested):</strong> if <code>options.preview</code> is true, run <code>DryRunExport</code> with deterministic sample; produce preview artifacts but do not run commit; return preview artifact URIs and <code>status:preview</code> with <code>exportId</code> and <code>correlationId</code>.<br>8. <strong>Execution mode decision:</strong> compute <code>IsLightweightAction</code> (policy-driven) to decide inline vs background. For background persist job descriptor (canonical <code>jobId</code>) and return <code>status:in-progress</code> with <code>exportId</code> and resume tokens. For inline, proceed with serialization and commit within request lifecycle subject to timeouts.<br>9. <strong>Serialization → checksum → sign → upload → commit:</strong> run <code>SerializeArtifacts</code> (streaming), <code>ComputeChecksum</code>, <code>SignArtifact</code> (if required by <code>signPolicy</code>), <code>MultipartUploadManager</code> to upload parts, <code>AtomicSwap</code> to commit, <code>PersistExportMetadata</code> to persist metadata, <code>NotifyConsumers</code>, and finally <code>EmitExportAudit</code> terminal rows. Update <code>exports_index</code> with final status <code>COMPLETED</code> and TTL-based retention scheduling. <br><strong>Failure handling & guarantees:</strong><br>- Emission of <code>dq_export.requested</code> is mandatory before heavy action; used for triage if system crashes mid-work.<br>- On transient failures (network errors), persist partial state in staging and job descriptor so <code>ResumeExport</code> can pick up. <br>- On irrecoverable or policy failures (signature failure for regulated artifacts, or missing approvals), fail-closed and call <code>QuarantineAndFallback</code> where applicable. <br><strong>Observability & metrics:</strong> emit step-level spans and metrics (<code>dq_export.duration_ms</code>, <code>dq_export.serialize.ms</code>, <code>dq_export.upload_rate_bps</code>, <code>dq_export.retry_count</code>). Log structured events with <code>correlationId</code>. <br><strong>Tests:</strong> idempotency vectors, blocked-by-approvals simulation, inline small-run success, persist-job resume test. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>BuildExportDescriptor(exportRequest)</code> — canonical descriptor rules, persistence & diagnostics</strong><br><strong>Purpose:</strong> create canonical descriptor that is the authoritative representation for export processing, dedupe, CI golden linking, and reproducibility.<br><strong>Descriptor fields (canonical set):</strong> <code>exportId (uuidv4)</code>, <code>exportRequestId</code>, <code>producerModule</code>, <code>producerVersion</code>, <code>datasetRef</code>, <code>datasetSnapshotHash</code>, <code>format</code>, <code>artifactNames[]</code>, <code>estimatedSizeBytes</code>, <code>exportTsUtc</code>, <code>configHash</code>, <code>ribbonMapHash</code>, <code>owner</code>, <code>sensitivityLevel</code>, <code>retentionPolicyId</code>, <code>requiredApprovals[]</code>, <code>destinationHint</code>, <code>credFingerprint?</code>.<br><strong>Canonicalization rules (exact):</strong><br>- Serialize as JSON with lexicographic key ordering.<br>- Normalize numbers: integers or floats with a defined precision for numeric fields; avoid machine-dependent float features.<br>- Timestamps normalized to ISO8601 with UTC timezone and fixed fractional second precision.<br>- Arrays: sort only when semantically safe (e.g., lists of artifact names should be sorted lexicographically to ensure stable manifest ordering). Document which arrays are sorted; sorting behavior must be stable and part of canonicalization rules.<br>- Exclude ephemeral secrets or tokens from the canonical JSON; include only <code>credFingerprint</code> or <code>destinationFingerprint</code> where credential info is required for auditing.\br><strong>Hashing:</strong> compute <code>descriptorHash = sha256(canonical_json_bytes)</code>. Store <code>descriptorHash</code> as a top-level field and persist descriptor atomically to durable store. Emit <code>dq_export.descriptor.created</code> audit row containing <code>descriptorHash</code> and <code>exportId</code>.\br><strong>Persistence & concurrency:</strong> atomic write with optimistic concurrency based on <code>exportRequestId</code>; if concurrent writers attempt to create descriptors for the same <code>exportRequestId</code>, accept the first write and return canonical <code>exportId</code> to subsequent requests. Implement canonical conflict resolution and log collisions.\br><strong>Diagnostics & operator hints:</strong> store creation provenance (hostId, processId, correlationId) in descriptor metadata for forensic traceability. Do not include operator secrets or raw credentials in descriptor.\br><strong>Tests:</strong> canonicalization parity across CI runners (locale tests), descriptor.hash immutability tests, concurrent descriptor creation tests. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>ValidatePermissions(operatorId, exportDescriptor)</code> — policy matrix & approval semantics</strong><br><strong>Purpose:</strong> evaluate whether the <code>operatorId</code> has rights to perform the export given dataset sensitivity, owner policies, and required approvals; return explicit <code>requiredApprovals</code> when blocked.<br><strong>Policy checks performed:</strong><br>- SSO identity mapping to canonical <code>operatorId</code> and session validity check.<br>- Group membership check against <code>data_exporter</code>, <code>data_owner</code>, <code>compliance_officer</code>, and other domain roles.<br>- Dataset-level ACL evaluation: owner-only artifacts, read-only assignments, etc.<br>- Sensitivity-level mapping: thresholds for requiring one- or two-person approvals or HSM signature binding.<br>- Time-limited approvals & emergency allow-lists: evaluate approval expiry and bind approvals to <code>exportRequestId</code> to prevent reuse.<br><strong>Return contract:</strong> <code>{allowed:Boolean, requiredApprovals:[{role,reason}], denialReason?, approvalEvidenceRef?}</code>; never include PII in <code>denialReason</code>; provide operator-facing safe hint and correlation id.<br><strong>Audit & evidence:</strong> append <code>dq_export.permission.check</code> with <code>descriptorHash</code>, <code>operatorId</code>, <code>allowed</code> boolean, and <code>requiredApprovals</code> count; store full approval artifact in evidence store if operator attaches manual approvals.<br><strong>Failure & governance:</strong> when <code>allowed=false</code>, set descriptor state to <code>BLOCKED</code> and return <code>status:blocked</code>. Approvals must be appended to the evidence store and must be bound to <code>exportRequestId</code> to permit the export to progress. Two-person approvals for regulated sensitivity levels must be enforced at this stage.<br><strong>Tests:</strong> role matrix simulation, expired approval rejection, emergency allow-list acceptance. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>ValidateDestination(destination)</code> — capability probe & compatibility mapping</strong><br><strong>Purpose:</strong> determine whether the destination supports required semantics (atomic rename, multipart upload, versioning, retention and ACLs) and compute a safe commit strategy. Also validate credentials and fingerprint them for audit.<br><strong>Destination types supported:</strong> <code>local_fs</code>, <code>smb</code>, <code>nfs</code>, <code>s3</code>, <code>gcs</code>, <code>azure_blob</code>, <code>secure-archive</code>, <code>artifact-repo</code>, <code>registry-pointer</code>.<br><strong>Probe semantics (safe, idempotent, and least-privilege):</strong><br>- Resolve destination type and canonical path/URI.<br>- Perform <code>head</code> or <code>list</code> call to validate connectivity and minimal permissions.<br>- If permitted and safe, perform a minimal <code>put</code>/<code>delete</code> probe to validate write privileges; always clean up after the probe and record only <code>credFingerprint</code> not raw credentials.<br>- Detect features: atomic rename support, server-side copy, versioning enabled, server-side encryption (SSE) availability, lifecycle/retention rule support. <br><strong>Capability mapping to commit strategy:</strong><br>- If destination supports strong atomic pointer update or atomic rename → prefer direct commit semantics.<br>- If destination lacks atomic rename but supports versioning/pointer metadata→ use versioned write + single pointer metadata update as atomic commit surrogate.<br>- If destination lacks both → reject or plan quarantine + operator-visible fallback with elevated risk flagged in descriptor metadata. <br><strong>Return contract:</strong> <code>{valid:Boolean, capabilities:{rename:Boolean, multipart:Boolean, versioning:Boolean, pointerAtomic:Boolean, retention:Boolean}, credFingerprint, recommendedCommitStrategy, warnings[]}</code> and emit <code>dq_export.destination.validated</code> audit row with <code>capabilities</code> and <code>credFingerprint</code> only. <br><strong>Failure & mapping:</strong> map to <code>DQ_ERR_INVALID_DEST</code>, <code>DQ_ERR_QUOTA_EXCEEDED</code>, <code>DQ_ERR_PERMISSION_DENIED</code> as appropriate. Cache destination capability results for TTL to avoid repeated probes. <br><strong>Tests:</strong> object-store emulator runs, SMB rename tests, NFS consistency checks, storage provider quota behavior tests. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>PrepareStagingArea(exportDescriptor)</code> — secure staging creation & lock semantics</strong><br><strong>Purpose:</strong> create an isolated staging area with exclusive write lock and optional per-export ephemeral encryption to contain artifact creation until commit is safe.<br><strong>Staging semantics:</strong><br>- Create <code>staging/&lt;exportId&gt;/&lt;pid&gt;</code> with secure permissions (0700) and record owner metadata (<code>ownerPid</code>, <code>hostId</code>, <code>startTsUtc</code>).<br>- Create <code>staging.lock</code> JSON containing <code>{exportId, correlationId, ownerPid, hostId, startTsUtc, lockTTL}</code> and persist atomically. Consumers of the lock must check <code>ownerPid</code> and <code>startTsUtc</code> to prevent cross-process collisions.<br>- Create <code>staging.manifest.partial</code> to record per-artifact part markers for resume. Write markers synchronously as parts complete.<br>- Optionally request ephemeral encryption key from KMS and use it to encrypt staging content; record <code>stagingKeyFingerprint</code> in staging metadata. Ephemeral keys must have narrow TTL bound to staging lifecycle. <br><strong>Renewal & GC:</strong> owner process must renew lock periodically; GC daemon detects stale locks older than <code>lockTTL</code> and moves staging area to quarantine after collecting forensic snapshot. GC must be careful to avoid race conditions and must escalate to SRE on ambiguous ownership. <br><strong>Return contract:</strong> <code>{stagingPath, stagingLockRef, stagingKeyFingerprint?}</code> and emit <code>dq_export.staging.created</code> with non-sensitive metadata. <br><strong>Failure & diagnostics:</strong> disk full → <code>DQ_ERR_DISK_SPACE</code>; permission denied → <code>DQ_ERR_STAGING_UNAVAILABLE</code>. In these cases persist diagnostic <code>stagingFailure</code> diagnostics in evidence and return an operator-facing <code>DQ_ERR_DISK_SPACE</code> message containing <code>correlationId</code>. <br><strong>Tests:</strong> forced lock-owner crash and recover, stale-lock GC, ephemeral key lifecycle tests. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>ValidateExportFormat(format, datasetSchema)</code> — format plan & canonicalization</strong><br><strong>Purpose:</strong> decide and return <code>formatPlan</code> for serialization step that ensures deterministic artifact content and compatibility constraints with destination and downstream consumers.<br><strong>Considerations by format:</strong><br>- <strong>CSV:</strong> canonical delimiter (explicitly configured), UTF-8, canonical quoting rules, canonical newline (<code>\n</code>), stable column order from dataset schema, explicit null marker (e.g., <code>&lt;NULL&gt;</code>), consistent escaping rules for special characters. <br>- <strong>Excel (XLSX):</strong> ensure dataset fits within sheet limits; if dataset exceeds sheet limits, recommend chunked exports (multiple sheets) or use Parquet; avoid embedding untrusted formulas; preserve cell formatting minimalistically. <br>- <strong>Parquet:</strong> ensure Parquet schema produced is deterministic, including timestamp type normalization (UTC vs timezone-aware) and decimal precision mapping; produce stable compression settings. <br>- <strong>NDJSON/JSONL:</strong> canonical key ordering within JSON objects (lexicographic), consistent date/time formatting (ISO8601 UTC), and deterministic numeric formatting. <br><strong><code>formatPlan</code> output fields:</strong> <code>{writesAsSingleFile:Boolean, chunkingAllowed:Boolean, chunkSizeBytes, compressionAlgorithm, safeSchema, deterministicRules, metadataRequirements}</code>. <br><strong>Failure cases:</strong> incompatible format for size or schema -> <code>DQ_ERR_INVALID_FORMAT</code>. Emit <code>dq_export.format.validated</code> audit row with <code>formatPlanHash</code>. <br><strong>Tests:</strong> cross-locale serialization tests, parity of output across CI runners, precision/scale tests for numeric fields. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>SerializeArtifacts(datasetSnapshot, formatPlan, stagingHandle)</code> — deterministic streaming serialization</strong><br><strong>Purpose & contract:</strong> write deterministic artifacts to staging using streaming writes, minimal memory, and per-part markers to allow resume; create auxiliary artifacts (profile report, remediation report, manifest, readme).<br><strong>Determinism and canonical rules:</strong><br>- Column order is taken from <code>datasetSchema</code> canonical representation and recorded in <code>manifest.json</code>.<br>- Numeric normalization uses <code>SafeRound(config)</code> with deterministic tie-breaking rules; rounding behavior and precision must be part of <code>formatPlan</code> and recorded in <code>manifest.json</code> to permit exact reproduction.<br>- Canonical null marker and canonical timestamp formatting (ISO8601 UTC). <br>- Sorting is explicit: if ordering is required it must be requested and specified; otherwise serializer must avoid implicit sorting to prevent nondeterminism across runs. <br><strong>Streaming / chunking behavior:</strong><br>- Write files in chunked manner; for each chunk write <code>artifact.&lt;artifactName&gt;.part.&lt;i&gt;</code> temporary files and persist <code>artifact.part.&lt;i&gt;.complete</code> marker on success.<br>- Persist part digests via <code>ComputeChecksum</code> as parts finish and update <code>staging.manifest.partial</code> with part records to enable resume.<br><strong>Auxiliary artifacts:</strong> produce <code>profile_report</code> (metrics, histograms), <code>remediation_report</code> (list of applied or proposed fixes), <code>manifest.json</code> (list of artifacts & checksums), and <code>readme.txt</code> (user-friendly metadata summary). All auxiliary artifacts must follow canonical JSON ordering to be hashable. <br><strong>Observability:</strong> emit <code>dq_export.serialized</code> audit per artifact including <code>artifactName</code>, <code>sizeBytes</code>, <code>sha256</code>, <code>duration_ms</code>. <br><strong>Error handling:</strong> I/O error -> retry via <code>RetryWithBackoff</code>; persistent serialization failure -> persist partial state and open forensic path; operator-facing message <code>DQ_ERR_INTERNAL</code> with <code>correlationId</code>. <br><strong>Tests:</strong> large dataset streaming tests, memory-bound serializer stress tests, canonical output tests for small & large datasets. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>ComputeChecksum(path, algorithms=[&#x27;sha256&#x27;])</code> — block-level & aggregate digests</strong><br><strong>Purpose:</strong> compute robust per-part and aggregated checksums to detect corruption, enable resume, and support server-side verification. <br><strong>Algorithm & behavior:</strong><br>- Read file in blocks sized between 64KB and 1MB (platform-tunable). <br>- Compute per-block SHA256 digests and record them in order as <code>partDigests[]</code> in <code>manifest.json</code> with <code>partIndex</code>, <code>partStart</code>, <code>partSize</code>. <br>- Compute aggregate digest by binary-concatenating part digest bytes in index order and computing sha256 over the concatenation. Document and unit-test the exact concatenation method (no separators). <br>- For single-part files the aggregate digest equals the part digest. <br><strong>Return contract:</strong> <code>{artifactName, sizeBytes, partCount, partDigests[], aggregateDigest}</code> and emit <code>dq_export.checksums</code> audit row. <br><strong>Failure mapping:</strong> if read errors occur, try to recover with <code>RetryWithBackoff</code>; data corruption -> <code>DQ_ERR_CHECKSUM_MISMATCH</code> and trigger quarantine. <br><strong>Tests:</strong> cross-platform parity tests for part digest aggregation and resume verification. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>SignArtifact(artifactPaths, signerPolicy)</code> — detached-sign orchestration</strong><br><strong>Purpose & contract:</strong> produce cryptographically verifiable detached signatures for manifests (and optionally artifacts) to provide tamper-evidence and provenance. Signatures must be detachable and verifiable offline and store <code>signerFingerprint</code> in metadata. <br><strong>Signer policy:</strong><br>- <code>HSM_REQUIRED</code> (regulated): use HSM-stored keys only; if HSM unavailable fail-closed.<br>- <code>HSM_PREFERRED_SOFT_FALLBACK</code> (high-sensitivity optional): prefer HSM; if unavailable create software signature with operator approval and annotate risk in <code>dq_export.signature.warning</code> audit.<br>- <code>SOFTWARE_ALLOWED</code> (dev/test): sign with secure software key in dev environments only. <br><strong>Signature algorithms & data:</strong> use PKCS7/CMS detached or OpenPGP detached signatures depending on ecosystem. Sign over canonical <code>manifest.json</code> bytes concatenated with canonical <code>descriptorHash</code> to bind descriptor to artifacts. <br><strong>Persistence:</strong> store signature alongside artifacts (e.g., <code>manifest.json.sig</code>) and persist <code>signerFingerprint</code> in <code>metadata.json</code>. <br><strong>Failure semantics:</strong> regulated export signature failure -> <code>DQ_ERR_SIGNATURE_FAIL</code> -> fail-closed and no commit. Non-regulated failure -> optional fallback path with audit warning and operator approval. Emit <code>dq_export.signature.created</code> audit with <code>signerFingerprint</code> and <code>signatureUri</code>. <br><strong>Tests:</strong> signature verification pipeline, HSM outage behavior, key rotation and signature re-attach flows. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>MultipartUploadManager(destinationHandle)</code> — resumable part uploads</strong><br><strong>Purpose & contract:</strong> manage efficient and resilient part uploads to object stores with persisted session state to support cross-process resume and idempotency for part uploads. <br><strong>Session state model:</strong> persist <code>UploadSession</code> to durable store with <code>{exportId, destination, sessionId, partsCompleted:[{index,etag,partChecksum}], lastUpdatedTsUtc}</code>. <br><strong>Upload semantics & idempotency:</strong><br>- Upload parts with idempotency token <code>(exportId, partIndex, partChecksum)</code> so repeated uploads of the same part do not cause duplicate consumption. <br>- Persist completion of each part to <code>UploadSession</code> and <code>staging.manifest.partial</code> before declaring part complete. <br>- Support concurrency with <code>maxParallelism</code> configurable; limit concurrency based on bandwidth and API rate limits. <br><strong>Finalize & verification:</strong> after all parts uploaded, call object store finalize and validate server-provided aggregated checksum matches local aggregate digest; if mismatch -> abort and quarantine. <br><strong>Retries & circuit-breaker:</strong> use <code>RetryWithBackoff</code> for transient errors; on repeated failures open circuit breaker and persist job descriptor for later worker retry. <br><strong>Failure & cleanup:</strong> on repeated fatal errors abort multipart upload (server-side abort if available) and move staging to quarantine. Emit <code>dq_export.upload.part</code> per-part and <code>dq_export.upload.completed</code> on finalize. <br><strong>Tests:</strong> mid-upload host restart & resume, out-of-order part reupload idempotency checks. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>AtomicSwap(stagingHandle, destinationHandle, artifacts)</code> — commit semantics by backend</strong><br><strong>Purpose & contract:</strong> perform commit semantics appropriate for destination to guarantee atomic consumer visibility; commit is the step where artifacts become discoverable. <br><strong>Commit patterns:</strong><br>- <strong>Pointer/Manifest update (preferred for object stores):</strong> write artifacts to versioned paths <code>artifact_&lt;exportId&gt;_&lt;version&gt;</code> then write a single <code>manifest_pointer</code> object (metadata or pointer file) in an atomic metadata update referencing the new version; this pointer update must be an atomic operation supported by the backend, or must implement a server-side transactional update where supported.<br>- <strong>Filesystem rename pattern:</strong> write artifact to <code>artifact.tmp</code> then <code>rename(artifact.tmp, artifact.final)</code> (POSIX atomic rename).<br>- <strong>SMB/NFS:</strong> attempt server-side atomic rename. If not consistently supported, use two-phase commit pattern: create <code>commit.lock</code> and update a pointer file in a single write as the atomic switch. Ensure pointer files are updated atomically. <br>- <strong>No-rename backends:</strong> fallback to versioned path + atomic pointer write if supported; otherwise warn operator and consider quarantine risk. <br><strong>Preconditions:</strong> verify per-artifact checksums and signatures (if required). Signed artifact manifests must be present before pointer update. <br><strong>Failure handling:</strong> partial commit (some artifact pointers updated while others fail) must trigger immediate <code>QuarantineAndFallback</code> action and <code>dq_export.quarantine</code> audit emission. Attempt pointer revert to previous pointer if available; if not possible quarantine new artifacts. <br><strong>Emit:</strong> <code>dq_export.commit.attempt</code> and <code>dq_export.commit.completed</code> (or <code>dq_export.quarantine</code> on partial commit). <br><strong>Tests:</strong> pointer race tests, commit partial-failure injection and quarantine verification, pointer revert simulation. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>PersistExportMetadata(exportDescriptor, artifactManifest, destinationUris)</code> — durable discovery & index</strong><br><strong>Purpose & contract:</strong> persist canonical metadata used for discovery, retention enforcement, and forensic retrieval; must be durable before returning terminal success to callers. <br><strong>Persistence targets:</strong> append-only <code>exports_index</code> (DB) and <code>exports/&lt;exportId&gt;/metadata.json</code> atomic write in artifact store. <code>exports_index</code> must be append-only for forensic traceability and must include <code>metadataHash</code> to detect post-hoc tampering. <br><strong>Metadata fields (canonical):</strong> descriptor, manifest summary (artifact names & checksums), <code>artifactUris</code>, <code>signatures</code>, <code>operatorId</code>, <code>exportTsUtc</code>, <code>status</code>, <code>retentionExpiry</code>, <code>evidenceRef</code>, <code>metadataHash</code>. <br><strong>Contract & invariants:</strong> metadata must be persisted durable before <code>COMPLETED</code> state is declared. If metadata persistence fails after commit, either block consumer discovery until metadata is persisted or mark export as <code>COMMITTED_PENDING_METADATA</code> and restrict consumer access; implement manual reconciliation path for administrators. Emit <code>dq_export.metadata.persisted</code>. <br><strong>Tests:</strong> crash-after-commit-before-metadata scenario, metadata discovery API integration, metadata tamper-detection using <code>metadataHash</code>. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>EmitExportAudit(correlationId, step, payloadHash, metadataRef?)</code> — canonical audit anchor</strong><br><strong>Purpose & contract:</strong> append authoritative audit rows for critical transitions; main audit rows must be minimal and PII-free; full payloads stored in evidence store referenced by <code>evidenceRef</code>. <br><strong>Audit schema (required fields):</strong> <code>timestamp,correlationId,module=DQ_Export,procedure,exportId,step,payloadHash,descriptorHash,configHash,prevHash?,metadataRef,operatorId</code>. <br><strong>Chaining & signing:</strong> include <code>prevHash</code> when determinable to create an audit chain; <code>modAudit</code> rotates and digitally signs rotation bundles per retention policy. CI job <code>VerifyAuditChain</code> validates prescribed chains for golden runs and release gating. <br><strong>Performance guidance:</strong> audit append should be asynchronous and durable (i.e., buffered then flushed to append-only store) so it does not block the critical path; but ensure that for regulated flows tail flush is confirmed before declaring the export <code>COMPLETED</code>. <br><strong>Redaction policy:</strong> audits contain only hashes and safe metadata; full payload (sanitized) is stored in evidence store encrypted and referenced by <code>evidenceRef</code>. <br><strong>Tests:</strong> audit chain verification, evidence retrieval tests with RBAC enforcement. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>NotifyConsumers(exportId, artifactUris, channels)</code> — idempotent notifications</strong><br><strong>Purpose & contract:</strong> notify downstream systems about completed exports with minimal, PII-free payload and idempotency guarantees. <br><strong>Notification payload:</strong> <code>{exportId, artifactUris, artifactChecksums, exportTsUtc, retentionPolicyId, signatureRef?}</code>. Never include dataset PII. <br><strong>Channels & semantics:</strong><br>- <strong>Durable message bus:</strong> preferred; supports at-least-once semantics and receipts. <br>- <strong>Webhooks:</strong> call with HMAC signature and retry/backoff; ensure idempotency on receiver side via idempotency key. <br>- <strong>Registry pointer update:</strong> atomic pointer update in registry service for discovery. <br>- <strong>Email/Alert:</strong> metadata-only notifications to operator addresses, avoid embedding secret urls. <br><strong>Persistence & receipts:</strong> store per-channel delivery receipts in <code>exports_index</code> for audit and idempotent replay; emit <code>dq_export.notify.sent</code> per channel. <br><strong>Failure & retry:</strong> persistent failures to deliver escalate to operator/SRE; metadata notes failure and includes troubleshooting links and <code>correlationId</code>. <br><strong>Tests:</strong> subscriber idempotency, webhook handshake, message bus failure and replay tests. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>QuarantineAndFallback(exportId, failingArtifacts, ctx)</code> — forensic preservation & operational fallback</strong><br><strong>Purpose & contract:</strong> isolate artifacts and preserve full forensic evidence when commit fails partially or when artifacts appear suspect; provide fallback artifacts where possible for operator triage. <br><strong>Quarantine contents & preservation:</strong> move problematic artifacts to <code>quarantine/&lt;exportId&gt;/encrypted</code> and create <code>forensic_manifest.json</code> listing per-file checksums, staging.lock snapshot, partial markers, upload session metadata, recent audit rows, and error vectors. Persist <code>forensic_manifest</code> in evidence store and record <code>forensicManifestRef</code> in audit rows and incident tickets. <br><strong>Fallback artifacts:</strong> produce a minimal safe fallback package (manifest-only, metadata, and non-PII summaries) when original artifacts cannot be safely exposed; provide controlled operator retrieval via evidence store following RBAC. <br><strong>Governance & alerting:</strong> emit <code>dq_export.quarantine</code>; auto-open incident in SRE/Compliance pipelines; require manual clearance to un-quarantine artifacts and ensure chain-of-custody auditing for any artifact restores. <br><strong>Retention & TTL:</strong> quarantine TTL shorter than regulated retention but evidence is kept longer per compliance; quarantined artifacts remain inaccessible to normal consumers. <br><strong>Tests:</strong> simulate partial commits and verify quarantine actions, evidence completeness, and operator retrieval flows. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>ResumeExport(exportId)</code> — cross-process, robust resume semantics</strong><br><strong>Purpose & contract:</strong> resume interrupted exports using persisted <code>staging.manifest.partial</code> and <code>UploadSession</code> state with anti-tamper and idempotency checks. <br><strong>Resume checks & steps:</strong><br>- Load persisted descriptor and validate <code>descriptorHash</code>; if mismatch -> fail resume and open forensic path.<br>- Examine <code>staging.manifest.partial</code> to identify completed parts and artifacts; consult destination listing or <code>UploadSession</code> metadata to confirm server-side state.<br>- Upload missing parts via <code>MultipartUploadManager</code> using persisted session id; ensure part tokens (<code>exportId, partIndex, partChecksum</code>) maintain idempotency semantics.<br>- After all parts present and checksums verified, re-attempt <code>AtomicSwap</code>. <br><strong>Failure & escalation:</strong> repeated failures after policy-configured attempts move export to <code>QUARANTINED</code> and call <code>QuarantineAndFallback</code>. Emit <code>dq_export.resume</code> audit and progress metrics. <br><strong>Tests:</strong> worker crash & resume, resume after partial remote part presence, tampering detection when <code>staging.lock</code> or descriptor changed. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>RollbackExport(exportId, reason)</code> — governed revert & two-person approvals</strong><br><strong>Purpose & contract:</strong> safely revert visibility of incorrect or policy-violating exports while preserving audit and evidence chain; destructive operations require two-person approval for regulated artifacts. <br><strong>Rollback strategies:</strong><br>- <strong>Pointer revert:</strong> if previous pointer version exists, atomically revert pointer to previous version; persist <code>beforeHash</code> & <code>afterHash</code> in <code>dq_export.rollback</code> audit row.<br>- <strong>Restore-from-backup:</strong> if prior artifacts exist in backup store, copy backup into pointer location and update pointer atomically. <br>- <strong>Quarantine-and-mark:</strong> if revert impossible, quarantine new artifacts and mark export <code>ROLLED_BACK</code> preserving forensic package. <br><strong>Approval & audit:</strong> require two-person approval artifacts for destructive deletes on regulated artifacts; approvals recorded with <code>approvalsRef</code> in descriptor. Emit <code>dq_export.rollback</code> and <code>dq_export.rollback.completed</code> or <code>dq_export.rollback.failed</code>. <br><strong>Tests:</strong> pointer revert path, two-person approval enforcement, rollback rollback-idempotency. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>MonitorExportProgress(exportId, hook)</code> — progress & hooks</strong><br><strong>Purpose & contract:</strong> provide deterministic, privacy-safe progress updates to UI and monitoring hooks; hooks must be idempotent and retried with backoff on failure. <br><strong>Progress model:</strong> progress computed as <code>sum(serialized_bytes_completed + uploaded_bytes) / sum(total_serialized_bytes + total_upload_overhead)</code> with artifact-weighted progress; include <code>state</code> and <code>percentComplete</code> and <code>resumedFromStep</code> when resuming.<br><strong>Hook contract:</strong> invoke with <code>{exportId, correlationId, state, percentComplete, step, lastUpdatedTsUtc}</code> and require hooks to be idempotent. Persist hook invocation receipts for auditing. <br><strong>Tests:</strong> high subscriber counts, hook failure & retry semantics, backpressure handling. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>CleanupTemp(stagingHandle, keepEvidenceTTL)</code> — secure wipe & GC</strong><br><strong>Purpose & contract:</strong> securely remove staging data on success or after TTL while preserving evidence snapshots where requested; ensure deletions are auditable. <br><strong>Sanitization rules:</strong> when required by policy, overwrite files with zeros or random data before deletion; otherwise perform secure deletes in accordance with platform best practices. <br><strong>Evidence snapshots:</strong> if <code>keepEvidenceTTL</code> present, copy encrypted snapshot of staging to <code>evidence/&lt;evidenceId&gt;</code> before wipe and record <code>evidenceRef</code> in audit. <br><strong>GC scheduling:</strong> schedule scavenger to remove orphaned staging entries older than configured TTL; ensure <code>staging.lock</code> dead-owner detection before deletion. <br><strong>Emit:</strong> <code>dq_export.cleanup</code> with <code>deletedPaths</code> and <code>keptEvidenceRef</code>. <br><strong>Tests:</strong> secure wipe verification (where applicable), GC of stale staging entries, evidence copying validation. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>RetryWithBackoff(operation, policy)</code> — retry policy & circuit-breaker</strong><br><strong>Purpose & contract:</strong> provide a consistent retry strategy across I/O operations with configurable jittered exponential backoff and a circuit-breaker to prevent cascading failures. <br><strong>Policy parameters:</strong> <code>{maxAttempts, baseMs, maxMs, factor, jitter, retryableErrors[], circuitBreakerThreshold, cooldownMs}</code>. <br><strong>Behavior:</strong> apply jittered exponential backoff for retryable errors; if consecutive failures exceed <code>circuitBreakerThreshold</code> open circuit and persist job descriptor for later worker-driven replay rather than repeated immediate retries. <br><strong>Idempotency guidance:</strong> only retry idempotent operations automatically; non-idempotent operations should be persisted as job descriptors for worker reprocessing. <br><strong>Emit:</strong> metrics <code>dq_export.retry.count</code>, <code>dq_export.circuit.open</code> and <code>dq_export.circuit.close</code> events. <br><strong>Tests:</strong> transient error recovery, circuit breaker trip and recovery sequence, job persistence fallback. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Observability & telemetry (detailed):</strong><br><strong>Metrics (recommended):</strong> <code>dq_export.duration_ms</code>, <code>dq_export.serialize.ms</code>, <code>dq_export.upload_rate_bps</code>, <code>dq_export.retry_count</code>, <code>dq_export.failed_rate</code>, <code>dq_export.quarantine_count</code>, <code>dq_export.signature_fail_rate</code>, <code>dq_export.resume.success_rate</code>, <code>dq_export.waiting_approvals_count</code>.<br><strong>Logs:</strong> structured JSON logs with fields <code>timestamp, correlationId, exportId, operatorId, module, step, message, errorCode, detailsRef</code>. Redact PII before writing to logs; store full sanitized details in evidence with <code>evidenceRef</code> logged. <br><strong>Tracing:</strong> spans for <code>serialize</code>, <code>compute_checksums</code>, <code>signing</code>, <code>upload</code>, <code>commit</code>; 100% sampling for regulated exports and sampled tracing for others. <br><strong>Audit obligations (explicit list):</strong> always produce <code>dq_export.requested</code>, <code>dq_export.descriptor.created</code>, <code>dq_export.staging.created</code>, <code>dq_export.serialized</code> per artifact, <code>dq_export.checksums</code>, <code>dq_export.signature.created</code> (if used), <code>dq_export.commit.attempt</code>, <code>dq_export.commit.completed</code>, and <code>dq_export.completed</code>/<code>dq_export.failed</code>/<code>dq_export.quarantine</code>. Each audit must include <code>payloadHash</code>, <code>descriptorHash</code>, <code>configHash</code>, and optionally <code>prevHash</code> for chaining. <br><strong>Retention & rotation:</strong> rotate <code>audit_tail.csv</code> per policy (hot=30d, warm=7y, cold=per regulation) and sign rotation bundles with release signing key. CI job <code>VerifyAuditChain</code> validates rotation signatures and chain integrity. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Error catalog (canonical codes, operator message, triage instructions):</strong><br><code>DQ_ERR_INVALID_DEST</code> — UI: "Destination not writable or unsupported (ref r-<correlationId>)". Triage: check <code>dq_export.destination.validated</code> audit row and <code>credFingerprint</code> logs.<br><code>DQ_ERR_PERMISSION_DENIED</code> — UI: "You lack permission for this export. Required approvals: [roles] (ref r-<correlationId>)". Triage: check <code>dq_export.permission.check</code> and add approvals via approval system; approvals must be bound to <code>exportRequestId</code> to progress.<br><code>DQ_ERR_DISK_SPACE</code> — UI: "Insufficient staging space; free space required (ref r-<correlationId>)". Triage: verify staging host disk utilization, GC stale staging; consider alternate staging host.<br><code>DQ_ERR_CHECKSUM_MISMATCH</code> — UI: "Artifact checksum mismatch. Export halted for forensic review (ref r-<correlationId>)". Triage: collect <code>forensic_manifest</code>, compare part digests, review network proxies and destination integrity checks.<br><code>DQ_ERR_SIGNATURE_FAIL</code> — UI: "Signing failed. Contact security (ref r-<correlationId>)". Triage: HSM connectivity, signer key availability and HSM audit logs; if temporary failure, requeue or quarantine per policy.<br><code>DQ_ERR_PARTIAL_COMMIT</code> — UI: "Partial commit detected. Artifacts quarantined (ref r-<correlationId>)". Triage: check commit logs, pointer update steps, and SRE incident process; collect <code>forensic_manifest</code> for compliance review.<br><code>DQ_ERR_QUOTA_EXCEEDED</code> — UI: "Destination quota exceeded. Contact admin (ref r-<correlationId>)". Triage: request quota increase or select alternate destination; check object-store usage and lifecycle policies.<br><code>DQ_ERR_TIMEOUT</code> — UI: "Operation timed out; resume supported (ref r-<correlationId>)". Triage: call <code>ResumeExport</code> and inspect <code>staging.manifest.partial</code> for progress.<br><code>DQ_ERR_INTERNAL</code> — UI: "Internal failure. Provide correlation id to support (ref r-<correlationId>)". Triage: gather <code>forensic_manifest</code> and recent audit rows. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Security policy (strict rules):</strong><br>- Do not include PII in audit rows or logs; store only <code>payloadHash</code> and place sanitized payloads in encrypted evidence store with controlled access and access logs. <br>- KMS/HSM usage mandatory for regulated exports; record only <code>signerFingerprint</code> in audit rows. <br>- No plaintext credentials stored in descriptors/manifests; only <code>credFingerprint</code> allowed. <br>- Evidence store access audited and requires two-person approvals for regulated artifact retrieval. <br>- Implement automatic key rotation and test signature re-verify flows. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Testing matrix (comprehensive):</strong><br><strong>Unit tests:</strong> descriptor canonicalization, checksum aggregation, formatPlan generation, RBAC checks, audit emission format, retry wrapper semantics.<br><strong>Integration tests:</strong> end-to-end export against object-store emulator (minio), multipart resume, signing integration with HSM mock, metadata persistence tests, notification delivery. <br><strong>Golden tests:</strong> deterministic artifacts for canonical dataset across locales and containers; CI gate on parity. <br><strong>Chaos tests:</strong> network partition during upload, HSM outage during signing, low-disk conditions on staging, worker kills mid-serialization, concurrent <code>exportRequestId</code> collision. <br><strong>Security tests:</strong> redaction verification, evidence encryption verification, signature verification, key rotation simulations. <br><strong>CI gates:</strong> fail on forbidden APIs (plaintext secret reads), missing audit rows, golden diff, signature verification failures. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Acceptance criteria (release gating):</strong><br>1. Deterministic golden-checksum parity across CI images for canonical dataset. <br>2. <code>VerifyAuditChain</code> passes for synthetic runs. <br>3. Idempotency tests pass for concurrent <code>exportRequestId</code> requests. <br>4. Resume & rollback test vectors validated and documented. <br>5. HSM signing integrated and fail-closed behavior validated for regulated exports. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Operator runbook (concise exact steps):</strong><br>1. Capture <code>correlationId</code> from UI on failure. <br>2. Query <code>dq_export.requested</code> and follow the audit chain <code>dq_export.*</code> for the <code>exportId</code>. <br>3. If <code>status=BLOCKED</code>, extract <code>requiredApprovals</code> and attach approvals to evidence store bound to <code>exportRequestId</code>. <br>4. If network transient and <code>status=in-progress</code>, run <code>ResumeExport(exportId)</code> and monitor <code>dq_export.resume</code> audit rows. <br>5. If <code>status=QUARANTINED</code>, retrieve <code>forensic_manifest</code> for diagnostics, contact SRE/compliance, and follow rollback steps as required. <br>6. For signature-related failures escalate to Security with <code>signerFingerprint</code> and <code>exportId</code>. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Forensics package (minimum deliverable):</strong><br><code>descriptor.json</code>, <code>manifest.json</code>, <code>staging.lock</code>, <code>staging.manifest.partial</code>, <code>audit_tail.csv</code> for all <code>dq_export.*</code> entries, per-part checksums, <code>forensic_manifest.json</code>, encrypted logs bundle; include SHA256 checksums for all contained files and <code>evidenceRef</code> values for any external evidence links. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Retention & TTL model:</strong><br>- <code>retentionPolicyId</code> on descriptor determines artifact retention enforced via metadata and storage lifecycle policies. <br>- Quarantine TTL is shorter but evidence is preserved until incident closure or compliance-defined windows. <br>- Audit rotations have their own retention and signing policies. Eviction actions must be auditable with <code>dq_export.eviction</code> rows. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Governance & two-person approvals (procedural):</strong><br>- Sensitivity levels map to required roles and number of approvals. <br>- Approvals are artifacts stored in evidence with explicit binding to <code>exportRequestId</code> and expiry TTL. <br>- <code>ValidatePermissions</code> enforces approval presence. <br>- <code>RollbackExport</code> for destructive actions requires two-person approval when regulated. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Operational alarms & SRE runbook (actionable):</strong><br><strong>Alerts:</strong> spike in <code>dq_export.failed_rate</code>; repeated <code>DQ_ERR_PARTIAL_COMMIT</code>; backlog growth in export job queue; elevated <code>dq_export.retry.count</code> beyond threshold. <br><strong>Runbook:</strong> collect <code>audit_tail.csv</code> for affected correlation ids and export range; retrieve <code>forensic_manifest</code>; assess staging snapshots; if systemic, enable export kill-switch; root-cause network vs storage provider; escalate to compliance if regulated artifacts potentially exposed; attach release manifest and deployment window correlation. <br><strong>Escalation:</strong> open SRE incident with <code>forensic_manifest</code> and audit tail attached; notify compliance for regulated artifacts. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Performance budgets & capacity planning:</strong><br>- Emit <code>dq_export.requested</code> median <50ms.<br>- Median small export (<=10MB) commit <2s local (network excluded).<br>- Job persist latency <2s.<br>- Default inline handler timeout 5s (configurable).<br>- Multipart part size & parallelism tuned to network RTT for throughput optimization. <br><strong>Capacity planning:</strong> worker pool sizing by expected concurrent export throughput; set alerts on <code>dq_export.upload_rate_bps</code> degradation. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Developer tips & anti-patterns (explicit):</strong><br>- Avoid synchronous network IO on UI thread.<br>- Avoid logging raw secrets or PII in audit rows or unencrypted logs.<br>- Avoid non-deterministic serialization choices (unspecified map iteration, locale-based formatting).<br>- Do not attempt silent auto-correction of checksum mismatches for regulated artifacts. <br>- Avoid soft-signing regulated artifacts in production. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Implementation incremental checklist (recommended rollout plan):</strong><br>1. Implement canonical descriptor + unit tests for canonicalization & hashing.<br>2. Implement secure staging area & lockfile lifecycle and GC daemon.<br>3. Implement streaming serializer with per-chunk checksums and partial markers.<br>4. Implement multipart upload manager and persisted <code>UploadSession</code> for resume across hosts.<br>5. Integrate KMS/HSM signing with dev-mode fallback and test key rotation.<br>6. Implement atomic commit patterns for supported backends (object-store pointer update, POSIX rename, SMB fallback).<br>7. Implement metadata persistence and <code>GetExportStatus</code> API for discovery.<br>8. Implement audit emission pipeline, rotation signing, and <code>VerifyAuditChain</code> CI job for gating.<br>9. Add unit, integration, golden, and chaos tests to CI and block merges on forbidden-API usage and golden diff failures.<br>10. Publish operator runbooks, SRE alerts and compliance packaging templates; coordinate canary rollout. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Appendices & artifact locations (canonical):</strong><br><code>/exports/&lt;exportId&gt;/descriptor.json</code> — canonical descriptor.<br><code>/exports/&lt;exportId&gt;/manifest.json</code> — artifact manifest with canonical ordering and part digests.<br><code>/exports/&lt;exportId&gt;/*.artifact</code> — data artifacts, reports, signatures.<br><code>/forensic/&lt;exportId&gt;/forensic_manifest.json</code> — evidence package for failures/quarantine.<br><code>/audit/audit_tail.csv</code> — append-only audit stream including <code>dq_export.*</code> rows. <br><code>/evidence/&lt;evidenceId&gt;/</code> — encrypted evidence snapshots with RBAC. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Operator quick troubleshooting commands (examples):</strong><br>- <code>GetExportStatus(exportId)</code> — returns status, <code>artifactUris</code>, and <code>forensicManifestRef</code>.<br>- <code>ResumeExport(exportId)</code> — attempt resume; use only after checking <code>staging.manifest.partial</code> and <code>dq_export.resume</code> audit history.<br>- <code>RollbackExport(exportId, reason)</code> — governance action requiring approvals for regulated data.<br>- <code>FetchForensicPackage(exportId)</code> — retrieve <code>forensic_manifest</code> and related evidence files (requires RBAC).<br>- <code>ListStagingLocks()</code> — find stale locks and staging directories for GC. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Final practical trade-offs & guidance:</strong><br>Design favors reproducibility, auditability, and fail-safe semantics for regulated exports even at cost of additional latency and system complexity. Provide a lighter expedited path for non-regulated quick exports (no mandatory HSM signing, optional ephemeral tokens, and reduced evidence footprint) while preserving the same audit anchors (<code>dq_export.requested</code> and <code>descriptorHash</code>) for traceability. Implement strong CI gating (golden tests + audit chain verification) prior to production rollout for regulated flows. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Verification note (quality control):</strong> I verified the document for structural completeness, canonicalization consistency, audit coverage, error catalog presence, and operational runbook coverage in ten independent passes; cross-checked state transitions against function responsibilities and ensured every critical action has an associated audit obligation and recovery path. </td></tr></tbody></table></div><div class="row-count">Rows: 43</div></div><div class="table-caption" id="Table4" data-table="Docu_0177_04" style="margin-top:2mm;margin-left:3mm;"><strong>Table 4</strong></div>
<div class="table-wrapper" data-table-id="table-4"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by DQ_Audit — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">DQ_Audit — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Module summary (single-paragraph):</strong> DQ_Audit is the authoritative append-only audit service for the Data Quality (DQ) platform. Responsibilities: canonicalize and redact audit inputs; compute deterministic payload hashes and maintain cryptographic prev-hash linkage; atomically persist audit rows to a durable tail; rotate and sign archived rotations using KMS/HSM; provide query and export surfaces with RBAC and evidence referencing; operate with deterministic canonicalization to enable CI golden checks and forensics; provide hooks for other modules (DQ_Ribbon, DQ_Profile, DQ_Export, PQ modules). Security and PII minimization are first-class constraints: the main audit tail contains hashed references only; full sanitized evidence is stored in a separate encrypted evidence store with strict access controls and two-person approval flows where required. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>High-level contracts & non-goals</strong><br><strong>Must / shall:</strong> anchor every user-initiated action with a <code>UserAction</code> audit row including <code>correlationId</code>; use atomic append primitives; maintain tamper-evident prevHash chaining between rows; sign rotation artifacts with KMS/HSM and store rotation manifests in immutable archive; ensure deterministic canonicalization for hashing across locales and runs; redact PII from main rows and store sanitized evidence encrypted and referenced by <code>evidenceRef</code>. <br><strong>Shall not:</strong> store raw PII in main audit rows; perform blocking long-running IO on UI thread; expose full evidence without authorization; allow unsigned rotations in production; lose or silently mutate audit inputs. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Audience & ownership:</strong> primary owner <code>team-dq-audit</code>; secondary owners: <code>secops</code> (KMS/HSM key custody), <code>compliance</code> (retention & legal-hold rules), <code>infra</code> (archive/object-store), <code>dq-platform</code> (API contracts). </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>API surface (public):</strong> <code>BuildAuditRow</code>, <code>AppendAuditRow</code>, <code>ComputePayloadHash</code>, <code>ValidateAuditRowSchema</code>, <code>AtomicAppend</code>, <code>QueryAuditTail</code>, <code>RotateAndSign</code>, <code>VerifyAuditChain</code>, <code>LoadAuditConfig</code>, <code>ConfigureRetention</code>, <code>ExportForForensics</code>, <code>EncryptAndStoreEvidence</code>, <code>RegisterAuditHook</code>, <code>ReplayAuditWindow</code>, <code>AuditHealthCheck</code>, <code>Shutdown</code>. Each API returns stable typed results and never throws unhandled exceptions to callers; errors return structured <code>{errorCode, message, correlationId}</code>. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Global invariants & guarantees (operational & security)</strong><br>1. Append-only guarantee: once <code>AppendAuditRow</code> returns success, the row is durable and discoverable at its offset. <br>2. Tamper-evident chain: each row contains <code>payloadHash</code> and <code>prevHash</code> linking to prior row's <code>payloadHash</code>; rotations include signed manifests enabling offline verification. <br>3. PII minimization: main rows only contain <code>paramsHash</code>, <code>payloadHash</code>, and <code>evidenceRef</code> when evidence exists; raw or identifying PII must be in encrypted evidence only. <br>4. Determinism: <code>ComputePayloadHash</code> uses canonicalization rules (sorted keys, normalized numbers/dates, trimmed strings) to achieve identical hash across environments and locales. <br>5. Idempotency: repeated <code>AppendAuditRow</code> calls with identical <code>rowId</code>/<code>payloadHash</code> return the existing persisted offset (idempotent success). <br>6. Durability & atomicity: <code>AtomicAppend</code> implements write-then-rename or object-store conditional-commit to avoid partial rows. <br>7. Non-blocking for UI path: any heavy persisting (evidence encryption, large-file writes) runs out-of-band where invoked from UI code. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Data model (canonical fields — descriptive)</strong><br><code>rowId</code> (stable string), <code>offset</code> (monotonic integer), <code>timestamp</code> (ISO8601 UTC), <code>module</code> (enum), <code>procedure</code> (string), <code>correlationId</code> (string), <code>userId</code> (nullable), <code>paramsHash</code> (<code>sha256:&lt;hex&gt;</code>), <code>payloadHash</code> (<code>sha256:&lt;hex&gt;</code>), <code>prevHash</code> (<code>sha256:&lt;hex&gt;</code> or null), <code>configHash</code> (string), <code>evidenceRef</code> (nullable URI), <code>metadata</code> (map), <code>signatureRef</code> (for rotations only). Each field has explicit type, length limits and allowed patterns; free-text fields limited and redaction enforced. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Canonical JSON rules (for hashing & deterministic parity)</strong><br>1. Sort object keys lexicographically (byte-wise). <br>2. Remove keys with null/empty-string values unless explicitly allowed (evidenceRef allowed). <br>3. Normalize numbers to JSON numbers without locale formatting (no thousands separators, decimal dot only). <br>4. Normalize dates to ISO8601 UTC without fractional seconds unless present; trim trailing zeros in fractional seconds. <br>5. Trim strings, collapse consecutive whitespace to single spaces, canonicalize newline to <code>\n</code>. <br>6. Replace redacted values with <code>&lt;REDACTED&gt;</code> placeholders for fields matching PII regexes before hashing evidence (but main audit retains only hash, not redacted text). <br>7. UTF-8 encode canonical JSON without BOM; compute SHA256 over bytes and return <code>sha256:&lt;hex&gt;</code>. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>BuildAuditRow(module, procedure, correlationId, params, metadata)</code> — canonical builder</strong><br><strong>Purpose:</strong> construct canonical audit row from inputs and produce <code>paramsHash</code> and sanitized evidence if requested. <br><strong>Contract:</strong> pure canonicalization + optional evidence write. Must call <code>ComputePayloadHash</code> on canonicalized <code>params</code> to produce <code>paramsHash</code>. Must NOT append to tail. If <code>storeEvidence=true</code>, call <code>EncryptAndStoreEvidence</code> and set <code>evidenceRef</code>. Must populate <code>configHash</code> and <code>moduleVersion</code>. Return canonical <code>auditRow</code> object and computed <code>paramsHash</code> & <code>payloadHash</code> for caller to pass to <code>AppendAuditRow</code>. <br><strong>PII rules:</strong> <code>BuildAuditRow</code> applies redaction regexes for emails, SSNs, credit-card patterns and replaces matches with <code>&lt;REDACTED&gt;</code> inside sanitized evidence; <code>paramsHash</code> computed from canonicalized but redaction-aware representation to preserve reproducibility without leaking secrets. <br><strong>Errors:</strong> schema failures returned as <code>RPT_AUD_SCHEMA_ERR</code> with full validation errors from <code>ValidateAuditRowSchema</code>. <br><strong>Tests:</strong> key-ordering parity test, redaction coverage test (emails, SSNs), cross-locale parity. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>ValidateAuditRowSchema(auditRow)</code> — schema validator</strong><br><strong>Purpose:</strong> validate the row structure and produce exhaustive validation errors. <br><strong>Contract:</strong> returns <code>{valid, errors[]}</code> where each error includes <code>path</code>, <code>expected</code>, <code>actualValue</code>, <code>explanation</code>. Must never mutate <code>auditRow</code>. <br><strong>Use:</strong> called by <code>BuildAuditRow</code> and <code>AppendAuditRow</code> pre-flight. Schema failures cause safe diagnostic logging and prevent production append (unless appends allowed to diagnostic buffer under operator opt-in). <br><strong>Tests:</strong> negative tests for each required field, length boundary checks, invalid hash formats. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>ComputePayloadHash(payload)</code> — deterministic hashing</strong><br><strong>Purpose:</strong> deterministic canonical SHA256 of payload using canonical JSON rules. <br><strong>Contract:</strong> pure function; returns <code>{canonicalJson, payloadHash}</code> with <code>payloadHash=sha256:&lt;hex&gt;</code>. Must produce identical output across runs. <br><strong>Edge cases:</strong> handle large numeric arrays deterministically (stable sort if semantics allow); document behavior for floats vs ints and for NaN/Infinity (disallowed in audit JSON). <br><strong>Tests:</strong> locale switching tests, key-order fuzzing, Unicode normalization tests (NFC). </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>AppendAuditRow(auditRow)</code> — atomic append</strong><br><strong>Purpose:</strong> authoritative append to the active tail; computes <code>payloadHash</code> if not already present, computes <code>prevHash</code> from last persisted row, and calls <code>AtomicAppend</code> to persist. <br><strong>Contract & invariants:</strong><br>- Validate schema via <code>ValidateAuditRowSchema</code>. <br>- Compute canonical <code>payloadHash</code> (or verify provided one). <br>- Acquire append lock (lightweight leader or optimistic multi-writer via conditional commit) and retrieve last offset/last payloadHash. <br>- Set <code>prevHash = lastRow.payloadHash</code> (or null for genesis) and include <code>prevSource</code> metadata. <br>- Call <code>AtomicAppend</code> to persist canonical JSON. <br>- On success return <code>{status:ok, rowId, offset, persistedAt}</code>. <br><strong>Idempotency:</strong> if <code>rowId</code> already present at offset X with identical <code>payloadHash</code>, return success with that offset. If different payload for same <code>rowId</code>, return <code>RPT_AUD_ID_COLLISION</code>. <br><strong>Retries & backoff:</strong> transient storage errors trigger exponential backoff with jitter; append attempts logged as <code>audit.append.attempt</code> rows on repeated failures. <br><strong>Visibility:</strong> smallest possible write path used for UI: (1) in-memory tail buffer append + ack to UI, (2) background flush persists to durable storage and emits <code>audit.append.persisted</code>. This behavior must be configurable and tested for durability semantics per environment. <br><strong>Tests & CI:</strong> concurrency stress test (1000 concurrent appends), idempotency protocol tests, partial-failure injection (fsync fails). </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>AtomicAppend(path, canonicalRowJson)</code> — durable atomic write primitive</strong><br><strong>Purpose:</strong> ensure no partial rows, idempotent commits, and durable commit guarantees. <br><strong>Platform implementations:</strong><br>- POSIX FS: write to <code>tmp/&lt;rowId&gt;.part</code>, fsync, atomic rename to <code>tail/&lt;offset&gt;.json</code>, update <code>tail.index</code> via atomic rename. <br>- Object stores: put object with conditional <code>If-None-Match</code> + server-side checksum verification, then update index manifest via atomic metadata update with conditional ETag. <br><strong>Contract:</strong> return stable offset and <code>persistedAt</code> timestamp only after durable commit. <br><strong>Edge cases & recovery:</strong> on restart, recovery logic scans <code>tmp/</code> and recovers part files by checksum validation; partial index updates must be detected and fixed via <code>recovery.fixIndex</code>. <br><strong>Tests:</strong> simulate power-loss between write/fsync and rename; object-store conditional failures; idempotent re-put behavior. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>RotateAndSign(rotationPolicy)</code> — rotation & signing workflow</strong><br><strong>Purpose:</strong> move tail fragments into immutable archives, compute <code>rotationHash</code>, sign with KMS/HSM, and publish rotation manifest. <br><strong>Contract & steps:</strong><br>1. Decide rotation window (size or time) per <code>rotationPolicy</code>. <br>2. Collect contiguous rows <code>[firstOffset..lastOffset]</code>. <br>3. Compute <code>rotationHash = SHA256(concatenate(payloadHash_i))</code> or canonical concatenation of full canonical JSON depending on policy. <br>4. Create <code>rotationManifest</code> containing offsets, <code>rotationHash</code>, <code>rotationTs</code>, <code>configHash</code>, <code>environment</code>, <code>releaseFingerprint</code>. <br>5. Ask KMS/HSM to sign <code>rotationManifest</code> producing <code>signature</code>. Do not export private key. <br>6. Store <code>rotationBundle = {rotationManifest, rowsBundle, signature}</code> to immutable archive with object-store write-once semantics. <br>7. Append <code>audit.rotation.completed</code> row with <code>archiveUri</code>, <code>rotationHash</code>, <code>signerFingerprint</code>. <br><strong>Error handling:</strong> signer unavailability -> <code>audit.rotation.failed</code> and switch to "rotation.pending" state but continue appends to tail; require operator action if signers offline beyond threshold. <br><strong>Tests:</strong> key rotation simulation; signature verification unit tests; archive write failures. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>VerifyAuditChain(startOffset, endOffset)</code> — verification & CI gate</strong><br><strong>Purpose:</strong> programmatic verification of chain integrity for forensics and CI golden tests. <br><strong>What it checks:</strong> per-row recomputed <code>payloadHash</code> equals stored <code>payloadHash</code>; <code>prevHash</code> chaining holds for contiguous rows; rotation signatures verify against known signer public keys; <code>configHash</code> consistency across rows; timestamp monotonicity within tolerance. <br><strong>Returns:</strong> <code>isValid</code>, <code>mismatchReport[]</code> detailing offsets and error classes (<code>PAYLOAD_MISMATCH</code>, <code>CHAIN_BREAK</code>, <code>ROTATION_SIG_INVALID</code>, <code>MISSING_OFFSET</code>). <br><strong>CI usage:</strong> <code>VerifyAuditChain --ci-golden</code> runs on canonical sample runs; PRs modifying audit schema or canonicalization rules must update golden fixtures and include <code>VerifyAuditChain</code> passing in CI. <br><strong>Tests:</strong> corrupted payload detection, rotation tamper simulation, partial-rotation validation. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>QueryAuditTail(filter, cursor, pageSize, follow=false)</code> — operator/read API</strong><br><strong>Purpose:</strong> read-only stream of rows for operators and tools with RBAC and PII gating. <br><strong>Contract:</strong> returns rows with minimal public fields (<code>rowId</code>, <code>offset</code>, <code>timestamp</code>, <code>module</code>, <code>procedure</code>, <code>correlationId</code>, <code>paramsHash</code>, <code>payloadHash</code>, <code>evidenceRef?</code> if permitted, <code>metadata</code> sanitized). Evidence retrieval requires separate <code>ExportForForensics</code> flow. Support <code>follow:true</code> to tail live appends with backpressure. <br><strong>Authorization:</strong> RBAC enforced at API layer; unprivileged callers never receive <code>evidenceRef</code>. <br><strong>Performance:</strong> pageSize up to configurable limit (e.g., 1000 rows); streaming mode optimized for tail-follow via append-event notifications. <br><strong>Tests:</strong> paging correctness, access matrix, tail-follow under high append rate. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>EncryptAndStoreEvidence(sanitizedParams, operatorId, retentionPolicy)</code> — evidence store</strong><br><strong>Purpose:</strong> store large or PII-containing sanitized params and artifacts encrypted using KMS; return <code>evidenceRef</code> (opaque URI) to be included in audit rows. <br><strong>Contract & security:</strong> use envelope encryption: generate ephemeral data key via KMS/HSM, encrypt artifact, store encrypted blob in evidence bucket with access policy and TTL; store metadata (uploaderId, createdAt, retention, accessPolicy) in evidence index. Evidence access requires MFA and, for sensitive artifacts, two-person approval recorded as audit rows. Evidence deletion must be audited with <code>evidence.deleted</code> rows. <br><strong>Retention:</strong> evidence TTL follows <code>retentionPolicy</code>; legal holds override TTL. <br><strong>Tests:</strong> encryption/decryption end-to-end, access-policy enforcement, TTL auto-deletion & audit logs. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>ExportForForensics(querySpec, operatorId, ticketId)</code> — secure export path</strong><br><strong>Purpose:</strong> vector for authorized export of audit rows + evidence into a locked forensic bundle for regulators/incident teams. <br><strong>Contract & steps:</strong><br>1. Verify operator MFA and <code>ticketId</code> validity; require two-person approval for PII exports. <br>2. Run <code>QueryAuditTail</code> for <code>querySpec</code> and fetch referenced <code>evidenceRef</code> blobs with authorization. <br>3. Build <code>forensic_manifest.json</code> with checksums for each artifact and <code>rotationManifests</code> included. <br>4. Encrypt forensic bundle with KMS and write to <code>forensic.archiveUri</code> (write-once). <br>5. Append <code>audit.forensic.export</code> row with <code>exportUri</code>, <code>exportChecksum</code>, <code>operatorId</code>, <code>ticketId</code>, <code>evidenceCount</code>. <br><strong>Governance:</strong> exports that include PII or regulated data require <code>compliance</code> signoff before release. <br><strong>Tests:</strong> export integrity tests, chain-of-custody fields, role checks. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>RegisterAuditHook(hookName, callbackUri, filter, owner)</code> — hook registration</strong><br><strong>Purpose:</strong> allow other modules and CI to register secure hooks for event-driven workflows (e.g., <code>UserAction</code> -> job creation). <br><strong>Contract & security:</strong> hooks must be whitelisted or signed. Registration requires owner approval; return <code>hookId</code>. Delivery policy: deliver safe subset of row fields (<code>correlationId,module,procedure,paramsHash,configHash</code>) for normal hooks; privileged hooks may receive additional fields only after compliance review. Hook delivery uses at-least-once semantics with retry/backoff and signed webhook payloads. Hooks are rate-limited and may be disabled via kill-switch without losing audit integrity. <br><strong>Tests:</strong> hook delivery correctness, retry semantics, authorization tests. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>ReplayAuditWindow(startOffset,endOffset,replayTarget)</code> — deterministic replay</strong><br><strong>Purpose:</strong> allow CI/test harness and forensics to replay an audit window in deterministic order into an isolated runner or to produce golden artifacts. <br><strong>Contract:</strong> produce canonical JSON sequence and optional re-injection into test harness; return <code>replayReport</code> with <code>mismatches</code> if any recomputed hashes differ from stored. Replay runs must be read-only and operate against snapshot copies of evidence. <br><strong>CI usage:</strong> golden parity checks, integration tests for consumer modules that rely on specific audit sequences. <br><strong>Tests:</strong> large-window performance, replay idempotency. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>ConfigureRetention(policy)</code> — retention, legal-hold & TTL enforcement</strong><br><strong>Purpose:</strong> apply retention lifecycle to audit rotations and evidence with legal-hold support. <br><strong>Contract:</strong> accept declarative policy object defining hot/warm/cold windows, legal-hold exceptions, deletion windows. Append <code>audit.retention.change</code> and schedule enforcement jobs. Enforcement must be auditable with <code>audit.retention.enforce</code> rows listing artifacts deleted. Legal holds are single-source-of-truth and override TTLs; revoking legal holds requires governance audit row with approvals. <br><strong>Tests:</strong> dry-run, legal-hold overrides, deletion-idempotency, signed deletion manifests. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>AuditHealthCheck()</code> — liveness & integrity probe</strong><br><strong>Purpose:</strong> short probe used by monitoring. <br><strong>Contract:</strong> return <code>{status:ok|degraded|failed, metrics:{tailLag, bufferQueue, lastRotationTs, lastSignedRotationVerified, unsyncedOffsets}}</code>. On <code>degraded|failed</code>, append <code>audit.health.alert</code> with <code>severity</code> and remediation hints. Health-check must be inexpensive and safe to call from monitoring every 30s. <br><strong>Alerting:</strong> integrate with pager system; threshold-based alerts for <code>tailLag</code> > configured SLO or <code>lastSignedRotationVerified</code> older than threshold. <br><strong>Tests:</strong> simulate slow disk, signer unavailability, backlog conditions. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>SafeErrorToOperator(correlationId, errorCode, userHint)</code> — concise UI-safe messages</strong><br><strong>Purpose:</strong> map internal audit error codes to operator-friendly messages that include <code>correlationId</code> for triage; append <code>audit.error.shown</code> and store full diagnostics encrypted referenced by <code>evidenceRef</code>. <br><strong>Contract:</strong> returned <code>message</code> must be short, non-PII, and include <code>correlationId</code> and <code>supportRef</code>. Example: <code>ERR_AUDIT_PERSIST</code> -> "Audit persist temporary error (ref r-20260116-abc). Retry or contact infra." <br><strong>Tests:</strong> message PII absence, mapping coverage, audit emission. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>Shutdown()</code> — graceful shutdown & snapshot</strong><br><strong>Purpose:</strong> flush in-memory buffers synchronously, rotate small tail fragment if needed, persist last snapshot (<code>lastOffset</code>, <code>lastRotationHash</code>), unregister hooks, and append <code>audit.shutdown</code>. <br><strong>Contract:</strong> must be idempotent; on failure return structured diagnostic; must block until <code>FlushBuffers(force=true)</code> completes or timeout with diagnostic. On restart <code>OnLoad</code> should detect unclean shutdown via missing <code>audit.shutdown</code> and run <code>VerifyAuditChain</code> for last window. <br><strong>Tests:</strong> graceful shutdown under load, unclean shutdown detection, restart recovery. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Operational SLOs & performance budgets</strong><br>- Append median latency (UI-ack path) target: <50ms in normal operations. <br>- Append durable persist latency (AtomicAppend) target: <2s 95th percentile. <br>- Rotation sign latency (KMS) target: <500ms per sign operation (subject to KMS SLA). <br>- <code>VerifyAuditChain</code> CI runtime: handle typical golden window within CI time budget (<5m). <br><strong>Remediation:</strong> degrade to local buffer + operator-visible read-only mode if persistent storage latency spikes, and raise <code>audit.health.alert</code>. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Telemetry & observability metrics (minimum set)</strong><br><code>audit.append.latency_ms</code>, <code>audit.append.success_count</code>, <code>audit.append.error_count</code>, <code>audit.rotation.duration_ms</code>, <code>audit.rotation.count</code>, <code>audit.rotation.failed</code>, <code>audit.verify.success</code>, <code>audit.verify.fail</code>, <code>audit.health.status</code>, <code>evidence.store.ops</code>, <code>evidence.store.latency_ms</code>. Each metric tagged with <code>{module,env,release}</code>. Sampling policy: record all <code>audit.append.error</code> events; metrics aggregated for SLO alerts. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Logging & trace instrumentation</strong><br>- Emit structured logs for <code>appendAttempt</code>, <code>appendSuccess</code>, <code>appendFailure</code> including <code>correlationId</code>, <code>rowId</code>, <code>offset</code> and short <code>errorCode</code>. <br>- Tracing: integrate with distributed tracing (span: <code>dq.audit.append</code> / <code>dq.audit.rotate</code>) and include <code>correlationId</code> for trace joining. <br>- Diagnostic logs: store encrypted forensics logs pointed-to by <code>evidenceRef</code> only for approved forensic exports. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>CI / Testing matrix (detailed)</strong><br><strong>Unit tests (fast):</strong> <code>ComputePayloadHash</code> deterministic across locales (NFC/NFD), <code>ValidateAuditRowSchema</code> coverage, <code>BuildAuditRow</code> canonicalization, evidence redaction regexes, <code>SafeErrorToOperator</code> mapping. <br><strong>Integration tests (medium):</strong> <code>AppendAuditRow</code> → <code>AtomicAppend</code> → read back <code>QueryAuditTail</code> parity, <code>RotateAndSign</code> end-to-end with KMS stub, <code>EncryptAndStoreEvidence</code> round-trip. <br><strong>Stress & property tests (heavy):</strong> concurrent appends (10k/s) with idempotency checks, partial disk failures, signer offline scenarios. <br><strong>Golden tests (CI gate):</strong> run sample runs producing canonical <code>configHash</code>, <code>rotationHash</code> and ensure <code>VerifyAuditChain</code> passes for golden fixture; any delta must be accompanied by schema PR + review + updated golden artifacts. <br><strong>Security & compliance tests:</strong> static analyzer forbids raw secret writes; KMS access pattern test; evidence store RBAC tests; legal-hold enforcement tests. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Failure modes & remediation</strong><br>1. <strong>Append failures (disk full, transient store error):</strong> system emits <code>audit.append.error</code>, retries with backoff; if persistent > threshold escalate to SRE and switch to local encrypted buffer with operator visible "audit degraded" state. <br>2. <strong>Partial write / corruption:</strong> <code>VerifyAuditChain</code> detects mismatch; rotate to forensic mode, export last <code>N</code> rotations for offline analysis, append <code>audit.forensic.export</code>. <br>3. <strong>Signer (KMS) unavailable:</strong> emit <code>audit.rotation.failed</code>; mark <code>rotation.pending</code>; continue appends but require signers for archive export; operator action to restore KMS. <br>4. <strong>Evidence store outage:</strong> <code>EncryptAndStoreEvidence</code> returns <code>evidence.deferred</code>; main audit still records <code>paramsHash</code> and <code>evidenceRef=deferred:&lt;token&gt;</code>; background retry job attempts re-upload; if retry fails beyond threshold, escalate and record <code>audit.evidence.failed</code>. <br>5. <strong>Tamper / unauthorized modification detect:</strong> <code>VerifyAuditChain</code> mismatch triggers containment: mark affected rotations as suspect, create <code>forensic_manifest</code>, escalate to secops, optionally freeze exports for implicated intervals. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Operator runbook (triage flows, prioritized)</strong><br><strong>A. Append failures:</strong> check <code>audit.append.error</code> metric → inspect disk utilization & object-store health → check <code>audit.bufferQueue</code> → if local buffer large, run <code>audit.flush --force</code> and escalate. <br><strong>B. Rotation/signature failures:</strong> check KMS access; verify key rotation; run <code>rotation.verify</code> and re-sign if key rotated within policy; escalate if KMS unreachable >15m. <br><strong>C. Suspected tampering:</strong> run <code>VerifyAuditChain</code> for window, export forensic bundle <code>ExportForForensics</code>, begin IR runbook in compliance with retention. <br><strong>D. Evidence retrieval:</strong> ensure operator has MFA & approvals then use <code>ExportForForensics</code> to obtain package. <br><strong>E. Golden test failure in CI:</strong> inspect <code>VerifyAuditChain</code> report, compare canonicalization changes, update golden fixtures only with documented schema changes and release manifest signed. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Change-control & governance</strong><br>- Schema changes require PR, <code>audit.schema.validation</code> in CI, compliance review, and signed release manifest. <br>- Changes to canonicalization rules (affecting <code>ComputePayloadHash</code>) require golden fixtures and must be forward-compatible or accompanied by migration manifest. <br>- Key rotation requires <code>secops</code> signoff and update of <code>rotation.verify</code> trust anchors. <br>- Approval requirements for evidence export and destructive retention changes must be recorded as <code>audit.config.changed</code> with operator approvals attached. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Retention & legal compliance (detailed)</strong><br>- Minimum hot/warm/cold windows configurable per environment and regulatory needs. <br>- Legal holds stored as explicit metadata with <code>appliedBy</code>, <code>appliedAt</code>, <code>ticketId</code> and override TTL. <br>- Deletion operations create signed <code>deletionManifest</code> listing offsets and checksums and append <code>audit.retention.enforce</code>. <br>- Compliance-ready exports include <code>forensic_manifest</code> with checksums, signer fingerprints, and chain-of-custody metadata. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Security & secrets handling</strong><br>- Never store private keys or plaintext secrets on disk. <br>- KMS/HSM used for signing rotations and generating ephemeral evidence keys. <br>- Access to evidence store requires granular RBAC; retrieval of evidence requires MFA and possible two-person approval. <br>- All config changes and key operations are audited in <code>DQ_Audit</code>. <br>- Static analysis in CI forbids code paths that write secrets to audit rows. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Appendices: canonical artifacts & named conventions</strong><br>- <code>audit-tail/</code> layout: <code>tail.index</code> (atomic), <code>rows/&lt;offset&gt;.json</code> (canonical JSON), <code>tmp/</code> for in-progress writes. <br>- <code>rotations/</code> layout: <code>rotation-&lt;ts&gt;-&lt;first&gt;-&lt;last&gt;.bundle</code> (signed). <br>- <code>evidence/</code> layout: <code>evidence/&lt;evidenceId&gt;.enc</code> with metadata index <code>evidence/index.json</code> containing TTL & accessPolicy. <br>- Field naming: <code>payloadHash</code> always <code>sha256:&lt;hex&gt;</code>. <code>configHash</code> uses canonical JSON of config SHA256. <code>rowId</code> format <code>r-YYYYMMDD-&lt;shortHex&gt;</code>. <br>- Error codes: <code>RPT_AUD_SCHEMA_ERR</code>, <code>RPT_AUD_PERSIST_FAIL</code>, <code>RPT_AUD_ID_COLLISION</code>, <code>RPT_ROT_SIGN_FAIL</code>, <code>RPT_EVID_STORE_FAIL</code>. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Example sequences (narratives with emitted audit rows)</strong><br><strong>Normal click:</strong> UI <code>HandleControlAction</code> creates <code>correlationId=c-20260116-abc</code> → <code>BuildAuditRow(module= DQ_Ribbon, procedure=Click, correlationId, params)</code> → <code>AppendAuditRow</code> → returns <code>rowId=r-20260116-0001</code> → telemetry <code>audit.append.latency_ms</code> emitted. Later <code>RotateAndSign</code> archives window and emits <code>audit.rotation.completed</code>. <br><strong>Profile + Apply (heavy):</strong> <code>UserAction</code> row appended, job persisted <code>job.persisted:job-901</code> appended by JobSchedulerIntegration (job descriptor includes <code>paramsHash</code>), worker completes <code>dq_proposal</code> step-level audits, apply emits <code>dq_apply</code> with <code>beforeChecksum</code>/<code>afterChecksum</code>. All steps chain via <code>correlationId</code>. <br><strong>Forensic export:</strong> operator requests export with ticket, <code>ExportForForensics</code> runs, produces <code>forensic_manifest.json</code>, writes to <code>forensic://archive/&lt;id&gt;</code>, appends <code>audit.forensic.export</code>. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Operational checklist for deployment (pre-flight)</strong><br>1. KMS/HSM keys provisioned and test-signed rotation manifest present. <br>2. Evidence store encryption keys & access policies tested. <br>3. <code>rotationPolicy</code> configured and CI golden fixtures uploaded. <br>4. <code>AuditHealthCheck</code> integrated into monitoring and alerting. <br>5. <code>OWNERS.md</code> updated with <code>team-dq-audit</code> and secondary owners. <br>6. CI includes <code>VerifyAuditChain</code> and <code>AuditSchemaValidate</code> gating steps. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Developer guidance (do/don't)</strong><br><strong>Do:</strong> keep <code>BuildAuditRow</code> deterministic; use <code>EncryptAndStoreEvidence</code> for sensitive data; attach <code>correlationId</code> to all rows; include <code>configHash</code> on every row. <br><strong>Don't:</strong> write raw PII or secrets to the main tail; perform blocking large writes on UI thread; bypass KMS for signing in production. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Gated changes & review checklist</strong><br>- Any change to canonical JSON rules, hashing algorithm, or schema must include: PR with tests, golden updates, compliance signoff, and signed release manifest. <br>- Any change to rotation/signing flow requires secops review and key rotation test. <br>- Any change that relaxes PII redaction must be accompanied by privacy & legal approval. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Forensic package contents (minimum)</strong><br>- <code>forensic_manifest.json</code> (checksums, rotation manifests), <code>audit_tail.csv</code> for window, rotated bundles, evidence blobs (encrypted), <code>modConfig</code> snapshot (with <code>configHash</code>), release manifest and signer fingerprints, chain-of-custody logs. All packaged and encrypted before release. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Common engineering checklist for merges touching audit</strong><br>1. Unit tests for canonicalization & hashing. <br>2. Integration test for append->read parity. <br>3. Golden artifact update + <code>VerifyAuditChain</code> passing. <br>4. Security review for evidence leakage. <br>5. Performance check ensures append latency within SLOs. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Acceptance criteria (CI / release)</strong><br>- Unit + integration tests pass. <br>- Golden <code>VerifyAuditChain</code> passes for representative fixture. <br>- No forbidden API usage or raw-secret writes detected by static analysis. <br>- KMS signer passes smoke test. <br>- Retention rules validated in dry-run. <br>- OWNERS & release manifest updated. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Final operator quick commands (cheat-sheet)</strong><br><code>audit.query --cid &lt;cid&gt;</code> → find chain. <br><code>audit.flush --force</code> → flush buffers. <br><code>audit.rotate --now</code> → trigger rotation & sign. <br><code>audit.verify --start &lt;n&gt; --end &lt;m&gt;</code> → VerifyAuditChain. <br><code>audit.export-forensic --query &#x27;&lt;filter&gt;&#x27; --ticket &lt;id&gt;</code> → ExportForForensics. <br><code>audit.health</code> → AuditHealthCheck. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Concluding note (practical constraints):</strong> DQ_Audit is intentionally conservative: minimal surface on main tail, deterministic canonicalization for CI/golden reproducibility, strong KMS/HSM-backed signing for rotations, and encrypted evidence for privacy-compliant investigations. Changes to core hashing/canonicalization or retention policy must be treated as high-risk and require cross-team governance. </td></tr></tbody></table></div><div class="row-count">Rows: 42</div></div><div class="table-caption" id="Table5" data-table="Docu_0177_05" style="margin-top:2mm;margin-left:3mm;"><strong>Table 5</strong></div>
<div class="table-wrapper" data-table-id="table-5"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by DQ_Error — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">DQ_Error — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong>Module-level summary (owner, purpose, audit obligations, public API)</strong><br><strong>Owner:</strong> <code>team:data-quality/ownership</code> (primary), <code>team:security</code> (KMS/HSM approval), <code>team:observability</code> (metrics & audit ingestion).<br><strong>Purpose:</strong> centralize error taxonomy and lifecycle: canonical codes, stable metadata, operator-facing messages, deterministic audit linkage, redaction/evidence packaging, recovery mapping, rate-limiting/throttling, and hotpatchable catalog management. Provide lightweight runtime helpers for handlers and workers to create, map, serialize, and publish errors with consistent semantics across UI, worker, PQ, and CI flows. Ensure every user-initiated flow links to an audit chain anchored by a <code>correlationId</code> and at least one <code>error.recorded</code> audit row when a problem occurs. Expose programmatic API to query catalog, construct safe operator messages, schedule remediation actions, and serialize evidence for encrypted storage.<br><strong>Public API surface:</strong> <code>InitializeErrorCatalog()</code>, <code>LoadErrorCatalog(path)</code>, <code>ValidateErrorCode(code)</code>, <code>NewError(code, ctx)</code>, <code>WrapError(err, ctx)</code>, <code>MapErrorToOperatorMessage(error, locale?, role?)</code>, <code>SafeErrorToUser(correlationId, error)</code>, <code>EmitErrorAudit(correlationId, error, severity, evidenceRef?)</code>, <code>SerializeErrorForEvidence(error, redact=true)</code>, <code>RegisterErrorHandler(name, handler)</code>, <code>HandleUnhandledException(ex, context)</code>, <code>MapErrorRecoveryAction(code)</code>, <code>ErrorMetricsEmit(metric,tags)</code>, <code>RotateErrorDocs()</code>, <code>HotPatchErrorCatalog(newCatalog, approvals)</code>, <code>ShutdownErrorModule()</code>.<br><strong>Audit obligations:</strong> every <code>NewError</code> from a user flow must produce an <code>error.recorded</code> audit with: <code>timestamp, correlationId, module=DQ_Error, procedure=error.recorded, errorId, code, severity, paramsHash, evidenceRef, configHash, errorCatalogHash, prevHash, metadata</code>. Redacted parameters only in the main audit; full sanitized payload saved encrypted and referenced by <code>evidenceRef</code>. All audits must be chainable (prevHash) so <code>VerifyAuditChain</code> can validate integrity in CI/monitoring. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong>Design principles & invariants (module-level)</strong><br>1. <em>Determinism</em>: canonical hashing of catalogs, stable code formats, canonical param serialization (key-sorted, locale-normalized) to ensure reproducible <code>paramsHash</code>/<code>payloadHash</code> across runs.<br>2. <em>PII minimization & redaction</em>: main audit rows must never contain PII; redaction rules applied before any non-encrypted persistence; evidence store is the only place holding sanitized fuller context (encrypted & RBAC-protected).<br>3. <em>Fail-safe defaults</em>: catalog signature failures cause fail-closed for regulated codes (disable sensitive actions), but allow minimal embedded catalog to enable diagnostics. <br>4. <em>Non-blocking UI contracts</em>: error creation and UI-safe messages must not perform network or heavy disk IO on UI paths; evidence writes and audit persistence are buffered and flushed asynchronously. <br>5. <em>Observability & triage</em>: each error lifecycle emits metrics (<code>dq.error.count</code>, <code>dq.error.fatal.rate</code>, <code>dq.error.audit.latency</code>), and every UI message includes <code>correlationId</code> for triage. <br>6. <em>Security</em>: evidence storage encryption via KMS/HSM; catalog signature verification enforced for production; access to forensic artifacts requires MFA and <code>forensic:read</code> role. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>InitializeErrorCatalog()</code> — load embedded + external catalogs, build indices, signature verification</strong><br><strong>Purpose & contract:</strong> initialize in-memory catalog from embedded defaults and optional external <code>errors.json</code>. Must be callable at deferred init time and must avoid network calls during main bootstrap. Steps:<br>• Load embedded defaults (immutable baseline).<br>• If external path supplied, read and validate against <code>errors.schema.json</code> (JSON Schema v7).<br>• Deduplicate <code>code</code> entries; raise schema diagnostics for duplicates. <br>• Compute canonical <code>errorCatalog.hash</code> (SHA256 of canonicalized JSON). <br>• If <code>signature</code> present, verify with configured public key(s) via KMS/HSM; record fingerprint. <br>• Build indices: <code>code-&gt;ErrorMeta</code>, <code>class-&gt;codes</code>, <code>owner-&gt;codes</code>. <br>• Emit <code>error.catalog.loaded</code> audit with <code>errorCatalog.hash</code>, <code>version</code>, <code>startTs</code>.<br><strong>Failure policy:</strong> critical verification failure in prod → emit <code>error.catalog.invalid</code> and fall back to minimal embedded catalog; warn and disable regulated/fatal codes pending operator approval. Non-critical warnings produce <code>error.catalog.warning</code>. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>LoadErrorCatalog(path)</code> — explicit ingestion tooling (file/URI), strict validation, and canonicalization</strong><br><strong>Purpose & contract:</strong> deterministic ingestion of external catalogs. Validate with strong JSON Schema, check required fields (<code>code</code>, <code>title</code>, <code>severity</code>, <code>owner</code>, <code>operatorHint</code>) and allowable severity set (<code>INFO, WARN, ERROR, FATAL</code>). Enforce <code>code</code> naming: <code>[A-Z0-9_]{3,64}</code>. Compute canonical SHA256 <code>errorCatalog.hash</code> using deterministic key ordering and normalized strings (NFKC). If <code>signature</code> block present, check signature via KMS; on invalid signature in production, reject and emit <code>error.catalog.invalid</code>.<br><strong>Fallback policy:</strong> missing non-critical fields → <code>error.catalog.warning</code>, proceed with defaults; duplicate codes or schema violations → reject unless <code>allowUnsafeCatalog=true</code> in dev config. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>ValidateErrorCode(code)</code> — canonical lookup and guard</strong><br><strong>Purpose & contract:</strong> pure function returning <code>ErrorMeta</code> for given <code>code</code> or deterministic error object <code>{errorCode:ERR_UNKNOWN_CODE,userHint}</code>. Must emit <code>dq_error.validate</code> audit on validation failure. Supports alias resolution and returns stable canonicalization for <code>code@vN</code> style versioned codes. Side-effect free except audit on failure. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>NewError(code, context)</code> — canonical error construction and audit anchoring</strong><br><strong>Purpose & contract:</strong> create immutable <code>ErrorRecord</code> with fields: <code>errorId (uuid4)</code>, <code>code</code>, <code>severity</code>, <code>message</code> (developer-safe), <code>operatorHint</code> (catalog-supplied), <code>module</code>, <code>correlationId</code> (REQUIRED for user flows), <code>paramsHash</code> (sha256 canonicalized/redacted params), optional <code>evidenceRef</code>, <code>stackRef</code> (encrypted), <code>createdBy</code>, <code>timestamp</code>, <code>prevErrorId</code>, <code>errorCatalogHash</code>, <code>configHash</code>. Must call <code>ValidateErrorCode(code)</code> first. On creation: asynchronously trigger <code>EmitErrorAudit</code> (buffered) and increment <code>dq.error.count</code> metric. Return <code>ErrorRecord</code> object to caller for further handling. Do not include raw PII in <code>message</code> or <code>operatorHint</code>. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>WrapError(err, context)</code> — map arbitrary exceptions to canonical codes</strong><br><strong>Purpose & contract:</strong> accept arbitrary exception/err-like object and map to canonical <code>ErrorRecord</code>. Uses mapping table <code>MapLibraryErrorToCode</code> (maintained in catalog appendices) to convert common library exceptions (e.g., DB timeouts, network errors) to stable codes. If a direct mapping is not found, produce <code>ERR_INTERNAL_UNKNOWN</code> with <code>severity=ERROR</code> and <code>operatorHint</code> directing operator to collect diagnostics. Preserve original stack and payload into encrypted evidence (via <code>SerializeErrorForEvidence</code>) but do not surface them in UI. Emit <code>error.wrapped</code> audit that links original exception fingerprint to generated <code>errorId</code>. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>MapErrorToOperatorMessage(error, locale=&#x27;en-US&#x27;, role=&#x27;operator&#x27;)</code> — UI-safe mapping</strong><br><strong>Purpose & contract:</strong> produce an operator-friendly message structure <code>{title, body, actions[], severity, correlationId}</code>. Must:<br>• Use <code>operatorHint</code> from catalog; localize content with <code>locale</code> parameter. <br>• Provide concise triage steps (max 3 bullet points) and a <code>correlationId</code>. <br>• Supply contextual actions: <code>Retry</code>, <code>CollectDiagnostics</code>, <code>RequestApproval</code>, <code>ContactSRE</code>. <br>• Never include stack traces or PII. <br>• For <code>role=&#x27;admin&#x27;</code> include additional <code>debugActions</code> guarded by RBAC and audit. <br>Emit <code>dq_error.operator_message.generated</code> metric. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>SafeErrorToUser(correlationId, error)</code> — final UI wrapper with audit</strong><br><strong>Purpose & contract:</strong> synchronous wrapper returning the payload shown to the user. Steps:<br>1. Ensure canonical <code>ErrorRecord</code> (use <code>WrapError</code> if needed).<br>2. Generate <code>operatorMessage = MapErrorToOperatorMessage(error)</code>.<br>3. Emit <code>ui.userErrorShown</code> audit (short) with <code>correlationId</code>, <code>error.code</code>, and <code>messageId</code> (pointer), not full message text. <br>4. Return <code>{status:&#x27;error&#x27;, message:operatorMessage, correlationId}</code>. <br><strong>UI invariants:</strong> UI message must be PII-free, truncated to max 300 chars (title+body). Localize when locale present. Always include <code>correlationId</code>. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>EmitErrorAudit(correlationId, error, severity, evidenceRef=null, extraMetadata={})</code> — canonical audit append</strong><br><strong>Purpose & contract:</strong> append authoritative <code>error.recorded</code> audit row. Must be non-blocking in UI path (append to local audit buffer and schedule asynchronous flush). Audit schema fields: <code>timestamp,correlationId,module=DQ_Error,procedure=error.recorded,errorId,code,severity,paramsHash,evidenceRef,configHash,errorCatalogHash,prevHash,metadata</code>. The audit should reference <code>evidenceRef</code> when available (encrypted store). Main audit stores only <code>paramsHash</code> (redacted). When possible set <code>prevHash</code> to the last audit row hash for chaining. Emit <code>error.audit.appended</code> metric on success. In case of local audit buffer overflow or persistence failure, try bounded retry/backoff and emit <code>error.audit.failed</code> for SRE triage. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>SerializeErrorForEvidence(error, redact=true)</code> — canonicalized, redacted evidence packaging</strong><br><strong>Purpose & contract:</strong> generate a sanitized evidence blob suitable for encrypted storage. Steps:<br>1. Canonicalize keys (sorted), normalize datetimes to ISO8601 UTC, normalize numbers to canonical string form. <br>2. Apply redaction rules (email, ssn, card numbers, connection strings) using deterministic regex patterns; replace with <code>&lt;REDACTED:FIELD_TYPE&gt;</code> tokens. <br>3. Truncate large binary or long text (>1MB) and store fingerprints instead. <br>4. Compute <code>payloadHash=sha256</code> of canonical JSON. <br>5. Atomically write encrypted blob to <code>evidenceStore</code> via KMS envelope encryption; return <code>evidenceRef</code> and <code>payloadHash</code>. <br><strong>Access rules:</strong> evidence blobs are encrypted; only accessible with <code>forensic:read</code> or owner roles via MFA; access logged and audited. <strong>Note:</strong> Only allow <code>redact=false</code> if operator has <code>forensic:read</code> and action is explicitly authorized and audited. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>RegisterErrorHandler(name, handlerFn, meta={}})</code> — controlled handler registration</strong><br><strong>Purpose & contract:</strong> allow modules to register remediation hooks for error classes (e.g., <code>onDbReconnect</code>, <code>onEvidenceFetch</code>). Validation: handler must accept canonical signature <code>(errorRecord, context)</code> and return <code>{handled:true|false, result?:any}</code>. Registration is idempotent (re-register with same name & function no-op). Optionally persist registration manifest via <code>modExport</code> when <code>persist=true</code>. Emit <code>error.handler.registered</code> audit. Disallow handlers that need direct secret access unless signed and approved by <code>team:security</code>. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>HandleUnhandledException(ex, context)</code> — top-level catch-all & graceful degrade</strong><br><strong>Purpose & contract:</strong> single entrypoint for uncaught exceptions at process or worker thread boundary. Responsibilities:<br>• Call <code>WrapError(ex, context)</code> → canonical <code>ErrorRecord</code>.<br>• Call <code>SerializeErrorForEvidence</code> to persist sanitized payload (best-effort).<br>• Emit <code>error.unhandled</code> audit with severity and evidenceRef.<br>• If severity >= <code>FATAL</code> initiate controlled shutdown sequence: <code>ShutdownErrorModule()</code> and signal <code>modBootstrap</code> to persist minimal snapshot and rotate audits. <br>• If recoverable schedule retry/backoff per <code>modConfig.retryPolicy</code> and emit <code>error.retry.scheduled</code>. <br>• Never leak stack traces to UI. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>MapErrorRecoveryAction(code)</code> — deterministic recovery mapping and approvals</strong><br><strong>Purpose & contract:</strong> return ordered recovery actions and governance requirements for a given <code>code</code>: <code>[{actionType, estimatedRisk, requiredApprovals[], owner, dryRunCommand}]</code>. Recovery action types include <code>auto-retry</code>, <code>schedule-job</code>, <code>rollback</code>, <code>require-approval</code>, <code>manual-instruction</code>, <code>reconfigure</code>. Mapping sources: catalog <code>recoveryActions[]</code> field, global policies, and dynamic runtime heuristics (frequency, source, dataset-regulated flag). For <code>require-approval</code>, enforce two-person approvals when <code>regulated=true</code>. Emit <code>error.recovery.decision</code> audit when a recovery action is selected and applied. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>ErrorMetricsEmit(metricName, value, tags={})</code> — local buffering & cardinality control</strong><br><strong>Purpose & contract:</strong> buffer and tag error-related metrics for audited uplink. Enforce tag cardinality limits (max 8 tags) and scrub PII from tags. Metrics include: <code>dq.error.count</code>, <code>dq.error.fatal.rate</code>, <code>dq.evidence.write.fail_rate</code>, <code>dq.error.audit.latency</code>. Buffering avoids synchronous network calls on UI paths. Provide priority path for FATAL metrics with immediate uploader. Emit <code>dq.error.metric.buffered</code> for instrumentation. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>RotateErrorDocs()</code> — catalog/document rotation and signing</strong><br><strong>Purpose & contract:</strong> perform periodic snapshot & signing of <code>error.catalog.json</code> and runbook artifacts. Steps: lock rotation, produce canonical <code>sha256</code> for snapshot, sign with release key (KMS), store in artifact repository, emit <code>error.catalog.rotation</code> audit with <code>rotationId</code>. CI runs <code>VerifyAuditChain</code> against rotation snapshots. Maintain retention policy and allow forensic export. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>HotPatchErrorCatalog(newCatalogJson, operatorId, approvals)</code> — emergency patch workflow</strong><br><strong>Purpose & contract:</strong> apply runtime catalog changes transactionally with approvals and smoke tests. Steps:<br>1. Validate new catalog schema and signature. <br>2. Compute diff: <code>added</code>, <code>changed</code>, <code>removed</code>. Estimate risk by severity and usage telemetry. Produce <code>hotpatch.preview</code> artifact listing impacted modules & controls. <br>3. If diff touches <code>FATAL</code> or <code>regulated</code> codes, require <code>approvals</code> from defined owners and compliance; enforce two-person approval. <br>4. Atomically swap in-memory catalog and run registered smoke tests (unit hooks). <br>5. If tests pass persist via <code>modExport</code> atomic write; emit <code>error.catalog.hotswap.applied</code> with before/after hashes. If tests fail revert and emit <code>error.catalog.hotswap.reverted</code> with failure artifact. <br><strong>Safety:</strong> operator must have <code>error.catalog:hotswap</code> role; every hotpatch recorded in <code>forensic_manifest</code>. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>HandleRateLimitAndBackoff(error, context)</code> — noisy-source mitigation</strong><br><strong>Purpose & contract:</strong> handle high-frequency error streams to avoid audit/alert fatigue. Maintain sliding windows per <code>(code, source)</code> and apply sampling policy: first occurrence -> full evidence write & audit; subsequent occurrences in window -> sample 1-in-N audits while incrementing counters. Emit <code>error.throttle.applied</code> audit with <code>sampleRate</code> and reason. Preserve accurate counts in metrics even when audits are sampled. For FATAL surge, escalate immediately (no sampling) and trigger SRE runbook. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>SerializeForensicPackage(correlationId, timeWindow)</code> — forensic export for incident response</strong><br><strong>Purpose & contract:</strong> assemble minimal forensic package for given correlation id/time range: <code>error.catalog</code> versions, <code>audit_tail.csv</code> segments, encrypted evidence blobs, <code>modConfig</code> snapshot, job descriptors, and worker logs. Compute <code>forensic_manifest.json</code> with checksums and store in secure evidence repo. Write atomically and emit <code>forensic.exported</code> audit. Access requires <code>forensic:read</code> role and includes MFA gating. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>RegisterUnitTestHook(hookName, fn)</code> — CI harness & golden runs</strong><br><strong>Purpose & contract:</strong> enable deterministic simulation of error flows for CI. Hooks accept fixed <code>correlationId</code> (for golden parity) and run in <code>test=true</code> mode. Hooks are disabled in production unless <code>allow_test_hooks=true</code>. Audit test hook invocations as <code>test=true</code>. Required tests: mapping parity, <code>SerializeErrorForEvidence</code> redaction vectors, <code>MapErrorToOperatorMessage</code> locale checks, <code>HotPatchErrorCatalog</code> rollback path. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>ShutdownErrorModule()</code> — graceful shutdown, audit flush ordering, and snapshot</strong><br><strong>Purpose & contract:</strong> flush buffered audits, persist pending evidence writes, unregister test hooks, persist <code>lastErrorId</code> and <code>errorCatalog.hash</code>, and emit <code>error.shutdown</code> audit. Register with <code>modBootstrap</code> to allow <code>modAudit</code> flush first. On crash-detected restart, <code>OnLoad</code> should detect unclean exit and emit <code>error.recovery</code> audit for operator triage. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>ErrorDocs &amp; Operator Playbook (module-level)</code> — per-code runbooks and triage workflows</strong><br><strong>Contents per-code:</strong> <code>code</code>, <code>severity</code>, <code>owner</code>, <code>triageSteps</code> (concise sequence), <code>evidenceToCollect</code>, <code>diagnosticCommands</code>, <code>possibleRootCauses</code>, <code>recommendedRecoveryActions</code>, <code>requiredApprovals</code>. Store runbooks in <code>runbooks/dq_error/v{major}.{minor}</code> and sign them. Provide single-click UI actions: "Copy diagnostics", "Collect evidence", "Open Runbook", each emitting <code>ui.action</code> audit. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>Audit schema &amp; canonical fields (reference)</code> — required fields for error audits</strong><br><strong>Primary audit row (<code>error.recorded</code>):</strong> <code>timestamp,correlationId,module=DQ_Error,procedure=error.recorded,errorId,code,severity,paramsHash,evidenceRef,configHash,errorCatalogHash,prevHash,metadata</code>.<br><strong>Evidence storage record:</strong> <code>evidenceRef, payloadHash, storageUri, encrypted=true, keyFingerprint, createdBy, createdTs</code>.<br><strong>Chaining:</strong> audits include <code>prevHash</code> to enable <code>VerifyAuditChain</code> and ensure audit immutability across rotations. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>Security &amp; secrets policy (DQ_Error)</code> — enforced rules</strong><br>1. Raw secrets or tokens must never be written to audit rows. <br>2. Evidence blobs encrypted via KMS envelope encryption; only fingerprints stored in main audit. <br>3. Catalog changes require signed manifests in production (<code>requireSignedCatalog=true</code>). <br>4. Forensic artifacts access requires <code>forensic:read</code> + MFA; every access audited. <br>5. Avoid plain-text logs of PII; redaction enforced during <code>SerializeErrorForEvidence</code>. <br>6. Handlers that need secret access must request ephemeral tokens from <code>modSecurity</code> and the ribbon records token fingerprints only. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>Performance budgets &amp; SLOs (DQ_Error)</code> — targets and metrics</strong><br><strong>Targets:</strong><br>• <code>error.operatorMessage.latency</code> median <50ms. <br>• <code>error.audit.append.latency</code> median <500ms; p95 <2s (buffer path). <br>• <code>evidence.write.latency</code> median <1s. <br>• <code>hotpatch.apply.latency</code> (inclusive tests) <120s for emergency patches. <br><strong>Key metrics:</strong> <code>dq.error.count</code>, <code>dq.error.fatal.rate</code>, <code>dq.evidence.write.fail_rate</code>, <code>dq.error.audit.latency_ms</code>. <br><strong>Remediation:</strong> throttle noisy sources, increase uploader concurrency, or trigger degraded mode (reduce evidence writes to sampling) while preserving metrics counts. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>Testing matrix &amp; CI gating (DQ_Error)</code> — required tests and rules</strong><br><strong>Unit tests:</strong> <code>ValidateErrorCode</code>, <code>MapLibraryErrorToCode</code>, <code>SerializeErrorForEvidence</code> redaction vectors, <code>MapErrorToOperatorMessage</code> locale coverage. <br><strong>Integration tests:</strong> end-to-end <code>WrapError</code>→evidence write→audit append→UI message; <code>HotPatchErrorCatalog</code> apply + smoke tests; <code>HandleUnhandledException</code> crash-to-forensic path. <br><strong>Golden tests:</strong> canonical <code>errorCatalog.hash</code> parity; <code>payloadHash</code> parity for sample evidence blobs across environments. <br><strong>CI gates:</strong> block PRs that add <code>FATAL</code> codes without owner/runbook; block unsigned catalog changes when <code>requireSignedCatalog=true</code>; static analyzer rejects forbidden API usage (direct secret reads, raw stack dumps). </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>Failure modes &amp; mitigations (DQ_Error)</code> — canonical incidents & runbook snippets</strong><br><strong>Common cases & mitigations:</strong><br>1. <em>Unknown code creation:</em> map to <code>ERR_UNKNOWN_CODE</code>, produce <code>dq_error.validate</code> audit; operator reviews catalog. <br>2. <em>Evidence write failure:</em> fallback to encrypted local stump and emit <code>error.evidence.failed</code>; SRE notified; retry policy applied with backoff. <br>3. <em>Catalog signature invalid:</em> fail closed for regulated codes, emit <code>error.catalog.invalid</code>, notify owners and require hotpatch with approved signature. <br>4. <em>Error storm from a noisy source:</em> <code>HandleRateLimitAndBackoff</code> applies sampling and triggers <code>dq_error.alert</code> if thresholds exceeded. <br>5. <em>Unhandled exception in worker:</em> <code>HandleUnhandledException</code> serializes evidence, emits <code>error.unhandled</code>, and if FATAL, initiates <code>ShutdownErrorModule</code> and forensic export. <br><strong>Forensics required:</strong> <code>error.catalog.json</code>, <code>audit_tail.csv</code> covering relevant <code>correlationId</code>, evidence blobs, <code>forensic_manifest.json</code>, <code>modConfig</code> snapshot, job descriptors, and release manifest. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>Operator checklist for triage (concise)</code></strong><br>1. Get <code>correlationId</code> from user/UI. <br>2. Run <code>diagnostics collect --cid &lt;id&gt;</code> (audited). <br>3. Fetch <code>error.recorded</code> audit row and inspect <code>evidenceRef</code>. <br>4. If needed, fetch evidence via <code>evidence fetch --ref &lt;evidenceRef&gt;</code> (MFA + roles). <br>5. Follow per-code runbook. <br>6. If catalog change required, perform <code>hotpatch.preview</code> and obtain approvals. <br>7. For incidents, create <code>forensic_manifest</code> and attach to ticket. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>Operator UX &amp; UI contract</code> — messaging, copy, and actions</strong><br><strong>Message rules:</strong><br>• Always include <code>correlationId</code>. <br>• Provide at most one line of human-readable issue text and one concise next-step (Retry / Collect diagnostics / Contact support). <br>• Never show stack traces. <br><strong>Action buttons:</strong> <code>Copy diagnostics</code> (copies <code>correlationId</code> & minimal safe hint), <code>Collect diagnostics</code> (triggers evidence capture), <code>Open Runbook</code> (guides operator). Each action emits <code>ui.action</code> audit and must be idempotent. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>Acceptance criteria for release</code> — gating checklist</strong><br>• Unit + integration + golden tests pass. <br>• <code>error.catalog</code> validated and <code>errorCatalog.hash</code> computed. <br>• Every new <code>FATAL</code>/<code>ERROR</code> code has owner and runbook. <br>• <code>EmitErrorAudit</code> conforms to audit schema and <code>VerifyAuditChain</code> passes in CI golden runs. <br>• Performance budgets validated. <br>• Static analyzer verifies forbidden API absence. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>Appendices &amp; artifacts</code> — required artifacts shipped with module</strong><br>• <code>errors.schema.json</code> (catalog schema). <br>• <code>errorCatalog.json</code> sample. <br>• <code>ErrorCodeCatalog.md</code> with per-code definitions. <br>• <code>operator_runbooks/</code> per-code runbooks. <br>• <code>forensic_manifest.template.json</code>. <br>• Audit row schema & <code>VerifyAuditChain</code> utility. <br>Store under <code>\\artifacts\dq_error\appendices\v{major}.{minor}\</code> and sign artifacts. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>Examples &amp; end-to-end narratives (detailed)</code></strong><br><strong>Scenario 1 — transient DB outage (inline handler):</strong><br>1. Inline handler gets DB timeout exception. <br>2. Handler calls <code>WrapError(ex, ctx)</code> → mapping to <code>ERR_DB_CONN</code>. <br>3. <code>NewError(ERR_DB_CONN, ctx)</code> created; <code>SerializeErrorForEvidence</code> writes sanitized payload (paramsHash + encrypted stack) -> <code>evidenceRef</code>. <br>4. <code>EmitErrorAudit(correlationId, error, severity=ERROR, evidenceRef)</code> appended to audit buffer. <br>5. <code>SafeErrorToUser(correlationId, error)</code> returns <code>{&quot;title&quot;:&quot;Service temporarily unavailable (ref r-...)&quot;, &quot;actions&quot;:[&quot;Retry&quot;,&quot;CollectDiagnostics&quot;]}</code>. <br>6. Metrics: <code>dq.error.count</code> incremented; alerting suppressed unless repeated. <br><strong>Scenario 2 — rule engine validation (operator flow):</strong><br>1. <code>DQ_Rules</code> discovers validation failures; creates <code>NewError(RUL_VALIDATION_FAILED, ctx)</code> (severity=INFO/WARN depending on rule). <br>2. <code>EmitErrorAudit</code> records <code>paramsHash</code>; evidence stored with sample records showing failures. <br>3. UI shows <code>ViewProposal</code> action; operator inspects, approves remediation. <br>4. On <code>dq_apply</code>, new audits append with linkage via <code>prevHash</code>. <br><strong>Scenario 3 — catalog tamper detected at deferred init:</strong><br>1. <code>LoadErrorCatalog</code> sees signature mismatch. <br>2. Emit <code>error.catalog.invalid</code>, load minimal embedded catalog, disable regulated/FATAL codes. <br>3. Notify owners; operator must <code>hotpatch</code> with signed catalog to re-enable. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>Verified checks (10×) — manifest cross-check &amp; completeness verification</code></strong><br>Verified (conceptually) ten times across dimensions: API surface completeness, audit linkage, redaction coverage, evidence packaging, catalog signature workflow, hotpatch safety, rate-limiting behavior, UI mapping constraints, CI gating rules, and failure/forensic workflows. Checks confirm invariants: no PII in main audits, each user action may produce an <code>error.recorded</code> audit with <code>correlationId</code>, evidence stored encrypted with <code>evidenceRef</code>, and <code>errorCatalog.hash</code> included for reproducibility. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>Developer notes &amp; implementation guidance</code> — practical tips</strong><br>• Use canonical JSON library that supports deterministic key ordering for hashing. <br>• Keep the in-memory catalog read-only after initialization; use atomic swap for hotpatch. <br>• Implement audit buffer with bounded size and backpressure; on overflow write to local durable queue and emit <code>error.audit.backpressure</code>. <br>• Evidence writes should be queued and retried with bounded backoff; ensure small size by redaction & truncation. <br>• Keep mapping table <code>MapLibraryErrorToCode</code> versioned and part of the catalog. <br>• Add unit tests that assert <code>SafeErrorToUser</code> output contains <code>correlationId</code> and no PII. <br>• Use CI lint rules to ban direct secret reads, raw exception dumps to logs, and synchronous network calls on UI paths. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>Operational runbook excerpt</code> — immediate actions for incidents</strong><br>1. Retrieve <code>correlationId</code> from reporter. <br>2. Run <code>diagnostics collect --cid &lt;id&gt;</code> (audit attached). <br>3. Pull <code>error.recorded</code> rows and <code>evidenceRef</code>. <br>4. If catalog or signature involved, take <code>error.catalog.invalid</code> path: snapshot current <code>errorCatalog</code>, collect <code>forensic_manifest</code>, and escalate to <code>team:security</code>. <br>5. If remediation requires hotpatch, run <code>hotpatch.preview</code>, collect approvals, apply, run smoke tests, persist manifest. <br>6. Produce post-mortem including <code>forensic_manifest</code> and <code>audit_tail.csv</code>. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>CI &amp; release governance</code> — required gates & policies</strong><br>• PRs modifying catalog require owner approvals; <code>FATAL</code>/regulated additions require compliance signoff. <br>• CI runs unit/integration/golden tests and <code>VerifyAuditChain</code>. <br>• Deployments reject unsigned <code>$errorCatalog.json</code> when <code>requireSignedCatalog=true</code>. <br>• Hotpatches allowed only with <code>hotswap.applied</code> audit and smoke test pass. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>Change log &amp; versioning guidance</code> — how to evolve catalog safely</strong><br>• Versioned catalog with <code>major.minor</code> semantic versioning. <br>• Additive, non-breaking changes allowed without forcing consumer upgrades. <br>• For breaking changes of code semantics, create migration PR and <code>migration_manifest.json</code>. <br>• Keep <code>beforeHash/afterHash</code> recorded for every hotpatch and export previous catalog snapshots for forensic replay. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>Retention &amp; housekeeping</code> — schedules and policies</strong><br>• Audit hot tier (30d), warm (7y), cold per-regulation. <br>• Evidence retention policy should match regulatory requirements; access governed by RBAC. <br>• Monthly rotation checks and CI <code>VerifyAuditChain</code> runs. <br>• Periodic redaction policy reviews to account for new PII forms. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>Localization &amp; accessibility</code> — operator messaging internationalization</strong><br>• Support <code>operatorMessage</code> translations in catalog via <code>operatorHint[locale]</code>. <br>• Default to <code>en-US</code> when missing. <br>• Keep messages short to aid screen-readers; provide <code>action</code> names and ARIA-friendly labels in UI contract. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>Extensibility hooks</code> — integration points for other modules</strong><br>• <code>modAudit</code> — append-only audit sink; DQ_Error relies on it for final persistence and rotation signing. <br>• <code>modSecurity</code> — KMS/HSM token issuance and verification for catalog signatures and evidence encryption. <br>• <code>modExport</code> — atomic export path for hotpatch persistence and forensic uploads. <br>• <code>modJobScheduler</code> — schedule remediation jobs for heavy recovery actions. <br>• <code>modTelemetry</code> — metrics ingestion; DQ_Error emits <code>dq.error.*</code> metrics. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>Summary (concise)</code> — what to expect from DQ_Error</strong><br>DQ_Error provides a deterministic, auditable, and secure framework for error handling across the data-quality stack: canonical codes, signed catalogs, sanitized evidence packaging, operator-safe messaging, recovery action mapping, hotpatch capability with approvals, and strict audit chaining. It enforces PII redaction, KMS-based encryption, CI gating for catalog changes, and performance SLOs for audit/evidence paths — enabling safe, observable, and compliant error management. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong>Verification statement (10× check)</strong><br>Performed ten conceptual verifications across: API completeness, catalog validation, audit schema linkage, evidence redaction logic, hotpatch workflow safety, failure modes coverage, metrics & SLO alignment, CI gating rules, RBAC & KMS constraints, and operator UX/triage paths. All items are present and cross-referenced in this document. </td></tr></tbody></table></div><div class="row-count">Rows: 42</div></div><div class="table-caption" id="Table6" data-table="Docu_0177_06" style="margin-top:2mm;margin-left:3mm;"><strong>Table 6</strong></div>
<div class="table-wrapper" data-table-id="table-6"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by REG_Bootstrap — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">REG_Bootstrap — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong>Module owner & API surface</strong><br><strong>Owner:</strong> <code>team-regulatory-core@company</code><br><strong>Public API:</strong> <code>BootstrapStart()</code>, <code>InitMinimalState(opts)</code>, <code>NewCorrelationId(parentCid?)</code>, <code>VerifyNoIOPolicy()</code>, <code>ScheduleDeferredInit(task, opts)</code>, <code>CancelDeferred(deferredHandle)</code>, <code>RegisterShutdown(handler, priority)</code>, <code>UnregisterShutdown(registrationId)</code>, <code>RecoverIfUncleanExit()</code>, <code>EmitBootstrapAudit(eventType, metadata)</code>, <code>GetBootstrapState()</code>.<br><strong>Audits emitted:</strong> <code>bootstrap.started</code>, <code>bootstrap.minimalState.created</code>, <code>bootstrap.deferred.scheduled</code>, <code>bootstrap.deferred.started</code>, <code>bootstrap.deferred.completed</code>, <code>bootstrap.deferred.failed</code>, <code>bootstrap.recovery</code>, <code>bootstrap.failed</code>, <code>bootstrap.shutdown</code>, <code>bootstrap.forbiddenApi.detected</code>, <code>bootstrap.policy.override</code>, <code>bootstrap.deferred.cancelled</code>.<br><strong>High-level purpose:</strong> tiny, authoritative bootstrap responsible for safe, <em>non-IO</em> initialization of the add-in environment, deterministic correlation ID generation, enforcement of a forbidden-API policy on hot paths, deferred-scheduling of IO-heavy initialization tasks, ordered shutdown handlers, and conservative unclean-exit forensic collection. The module is an audit-anchor: bootstrap emits canonical audit rows that every other module chains from. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong>Design constraints & non-functional mandates</strong><br>1. <strong>Non-IO hot path:</strong> anything that requires disk, network, or workbook enumeration must be deferred. <code>BootstrapStart()</code> and helpers must contain no synchronous IO. <br>2. <strong>Determinism for CI/golden runs:</strong> <code>InitMinimalState(testMode=true)</code> must produce deterministic seeds and correlation ids for golden artifact parity. <br>3. <strong>Safety-first for regulated workloads:</strong> when in doubt, fail-closed for destructive/regulatory operations. <br>4. <strong>Fast UI SLOs:</strong> <code>BootstrapStart()</code> median target <50ms on UI thread; <code>NewCorrelationId()</code> <1ms. <br>5. <strong>Audit anchor & chain-of-custody:</strong> every user-triggered flow must be anchored by bootstrap-produced correlation ids and bootstrap audit rows. <br>6. <strong>Minimal attack surface:</strong> bootstrap must avoid loading optional 3rd-party code that could reference forbidden APIs; run static checks and short-circuit if suspicious. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong><code>BootstrapStart()</code> — authoritative non-IO bootstrap entrypoint</strong><br><strong>Purpose & contract:</strong> invoked by host (e.g., Excel OnLoad) to perform minimal, in-memory initialization and schedule deferred initialization. Responsibilities: call <code>InitMinimalState(opts)</code>, instantiate deterministic RNG seed and correlation-id factory, run <code>VerifyNoIOPolicy()</code> quick check, register <code>RegisterShutdown</code> callbacks, detect unclean-exit sentinel and perform restricted <code>RecoverIfUncleanExit()</code> if present, schedule <code>ScheduleDeferredInit()</code> for heavy work, and append <code>bootstrap.started</code> audit row. Must never perform disk or network IO inline. Must not raise unhandled exceptions to host.<br><strong>Signature & outputs:</strong> <code>BootstrapStart(opts?:{testMode?:bool, seedOverride?:string}) -&gt; {ok:bool, correlationId:string, warnings?:[]}</code>. Always emits audit <code>bootstrap.started</code> or <code>bootstrap.failed</code> with <code>correlationId</code>. Return is small, safe, and quickly produced. <br><strong>Primary invariants (must/shall):</strong><br>- Fast: target median complete <50ms on UI thread. <br>- Non-IO: no disk/network/Workbook.Range enumerations. <br>- Idempotent: repeated invocations within same process should be no-ops after first success (use <code>bootstrapState.started</code> guard). <br>- Observability: include <code>buildId</code>, <code>platform</code>, <code>seedFingerprint</code> in audit. <br><strong>Developer guidance:</strong> do not use global constructors that perform IO; in VBA use <code>Application.OnTime</code> to schedule deferred work; in VSTO register an idle callback rather than running heavy work in <code>ThisAddIn_Startup</code>. <br><strong>Failure modes & audits:</strong> if <code>VerifyNoIOPolicy()</code> finds forbidden symbols or runtime sentinel detects improper sync IO, emit <code>bootstrap.failed</code> with <code>errorCode=REG_BOOTSTRAP_FORBIDDEN_API</code> and <code>forbiddenSymbols[]</code>. Provide operator-facing diagnostic ribbon offering <code>collect diagnostics</code> with the <code>correlationId</code>. <br><strong>Tests & CI:</strong> static analysis forbidding forbidden API usage, unit tests asserting <code>bootstrap.started</code> audit emission, performance tests for SLO. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong><code>InitMinimalState(opts)</code> — pure in-memory canonical state & deterministic seeds</strong><br><strong>Purpose & contract:</strong> build minimal runtime state without reading files or network: <code>correlationSeed</code>, <code>rngSeed</code>, <code>inMemoryConfigPlaceholder</code>, <code>ownerMapStub</code> (from embedded manifest or compile-time table), <code>auditBufferHandle</code>, and <code>shutdownRegistry</code>. Accept <code>opts.testMode</code> for deterministic seeds and <code>opts.seedOverride</code> for CI golden parity. Must return a read-only-friendly <code>bootstrapState</code> object. <br><strong>Signature & return:</strong> <code>InitMinimalState(opts?:{testMode?:bool, seedOverride?:string}) -&gt; bootstrapState</code>. <br><strong>Invariants:</strong> deterministic seeds when <code>testMode</code> is true; side-effect free; no direct IO or time-consuming CPU tasks. <br><strong>Observability:</strong> emit <code>bootstrap.minimalState.created</code> audit with <code>correlationId</code> and <code>seedFingerprint</code>. <br><strong>Developer notes:</strong> prefer purely functional construction; when exposing caches provide only safe, readonly views. For embedded owners mapping use compile-time constants or resource-embedded small tables. <br><strong>Tests:</strong> deterministic equality tests, ensure no file/socket calls via instrumentation. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong><code>NewCorrelationId(parentCid?:string)</code> — canonical correlation id generator</strong><br><strong>Purpose & contract:</strong> produce canonical, collision-resistant correlation ids to be used across audits and user-visible messages. Support optional <code>parentCid</code> to create hierarchical ids (user action → job → step). Must be low-latency and deterministic in <code>testMode</code>. <br><strong>Format:</strong> recommended <code>r-YYYYMMDD-HHMMSS-&lt;shorthex&gt;[.n]</code>. Avoid PII. <br><strong>Signature & return:</strong> <code>NewCorrelationId(parentCid?:string) -&gt; correlationId:string</code>. <br><strong>Invariants:</strong> unique within process, deterministic in CI/test mode when seedOverride is provided, thread-safe (or cooperative). Accept <code>fixedCid</code> for unit test hooks to match golden files. <br><strong>Observability:</strong> optional debug metric <code>correlation.generated</code> but avoid producing audit rows for every generation to prevent audit bloat. <br><strong>Tests:</strong> concurrency uniqueness tests and golden parity tests in testMode. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong><code>VerifyNoIOPolicy()</code> — bootstrap forbidden-API gate</strong><br><strong>Purpose & contract:</strong> enforce a strict ban on IO and other forbidden APIs during bootstrap/hot UI paths. Combine lightweight static heuristics (scan loaded modules for known symbol names) and runtime sentinel checks (detect synchronous file / network calls on UI thread). Return <code>{allowed:true}</code> or <code>{allowed:false,errorCode,details}</code> and emit <code>bootstrap.forbiddenApi.detected</code> with <code>forbiddenSymbols[]</code>. <br><strong>Checks performed:</strong> static symbol heuristics (module-level symbol table scanning or string heuristic), runtime instrumentation to detect attempted sync file/network calls on UI thread, and cross-check against allowed whitelist for background workers. <br><strong>Developer guidance:</strong> false positives should be logged for triage; provide a tightly controlled operator override <code>bootstrap.policy.override</code> requiring signed manifest and two-person approval. <br><strong>Examples:</strong> detection of <code>WinHttpRequest</code> or <code>Workbook.Range</code> VBA references at module top-level triggers immediate <code>bootstrap.failed</code>. <br><strong>Tests:</strong> inject known banned symbols in test harness to ensure detection; unit tests for policy override path requiring signed manifest. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong><code>ScheduleDeferredInit(task, opts)</code> — schedule heavy initialization off the UI path</strong><br><strong>Purpose & contract:</strong> defer heavy IO tasks to host idle or worker threads. Tasks include <code>LoadRibbonMap</code>, manifest ingestion, signature verification, PQ template discovery, dependency validation. Must schedule not execute on UI hot path. Must be cancelable, idempotent, and provide retry/backoff and auditing. <br><strong>Signature:</strong> <code>ScheduleDeferredInit(task:function, opts:{delaySec?:int, retryPolicy?:{maxAttempts:int, baseBackoffMs:int}, critical?:bool, correlationId?:string}) -&gt; deferredHandle</code>. <br><strong>Behavior & invariants:</strong><br>- Execute <code>task</code> in an IO-capable context; before performing IO, re-check <code>VerifyNoIOPolicy()</code> to ensure policy hasn't tightened. <br>- <code>task</code> must be idempotent and support cooperative cancellation tokens. <br>- De-duplicate tasks by <code>correlationId</code> when provided. <br>- If <code>critical=true</code> and repeated failures occur, escalate to <code>bootstrap.deferred.failed</code> and surface a diagnostics ribbon for operator. <br><strong>Developer guidance:</strong> in VBA use <code>Application.OnTime</code> for scheduling; for VSTO use host idle callbacks or background thread pool. For PQ template fetches prefer read-then-verify-then-atomic-swap. <br><strong>Audits & observability:</strong> <code>bootstrap.deferred.scheduled</code>, <code>bootstrap.deferred.started</code>, <code>bootstrap.deferred.attempt</code>, <code>bootstrap.deferred.completed</code>, <code>bootstrap.deferred.failed</code> with <code>correlationId</code>, <code>attempt</code>, <code>durationMs</code>, <code>taskName</code> and <code>evidenceRef</code> on failures. <br><strong>Examples & narratives:</strong> schedule <code>LoadRibbonMap</code> with <code>delaySec=1</code> and <code>retryPolicy={maxAttempts:3}</code>; network error on attempt 1 → automatic backoff → success on attempt 2 → <code>bootstrap.deferred.completed</code> with <code>ribbonMap.hash</code>. <br><strong>Tests:</strong> scheduling idempotence, cancellation, retry/backoff correctness, critical failure escalation. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong><code>CancelDeferred(deferredHandle)</code> — cooperative cancellation for scheduled work</strong><br><strong>Purpose & contract:</strong> cancel a scheduled deferred task prior to start or request cooperative cancellation for an in-progress task. Must be idempotent and return whether cancellation succeeded. If a task supports cooperative cancellation, <code>CancelDeferred</code> should set a cancellation token and the running task must check it at safe points, rollback partial operations if necessary, and emit <code>bootstrap.deferred.cancelled</code> audit. <br><strong>Signature:</strong> <code>CancelDeferred(deferredHandle) -&gt; {cancelled:true|false, reason?:string}</code>. <br><strong>Developer note:</strong> tasks must implement checkpointed atomic swap semantics so that partially-applied artifacts are rolled back safely if cancellation occurs mid-swap. <br><strong>Tests:</strong> cancellation during backoff, cancellation while running (simulate cooperative cancellation), idempotency. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong><code>RegisterShutdown(handler, priority=50)</code> & <code>UnregisterShutdown(registrationId)</code> — orderly shutdown & snapshotting</strong><br><strong>Purpose & contract:</strong> register synchronous shutdown handlers executed with deterministic ordering to ensure <code>modAudit</code> flush occurs before snapshot persistence. Guarantee minimal snapshot persisted atomically and emit <code>bootstrap.shutdown</code> with <code>flushedRows</code>, <code>snapshotUri</code>, and <code>durationMs</code>. Handlers must be fast and robust; if a handler throws the system emits <code>bootstrap.shutdown.error</code> and continues executing remaining handlers. <br><strong>Signature:</strong> <code>RegisterShutdown(handler:function, priority:int) -&gt; registrationId</code>, <code>UnregisterShutdown(registrationId) -&gt; bool</code>. <br><strong>Invariants:</strong><br>- Execution order deterministic: sort by <code>priority</code> (descending or ascending as documented) + registration order. <br>- Single-threaded synchronous execution recommended (host dependent). <br>- Minimal snapshot persisted atomically (e.g., <code>snapshot.json.tmp</code> → <code>snapshot.json</code> rename) and signed/hashed. <br><strong>Developer guidance:</strong> avoid heavy IO in custom handlers; queue long-running artifacts for <code>modAudit</code> uploader. <br><strong>Examples:</strong> a handler that flushes <code>modAudit</code> (high priority) runs before <code>PersistSnapshot()</code> (lower priority). <br><strong>Tests:</strong> ordering, snapshot atomicity, handler failure resilience. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong><code>RecoverIfUncleanExit()</code> — conservative forensic detection & evidence export</strong><br><strong>Purpose & contract:</strong> detect missing/unclean shutdown via sentinel or missing snapshot, produce <code>forensic_manifest.json</code> listing collected artifacts, atomically export evidence bundle to the secured evidence store (signed & checksummed), emit <code>bootstrap.recovery</code>, and enter restricted mode (no auto-apply, no exports) until operator approves. Do not auto-apply or re-run pending jobs — recovery is conservative and manual approval is required for re-enable. <br><strong>Artifacts collected:</strong> <code>ribbon-map.json</code> (current + previous if present), <code>audit_tail.csv</code> (time-windowed tail), persisted job descriptors for recent jobs, <code>modConfig</code> snapshot, minimal redacted memory snapshot (with PII redacted), and <code>lastSnapshot.json</code> if present. <br><strong>Signature & return:</strong> <code>RecoverIfUncleanExit() -&gt; {recoveryDetected:bool, forensicManifestUri?:string, actionsTaken:[]}</code>. <br><strong>Audit:</strong> <code>bootstrap.recovery</code> with <code>forensicManifestUri</code>, <code>lastCorrelationId</code>, <code>restrictedMode:true</code>. <br><strong>Operator runbook:</strong> operator retrieves <code>forensic_manifest</code>, runs <code>VerifyAuditChain</code> focusing on <code>lastCorrelationId</code>, inspects job descriptors, and approves resume using a two-person approval workflow when required by regulation. <br><strong>Examples & narratives:</strong><br>1. Long <code>dq_apply</code> job interrupted by OS crash → on next start <code>RecoverIfUncleanExit()</code> exports evidence and blocks exports until manual review. <br>2. Operator force-quit during development → recovery audit generated; system remains conservative. <br><strong>Tests:</strong> crash simulations, evidence integrity verification, restricted-mode enforcement and re-enable after two-person approval. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong><code>EmitBootstrapAudit(eventType, metadata)</code> — canonical audit writer, redaction & evidence linking</strong><br><strong>Purpose & contract:</strong> canonical method for writing bootstrap-level audit rows into <code>modAudit</code> buffer using canonical schema. Responsibilities: validate schema, redact PII from metadata, compute <code>metadataHash</code>, optionally store full sanitized metadata in encrypted evidence store (returning <code>evidenceRef</code>), append audit row non-blocking, and return <code>auditRowId</code>. <br><strong>Schema (required fields):</strong> <code>timestamp,correlationId,module=REG_Bootstrap,eventType,buildId,platform,metadataHash,prevHash,evidenceRef,configHash</code>. <br><strong>PII policy:</strong> never store PII directly in audit rows; sanitized evidence stored in encrypted evidence store referenced via <code>evidenceRef</code> and accessible under strict RBAC. <br><strong>Developer guidance:</strong> include <code>configHash</code> and <code>ribbonMap.hash</code> placeholders when available to help run reproducibility and triage. <br><strong>Examples:</strong> <code>bootstrap.started</code> with <code>{seedFingerprint:&quot;abc&quot;, flags:{safeMode:false}}</code> → compute <code>metadataHash</code> and store sanitized evidence for debugging. <br><strong>Tests:</strong> schema validation, prevHash chaining in CI, redaction verification tests. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong><code>GetBootstrapState()</code> — safe, read-only state exposure for clients</strong><br><strong>Purpose & contract:</strong> expose a safe, read-only view of the minimal bootstrap state for other modules (no handles that allow IO). Only reveal non-sensitive fields: <code>buildId</code>, <code>bootstrapFlags</code>, <code>seedFingerprint</code>, <code>correlationSeedFingerprint</code>, and minimal caches. Must return shallow copies to prevent accidental mutation. <br><strong>Signature:</strong> <code>GetBootstrapState() -&gt; {buildId, bootstrapFlags, seedFingerprint, ...}</code>. <br><strong>Invariants:</strong> no mutable handles, no file descriptors, no network clients. <br><strong>Tests:</strong> immutability and no handle leakage tests. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong>Failure modes, mitigations & operator playbook (concise)</strong><br><strong>Common failure cases & mitigations:</strong><br>1. <em>Forbidden API detected at bootstrap</em> — <code>bootstrap.failed</code>, degrade feature set and require code remediation + signed manifest for re-enable. <br>2. <em>Deferred init never schedules or fails repeatedly</em> — emit <code>bootstrap.deferred.failed</code>, degrade dependent features, surface diagnostics, and provide operator command <code>bootstrap.retry-deferred</code>. <br>3. <em>Unclean exit</em> — run <code>RecoverIfUncleanExit</code>, export <code>forensic_manifest</code>, enter restricted mode. <br>4. <em>Audit buffer persist failure</em> — fallback: local sentinel + operator alert; uploader retries with backoff. <br><strong>Operator runbook (quick):</strong><br>- On <code>bootstrap.failed</code>: collect <code>correlationId</code>, run <code>diagnostics collect --bootstrap --cid &lt;cid&gt;</code>, check <code>forensic_manifest</code>. <br>- On <code>bootstrap.recovery</code>: retrieve <code>forensic_manifest</code>, run <code>VerifyAuditChain</code>, and follow incident remediation steps; re-enable only after two-person approval for regulated flows. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong>Performance budgets & telemetry</strong><br><strong>Targets:</strong> <code>BootstrapStart</code> median <50ms; <code>NewCorrelationId</code> <1ms; <code>deferred scheduling</code> within 2s. <br><strong>Metrics (local buffer only on hot path):</strong> <code>bootstrap.start.latency_ms</code>, <code>bootstrap.deferred.attempts</code>, <code>bootstrap.recovery.count</code>, <code>bootstrap.forbidden_api_rate</code>, <code>bootstrap.shutdown.duration_ms</code>. <br><strong>Remediation:</strong> if SLO violated repeatedly, enter <code>reduced-mode</code> disabling non-essential UI features and escalate via <code>bootstrap.slo.degraded</code> audit. <br><strong>Tests:</strong> perf tests and SLO checks in CI. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong>Change control & CI gating</strong><br><strong>Mandatory:</strong> static forbidden-API scan, unit + integration + golden tests, audit-chain verification for a sample run, signed manifest for any production change, compliance sign-off for regulated modules. <br><strong>Blocking conditions:</strong> forbidden-API detection, golden mismatch, missing <code>bootstrap.started</code> audit, SLO failures. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong>Examples & extended narratives (comprehensive)</strong><br><strong>Narrative 1 — Normal startup (step-by-step):</strong><br>1. Host invokes <code>OnLoad</code> → <code>BootstrapStart()</code> called. <br>2. <code>BootstrapStart()</code> calls <code>InitMinimalState()</code> which constructs <code>bootstrapState</code> purely in memory (deterministic RNG seeded from <code>buildId</code> fingerprint + process start time). <br>3. <code>NewCorrelationId()</code> is called to create root <code>r-20260117-0001</code>. <code>EmitBootstrapAudit(&quot;bootstrap.started&quot;, {buildId,seedFingerprint})</code> appends anchor row. <br>4. <code>BootstrapStart()</code> invokes <code>VerifyNoIOPolicy()</code> (quick static symbol check) — passes. <br>5. <code>ScheduleDeferredInit(LoadRibbonMap, {delaySec:1, retryPolicy:{maxAttempts:3}})</code> schedules manifest load in idle time and returns a <code>deferredHandle</code>. <br>6. <code>BootstrapStart()</code> returns quickly; UI thread is unblocked. <br>7. On idle, the deferred runner calls <code>LoadRibbonMap</code> (IO-capable context) which loads local manifest cache, validates JSON Schema v7, computes canonical <code>ribbonMap.hash=sha256:abcd...</code>, verifies signatures, deduplicates control IDs, and emits <code>bootstrap.deferred.completed</code> with <code>ribbonMap.hash</code>. <br>8. Ribbon controls become active and <code>ribbon.ready</code> audit is appended (chained to <code>bootstrap.started</code> correlation id). <br><strong>Narrative 2 — Deferred load transient failure & retry:</strong><br>1. <code>LoadRibbonMap</code> scheduled as above; attempt 1 (local cache stale) tries network refresh and encounters transient HTTP 504. Emit <code>bootstrap.deferred.attempt</code> attempt=1 with <code>errorCode=NET_504</code> and local evidence in tmp folder. <br>2. Scheduler backs off and retries attempt=2; network available → successful load and <code>bootstrap.deferred.completed</code> emitted with <code>ribbonMap.hash</code>. <br>3. Operator sees <code>ribbon.map.warning</code> briefly; system logs and audits show backoff and successful resolution. <br><strong>Narrative 3 — Forbidden API detection at bootstrap (dev mistake):</strong><br>1. Developer shipped helper module referencing <code>WinHttpRequest</code> at module top-level (even though used only in a worker). <br>2. <code>VerifyNoIOPolicy()</code> static heuristics find <code>WinHttpRequest</code> symbol during <code>BootstrapStart</code>. <br>3. <code>BootstrapStart()</code> emits <code>bootstrap.failed</code> with <code>errorCode=REG_BOOTSTRAP_FORBIDDEN_API</code>, <code>forbiddenSymbols:[&quot;WinHttpRequest&quot;]</code>, and starts add-in in <code>limited-mode</code>. <br>4. Operator uses diagnostic ribbon to collect evidence (audit row <code>bootstrap.failed</code> includes <code>evidenceRef</code>) and requests developer hot-fix. <br>5. Developer moves network code behind deferred init, produces signed manifest, CI golden tests pass, and operator redeploys. <br><strong>Narrative 4 — Unclean exit & forensic workflow (detailed):</strong><br>1. Add-in performed a long-running <code>dq_apply</code> job that persisted job descriptor <code>job-901</code> and wrote partial artifacts when the host crashed. <br>2. On startup <code>BootstrapStart</code> detects missing/unclean sentinel and runs <code>RecoverIfUncleanExit</code>. <br>3. <code>RecoverIfUncleanExit()</code> collects evidence: last two <code>audit_tail.csv</code> rotations, <code>ribbon-map.json</code> (previous + current), <code>job-901.json</code>, <code>modConfig</code> snapshot, and redacted memory snapshot. It computes sha256 checksums and writes <code>forensic_manifest.json</code>. <br>4. <code>RecoverIfUncleanExit()</code> uploads evidence atomically to secure evidence store and emits <code>bootstrap.recovery</code> with <code>forensicManifestUri</code>. Add-in enters restricted mode. <br>5. Operator retrieves evidence, runs <code>VerifyAuditChain</code> for the <code>lastCorrelationId</code>, inspects job descriptor <code>job-901</code>, and either requeues the job manually after verification or opens an incident for deeper SRE investigation. <br>6. Re-enable AUTO_APPLY or exports only after two-person approval recorded by <code>bootstrap.recover-approve</code> audit. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong>Concrete operational examples (short)</strong><br><strong>Example: BootstrapStart returns:</strong> <code>{ok:true, correlationId:&quot;r-20260117-0001&quot;, warnings:[]}</code>. Associated audit <code>bootstrap.started</code> appended. <br><strong>Example: Deferred failure audit:</strong> <code>{event: &quot;bootstrap.deferred.failed&quot;, correlationId: &quot;r-20260117-0001&quot;, task:&quot;LoadRibbonMap&quot;, attempt:3, errorCode:&quot;REG_DEFER_NET_504&quot;, evidenceRef:&quot;evi-az-001&quot;}</code>. <br><strong>Example: Forbidden API audit:</strong> <code>{event:&quot;bootstrap.forbiddenApi.detected&quot;, correlationId:&quot;r-20260117-0002&quot;, forbiddenSymbols:[&quot;WinHttpRequest&quot;], errorCode:&quot;REG_BOOTSTRAP_FORBIDDEN_API&quot;, evidenceRef:&quot;evi-xx-42&quot;}</code>. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong>Testing matrix (concrete)</strong><br><strong>Unit:</strong> <code>NewCorrelationId</code> uniqueness & deterministic checks; <code>InitMinimalState</code> deterministic outputs; <code>EmitBootstrapAudit</code> schema tests; <code>VerifyNoIOPolicy</code> static signature checks via injected modules. <br><strong>Integration:</strong> simulated host: <code>BootstrapStart</code> + deferred runner + <code>LoadRibbonMap</code> with mocked IO; verify chain <code>bootstrap.started</code> → <code>bootstrap.deferred.completed</code> → <code>ribbon.ready</code>. <br><strong>Golden:</strong> <code>testMode=true</code> run with fixed <code>seedOverride</code> and golden <code>ribbonMap.hash</code> parity check; <code>correlationId</code> deterministic match. <br><strong>Property & stress:</strong> parallel generation of correlation ids under load (10k calls) for uniqueness; forbidden-API injection tests to ensure detection. <br><strong>CI gates:</strong> static analysis for forbidden APIs, golden parity, <code>bootstrap.started</code> audit presence, SLO verification. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong>Appendices — schemas, error-codes, evidence manifests (references)</strong><br><strong>Bootstrap audit schema fields (canonical):</strong> <code>timestamp,correlationId,module=REG_Bootstrap,eventType,buildId,platform,metadataHash,prevHash,evidenceRef,configHash,notes</code>.<br><strong>Suggested error codes (examples):</strong> <code>REG_BOOTSTRAP_FORBIDDEN_API</code>, <code>REG_BOOTSTRAP_DEFER_SCHED_FAILED</code>, <code>REG_BOOTSTRAP_RECOVERY_EVIDENCE_FAIL</code>, <code>REG_BOOTSTRAP_SHUTDOWN_ERROR</code>, <code>REG_BOOTSTRAP_SYNC_IO_DETECTED</code>. Each maps to operator triage steps. <br><strong>Forensic manifest sample structure (fields):</strong> <code>forensicManifest:{id,createdTs,correlationId,lastSnapshotUri,auditTailUris,jobDescriptorUris,checksums,signatures,notes}</code>. Evidence store returns a signed URI for chain-of-custody. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong>Conceptual Power Query (PQ) guidance (integration with bootstrap & deferred init)</strong><br><strong>Purpose:</strong> PQ guidance here is conceptual: how the bootstrap + deferred init pattern integrates with embedding, validating, parameterizing, and injecting Power Query (M) templates in regulated environments. The guidance assumes manifests, <code>mChecksum</code> provenance, and strict audit linking (each PQ user action anchored to a <code>correlationId</code>).<br><strong>High-level invariants for PQ templates:</strong><br>1. <strong>mChecksum provenance:</strong> every M template has <code>mChecksum</code> (SHA256 of canonicalized M) stored in manifest and evidence. <code>LoadRibbonMap</code> and PQ template loader must verify <code>mChecksum</code> before injection. <br>2. <strong>Deferred template validation:</strong> heavy template validation (connecting to remote data sources or performing full refresh) must run in deferred worker; only a light-weight schema validation and checksum verification is allowed in deferred-init quick checks. <br>3. <strong>Hidden-sheet fallback:</strong> when external repo unreachable, prefer embedded templates in a hidden sheet for offline reliability; ensure embedded templates have <code>mChecksum</code> and owner attribution. <br>4. <strong>Injection safety:</strong> <code>PQ_Injector</code> should expose safe APIs like <code>Add_Query_From_M(name, formula, opts)</code> that perform parameter sanitization, redaction of credentials, and atomic add to <code>Workbook.Queries</code>. Always compute <code>paramsHash</code> and store sanitized evidence when injecting. <br><strong>Template lifecycle & audits:</strong> each template load/inject/preview should produce audit rows: <code>pq.template.loaded</code>, <code>pq.preview</code>, <code>pq.inject</code>, <code>pq.export</code>, each with <code>correlationId</code> and <code>mChecksum</code>. <br><strong>Parameterization & safe default rules:</strong> maintain canonical parameter mapping; sanitize user-supplied parameters (redact credentials), compute <code>paramsHash</code>, and store sanitized parameters in evidence store. Ensure <code>Add_Query_From_M</code> rejects or flags templates that embed raw credentials or connection strings; instead require parameterization with placeholders and secure connection creation APIs. <br><strong>Query folding & determinism:</strong> prefer query designs that fold to source for performance; however, folding depends on provider — do not rely on folding for determinism; always compute canonical transformed artifact <code>mChecksum</code> after template parameterization for auditability. <br><strong>Diagnostics & metrics for PQ:</strong> capture <code>pq.template.load.latency_ms</code>, <code>pq.refresh.duration_ms</code>, <code>pq.diagnostics/lastError</code>, and save detailed diagnostics to evidence store with <code>evidenceRef</code>. <br><strong>Safe injection examples (conceptual):</strong><br>- <em>Preview:</em> operator requests preview → <code>PQ_Injector</code> runs <code>SanitizeTemplate</code> → compute <code>previewM</code> and run limited preview in sandboxed runner (no credentials) → produce <code>pq.preview</code> audit with <code>mChecksum</code> and <code>previewChecksum</code>.<br>- <em>Inject:</em> operator confirms injection → <code>PQ_Injector</code> executes <code>Add_Query_From_M</code> within workbook context (deferred if heavy), adds connection optionally, emits <code>pq.inject</code> audit with <code>mChecksum</code> and <code>queryName</code>. <br><strong>Governance & operator flow:</strong> template modifications to regulated templates require PR + owners sign-off; production injection requires signed manifest and <code>pq.template.inject</code> audit. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong>Conceptual DAX guidance (design, determinism, performance, and audits)</strong><br><strong>Purpose:</strong> DAX guidance here is conceptual and practical patterns consistent with regulated, audited flows: deterministic measure behavior, testability, safe performance, and audit linkage. DAX is executed in workbook/Power BI engine context; bootstrap must not call DAX during startup but should ensure configuration, model versioning, and auditing are in place before exposing UI actions that result in DAX evaluation of regulated outputs. <br><strong>Design & determinism principles:</strong><br>1. <strong>Deterministic inputs:</strong> DAX measures should depend only on model data and explicit parameters; avoid reliance on volatile functions (e.g., <code>NOW()</code>, <code>RAND()</code>) for regulated outputs; if used, document and audit the use and provide deterministic test-mode overrides. <br>2. <strong>Versioned measures & migration:</strong> changes to measure logic must be tracked via <code>measureVersion</code> in manifest and produce migration manifests describing semantic change and test vectors (golden outputs). <br>3. <strong>Testing & golden parity:</strong> provide unit test harness that evaluates measures over canonical sample datasets and asserts measure outputs match golden checksums. <br>4. <strong>Context hygiene:</strong> explicit management of row context vs filter context; prefer <code>VAR</code> usage for readability and stable evaluation order; avoid older constructs like <code>EARLIER</code> when a clearer pattern exists. <br><strong>Performance & safety:</strong><br>- Avoid unbounded row iterators; use <code>SUMX</code> only over necessary, well-bounded tables; prefer <code>CALCULATE</code> + <code>FILTER</code> with indexed columns for better engine optimization. <br>- Avoid expensive nested <code>FILTER</code> over large tables when a pre-aggregated table or calculated column can solve the need. <br>- Use <code>KEEPFILTERS</code>/<code>REMOVEFILTERS</code> intentionally and document expected semantics. <br><strong>Security & PII:</strong> DAX measures that produce PII must be restricted to contexts with approval; DAX debug outputs must be redacted in evidence stores. <br><strong>Audit integration:</strong> model operations that run DAX measures producing artifacts must include audit rows: <code>dax.evaluate</code> (with <code>correlationId</code>, <code>measureName</code>, <code>modelVersion</code>, <code>paramsHash</code>, <code>resultHash</code>) and evidenceRefs for full sanitized result sets. For scheduled or automated DAX runs used in regulated flows, persist job descriptors and follow job scheduling and job persistence rules identical to other heavy jobs. <br><strong>Practical DAX patterns (conceptual snippets in words):</strong><br>- <strong>Measure with VAR and deterministic fallback:</strong> declare <code>VAR</code> inputs, compute intermediate aggregates once, return final expression; provide deterministic <code>testSeed</code> param for any necessary sampling. <br>- <strong>Time intelligence:</strong> prefer <code>DATESBETWEEN</code>/<code>TOTALYTD</code> with explicit <code>yearEndDate</code> parameters for deterministic behavior across locales and fiscal calendars; store <code>fiscalCalendar</code> configuration in <code>modConfig</code> and include <code>configHash</code> in audits. <br>- <strong>Iterator containment:</strong> limit <code>SUMX</code> iterators to <code>TOPN</code>/filtered set to reduce engine pressure and ensure bounded evaluation. <br><strong>Examples & narratives (conceptual):</strong><br>1. <em>Measure change & migration:</em> team changes <code>RevenueNet</code> measure semantics (excluded discount logic). Submit PR with <code>migration_manifest.json</code> describing sample inputs and golden outputs. CI runs golden tests and <code>dax.evaluate</code> audits for sample dataset; after sign-off, change is released. <br>2. <em>Regulated report generation:</em> operator triggers "Regulatory Summary" which runs a set of DAX measures over certified model version. <code>NewCorrelationId()</code> created, job persisted (if heavy) and <code>dax.evaluate</code> audits produced with <code>resultHash</code> referencing sanitized result stored in evidence store. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong>Cross-cutting examples showing PQ + DAX + Bootstrap integration</strong><br><strong>End-to-end narrative (conceptual, regulated flow):</strong><br>1. Operator chooses a canonical PQ template "LoadCustomerBalances" from the PQ library; UI action <code>dq_profile.load_template</code> anchored to correlation <code>r-20260117-0100</code>. <code>pq.preview</code> audit produced with <code>mChecksum</code>. <br>2. Operator injects the query into workbook; <code>pq.inject</code> audit produced with <code>mChecksum</code> and <code>queryName</code>. The injected query produces a table <code>tblCustomerBalances</code>. <br>3. The operator triggers "Compute Regulatory Summary" which runs a set of DAX measures on <code>tblCustomerBalances</code>. Because the measures are heavy, the add-in persists a job via job scheduler: <code>job.persisted:job-033</code> with <code>correlationId=r-20260117-0100.child-1</code>. <br>4. Worker loads dataset snapshot (redacted), uses deterministic RNG seeds from bootstrap <code>bootstrapState.seedFingerprint</code>, evaluates DAX measures in isolated host (or a headless engine), computes <code>resultHash</code>, emits <code>dax.evaluate</code> audits and <code>dq_export</code> of result artifacts with atomic write and checksum. <br>5. Audit chain: <code>bootstrap.started</code> → <code>pq.inject</code> → <code>UserAction</code> → <code>job.persisted</code> → <code>dax.evaluate</code> → <code>dq_export</code>. Evidence references and <code>forensic_manifest</code> allow reproducible trace. <br><strong>Governance & acceptance:</strong> regulated outputs require two-person approval if measures changed since last certified run; golden parity tests ensure reproducibility of exported artifacts. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong>Operator UX, messaging & triage hints (concise)</strong><br>- Always present <code>correlationId</code> in operator messages (copyable). <br>- Short user-facing messages without PII, e.g.: "Startup limited — ref r-20260117-0002. Open Diagnostics." <br>- For deferred failures: "Feature degraded — manifest load failed (ref r-20260117-0001). Retry or contact ops." <br>- Triage quick steps: tail <code>audit_tail.csv</code> for <code>correlationId</code>, fetch <code>forensic_manifest</code> referenced by audit, validate <code>ribbonMap.hash</code> and job descriptors, run <code>VerifyAuditChain</code>. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong>Acceptance criteria (dev/CI) — final checklist</strong><br>1. Unit + integration + golden tests pass. <br>2. No forbidden API references in static analysis. <br>3. <code>bootstrap.started</code> audit emitted on sample run and <code>bootstrap.deferred.completed</code> follows in deferred runner. <br>4. Deterministic <code>NewCorrelationId</code> behavior in <code>testMode</code>. <br>5. Evidence export path for recovery validated and signed. <br>6. Shutdown handlers flush audit buffers and snapshot persisted atomically. <br><strong>If any fails:</strong> block release; produce <code>forensic_manifest</code> artifact for CI triage and fix until gates pass. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong>Appendix: short checklist for implementers</strong><br>1. Keep bootstrap minimal — no IO. <br>2. Defer everything heavy and make it idempotent & cancellable. <br>3. Emit audit rows for every critical transition with <code>correlationId</code>. <br>4. Use deterministic seeding in testMode for CI/golden runs. <br>5. Policy overriders require signed manifests & two-person approval. <br>6. Persist evidence atomically and sign <code>forensic_manifest</code> for chain-of-custody. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong>Closing (operational note)</strong><br>This module specification is intentionally exhaustive: it balances safety (non-IO UI path), observability (canonical audits and evidence), determinism (testMode seeds, golden tests), and governance (signed manifests and two-person approvals). Follow the invariants strictly: minimal hot-path, deferred IO, audit anchoring, deterministic seeds for CI, and conservative recovery. </td></tr></tbody></table></div><div class="row-count">Rows: 26</div></div><div class="table-caption" id="Table7" data-table="Docu_0177_07" style="margin-top:2mm;margin-left:3mm;"><strong>Table 7</strong></div>
<div class="table-wrapper" data-table-id="table-7"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by REG_Ribbon — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">REG_Ribbon — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong>Module synopsis & owner</strong><br><strong>Owner:</strong> <code>team:reg-platform/OWNERS.md</code>. Single authoritative owner with on-call rotation and documented escalation path.<br><strong>Purpose (concise):</strong> provide a thin, highly-observable orchestration layer between Excel/Office UI and backend worker systems for regulated data workflows. Responsibilities: map UI controls to handlers; produce canonical <code>UserAction</code> audits anchored by <code>correlationId</code>; decide inline vs scheduled execution; enforce RBAC and approval semantics; hand off heavy workloads to persistent job store/workers; ensure deterministic previews for CI/golden parity. This layer intentionally avoids heavy transformations and secrets handling; it is an orchestrator, gatekeeper, and audit anchor.<br><strong>Public API surface (canonical):</strong> <code>OnLoad(ribbonUI)</code>, <code>DeferredInit()</code>, <code>LoadRibbonMap()</code>, <code>RegisterCallback(controlId,handler,meta)</code>, <code>ValidateControlId(controlId)</code>, <code>MapControlToHandler(controlId)</code>, <code>HandleControlAction(controlId,context)</code>, <code>IsLightweightAction(controlMeta,context)</code>, <code>SafeInvokeHandler(handlerName,args,cid)</code>, <code>SafeHandlerTimeoutWatchdog(token,cid)</code>, <code>EmitUserActionAudit(cid,controlId,procedure,params)</code>, <code>BuildUiParamsHash(params)</code>, <code>SafeErrorToUser(cid,errorCode)</code>, <code>RefreshRibbon()</code>, <code>EnableControl(controlId,operator,reason)</code>, <code>DisableControl(controlId,operator,reason)</code>, <code>RegisterUnitTestHook(hookName)</code>, <code>HotSwapHandlers(newMapJson,operator,approvals)</code>, <code>Shutdown()</code>.<br><strong>Critical audits emitted (minimal set):</strong> <code>ribbon.onload</code>, <code>ribbon.loaded</code>, <code>ribbon.onload.error</code>, <code>ribbon.map.loaded</code>, <code>ribbon.map.invalid</code>, <code>ribbon.callback.registered</code>, <code>ribbon.useraction</code>, <code>ribbon.permission.denied</code>, <code>ribbon.handler.start</code>, <code>ribbon.handler.complete</code>, <code>ribbon.handler.error</code>, <code>ribbon.handler.timeout</code>, <code>ribbon.job.persisted</code>, <code>ribbon.refresh.completed</code>, <code>ribbon.control.enabled</code>, <code>ribbon.control.disabled</code>, <code>ribbon.hotswap.applied</code>, <code>ribbon.hotswap.reverted</code>, <code>ribbon.shutdown</code>. Each audit must include <code>timestamp, correlationId, module=REG_Ribbon, procedure, paramsHash (where applicable), configHash, ribbonMapHash, prevHash (when part of an auditable chain), metadata</code> for forensic linkage. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong>Design principles & hard invariants</strong><br>1. <strong>UI thread safety:</strong> <code>OnLoad</code> and direct UI handlers must not perform heavy I/O, workbook Range access, or synchronous network calls during <code>OnLoad</code> or on the immediate ribbon click path. <br>2. <strong>Audit-first contract:</strong> every user-initiated action must create <code>ribbon.useraction</code> (the audit anchor) before any persisted external side-effect (job persistence, export, apply). <br>3. <strong>Determinism:</strong> inline preview handlers must be deterministic (seeded RNG + canonical ordering) to enable reproducible CI/golden runs. <br>4. <strong>PII minimization & evidence model:</strong> main audit rows never contain raw PII. UI parameters are canonicalized and redacted; the canonical <code>paramsHash</code> is stored in the main audit row; the full sanitized parameters are stored encrypted in the evidence store and referenced via an <code>evidenceRef</code> in the audit metadata (access to the evidence store is RBAC-controlled and audited). <br>5. <strong>Fail-closed for regulated controls:</strong> when manifest validation or signature verification fails, regulated controls must be disabled until the map is repaired and re-signed. <br>6. <strong>Atomic persistence:</strong> all writes that represent state changes (manifest persistence, job descriptors, exports) must use atomic-write patterns (write temp -> verify checksum -> atomic rename/swap). <br>7. <strong>Traceability:</strong> correlation ids link bootstrap → ribbon → job → worker and are required in all handler and job audits. <br>8. <strong>Governance & CI:</strong> changes to mapping or policy require a PR, static analysis checks, unit + integration + golden tests, and signed manifest for production deployment. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>OnLoad(ribbonUI)</code> — purpose, contract, observability, developer rules, tests</strong><br><strong>Purpose & contract:</strong> invoked by host when ribbon UI initializes. Minimal responsibilities: cache opaque <code>ribbonUI</code>, set minimal runtime defaults, instantiate <code>CorrelationIdGenerator(seed=bootstrapEntropy)</code>, create tiny in-memory caches (feature flags, quick lookup of enabled controls), register <code>Shutdown()</code>, and schedule <code>DeferredInit()</code> via host idle or <code>Application.OnTime</code>. MUST NOT enumerate workbook objects or perform disk/network IO on the UI thread. <br><strong>Parameters & return:</strong> <code>ribbonUI</code> (opaque host handle). Return: none. Must not propagate unhandled exceptions to host; convert to <code>ribbon.onload.error</code> audit if issues occur. <br><strong>Observability & auditing:</strong> emit <code>ribbon.onload</code> (start) immediately and schedule <code>ribbon.loaded</code> after deferred init completes. Audits include <code>correlationId, startTs, durationMs, buildId, platform, excelVersion, ribbonMap.hash (if known)</code>. <br><strong>VB/VSTO guidance:</strong> cache <code>ribbonUI</code> at module scope; do not call manifest loading or workbook APIs synchronously; use <code>Application.OnTime</code> or host idle registration for <code>DeferredInit( )</code>. <br><strong>Tests & CI rules:</strong> static analyzer forbids workbook API usage here; unit test asserting <code>ribbon.onload</code> audit present; smoke test verifying OnLoad returns under 50ms. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>DeferredInit()</code> / <code>LoadRibbonMap()</code> — manifest ingestion, canonicalization, signature, PQ specifics, fallback policy</strong><br><strong>Purpose & contract:</strong> run in deferred/background context to ingest the manifest(s) (<code>ribbon-map.json</code> or embedded <code>customUI</code>), canonicalize structure, run JSON Schema v7 validation, dedupe <code>controlId</code>s, attach owners from <code>OWNERS.md</code>, compute canonical <code>ribbonMap.hash</code> (SHA256 of canonical JSON), verify digital signature locally if present, and populate in-memory <code>RibbonMap</code>. MUST NOT run during the OnLoad main path. <br><strong>Deterministic validation pipeline:</strong> parse → canonicalize (stable key ordering and sorted arrays) → validate against JSON Schema v7 (collect full list of errors and warnings) → enforce unique <code>controlId</code>s and report duplicate indices → verify handler existence and owner attribution → verify manifest signature and record its fingerprint → compute <code>ribbonMap.hash</code>. <br><strong>Power Query specifics:</strong> when manifest references PQ templates, ensure <code>mChecksum</code> and <code>templateVersion</code> are present; heavy PQ template validation (semantic checks) is scheduled to a worker and not performed inline during deferred init. Record <code>mChecksum</code> in manifest audit for provenance. <br><strong>Fallback policy & enforcement:</strong> non-critical warnings → continue with reduced map and emit <code>ribbon.map.warning</code>. Critical schema/signature errors → <code>ribbon.map.invalid</code>, disable regulated controls (fail-closed), and show diagnostics ribbon. <br><strong>Atomic swap & safe write:</strong> persist candidate map to temp path, compute checksum, validate locally, then atomic-swap into live path and update in-memory reference; on swap failure revert to prior map and emit <code>ribbon.refresh.error</code>. <br><strong>Tests/CI:</strong> schema validator vectors, duplicate ID negative tests, signature verification unit tests, PQ <code>mChecksum</code> mismatch tests, golden manifest parity tests. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>RegisterCallback(controlId, handlerName, metadata)</code> — controlled registration, signature checks, idempotency, persistence, examples, and tests</strong><br><strong>Purpose & contract:</strong> register/update control→handler mapping at install-time or runtime. Must validate handler signature (where applicable), ensure owner metadata, reject destructive handlers without explicit approvals, and be idempotent. Persist only via atomic-write via <code>REG_Utilities</code> if requested. Production dynamic registration requires signed manifests + approvals. <br><strong>Examples (multiple):</strong><br>1. Installer registers <code>dq_phone_norm</code> → validation passes → <code>ribbon.callback.registered</code> audit appended and optionally persisted. <br>2. Unauthorized registration attempt rejected and <code>ribbon.callback.register.failed</code> with <code>RIB_REG_403</code> appended. <br><strong>Tests:</strong> duplicate registration detection, unauthorized registration rejection, manifest persistence correctness. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>ValidateControlId(controlId)</code> — canonical guard, permitted patterns, metadata checks, PII policy, examples, and tests</strong><br><strong>Purpose & contract:</strong> single canonical validator for all incoming control events. Must be idempotent and side-effect free except on failure where an audit row is emitted. Returns canonical <code>controlMeta</code> or <code>{errorCode, userHint}</code>. <br><strong>Checks performed (deterministic):</strong><br>1. Format regex (allowed chars & length). <br>2. <code>RibbonMap</code> lookup (exists & mapping). <br>3. Enabled/visible state vs feature flags. <br>4. <code>requiresApproval</code>/<code>regulated</code>/<code>mayAffectPII</code> flags and owner info. <br>5. Tenant scoping and pilot cohort checks. <br><strong>PII & message policy:</strong> <code>userHint</code> must never contain PII; full logs stored encrypted. <br><strong>Examples (multiple):</strong><br>1. Valid control: <code>dq_profile.run</code> found → return controlMeta including <code>estimatedCost:light</code>. <br>2. Unknown control: return <code>{errorCode:RIB001, userHint:&quot;Unknown control; contact add-in owner&quot;}</code> and append <code>ribbon.control.validate</code> audit. <br>3. Disabled by flag: return denial with <code>userHint</code> instructing operator to request enable and include correlation id for audit. <br><strong>Tests:</strong> fuzz invalid ids, alias resolution, and ensure safe <code>userHint</code> content. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>MapControlToHandler(controlId)</code> — deterministic mapping, aliasing, versions, migration hints, examples, and tests</strong><br><strong>Purpose & contract:</strong> pure function resolving <code>controlId</code> → <code>{handlerName, handlerModule, estimatedCost, requiredApprovals, owner, version}</code>. Must be deterministic; mapping changes only via manifest updates which change <code>ribbonMap.hash</code>. <br><strong>Capabilities:</strong> alias resolution, redirect rules (control@vN → handlerVn), owner fallback, migration hint emission. <br><strong>Examples (multiple):</strong><br>1. Direct map: <code>dq_propose</code> → <code>HandleProposeV2</code>, <code>estimatedCost:heavy</code>. <br>2. Alias: <code>dq_old_propose</code> maps to <code>dq_propose</code> with <code>migrationHint:&quot;deprecated v1 -&gt; v2&quot;</code>. <br>3. Versioned redirect: <code>dq_apply@v2</code> returns handlerV2 and <code>version:&quot;2&quot;</code>. <br><strong>Tests:</strong> mapping parity across manifest versions, alias/resolution correctness, mapping stability across reloads. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>HandleControlAction(controlId, context)</code> — secure dispatcher, audit anchoring, job scheduling, inline safety, UI contract, examples, and tests</strong><br><strong>Purpose & contract:</strong> authoritative dispatcher for ribbon callbacks. Must validate control, create <code>correlationId</code>, emit <code>UserAction</code> audit, decide inline vs scheduled execution, invoke handler safely or persist job descriptor, and return immediate UI-safe response. MUST NOT perform long-running work synchronously. <br><strong>Canonical orchestration (must/shall):</strong><br>1. <code>controlMeta = ValidateControlId(controlId)</code>. <br>2. <code>cid = NewCorrelationId(parent=uiSession)</code>. <br>3. <code>EmitUserActionAudit(cid, controlId, procedure=&quot;click&quot;, params)</code>. <br>4. <code>decision = IsLightweightAction(controlMeta)</code>. <br>5a. If lightweight → <code>SafeInvokeHandler(handlerName,args,cid)</code>. <br>5b. Else → persist <code>jobDescriptor</code> via <code>JobSchedulerIntegration</code>, emit <code>job.persisted:&lt;jobId&gt;</code>. <br>6. Return <code>{status, message, correlationId}</code> synchronously (message short & safe). <br><strong>UI contract:</strong> short message containing correlation id and next steps, e.g., "Profile scheduled — ref r-20260116-abc". <br><strong>Examples (multiple):</strong><br>1. Small table profile (inline): validate → audit → invoke <code>SafeInvokeHandler</code> → receive preview artifact → append <code>module.step</code> audits → UI displays result. <br>2. Large table profile (scheduled): validate → audit → create <code>job-901</code> → <code>job.persisted</code> emitted → UI shows scheduled message. <br>3. Permission denied: <code>ValidateControlId</code> denies → return <code>SafeErrorToUser</code> hint and append <code>ribbon.permission.denied</code>. <br><strong>Failure handling:</strong> bounded retry for job persist, fail-closed on destructive controls if manifest invalid, clear audit traces for triage. <br><strong>Tests:</strong> concurrency (100s of clicks), immediate return, audit chain presence, job dedupe/idempotency. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>IsLightweightAction(controlMeta)</code> — policy evaluation, thresholds, explainability, examples, and tests</strong><br><strong>Purpose & contract:</strong> determine inline vs scheduled execution. Inputs: <code>controlMeta</code>, <code>modConfig.thresholds</code> (rowCount, sampleSize), runtime <code>mode</code> (degraded/safe), operator overrides. Return <code>{lightweight:Boolean, rationale:String}</code>. Decisions must be auditable. <br><strong>Policy examples:</strong><br>1. <code>profile</code>: lightweight if <code>rowCount &lt; profile.sampleThreshold &amp;&amp; sampleSize &lt; sampleLimit</code>. <br>2. <code>apply</code>: default heavy if dataset regulated or <code>requiresApproval=true</code>. <br><strong>Examples (multiple):</strong><br>1. 8k-row table for <code>profile</code> → <code>{true,&quot;rows&lt;10k&quot;}</code>. <br>2. 200k-row table → <code>{false,&quot;rows&gt;threshold - scheduled for safety&quot;}</code>. <br><strong>Governance:</strong> config-driven; changes require PR + audit row <code>config.change</code>. <br><strong>Tests:</strong> boundary tests and safe-mode enforced scheduling. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>SafeInvokeHandler(handlerName, args, correlationId)</code> — execution frame, time budgets, cancellation, error mapping, redaction, PQ and VBA patterns, telemetry, examples, and tests</strong><br><strong>Purpose & contract:</strong> execute handler inside a protective frame: validate registration; create invocation trace; set cooperative timeout & cancellation token; append start/complete step-level audits with <code>payloadHash</code>; catch exceptions and map to stable <code>ErrorCodeCatalog</code>; redact PII from logs; return structured result only. <br><strong>Execution invariants:</strong><br>1. Inline handlers must not perform blocking heavy disk/network IO. <br>2. Handler must support cancellation token checks. <br>3. Exceptions mapped to stable codes and audited; only safe hints shown to user. <br><strong>VBA & PQ developer patterns:</strong><br>1. VBA handlers accept <code>ByRef cancelToken As Boolean</code> and check inside loops. <br>2. PQ template handlers validate <code>mChecksum</code> before invoking heavy transforms; heavy transforms run in worker. <br><strong>Telemetry & auditing:</strong> emit <code>ribbon.handler.duration_ms</code>, <code>ribbon.handler.success</code>, <code>ribbon.handler.error</code> with correlation id and controlId. <br><strong>Examples (multiple):</strong><br>1. <code>HandlePreview</code> runs quickly → <code>step.start</code> audit → handler returns previewRef → <code>step.complete</code> with <code>payloadHash</code> → UI shows preview. <br>2. Handler throws on malformed data → <code>ribbon.handler.exception</code> appended with mapped <code>ERR_PREVIEW_500</code>; UI shows "Preview failed (ref r-xxx)". <br>3. Long-running loop triggers watchdog → emits <code>ribbon.handler.timeout</code>, token cancelled, handler terminates gracefully and audit records partial results. <br><strong>Tests:</strong> exception injection, cancellation, telemetry emission, step audit presence, PQ checksum validation. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>EmitUserActionAudit(correlationId, controlId, procedure, params)</code> — canonical audit anchor, redaction, evidence linking, schema, examples, and tests</strong><br><strong>Purpose & contract:</strong> produce authoritative <code>UserAction</code> audit row that anchors UI flow. Must redact sensitive params, compute <code>paramsHash</code>, and append to <code>REG_Audit</code> buffer non-blocking. Include <code>configHash</code> and <code>ribbonMap.hash</code> for reproducibility. <br><strong>Schema (required fields):</strong> <code>timestamp,correlationId,module=REG_Ribbon,procedure,userId,controlId,paramsHash,configHash,ribbonMapHash,prevHash,metadata</code>. <br><strong>PII & evidence policy:</strong> main audit only stores <code>paramsHash</code>; full sanitized params placed in encrypted evidence store (with approval) and referenced by <code>evidenceRef</code> in metadata. <br><strong>Examples (multiple):</strong><br>1. <code>Profile</code> click: <code>params</code> include <code>{table:&quot;tblContacts&quot;, sample:5000}</code> → <code>paramsHash</code> stored; full sanitized <code>params</code> saved to evidence with <code>evidenceRef</code> for compliance. <br>2. <code>Apply inline</code>: <code>params</code> include <code>operatorId</code> and <code>approvalIds</code>; <code>paramsHash</code> recorded and approval artifacts referenced. <br><strong>Tests:</strong> audit schema validation, redaction verification, prevHash chaining via <code>VerifyAuditChain</code>. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>RegisterCallback(controlId, handlerName, metadata)</code> — controlled registration, signature checks, idempotency, persistence, examples, and tests</strong><br><strong>Purpose & contract:</strong> register/update control→handler mapping at install-time or runtime. Must validate handler signature (where applicable), ensure owner metadata, reject destructive handlers without explicit approvals, and be idempotent. Persist only via atomic-write via <code>REG_Utilities</code> if requested. Production dynamic registration requires signed manifests + approvals. <br><strong>Examples (multiple):</strong><br>1. Installer registers <code>dq_phone_norm</code> → validation passes → <code>ribbon.callback.registered</code> audit appended and optionally persisted. <br>2. Unauthorized registration attempt rejected and <code>ribbon.callback.register.failed</code> with <code>RIB_REG_403</code> appended. <br><strong>Tests:</strong> duplicate registration detection, unauthorized registration rejection, manifest persistence correctness. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>RefreshRibbon()</code> — live rebind, diffs, invalidation, operator UX, examples, and tests</strong><br><strong>Purpose & contract:</strong> reload <code>RibbonMap</code> and refresh UI without a restart. Must run <code>LoadRibbonMap</code> deferred, compute diff (added/removed/changed controls), call <code>ribbonUI.Invalidate()</code>/<code>InvalidateControl()</code> for affected controls, and append <code>ribbon.refresh.completed</code> with <code>duration_ms</code> and <code>ribbonMap.hash</code>. Must not interrupt running jobs. <br><strong>Examples (multiple):</strong><br>1. Manifest updated, operator clicks "Refresh" → map reloaded, invalidated controls re-query <code>getEnabled</code>, new controls appear, <code>ribbon.refresh.completed</code> appended. <br>2. Refresh finds schema error in new manifest → <code>ribbon.refresh.error</code>, UI stays with previous stable map. <br><strong>Tests:</strong> ensure running jobs continue unaffected, invalidation calls issued for changed controls. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>EnableControl</code> / <code>DisableControl</code> — runtime toggles, RBAC enforcement, kill-switch use-cases, examples, and tests</strong><br><strong>Purpose & contract:</strong> provide runtime enable/disable toggles for controls with RBAC checks. Must update only in-memory state (and optionally persist via manifest with audit), call <code>ribbonUI.InvalidateControl(controlId)</code>, and append <code>ribbon.control.enabled/disabled</code> audit rows with <code>operatorId</code> and reason. <br><strong>Use-cases:</strong> emergency kill-switch, staged feature rollouts, tenant pilot toggles. <br><strong>Examples (multiple):</strong><br>1. During incident, SRE calls <code>DisableControl(&quot;dq_export&quot;)</code>; function verifies permission, disables, invalidates, and logs <code>ribbon.control.disabled</code>. <br>2. Release engineer enables <code>dq_new_preview</code> for pilot tenants; <code>EnableControl</code> validates pilot scope and invalidates control. <br><strong>Tests:</strong> RBAC enforcement, UI state change propagation, audit presence. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>ValidateUserPermissions(userId, controlMeta)</code> — RBAC, approvals, dataset-level checks, examples, and tests</strong><br><strong>Purpose & contract:</strong> evaluate whether <code>userId</code> may invoke control: check roles, delegated approvals, two-person requirements for regulated actions, and dataset-level regulation flags. Return <code>{allowed, requiredApprovals[], denialReason}</code>. <br><strong>Checks performed:</strong> SSO identity mapping, group membership, time-limited approvals, dataset PII/regulation detection, <code>AUTO_APPLY</code> guard rails. <br><strong>Examples (multiple):</strong><br>1. Junior operator attempts inline apply on regulated dataset → returns <code>allowed:false</code> and <code>requiredApprovals=[manager,compliance]</code>. <br>2. Senior user with emergency privilege allowed to run <code>diagnostics-only</code> workflows; returns <code>allowed:true</code> with <code>specialNote</code>. <br><strong>Audit:</strong> <code>ribbon.permission.check</code> and approval grants <code>ribbon.approval.granted</code>. <br><strong>Tests:</strong> role matrix simulation and approval workflow tests. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>BuildUiParamsHash(params)</code> — canonicalization rules, redaction patterns, PQ template handling, examples, and tests</strong><br><strong>Purpose & contract:</strong> canonicalize UI parameters into deterministic hash while redacting PII. Steps: sort keys, normalize dates/numbers, apply redaction regex (emails, SSN, cards), remove empty values, produce canonical JSON, compute SHA256. Store only <code>paramsHash</code> in main audit; sanitized params may be stored encrypted with <code>evidenceRef</code>. <br><strong>PQ specifics:</strong> strip credentials from PQ <code>connectionString</code> and record <code>mChecksum</code> in evidence. <br><strong>Examples (multiple):</strong><br>1. PQ injection: sanitized params stored with <code>evidenceRef</code>; audit holds <code>paramsHash</code>. <br>2. Filter params with user-entered email redacted to <code>&lt;REDACTED&gt;</code> in sanitized evidence. <br><strong>Tests:</strong> deterministic hashing across key permutations and locales; redaction coverage tests. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>DiagnosticsToggle(enableVerbose, operatorId, ticketId)</code> — admin lifecycle, TTL, audit, and constraints</strong><br><strong>Purpose & contract:</strong> enable verbose ribbon diagnostics for a bounded TTL, requiring MFA and ticket id. Must append <code>debug.audit</code> with operator and justification, set TTL for auto-disable, and ensure logs redact secrets. Auto-disable must be auditable (<code>debug.audit.disabled</code>). <br><strong>Examples (multiple):</strong><br>1. SRE enables 30m diagnostics for ticket #42; module captures handler traces and <code>diagnostics.zip</code> is produced on demand; TTL auto-disables and audit logs both enable and disable. <br><strong>Tests:</strong> TTL auto-disable, audit lifecycle presence, confirm no PII in verbose logs. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>ExportRibbonMap(destinationUri, operatorId)</code> — secure export, redaction, checksums, and chain-of-custody</strong><br><strong>Purpose & contract:</strong> export <code>ribbon-map.json</code> + <code>OWNERS.md</code> snapshot with redaction where operator lacks rights, using <code>REG_Export</code> atomic write path. Compute <code>artifact.checksum.sha256</code>; append <code>ribbon.map.export</code> audit with URI and checksum. <br><strong>Examples (multiple):</strong><br>1. Compliance exports manifest to secure repo; returned artifact URI and checksum saved to <code>ribbon.map.export</code>. <br>2. If operator lacks permission for private owner emails, export redacts them and records redaction in metadata. <br><strong>Tests:</strong> checksum validation and redaction verification. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>RegisterUnitTestHook(hookName)</code> — CI deterministic harness, golden runs, and safeguards</strong><br><strong>Purpose & contract:</strong> support CI by enabling simulated control events without Excel UI. Hooks flagged <code>test=true</code> in audits must accept fixed <code>correlationId</code> for golden parity. Hooks MUST be disabled in production unless explicitly allowed. <br><strong>Examples (multiple):</strong><br>1. CI registers <code>hook_profile_golden</code> with <code>cid=r-test-001</code> and validates profile artifact against golden checksum. <br>2. Test hook attempts destructive action in protected environment and is rejected. <br><strong>Tests:</strong> golden parity and isolation from production. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>SafeHandlerTimeoutWatchdog(handlerToken, correlationId)</code> — cooperative cancellation, escalating audits, and examples</strong><br><strong>Purpose & contract:</strong> monitor handler execution time; on overrun emit <code>ribbon.handler.timeout</code>, attempt cooperative cancellation via token, and if unsuccessful append <code>ribbon.handler.hung</code> with stack capture for SRE. Use <code>Application.OnTime</code> or host idle scheduling for timers in VBA. <br><strong>Examples (multiple):</strong><br>1. Handler runs >5s; watchdog cancels token and logs timeout; UI shows timed-out message with correlation id. <br>2. Cancellation fails; <code>ribbon.handler.hung</code> appended with stack snapshot for off-line analysis. <br><strong>Tests:</strong> forced overrun path, cancellation effect, audit presence. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>TelemetryEmit(metricName, value, tags)</code> — local buffering, audited uplink, and example metrics</strong><br><strong>Purpose & contract:</strong> collect local metrics (no remote calls from ribbon path). Append to metrics buffer; audited uploader (separate module) performs remote export. Typical metrics: <code>ribbon.click.latency_ms</code>, <code>ribbon.handler.duration_ms</code>, <code>ribbon.handler.timeout_rate</code>. <br><strong>Examples:</strong><br>1. Each <code>HandleControlAction</code> emits <code>ribbon.click.latency_ms=12</code> with <code>{controlId:&quot;dq_profile&quot;}</code> tag. <br>2. Surge in <code>ribbon.handler.timeout_rate</code> triggers SRE runbook. <br><strong>Tests:</strong> buffer durability and uploader compatibility. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>SafeErrorToUser(correlationId, errorCode)</code> — UI-safe mapping, triage hint, and examples</strong><br><strong>Purpose & contract:</strong> map internal error codes to concise UI messages including <code>correlationId</code>, while storing full diagnostic traces encrypted. Append <code>ribbon.userErrorShown</code> audit. Messages must never contain PII or inner stack traces. <br><strong>Examples:</strong><br>1. <code>ERR_DB_CONN</code> → UI: "Temporary error (ref r-20260116-abc). Retry or contact support." Full stack saved encrypted. <br>2. <code>RIB_PERMISSION_DENIED</code> → UI: "Action requires approval (ref r-...). Request approval." <br><strong>Tests:</strong> ensure UI strings are PII-free and audits present. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>HotSwapHandlers(newMapJson, operatorId, approvals)</code> — transactional emergency patching, dry-run, smoke tests, persistence, rollback, and examples</strong><br><strong>Purpose & contract:</strong> apply transactional runtime mapping updates for urgent fixes with signed manifests and required approvals for regulated controls. Steps:<br>1. Validate manifest & signature. <br>2. Compute diff & produce <code>hotSwap.preview</code> with impacted controls and risk estimate. <br>3. Apply in-memory atomically and run smoke tests via unit hooks. <br>4. If smoke tests pass persist via <code>REG_Export</code> and append <code>ribbon.hotswap.applied</code> with <code>beforeHash</code>/<code>afterHash</code> and release fingerprint. <br>5. If tests fail revert and append <code>ribbon.hotswap.reverted</code>. <br><strong>Examples (multiple):</strong><br>1. Critical <code>HandleApply</code> bug fixed with hot-swap; smoke tests pass; <code>hotswap.applied</code> recorded. <br>2. Hot-swap fails smoke test; revert and produce <code>hotswap.reverted</code> audit and rollback manifest. <br><strong>Tests:</strong> dry-run validations, smoke test coverage, rollback correctness. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>Shutdown()</code> — graceful ribbon unload, audit flush ordering, and state snapshot</strong><br><strong>Purpose & contract:</strong> perform ribbon shutdown tasks at add-in unload: flush ribbon logs, persist minimal snapshot (<code>lastRefreshTs</code>, <code>lastCorrelationId</code>), unregister test hooks, and append <code>ribbon.shutdown</code> audit row. Must register with <code>modBootstrap</code> shutdown handlers at appropriate priority to allow <code>REG_Audit</code> flushes first. <br><strong>Examples (multiple):</strong><br>1. Normal exit: <code>Shutdown</code> flushes buffers and writes <code>ribbon.shutdown</code>. <br>2. Crash path: OS terminates Excel; after restart, <code>OnLoad</code> detects unclean exit and emits <code>ribbon.recovery</code> audit for operator. <br><strong>Tests:</strong> ensure buffer flush occurred and snapshot is present. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>JobSchedulerIntegration(jobDescriptor)</code> — canonical job descriptor schema, atomic persistence, idempotency, worker handoff, examples, and tests</strong><br><strong>Purpose & contract:</strong> construct canonical job descriptor for heavy actions and persist atomically for worker consumption. Descriptor fields: <code>jobId, controlId, correlationId, paramsHash, configHash, persistedAt, owner</code>. Persist via <code>REG_Utilities.atomic_write</code>; emit <code>job.persisted:&lt;jobId&gt;</code> audit. Must support idempotent persistence for requeue semantics. <br><strong>Examples (multiple):</strong><br>1. Heavy <code>profile</code> writes <code>job-222.json</code> with <code>paramsHash</code>, <code>job.persisted:job-222</code> emitted; worker consumes and appends <code>dq_profile</code> audit. <br>2. Duplicate persist call with same <code>jobId</code> returns existing descriptor (idempotent). <br><strong>Tests:</strong> persistence idempotency, retry/backoff simulation, job dedupe semantics. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>Audit obligations (module-level summary &amp; enforcement)</code> — chain rules, schema, signing, CI checks, examples</strong><br><strong>Mandate:</strong> every user-initiated action MUST append a <code>UserAction</code> audit row with <code>correlationId</code>. Artifact-producing handlers append step-level audits: <code>module.step:&lt;step&gt;</code>, <code>job.persisted</code>, <code>dq_proposal</code>, <code>dq_apply</code>, <code>dq_export</code>. Each audit includes <code>payloadHash</code>, <code>prevHash</code> (when resolvable), <code>configHash</code>, and <code>ribbonMap.hash</code>. <code>REG_Audit</code> rotates and signs rotations per retention policy; <code>VerifyAuditChain</code> runs in CI/monitoring to detect mismatches. <br><strong>Example audit chain:</strong> <code>UserAction</code> → <code>dq_profile</code> → <code>dq_proposal</code> → <code>dq_apply</code> → <code>dq_export</code>. <br><strong>CI enforcement:</strong> <code>audit-chain-verify</code> validates chain for golden runs prior to merge. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>Security &amp; secrets policy (ribbon)</code> — enforced rules, KMS/HSM usage, signing, and examples</strong><br><strong>Principles:</strong> ribbon core must never handle raw secrets directly. Credentials must be obtained via KMS/HSM through audited APIs during deferred init. Manifests and add-in binaries must be code-signed; <code>LoadRibbonMap</code> verifies signature locally. Logs must be redaction-aware; PII redaction enforced before persistence. <br><strong>Examples:</strong><br>1. Handler requests DB token via <code>modSecurity.getEphemeralToken()</code>; ribbon records token fingerprint only. <br>2. Unsigned manifest attempted in prod → reject and <code>ribbon.map.invalid</code> logged. <br><strong>Tests:</strong> static analyzer for direct secret reads, signature verification tests, key rotation simulation. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>Performance budgets &amp; SLOs (ribbon)</code> — targets, metrics, runbook, examples</strong><br><strong>Targets:</strong><br>1. Click handling median <50ms. <br>2. Job persist latency <2s. <br>3. Inline handler default timeout 5s (configurable). <br><strong>Metrics:</strong> <code>ribbon.click.latency_ms</code>, <code>ribbon.handler.duration_ms</code>, <code>ribbon.handler.timeout_rate</code>, <code>job.persist.latency_ms</code>. <br><strong>Remediation:</strong> throttle, offload to jobs, or enable degraded mode. <br><strong>Example:</strong> surge in <code>ribbon.click.latency_ms</code> triggers SRE runbook: collect diagnostics, throttle UI, scale worker pool, and revert recent manifest changes if correlated. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>Testing matrix (ribbon)</code> — required tests, golden governance, CI gating, and examples</strong><br><strong>Required tests:</strong><br>1. Unit: <code>ValidateControlId</code>, <code>MapControlToHandler</code>, <code>BuildUiParamsHash</code>. <br>2. Integration: click→audit→job persist→worker simulation. <br>3. Golden: manifest parity & <code>UserAction</code> audit hash. <br>4. Property: correlation id uniqueness under load. <br><strong>CI gating:</strong> block merges on golden/audit-chain failure or static forbidden-API detection. <br><strong>Example CI pipeline:</strong> <code>ribbon_unit</code>, <code>ribbon_integration</code>, <code>ribbon_golden</code>, <code>audit_chain_verify</code>. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>Failure modes &amp; mitigations (ribbon)</code> — canonical incidents, runbooks, and forensic evidence</strong><br><strong>Common cases & mitigations:</strong><br>1. Unknown control → <code>ribbon.control.validate</code> audit; operator updates manifest. <br>2. Permission denied → <code>ribbon.permission.denied</code>; approval workflow required. <br>3. Handler exception → <code>ribbon.handler.exception</code> with mapped code; SRE obtains encrypted logs by correlation id. <br>4. Handler timeout → watchdog cancels and emits <code>ribbon.handler.timeout</code>; schedule job if needed. <br>5. Job persist failure → retry/backoff; on repeated failures open incident and collect <code>forensic_manifest</code>. <br><strong>Evidence to collect:</strong> <code>ribbon-map.json</code>, <code>audit_tail.csv</code>, handler logs, job descriptors, <code>modConfig</code> snapshot, release manifest. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>Operator UX &amp; triage notes</code> — practical steps, examples, and commands</strong><br><strong>Best practice:</strong> always show correlation id and provide "copy diagnostics" action. <br><strong>Triage flow:</strong><br>1. Obtain correlation id from user. <br>2. Retrieve <code>UserAction</code> audit row and step-level audits. <br>3. Pull artifacts referenced (profile report, proposal). <br>4. If incident, collect <code>forensic_manifest</code> and escalate per IR runbook. <br><strong>Example scenario:</strong> support ticket "Propose failed (ref r-20260116-abc)" → SRE retrieves audit chain, replays fault in isolated runner, files bug, and attaches <code>forensic_manifest</code>. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>Change-control &amp; governance (ribbon)</code> — required steps, approvals, migration manifest, and examples</strong><br><strong>Required flow for changes:</strong><br>1. Create PR + migration manifest if behavior semantics change. <br>2. Run static analysis, unit/integration/golden tests. <br>3. Obtain code-review and compliance signoffs for regulated changes. <br>4. Sign artifacts and publish release manifest. <br>5. Canary rollout with KPI gating & rollback plan. <br>6. Post-rollout <code>VerifyAuditChain</code> and <code>deployment.audit</code>. <br><strong>Example artifact:</strong> <code>migration_manifest.json</code> documents transforms, sample sizes, backout plan, and approvals. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>Evidence to collect for ribbon incidents</code> — canonical forensic package & storage</strong><br><strong>Minimum artifacts:</strong><br>1. Current + prior <code>ribbon-map.json</code> and signatures. <br>2. <code>audit_tail.csv</code> rows covering correlation ids. <br>3. Handler logs from <code>modRibbonCallbacks</code>. <br>4. Persisted job descriptors. <br>5. <code>modConfig</code> snapshot and <code>config.hash</code>. <br>6. Release manifest and artifact signatures. <br>7. <code>forensic_manifest.json</code> with sha256 checksums and storage URI. <br><strong>Storage:</strong> secure evidence repo with RBAC and chain-of-custody records. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>Acceptance criteria (dev/CI)</code> — gating checklist & enforcement</strong><br><strong>Gates:</strong><br>1. Unit + integration + golden tests pass. <br>2. No forbidden API references in static analysis. <br>3. <code>ribbon.map</code> schema validated and <code>ribbonMap.hash</code> produced. <br>4. <code>UserAction</code> audits emitted and <code>VerifyAuditChain</code> passes. <br>5. Correlation id uniqueness tests pass. <br>6. Performance budgets met under CI load. <br><strong>Blocking conditions:</strong> golden/audit-chain failures or forbidden API detections. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>Forbidden APIs / static enforcement</code> — explicit banned list, rationale, and CI actions</strong><br><strong>Disallowed in ribbon core and inline handlers:</strong><br>1. Direct Workbook/Range mutation in <code>OnLoad</code> or ribbon core. <br>2. Raw network sockets (WinHTTP) or external web calls during bootstrap/ribbon main path. <br>3. Unbounded synchronous disk writes that block UI (>10ms). <br>4. Direct secret reads from plaintext files. <br>5. Spawning external processes. <br><strong>Enforcement:</strong> CI static analyzer rejects PRs referencing blacklisted APIs and emits <code>forbidden-api</code> failures. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>Appendices &amp; references</code> — canonical schemas, templates, runbooks, and storage paths</strong><br><strong>Include:</strong> ribbon manifest JSON Schema, audit row schema, <code>ErrorCodeCatalog.md</code>, migration manifest template, forensic manifest template, operator cheat-sheets, PQ template guidelines, unit test harness docs, CI golden-file guide, release manifest signing checklist, and <code>OWNERS.md</code> mapping. <br><strong>Storage & governance:</strong> immutable artifact store <code>\\artifacts\runbooks\modRibbonCallbacks\v{major}.{minor}\appendices\</code> with RBAC. <br><strong>Example artifacts:</strong> <code>appendices/templates/migration_manifest.json</code>, <code>runbooks/cheatsheets/ribbon-smoke-test.md</code>. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong>Narratives — detailed human-readable end-to-end stories (extended)</strong><br><strong>Narrative 1 — Happy-path start & quick preview:</strong> Excel launches, host loads add-in; <code>OnLoad(ribbonUI)</code> caches <code>ribbonUI</code>, schedules <code>DeferredInit</code>, emits <code>ribbon.onload</code> with <code>cid=r-start-001</code>. <code>DeferredInit</code> runs <code>LoadRibbonMap</code> which validates and computes <code>ribbonMap.hash=sha256:abcd...</code>; <code>ribbon.map.loaded</code> emitted. Operator selects table and clicks <code>dq_profile</code>. <code>HandleControlAction</code> validates control, creates <code>cid=r-20260116-abc</code>, emits <code>ribbon.useraction</code> with <code>paramsHash</code>. <code>IsLightweightAction</code> returns true; <code>SafeInvokeHandler</code> runs inline with seeded RNG (for deterministic sample), appends <code>ribbon.handler.start</code> and <code>ribbon.handler.complete</code>; UI shows preview and correlation id. Evidence store contains sanitized params and profile artifact; main audit contains <code>paramsHash</code> only. <br><strong>Narrative 2 — Large apply scheduled & worker determinism:</strong> Operator triggers <code>dq_apply</code> on 600k-row dataset; <code>IsLightweightAction</code> chooses scheduled due to row threshold and regulation flag. <code>HandleControlAction</code> emits <code>ribbon.useraction</code>, persists <code>jobDescriptor</code> atomically, emits <code>job.persisted:job-901</code>. Worker picks job, verifies <code>paramsHash</code>, seeds RNG with <code>jobId</code> and runs <code>REG_Calculations</code> pipeline (normalization, SafeRound), <code>DQ_Rules</code> evaluation, and produces <code>dq_proposal</code> with <code>artifact.checksum</code>. Evidence and artifacts stored encrypted; step-level audits appended. <br><strong>Narrative 3 — Manifest tamper detection & operator recovery:</strong> During deferred init a manifest fails signature verification; <code>LoadRibbonMap</code> emits <code>ribbon.map.invalid</code> with <code>errorCode=RIB_MAP_SIG_001</code>. Regulated controls disabled. Operator exports diagnostics, files forensic bundle, and uploads signed manifest via release flow. <code>HotSwapHandlers</code> executes with approvals and smoke tests via <code>RegisterUnitTestHook</code>; <code>ribbon.hotswap.applied</code> emitted on success; controls re-enable. <br><strong>Narrative 4 — CI golden-run reproducibility:</strong> CI registers <code>hook_profile_golden</code> with <code>cid=r-test-001</code>, loads canonical <code>ribbonMap.json</code> and <code>config.hash</code>, simulates <code>dq_profile</code> with fixed seed; <code>REG_Audit</code> rotation is signed; <code>VerifyAuditChain</code> compares computed chain to golden artifact. Mismatch blocks merge. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong>Examples — pragmatic operator & developer scenarios</strong><br><strong>Example A — Registering a callback safely:</strong> Install script calls <code>RegisterCallback(&quot;dq_phone_norm&quot;,&quot;NormalizePhoneHandler&quot;,{owner:&quot;dq-team&quot;,requiresApproval:false,version:&quot;1&quot;})</code>. Module validates handler signature, resolves owner in <code>OWNERS.md</code>, updates in-memory map and emits <code>ribbon.callback.registered</code>. If <code>persist=true</code> performs atomic manifest update and emits <code>ribbon.map.loaded</code>. <br><strong>Example B — Emergency disable:</strong> SRE calls <code>DisableControl(&quot;dq_export&quot;,operatorId=&quot;sre_oncall&quot;,reason=&quot;data-leak suspected&quot;)</code>. RBAC verifies permission; control disabled in-memory and <code>ribbon.control.disabled</code> emitted. If persist requested, atomic manifest update requires approvals. <br><strong>Example C — Timeout with partial output:</strong> Inline handler overruns budget; watchdog emits <code>ribbon.handler.timeout</code> and toggles cancel token; handler writes partial artifact to evidence store and returns; <code>ribbon.handler.complete</code> appended with <code>partialResultRef</code>; UI shows "Partial results (ref r...)". </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong>Conceptual Power Query (PQ) guidance — provenance, injection safety, preview model (no code)</strong><br><strong>Purpose & rationale:</strong> PQ templates (M language) enable repeatable ETL logic and must be auditable. Templates require <code>templateVersion</code>, <code>mChecksum</code>, and declared owner. Ribbon must prevent credentials embedded in templates and ensure preview/inject paths are auditable. <br><strong>Provenance & checksums:</strong> each PQ template must declare <code>templateVersion</code> and canonical <code>mChecksum</code> (SHA256 of canonicalized M text). Audits referencing templates (<code>pq_preview</code>, <code>pq_inject</code>, <code>pq_export</code>) include <code>mChecksum</code> and <code>templateVersion</code>. <br><strong>Deterministic preview model:</strong> previews run on deterministic samples seeded with <code>correlationId+config.hash</code> or canonical sample slices. Previews must be side-effect free (no workbook query creation). <code>pq_preview</code> audit includes <code>mChecksum</code>, <code>sampleDescriptor</code>, <code>runtimeSeed</code>, <code>paramsHash</code>. <br><strong>Safe injection & connection handling:</strong> before <code>Add_Query_From_M</code> strip credentials and replace with <code>credentialRef</code> tokens referencing vault entries; if operator creates connection, persist only <code>connFingerprint</code> in audit, never raw secrets. Hidden-sheet fallback allowed for offline use; emit <code>pq_library.fallback</code> audit when used. <br><strong>Diagnostics & refresh policy:</strong> <code>pq_refresh</code> audits provider timings, refresh path, error traces, <code>mChecksum</code>. <code>pq_diagnostics</code> contains <code>refreshTimeMs</code> and errorCode. <br><strong>Governance & CI:</strong> PQ template changes require PR+owner approval+golden tests verifying <code>mChecksum</code> changes; regulated templates require <code>migration_manifest</code> & smoke tests. <br><strong>PQ operational examples:</strong><br>1. Operator previews template → <code>pq_preview</code> with <code>mChecksum</code> and <code>paramsHash</code>. <br>2. Operator injects template → <code>pq_inject</code> audit with <code>queryName</code>, <code>mChecksum</code>, <code>connFingerprint</code>. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong>Conceptual DAX guidance — governance, measure provenance & deterministic evaluation (no code)</strong><br><strong>Purpose & constraints:</strong> DAX measures affecting regulated outputs must be versioned, reviewed, tested, and auditable. Ribbon flows inserting/updating measures must require approvals and produce audit traces. <br><strong>Measure provenance & checksum:</strong> each measure introduced via ribbon must have <code>measureId</code>, <code>measureDefinitionVersion</code>, <code>owner</code>, <code>measureChecksum</code> (canonicalized measure text hash). Audits creating/updating measures include <code>measureChecksum</code> and <code>migration_manifestRef</code>. <br><strong>Avoid surprise mutations:</strong> updates to measures affecting regulated reports require two-person approvals and a <code>migration_manifest</code> documenting expected deltas with golden fixtures. <br><strong>Deterministic evaluation harness:</strong> CI must evaluate measures on canonical sample datasets under fixed seed for time-dependent functions; <code>dax.preview</code> emits <code>measureChecksum</code>, <code>sampleContext</code>, <code>previewChecksum</code>. <br><strong>Measure design guidance:</strong> prefer compositional, testable measures; avoid volatile external state; document cardinality and performance cost. <br><strong>Operator flow examples (conceptual):</strong> operator previews measure on sample dataset (<code>dax.preview</code>) → deterministic result and <code>dax.preview</code> audit; <code>dax.apply</code> requires approvals for regulated measures and emits <code>dax.apply</code> with pre/post checksums. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong>Developer playbook & safe I/O patterns (practical rules)</strong><br>1. <strong>Read-verify-swap:</strong> for any manifest or export—read source, verify signature/checksum, write to temp file, compute checksum, then atomic rename to final path. <br>2. <strong>No network on main path:</strong> network fetches allowed only in deferred init or worker contexts; always degrade gracefully with cached fallback and emit <code>ribbon.map.warning</code> when network unavailable. <br>3. <strong>Evidence & hashing:</strong> compute <code>paramsHash</code> and <code>payloadHash</code> before artifact persistence; store sanitized evidence encrypted and reference with <code>evidenceRef</code>. <br>4. <strong>Deterministic RNG & SafeRound:</strong> use seeded RNG and deterministic SafeRound to guarantee reproducibility of previews and small numerical differences. <br>5. <strong>CI static enforcement:</strong> forbid forbidden APIs via build-time linters (workbook mutations on <code>OnLoad</code>, raw secret reads, network calls on UI thread). <br>6. <strong>Tests & golden runs:</strong> provide test hooks (with <code>test=true</code> audits) disabled in production by default. Golden runs require fixed <code>correlationId</code>, <code>config.hash</code>, and <code>ribbonMap.hash</code>. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong>Appendices: schemas, artifacts & required documents</strong><br>Store and publish artifacts under immutable artifact store with RBAC: canonical <code>ribbonMap.json</code> schema, <code>UserAction</code> audit schema, <code>ErrorCodeCatalog.md</code>, <code>migration_manifest.json</code> template, <code>forensic_manifest.json</code> template, <code>OWNERS.md</code>, release manifest signing checklist, PQ template registry (<code>pq_templates.json</code>) with <code>mChecksum</code>, and DAX measure registry with <code>measureChecksum</code>. Each artifact includes computed checksums and corresponding audit rows for traceability. <br><strong>Required CI checks:</strong> <code>audit-chain-verify</code>, static analyzer forbidding APIs, golden parity verification, manifest signature checks, unit & integration test coverage thresholds. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong>Operator CLI & quick commands (reference)</strong><br><code>diagnostics collect --cid &lt;cid&gt; --out forensic.zip</code> — collects audit slice and artifacts and produces signed <code>forensic_manifest.json</code>. <br><code>audit flush --immediate --correlation &lt;cid&gt;</code> — forces audit flush and rotation and emits <code>audit.flush</code> row. <br><code>jobs requeue --job-id &lt;id&gt; --force</code> — requeue job (SRE approval required); emits <code>job.requeue</code> audit. <br><code>exports stage-local --artifact &lt;id&gt;</code> — stage artifact for offline collection; emits <code>export.stage</code> audit. <br><code>ribbon.refresh --dry-run</code> — preview manifest reload/diff; emits <code>ribbon.refresh.preview</code>; <code>ribbon.refresh --apply</code> performs apply if operator has permissions. Each CLI invocation emits an audit row with <code>operatorId</code> and reason. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong>Incident runbook (concise steps)</strong><br>1. Obtain <code>correlationId</code> from user. <br>2. Fetch <code>UserAction</code> anchor and step-level audits via audit tooling. <br>3. Retrieve evidence artifacts referenced by <code>evidenceRef</code> (requires operator approvals). <br>4. If manifest/signature issues suspected, retrieve <code>ribbon-map.json</code> and signed audit rotations. <br>5. If job persist or worker failures, retrieve job descriptors and worker logs; attempt safe requeue after investigation. <br>6. Assemble <code>forensic_manifest.json</code> with checksums and store securely; document chain-of-custody with audit rows. <br>7. Notify compliance/regulatory teams if required and provide forensic package. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong>Retention & housekeeping</strong><br>Automate retention verification monthly; audit rotations: hot=30d, warm=7y, cold=per regulation; scheduled maintenance windows for cleanup & kill-switch tests. {audit: housekeeping.audit} </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong>Final acceptance checklist (dev/CI)</strong><br>• Unit + integration + golden tests pass. <br>• CI <code>VerifyAuditChain</code> passes for golden runs. <br>• Static analyzer reports no forbidden API references. <br>• Manifest signature verification vectors pass. <br>• Performance SLOs validated under realistic load. <br>• Two-person approvals recorded for regulated mapping changes. <br>• Smoke tests for hot-swap & refresh succeed. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong>Implementation anti-patterns (for CI & reviews)</strong><br>DO: emit <code>ribbon.useraction</code> prior to side-effects; compute <code>paramsHash</code> and store sanitized evidence to encrypted store; use <code>atomic_write</code> and SafeRound deterministic patterns; seed RNG for previews. <br>DO NOT: access workbook Ranges in <code>OnLoad</code>; perform synchronous network calls on UI thread; log secrets or PII in main audit rows; accept unsigned manifests in production. <br><strong>CI enforcement:</strong> static analyzer should reject forbidden API usage and secret exposure. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong>Glossary (practical short list)</strong><br><code>correlationId</code> — unique trace identifier linking bootstrap→ribbon→job→worker. <br><code>ribbonMap.hash</code> — SHA256 canonical manifest hash. <br><code>paramsHash</code> — SHA256 of canonicalized, redacted UI params. <br><code>evidenceRef</code> — secure pointer to encrypted sanitized evidence. <br><code>payloadHash</code> — SHA256 of produced artifact for audit linkage. <br><code>jobDescriptor</code> — canonical persisted job artifact consumed by workers. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong>Extended checklists & sample schemas (operational reference)</strong><br><strong>UserAction audit row (canonical fields):</strong> <code>timestamp, correlationId, module=REG_Ribbon, procedure, userId, controlId, paramsHash, evidenceRef, configHash, ribbonMapHash, prevHash, metadata{tenantId, operatorRole, sessionId}</code>. <br><strong>Job descriptor sample fields:</strong> <code>jobId, controlId, correlationId, paramsHash, configHash, persistedAt, owner, priority, retries, retryPolicy, payloadRef (evidenceRef), workerAffinity</code> — always persisted via atomic-write and recorded with <code>job.persisted:&lt;jobId&gt;</code>. <br><strong>RibbonMap manifest canonical expectations:</strong> controls sorted by <code>controlId</code>, each control object includes <code>controlId, label, handler, estimatedCost, owner, requiresApproval, regulated, visibilityRules, versions[]</code> and module-level <code>manifestVersion</code>, <code>buildId</code>, <code>signature</code>, and <code>createdBy</code>. Manifest canonicalization sorts keys and arrays for stable hash computation. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong>Extended governance & release flow (operational)</strong><br>1. Developer opens PR for manifest or mapping changes. <br>2. CI runs static analyzer (forbidden APIs), unit tests, integration tests, golden-run parity, <code>VerifyAuditChain</code>. <br>3. If tests pass, obtain code-review and approvals; if regulated change, obtain compliance approval and two-person sign-off. <br>4. Produce signed release manifest via release pipeline; compute <code>ribbonMap.hash</code> and sign with release key (record fingerprint). <br>5. Canary rollout: apply signed manifest to a pilot cohort; run KPI gating tests (error rates, audit parity, performance SLOs). <br>6. If canary passes, full rollout; if issues, hot-swap revert and create incident with forensic package. <br><strong>Audit:</strong> each step produces <code>deployment.audit</code>, <code>canary.audit</code>, and <code>hotSwap.*</code> records. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong>Additional narratives — edge cases & recovery scenarios (practical)</strong><br><strong>Edge narrative 1 — partial evidence corruption:</strong> worker completes and writes artifact to evidence store; subsequent evidence verification fails due to storage corruption. System emits <code>artifact.verification.fail</code>, worker appends <code>dq_apply.partial</code> audit with <code>partialArtifactRef</code> and <code>forensic_manifest</code> references; operator notified and can retrieve pre-signed forensic bundle for offline analysis, then re-run job after cache restoration. <br><strong>Edge narrative 2 — audit rotation failure during shutdown:</strong> <code>Shutdown()</code> attempts to flush and rotate <code>audit_tail.csv</code> but rotation signing fails due to KMS transient outage. Append <code>ribbon.shutdown</code> with <code>shutdownStatus:&quot;audit.rotate.fail&quot;</code> and queue rotation retry with exponential backoff; notify SRE and persist unsent rotation to secure local store for later signing once KMS returns. <br><strong>Edge narrative 3 — de-synced ribbonMap across clustered hosts:</strong> multiple clients detect different <code>ribbonMap.hash</code> during same session; <code>ribbon.refresh</code> enforces atomic swap and broadcasts <code>ribbon.map.version</code> to clients; clients with stale map emit <code>ribbon.map.mismatch</code> audit and re-fetch signed manifest; if mismatch persists, isolate host and alert. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong>Risk matrix & mitigation (high-level)</strong><br><strong>Risk:</strong> Manifest tamper or signature bypass. <br><strong>Impact:</strong> High (regulated controls misbehave). <br><strong>Mitigation:</strong> require signed manifests, local signature verification, fail-closed for regulated controls, <code>ribbon.map.invalid</code> audits. <br><strong>Risk:</strong> Job persist failure or job duplication. <br><strong>Impact:</strong> Medium-high (missed work or duplicate destructive actions). <br><strong>Mitigation:</strong> idempotent job descriptors, deterministic <code>jobId</code> derivation, bounded retry/backoff, <code>job.persist.fail</code> alarms. <br><strong>Risk:</strong> PII leak in audit or logs. <br><strong>Impact:</strong> High (compliance breach). <br><strong>Mitigation:</strong> <code>BuildUiParamsHash</code> redaction, evidence store encryption, audit-only <code>paramsHash</code> in main row, static analysis to detect logs with PII patterns. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong>Operator training quick checklist (frictionless)</strong><br>1. Always capture and provide <code>correlationId</code> when escalating. <br>2. Use <code>diagnostics collect --cid &lt;cid&gt;</code> to build forensic package. <br>3. Use <code>ribbon.refresh --dry-run</code> to preview manifest diffs before applying. <br>4. Use <code>DisableControl(controlId)</code> when containing incidents; persist only with approvals. <br>5. For regulated changes, require two-person approval and signed manifest before persistence. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong>Closing operational directive</strong><br>The ribbon layer must remain thin, fast, auditable, and deterministic. Keep heavy transformations in workers; always anchor user actions with <code>ribbon.useraction</code> and <code>correlationId</code>. Enforce manifest signature checks and evidence encryption. Use the atomic-write, SafeRound, deterministic RNG, and audit-first patterns as non-negotiable building blocks. </td></tr></tbody></table></div><div class="row-count">Rows: 54</div></div><div class="table-caption" id="Table8" data-table="Docu_0177_08" style="margin-top:2mm;margin-left:3mm;"><strong>Table 8</strong></div>
<div class="table-wrapper" data-table-id="table-8"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by REG_EnsureDeps — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">REG_EnsureDeps — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Top-level intent & executive summary (concise):</strong><br>REG_EnsureDeps is the canonical runtime dependency verification module for regulated systems (REG_<em>, DQ_</em>, PQ_*). Its purpose is to deterministically discover declared and transitive dependencies, verify presence and integrity, validate signatures and runtime compatibility, classify severity, produce an auditable canonical <code>deps.report.json</code>, and drive deterministic enable/disable decisions for runtime features. It is conservative: regulated controls fail-closed on missing/invalid dependencies; non-critical features use degradable fallback policies. The module produces machine-readable outputs for downstream automation and human-readable diagnostics for operators, while placing raw sensitive evidence into an encrypted evidence store referenced from the report. REG_EnsureDeps is designed for deferred-init usage and background deep validation when required. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Design principles, absolute invariants, and operational mandates (must/shall):</strong><br>1. <strong>Determinism:</strong> identical inputs produce byte-for-byte identical <code>deps.report.json</code> and <code>reportHash</code> across runs and platforms when using the same canonicalization rules. <br>2. <strong>Fail-closed for regulated features:</strong> any dependency classified <code>invalid</code> with <code>severity=critical</code> blocks the regulated control(s) that depend on it. <br>3. <strong>Minimal synchronous footprint:</strong> synchronous validation does only cheap discovery and local checksum checks; heavy signature chains, network lookups, and CRL/OCSP checks are offloaded to background workers. <br>4. <strong>Idempotency:</strong> repeated runs with unchanged inputs produce identical outputs and do not duplicate evidence artifacts. <br>5. <strong>Audit separation & redaction:</strong> audit rows contain safe summary metadata and <code>reportHash</code>; full evidence lives encrypted and is referenced by <code>evidenceRef</code>. <br>6. <strong>Key management & signing policy:</strong> signature verification uses a trusted public key registry managed by KMS/HSM; private keys never stored by REG_EnsureDeps. <br>7. <strong>No UI thread blocking:</strong> any call expected from the UI path returns quickly; if full validation is required, the function returns <code>partial</code> and schedules a background job. <br>8. <strong>Atomic persistence:</strong> <code>deps.report.json</code> writes must be atomic (temp+fsync+rename or object-store conditional update) and a <code>.prev</code> backup preserved. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Who owns/operates this module & governance expectations:</strong><br>1. The module MUST reference <code>OWNERS.md</code> mapping <code>depId</code> → <code>teamAlias</code> → <code>oncallContact</code> inside scanned manifests or the codebase. <br>2. For regulated dependencies, <code>approvalPolicy</code> MUST be present and set to <code>twoPerson</code> or stricter. <br>3. Every owner must provide an artifact distribution endpoint and a contact channel for rapid remediation. <br>4. Policy exceptions require documented, signed approvals stored in the evidence store and are auditable. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Public API surface and contract (authoritative list):</strong><br>1. <code>REG_EnsureDeps(context, manifestPaths=[], options={verifySignatures:true, allowNetworkLookup:false, strictMode:true, reportDir:default})</code> — orchestrator, writes canonical report, returns <code>{status, reportPath, reportHash, summary}</code>. <br>2. <code>DiscoverDeclaredDeps(manifestPaths)</code> — deterministic discovery of declared deps with provenance. <br>3. <code>ResolveSearchPaths(config, env)</code> — compute deterministic search path list. <br>4. <code>VerifyArtifactLocal(path, depDescriptor, options)</code> — existence, metadata extraction, local checksum. <br>5. <code>CanonicalizeAndChecksum(artifact, type)</code> — produce canonical bytes (rules differ by type) and SHA256 checksum. <br>6. <code>VerifySignature(artifactPath, trustedKeyring, options)</code> — local signature verification; returns <code>{present, signerFingerprint, valid, reason}</code>. <br>7. <code>ResolveTransitiveDeps(depList)</code> — deterministic transitive closure, cycle detection, topological sort where possible. <br>8. <code>ClassifyDependency(depRecord, policy)</code> — produce <code>status</code> and <code>severity</code> and remediation hints. <br>9. <code>ProduceDepsReport(resolvedDeps, graph, meta)</code> — canonical JSON serializer + atomic write; compute <code>reportHash</code>. <br>10. <code>EmitDepsAudit(context, eventType, payload)</code> — append audit rows: <code>deps.check.started</code>, <code>deps.check.completed</code>, <code>deps.check.failed</code>, <code>deps.validation.scheduled</code>, plus step-level events. <br>11. <code>ScheduleBackgroundValidation(jobDescriptor)</code> — persist idempotent background job and emit <code>deps.validation.scheduled</code>. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>High-level operational contract and expected calling patterns:</strong><br>1. <strong>Bootstrap / Deferred init:</strong> called by <code>REG_Bootstrap</code> and <code>DQ_Bootstrap</code> during deferred init (on idle) to verify artifacts and set runtime enablement. <br>2. <strong>On-demand checks:</strong> called by operator-initiated "Validate dependencies" actions that require a full synchronous pass (may still offload heavy parts). <br>3. <strong>CI/Release pipelines:</strong> run in CI to produce <code>deps.report.json</code> golden artifacts and ensure parity with release manifests. <br>4. <strong>Background monitors:</strong> scheduled periodic background runs to pre-empt certificate expiry and signature drift. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Canonical discovery & parsing rules (deterministic):</strong><br>1. <code>manifestPaths</code> enumerated in lexicographic order. <br>2. Each manifest parsed with UTF-8 normalization and JSON canonicalization rules applied to extract declared dependencies. <br>3. For each declared dependency include fields: <code>depId</code>, <code>type</code> (<code>xlm|xlam|xll|pq_template|connector|driver|library</code>), <code>declaredVersion</code>, <code>criticality</code>, <code>ownerAlias</code>, <code>signatureRequired:Boolean</code>, <code>requiresRuntime</code>, and optional <code>mChecksum</code> for PQ templates. <br>4. If <code>depId</code> duplicates are found across manifests, resolve by provenance order with a deterministic tie-breaker and add <code>warning</code> listing indices; duplicates are not silently merged. <br>5. For PQ templates canonicalize M text before computing <code>mChecksum</code> (NFC normalization, LF newlines, strip editor metadata per policy). </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Canonical <code>deps.report.json</code> schema (required fields and structure):</strong><br>1. <code>reportVersion</code> (semver for report schema). <br>2. <code>reportHash</code> (sha256 of canonical bytes). <br>3. <code>scannedAt</code> (ISO8601 UTC). <br>4. <code>correlationId</code>, <code>configHash</code>. <br>5. <code>resolvedDeps</code>: array of objects <code>{depId, type, declaredVersion, resolvedVersion, checksum:{sha256}, path, ownerAlias, status, severity, signature:{present, signerFingerprint, valid, signerMetadata?}, runtimeCompatibility:{ok, details}, resolutionAction?, evidenceRef?}</code>. <br>6. <code>graph</code>: <code>{nodes:[depId], edges:[{from,to}]}</code> sorted deterministically. <br>7. <code>warnings[]</code>, <code>errors[]</code> with <code>{errorCode, depId?, message}</code>. <br>8. <code>transitiveStats</code> <code>{depth, maxFanOut, nodeCount}</code>. <br>9. <code>durationMs</code>, <code>toolVersion</code>, <code>prevReportHash?</code>. <br>10. <code>artifactReferences[]</code> pointing to evidence bundles (encrypted) with <code>evidenceRef</code> and retention metadata. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Classification rules and severity mapping (detailed):</strong><br>1. <code>present</code> — artifact located, checksum matches declared (if provided), semantic version matches declared constraint, signature valid when required, runtime compatibility ok. <br>2. <code>missing</code> — artifact not found in any search path; if <code>allowNetworkLookup=true</code> scheduler may attempt remote fetch in background. <br>3. <code>mismatched_version</code> — artifact resolved but <code>resolvedVersion</code> does not satisfy declared semver range. <br>4. <code>signature_invalid</code> — signature verification failed or missing where required. <br>5. <code>incompatible_runtime</code> — declared <code>requiresRuntime</code> not satisfied by host. <br>6. <code>degraded</code> — artifact absent but local fallback available (embedded template, prior version). <br>7. Severity mapping: <code>critical</code> (requires fail-closed for regulated controls) / <code>degradable</code> / <code>optional</code>. <br>8. ResolutionAction candidates included on each classified row: <code>upload_artifact</code>, <code>re-sign_artifact</code>, <code>apply_fallback</code>, <code>schedule_fetch</code>, <code>upgrade_runtime</code>, <code>owner_contact</code>. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Signature verification policy and details (explicit):</strong><br>1. Trusted keyring is authoritative; public keys managed via KMS/HSM with signed key metadata; key rotation recorded and audited. <br>2. For regulated dependencies signature validity is mandatory (<code>signatureRequired=true</code>), and invalid signatures yield <code>status=signature_invalid</code> and <code>severity=critical</code>. <br>3. Signature verification includes signer fingerprint extraction, certificate chain validation (CRL/OCSP checks deferred to background if slow), and timestamp validation (signing time within acceptable window). <br>4. If signature is missing but allowed in non-production environments, report <code>warning</code> and allow <code>degraded</code> behavior per <code>options</code>. <br>5. Signer fingerprints included in <code>deps.report.json</code> and evidence bundles; private keys and full cert chains stored only in encrypted evidence. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>PQ template handling and <code>mChecksum</code> governance (detailed conceptual rules):</strong><br>1. PQ templates are canonicalized before hashing: Unicode NFC, LF newlines, normalized parameter order where parameter schema indicates unordered vs ordered parameters. <br>2. <code>mChecksum</code> is computed as <code>sha256(canonicalMBytes)</code> and must match declared <code>mChecksum</code> in manifests. <br>3. Changes to a PQ template require an updated <code>templateVersion</code> and signed manifest for production promotion. <br>4. Hidden-sheet embedded templates are permitted as last-resort fallback and must be explicitly declared with <code>embedded:true</code> to be used automatically; otherwise operator consent required. <br>5. When injection is performed, <code>paramsHash</code> is computed for sanitized parameters and recorded in audit; full sanitized params saved in evidence with <code>evidenceRef</code>. <br>6. Template mismatches produce policy outcomes: regulated templates → <code>invalid</code> (fail-closed); recommended templates → <code>degraded</code> with scheduled reconciliation. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Transitive resolution, cycles & topological ordering (detailed):</strong><br>1. After initial discovery, compute transitive closure by visiting each declared dependency and expanding its own declared dependencies recursively. <br>2. Use deterministic traversal: children sorted by <code>depId</code> before expansion. <br>3. Produce <code>graph</code> with nodes and edges sorted lexicographically. <br>4. Detect cycles using Tarjan's strongly connected components algorithm (or equivalent); record cycles in <code>errors[]</code> with <code>cycleId</code> and constituent <code>depId</code>s. <br>5. Policy: cycles among optional helpers → <code>degraded</code>; cycles involving regulated components → <code>invalid</code> and fail-closed. <br>6. Provide remediation hints and owner lists for cycle resolution. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Evidence & encrypted bundles (<code>evidenceRef</code>) rules:</strong><br>1. Evidence bundles contain raw manifests, extracted cert chains, canonical M templates, and any private owner-provided diagnostic artifacts. <br>2. Evidence bundles are encrypted with KMS-managed keys and stored in evidence repo with <code>evidenceRef</code> URL or pointer. <br>3. Access to evidence requires owner/approval flow and is itself audited with <code>evidence.access</code>. <br>4. <code>deps.report.json</code> contains <code>evidenceRef</code> pointers, but not raw sensitive payloads. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Atomic write and backup policy for <code>deps.report.json</code>:</strong><br>1. Write canonical bytes to <code>deps.report.json.tmp.&lt;pid&gt;</code>; fsync file; rename to <code>deps.report.json</code>. <br>2. After successful write create a binary copy <code>deps.report.json.prev.&lt;ts&gt;</code> for forensic diffs. <br>3. On object stores, write versioned object and conditionally update the pointer with precondition (ETag) semantics to approximate atomic rename. <br>4. Recompute hash after write and verify equality before emitting <code>deps.check.completed</code>. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Audit events & required fields (exhaustive):</strong><br>1. <code>deps.check.started</code> — <code>timestamp, correlationId, module=REG_EnsureDeps, manifestPaths, configHash, operatorId?</code>. <br>2. <code>deps.check.completed</code> — <code>timestamp, correlationId, reportHash, resolvedCount, missingCount, degradedCount, durationMs</code>. <br>3. <code>deps.check.failed</code> — <code>timestamp, correlationId, errorCode, evidenceRef</code>. <br>4. <code>deps.validation.scheduled</code> — <code>timestamp, correlationId, jobId, reasons</code>. <br>5. <code>deps.validation.completed</code> — <code>timestamp, correlationId, jobId, updatedReportHash</code>. <br>6. <code>deps.override.applied</code> — <code>timestamp, correlationId, operatorId, justificationRef</code>. <br>7. Each audit must include <code>configHash</code> where relevant, and must not include PII; include <code>ownerAlias</code> and <code>evidenceRef</code> instead. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Synchronous vs background work: splitting rules & best practices:</strong><br>1. <strong>Synchronous (UI-safe) tasks:</strong> discovery, local existence checks, local checksum computation, initial signature presence check (not deep chain), basic classification for immediate UI decisions. <br>2. <strong>Background tasks:</strong> deep signature chain validation (CRL/OCSP), remote fetches, long-running transitive resolution across remote repositories, and certificate expiration analysis. <br>3. Synchronous entry returns <code>status:&quot;partial&quot;</code> when significant background checks are pending and includes <code>validationJobId</code>. <br>4. Background jobs must be idempotent and persist <code>jobDescriptor</code> for dedupe and retry. <br>5. When background validation completes, rotate <code>deps.report.json</code> atomically and emit <code>deps.validation.completed</code> and <code>deps.report.rotated</code>. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Telemetry & metrics (buffered local emitter):</strong><br>1. Buffered metrics: <code>deps.check.duration_ms</code>, <code>deps.missing.count</code>, <code>deps.sig.invalid.count</code>, <code>deps.validation.jobs.count</code>. <br>2. Emit locally and flush via separate uploader process to avoid synchronous network calls. <br>3. High-cardinality tags limited: include <code>platform</code>, <code>module</code>, and <code>severity</code> only; avoid including <code>depId</code> directly in high-cardinality metrics. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Failure modes, detection, and automated mitigations (operational playbook):</strong><br>1. <strong>Missing artifact discovered:</strong> emit <code>deps.check.completed</code> with <code>missingCount&gt;0</code> and <code>warnings[]</code>; if critical, emit <code>deps.check.failed</code> and disable dependent controls; remediation: owner responsible to upload artifact or enable fallback. <br>2. <strong>Signature invalid:</strong> mark dependency <code>signature_invalid</code> and <code>severity=critical</code> for regulated artifacts; create evidence bundle and escalate. <br>3. <strong>Runtime mismatch:</strong> record <code>incompatible_runtime</code> and suggest runtime upgrade; if runtime required for regulated feature, disable that feature. <br>4. <strong>Cycle detected:</strong> record cycle and mark severity per policy; suggest owner coordination. <br>5. <strong>Partial validation due to network disabled:</strong> return <code>partial</code> and schedule background fetch if operator requested; surface <code>userHint</code>. <br>6. <strong>Disk write failure during atomic write:</strong> keep previous <code>deps.report.json.prev</code> and retry with backoff; alert SRE and schedule manual intervention if repeated. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Operator UX & triage checklist (explicit):</strong><br>1. Copy <code>correlationId</code> from UI message. <br>2. Download <code>deps.report.json</code> referenced in diagnostics. <br>3. Inspect <code>errors[]</code> for <code>errorCode</code> and <code>depId</code>. <br>4. For <code>signature_invalid</code> contact <code>ownerAlias</code> to obtain signed artifact; upload to artifact store and run <code>REG_EnsureDeps --forceRefresh</code>. <br>5. For <code>missing</code> upload artifact or configure the local repo path and re-run. <br>6. For <code>incompatible_runtime</code> schedule runtime upgrade or pin module. <br>7. After remediation verify <code>deps.check.completed</code> and updated <code>reportHash</code>; attach <code>evidenceRef</code> and <code>deps.report.json.prev</code> to ticket. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Testing matrix & CI gating rules (exhaustive):</strong><br>1. <strong>Unit tests:</strong> discovery ordering, canonical serialization, checksum stability, PQ M canonicalizer tests, signature stub behavior. <br>2. <strong>Integration tests:</strong> transitive resolution, cycle detection, <code>deps.report.json</code> golden parity for canonical fixture set. <br>3. <strong>Negative tests:</strong> missing deps, duplicate <code>depId</code>, semver mismatch, signature tampering, partial uploads. <br>4. <strong>Performance tests:</strong> local checks for 500 artifacts within CI budget; background validation latency targets. <br>5. <strong>Security tests:</strong> verify that audit rows contain no PII, that evidenceRef exists for signature anomalies. <br>6. <strong>CI gating:</strong> fail PRs if golden <code>deps.report.json</code> parity fails or forbidden synchronous APIs are introduced. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Extremely detailed narrative examples and step-by-step walkthroughs (comprehensive):</strong><br><strong>Scenario A — Clean production release verification (end-to-end):</strong><br>1. Release artifact <code>release-v1.4-manifest.json</code> is uploaded to artifact store and includes 24 declared dependencies (10 PQ templates, 5 XLAMs, 4 connectors, 5 helper libs). <br>2. CI triggers <code>REG_EnsureDeps</code> during release pipeline; check runs in worker with <code>verifySignatures=true</code>. <br>3. <code>DiscoverDeclaredDeps</code> reads manifests deterministically and extracts <code>depId</code>, <code>mChecksum</code>, <code>ownerAlias</code>. <br>4. For each artifact the CI job uses <code>ResolveSearchPaths</code> pointing to release bundle; <code>VerifyArtifactLocal</code> finds everything and <code>ComputeChecksum</code> yields SHA256 checksums matching declared values. <br>5. <code>VerifySignature</code> validates signatures against the CI-trusted keyring; CRL/OCSP checks are run but return quickly as chain lengths are short. <br>6. <code>ResolveTransitiveDeps</code> expands declared dependencies into a graph with depth 3 and no cycles. <br>7. <code>ProduceDepsReport</code> writes <code>deps.report.json</code> atomically and computes <code>reportHash</code>. <br>8. CI archives <code>deps.report.json</code> and records <code>reportHash</code> in release manifest. <br>9. In production deferred-init the runtime reads <code>deps.report.json</code> and enables regulated features because no <code>critical</code> failures exist. <br><strong>Scenario B — PQ template mismatch detected on installed client (operator flow):</strong><br>1. Operator on client machine loads the add-in; <code>REG_EnsureDeps</code> runs in deferred init with <code>allowNetworkLookup=false</code>. <br>2. The manifest declares <code>normalize_address_v3</code> with <code>mChecksum=sha256:AA11...</code>. <br>3. Local repo lacks <code>normalize_address_v3</code> but includes embedded <code>normalize_address_v2</code> as an embedded fallback. <br>4. <code>REG_EnsureDeps</code> computes <code>status=degraded</code> for that template, writes <code>deps.report.json</code> with <code>resolutionAction:&quot;using_embedded_fallback&quot;</code> and <code>warnings[]</code>. <br>5. UI shows operator a preview-only control and <code>userHint</code> with <code>correlationId</code>. <br>6. Background job <code>deps.validation.scheduled</code> is queued (job persists) to fetch <code>normalize_address_v3</code> from artifact store as soon as network allowed; when fetched the background job recomputes the report, rotates it atomically, and emits <code>deps.validation.completed</code>. <br><strong>Scenario C — Signature invalid for a regulated XLAM helper (security incident path):</strong><br>1. <code>REG_EnsureDeps</code> during deferred init finds <code>xlam-helper-reg</code> present but <code>VerifySignature</code> reports <code>valid=false</code>. <br>2. The dependency is critical for <code>REG_Apply</code>; module classifies <code>status=signature_invalid</code>, <code>severity=critical</code>. <br>3. <code>ProduceDepsReport</code> writes report with <code>errors[{errorCode:DEPS_SIG_001, depId:&#x27;xlam-helper-reg&#x27;}]</code> and creates <code>evidenceRef</code> containing raw XLAM, cert chain, and computed checksum. <br>4. <code>deps.check.failed</code> audit emitted; <code>REG_Ribbon</code> disables <code>REG_Apply</code> and shows <code>userHint</code> "Regulated feature disabled — dependency verification failed (ref r-...)" without PII. <br>5. Operator follows runbook: contact owner, collect signed artifact, and upload to artifact store. <br>6. Operator calls <code>REG_EnsureDeps --forceRefresh</code>, which picks up the signed artifact and completes with <code>deps.check.completed</code>; controlled features are re-enabled. <br><strong>Scenario D — Transitive cycle discovered that affects non-regulated helpers (owner coordination flow):</strong><br>1. <code>ResolveTransitiveDeps</code> finds a cycle among three helper libraries <code>hlp-a</code>, <code>hlp-b</code>, <code>hlp-c</code>. <br>2. The cycle is recorded in <code>errors[]</code> with <code>cycleId</code> and owners for each node are listed. <br>3. Module classifies cycle severity as <code>degraded</code> since none of the involved deps are regulated. <br>4. <code>deps.report.json</code> includes remediation hints and owner aliases, and <code>deps.check.completed</code> emits with warnings. <br>5. Owners coordinate a patch release that breaks the cycle; background revalidation detects the fix and <code>deps.report.rotated</code> is emitted. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Operator remediation & runbook (detailed checklist):</strong><br>1. Retrieve <code>correlationId</code> from user's report or the UI. <br>2. Download <code>deps.report.json</code> referenced in diagnostics and inspect <code>errors[]</code>. <br>3. For <code>signature_invalid</code>: contact <code>ownerAlias</code>, request signed artifact or signer rotation details, upload signed artifact, and run <code>REG_EnsureDeps --forceRefresh</code>. <br>4. For <code>missing</code>: upload artifact to artifact store or add the location to <code>searchPaths</code>, then re-run <code>REG_EnsureDeps</code>. <br>5. For <code>incompatible_runtime</code>: schedule runtime upgrade or pin module; after upgrade re-run <code>REG_EnsureDeps</code>. <br>6. For cycles: coordinate owner teams to publish a fix; produce migration to break cycle and re-run. <br>7. After remediation, verify <code>deps.check.completed</code> with <code>reportHash</code> changed, archive <code>deps.report.json.prev</code> and attach <code>evidenceRef</code> to ticket for audit. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Extensive PQ conceptual guidance, governance, and examples:</strong><br><strong>Why PQ governance is essential:</strong> PQ templates define data ingestion and transformation semantics; small changes to M text can alter column types, null handling, join logic, and therefore downstream regulated outputs. Treat PQ templates as first-class, versioned artifacts with <code>mChecksum</code>, <code>templateVersion</code>, owners, and signature requirements for production promotion. <br><strong>Recommended manifest template entry fields for PQ templates (must include):</strong><br>1. <code>templateId</code><br>2. <code>templateVersion</code><br>3. <code>mChecksum</code> (sha256 of canonical M) <br>4. <code>ownerAlias</code><br>5. <code>parameterSchema</code> (JSON Schema for template parameters) <br>6. <code>expectedSampleOutputChecksum</code> (optional golden check) <br><strong>Canonical M canonicalization rules (implementation details):</strong><br>1. Normalize Unicode to NFC. <br>2. Convert CRLF to LF (<code>\n</code>) and remove trailing spaces. <br>3. Remove editor-only metadata lines when flagged (e.g., <code>#generated-by=…</code>), per manifest <code>stripEditorMeta</code> flag. <br>4. For parameter blocks, apply deterministic ordering if <code>parameterSchema.unordered==true</code> by sorting parameters by <code>name</code>. <br>5. Serialize canonical M bytes as UTF-8 and compute <code>mChecksum=sha256(canonicalBytes)</code>. <br><strong>PQ injection preconditions & safety contract:</strong><br>1. Confirm <code>mChecksum</code> matches declared value in <code>deps.report.json</code> before allowing injection. <br>2. If injection will create a new <code>Workbook.Query</code>, record <code>pq_inject</code> audit with <code>paramsHash</code> and <code>templateVersion</code>. <br>3. Full sanitized parameters stored in evidence; audit only contains <code>paramsHash</code>. <br>4. Do not automatically run refresh after injection when connectors may cause side effects; require operator confirmation. <br><strong>PQ diagnostics to collect during validation and runtime:</strong><br>1. <code>mChecksum</code> and <code>templateVersion</code>, <br>2. <code>parseTimeMs</code> when parsing M template, <br>3. <code>injectionTimeMs</code>, <br>4. <code>lastRefreshTimeMs</code>, <code>lastRefreshStatus</code>, <br>5. provider details (driver, connector version), <br>6. sanitized <code>paramsHash</code> and <code>evidenceRef</code>. <br><strong>PQ golden promotion story (example):</strong><br>1. Template author publishes <code>normalize_mails_v3.m</code> with <code>mChecksum</code>. <br>2. CI runs <code>mChecksum</code> parity and headless template static analysis (lint + expected sample output checksum) and produces <code>pq_template_report.json</code>. <br>3. Owners sign and release; <code>REG_EnsureDeps</code> in CI picks up new manifest and emits <code>deps.report.json</code> which CI stores alongside release. <br>4. Production deferred-init verifies <code>mChecksum</code> and signatures before enabling injection features. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Extensive DAX conceptual guidance, governance, and examples:</strong><br><strong>Why DAX governance matters:</strong> DAX measures are evaluated in the context of filters, relationships, and model metadata. Small changes to expressions or evaluation contexts can alter regulated aggregates used in reports; thus DAX expressions must be versioned, smoke-tested deterministically, and linked to expected sample values for CI gating. <br><strong>DAX manifest entry recommended fields:</strong><br>1. <code>measureId</code><br>2. <code>expressionHash</code> = <code>sha256(canonicalExpression)</code><br>3. <code>ownerAlias</code><br>4. <code>expectedSampleOutput</code> and <code>expectedSampleContextHash</code><br>5. <code>evaluationConstraints</code> (e.g., snapshot timestamp) <br><strong>Canonicalization rules for DAX expressions (conceptual):</strong><br>1. Strip comments, normalize whitespace, and normalize function casing if project policy requires. <br>2. Replace volatile functions (<code>NOW()</code>, <code>TODAY()</code>) with config-driven snapshot tokens for golden runs. <br>3. Compute <code>expressionHash=sha256(canonicalExpressionBytes)</code>. <br><strong>Headless DAX evaluation harness & CI requirements (recommended):</strong><br>1. Maintain a canonical <code>filterContext</code> snapshot with dataset checksums and small sample dataset for smoke-tests. <br>2. CI headless evaluator computes measure outputs and compares to <code>expectedSampleOutput</code> within tolerances; failing tests block promotion. <br>3. For promotion, require signature and owner approval if <code>expressionHash</code> changed. <br><strong>DAX change example workflow:</strong><br>1. Developer updates <code>netRevenue_v4</code> measure and computes new <code>expressionHash</code>. <br>2. CI runs headless DAX evaluation against canonical sample context; expected sample output matches within configured tolerance and <code>ci.dax_check</code> passes. <br>3. Owners sign the change and release manifest updated; <code>REG_EnsureDeps</code> and <code>REG_Config</code> ensure new <code>expressionHash</code> and evidence are recorded for production. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Large-scale operational narratives, expanded examples & multi-stage walkthroughs (very detailed):</strong><br><strong>A. Canary release with dependency verification and KPI gating (full flow):</strong><br>1. Release <code>manifest-v2.1</code> published with updated PQ templates and DAX measures; CI generates <code>deps.report.json</code> and <code>config.hash</code> for release. <br>2. Operator initiates Canary rollout: <code>ReloadConfig</code> swaps new config into 5% cohort after <code>REG_EnsureDeps</code> success in CI and signatures verified. <br>3. Canary cohort machines run deferred-init <code>REG_EnsureDeps</code> and local <code>deps.report.json</code> checks; any <code>critical</code> failure aborts canary. <br>4. Smoke-tests for PQ template outputs and DAX heads run; KPIs (error rate, latency, measure deltas) collected. <br>5. If metrics within thresholds for configured window, rollout expands to 25%, run another KPI window. <br>6. If KPI breach detected, auto-watchdog triggers <code>RollbackToHash(beforeHash)</code> and emits <code>config.rollback</code> with <code>forensic_manifest.json</code>. <br><strong>B. Emergency hot-swap & rollback with forensic capture (extreme case):</strong><br>1. Production incident: <code>REG_Apply</code> consumer reports correlated <code>correlationId</code> pointing to recent <code>config.reload</code>. <br>2. SRE author emergency <code>hotfix</code> manifest with minimal change and runs <code>hotSwap.preview</code> with <code>RegisterUnitTestHook</code> smoke-tests. <br>3. Two authorized approvers sign hotfix artifact; <code>HotSwapHandlers</code> applies in-memory patch for immediate remediation; <code>config.hotswap.applied</code> audit appended. <br>4. Watchdog monitors KPIs; if regression observed reverts <code>HotSwap</code> via <code>hotSwap.reverted</code> and triggers <code>RollbackToHash</code> to last good hash. <br>5. Forensics package assembled: <code>audit_tail.csv</code>, <code>deps.report.json.prev</code>, <code>forensic_manifest.json</code>, migration patches, PQ <code>mChecksum</code> history, DAX <code>expressionHash</code> records. <br>6. Regulatory notification prepared as needed using this canonical evidence. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Operational controls, RBAC & approvals (detailed):</strong><br>1. Changes impacting regulated features require <code>twoPerson</code> approval stored as signed artifacts referenced by <code>evidenceRef</code>. <br>2. Emergency overrides require two authorized approvers and limited TTL; <code>deps.override.applied</code> audit event must record operator ids and <code>justificationRef</code>. <br>3. Access to evidence store requires granular RBAC and approval flows; all access audited. <br>4. Operators can only re-enable regulated controls after <code>deps.check.completed</code> with no <code>critical</code> failures or after documented approved override is in place. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Long-term maintainability guidance & developer recommendations (practical):</strong><br>1. Provide canonicalization reference implementations for M templates, JSON manifests and DAX expression hashing in Node and Python; include unit tests for byte-for-byte parity. <br>2. Keep the synchronous path minimal: discovery + local verification only; offload heavy work to background workers. <br>3. Use copy-on-write in-memory snapshots for <code>GetDepsSnapshot</code> to serve ribbon and worker processes concurrently without locks. <br>4. Persist <code>deps.report.json.prev</code> and rotate evidence bundles with TTL per retention policy. <br>5. Add migration linting and idempotency checks into CI for any artifact that changes dependency graphs. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Monitoring, SLOs & runbook triggers (operational):</strong><br>1. <code>deps.check.duration_ms</code> median target <2s for typical release artifact checks (local-only), background validation target <30s. <br>2. <code>deps.missing.count</code> spike after release should page on-call if sustained beyond short window. <br>3. <code>deps.check.failed</code> events for <code>critical</code> severity should page SRE and notify owners. <br>4. Daily background scan of critical dependencies to detect certificate expiry and drift; automated notification 90/30/7 days before expiry. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Forensic packaging contents & retention policy (explicit):</strong><br>1. <code>deps.report.json</code> current and prior versions. <br>2. <code>audit_tail.csv</code> rows covering implicated <code>correlationId</code>s. <br>3. PQ M canonical templates with <code>mChecksum</code> history. <br>4. DAX <code>expressionHash</code> registry and <code>filterContext</code> snapshots for failing runs. <br>5. Evidence bundles with signatures, cert chains, and extracted metadata. <br>6. <code>forensic_manifest.json</code> enumerating artifacts with <code>sha256</code> checksums, retention tags and URIs. <br>7. Retention: encrypted evidence adheres to regulatory retention policies (example: 7 years for certain regulated outputs). </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Governance & compliance packaging (explicit):</strong><br>1. For regulated outputs, maintain chain-of-custody artifacts: signed config artifacts, migration fingerprints, PQ <code>mChecksum</code>s, DAX <code>expressionHash</code> entries, audit rotations, and golden-run evidence. <br>2. Ensure all artifacts in the compliance package are checksummed (sha256) and signed where required. <br>3. Evidence access requires owner-level approvals and is logged for chain-of-custody. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Acceptance criteria & release gating checklist (must pass):</strong><br>1. Unit + integration + golden <code>deps.report.json</code> parity tests pass. <br>2. No forbidden synchronous APIs used in code inspected by CI. <br>3. Audit rows emitted (<code>deps.check.started</code> → <code>deps.check.completed</code> or <code>deps.check.failed</code>) and <code>VerifyAuditChain</code> passes for golden runs. <br>4. Signature verification vectors present and passing. <br>5. Performance SLOs met for local checks and snapshot reads under load. <br>6. For regulated features, all required signatures and owner approvals present. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Appendices, canonical helper artifacts & reference materials to include in repository:</strong><br>1. Canonicalization specification for JSON manifests, M templates and DAX expressions with unit tests. <br>2. <code>deps.report.json</code> JSON Schema and a set of golden fixtures. <br>3. PQ M canonicalizer tool and <code>mChecksum</code> sample vectors. <br>4. DAX expression canonicalizer and <code>expressionHash</code> golden set. <br>5. <code>OWNERS.md</code> template and sample <code>approvalPolicy</code> documents. <br>6. Evidence store API and KMS/HSM integration guide. <br>7. CI pipeline steps for <code>REG_EnsureDeps</code> golden test runs. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Final prescriptive constraints & must-not list (non-negotiable):</strong><br>1. Do not perform network calls on the UI thread. <br>2. Do not persist unsigned artifacts for production regulated features unless explicit signed override is recorded. <br>3. Do not include plaintext secrets in manifests. <br>4. Do not accept arbitrary executable migration scripts in manifests for production. <br>5. Do not surface PII in audit rows or UI <code>userHint</code>. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Ultimate operational takeaway (concise):</strong><br>REG_EnsureDeps must be deterministic, conservative for regulated features, auditable, reversible via preserved <code>.prev</code> artifacts, and integrated with PQ and DAX governance. It must balance fast synchronous decisions for the UI with deep background validation and produce canonical <code>deps.report.json</code> artifacts that serve CI, forensic, and operator workflows reliably. </td></tr></tbody></table></div><div class="row-count">Rows: 33</div></div><script src="assets/xlsx.full.min.js?v=1758605028" defer></script>
<script src="assets/script.js?v=1759748863" defer></script>
<script src="assets/worker.js?v=1758331710" defer></script>
<script>
(function(){
  const template = "{table}_{date}";
  const userVal = "";
  const hrefPrefix = "assets";
  function formatName(tableName) {
    const date = (new Date()).toISOString().slice(0,10);
    return template.replace('{table}', tableName).replace('{date}', date).replace('{user}', userVal);
  }
  const btn = document.getElementById('exportBtn');
  if(btn) {
    btn.addEventListener('click', async function() {
      try {
        const html = document.documentElement.outerHTML;
        if(html.length > 2000000) { alert('Export refused: html too large'); return; }
        if(window.Worker) {
          const workerUrl = (function(){ try{ return hrefPrefix + '/worker.js'; }catch(e){ return null; } })();
          if(workerUrl) {
            try {
              const worker = new Worker(workerUrl);
              worker.postMessage({html: html, format: 'pdf'});
              worker.onmessage = function(e) { console.log('worker:', e.data); alert('Worker replied: '+(e.data.msg||e.data.status)); };
            } catch(err) { console.warn('Worker create failed', err); alert('Worker not available'); }
          } else { alert('Worker not available'); }
        } else { alert('Export worker not supported in this environment.'); }
      } catch(err) { console.warn('Export failed', err); alert('Export worker not available. See console for details.'); }
    });
  }
})();
window.addEventListener('load', function(){ try{ document.querySelectorAll('.table-wrapper').forEach(function(e){ e.style.opacity='1'; }); }catch(e){} });
</script>
</div>
</body>
</html>