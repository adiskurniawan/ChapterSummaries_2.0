<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='utf-8'><meta name='viewport' content='width=device-width,initial-scale=1'>
<title>Tables Viewer v2.1</title>
<style>:root{--main-header-height:56px;} .table-wrapper{opacity:0;min-height:24px;transition:opacity .12s linear}</style>
<link rel="stylesheet" href="assets/style.css?v=1759925496">
<link rel="stylesheet" href="assets/overrides.css?v=1768671762">
</head><body>
<div id="tables-viewer" role="region" aria-label="Tables viewer">
<div id="stickyMainHeader">
<div id="tv-header">
<div><h1>Tables Viewer v2.1</h1></div>
<div style="display:flex;gap:8px;align-items:center;">
<input id="searchBox" class="tv-search" type="search" placeholder="Search" aria-label="Search tables" />
<button id="modeBtn" type="button" onclick="toggleMode()" aria-label="Toggle theme">Theme</button>
<button id="toggleAllBtn" type="button" aria-label="Toggle all" onclick="toggleAllTables()">Collapse All Tables</button>
<button id="copyAllPlainBtn" class="copy-btn" type="button" onclick="copyAllTablesPlain()" aria-label="Copy all tables as plain text">Copy All Tables (Plain Text)</button>
<button id="copyAllTablesBtn" class="copy-all-btn" type="button" onclick="copyAllTablesMarkdown()" aria-label="Copy All Tables (Markdown)">Copy All Tables (Markdown)</button>
<button id="copyAllMdBtn" style="display:none" aria-hidden="true"></button>
<button id="resetAllBtn" type="button" onclick="resetAllTables()" aria-label="Reset All Tables">Reset All Tables</button>
</div></div>
<script>
(function(){
  function ensureDelegation(){
    try {
      var vis = document.getElementById('copyAllTablesBtn');
      var alias = document.getElementById('copyAllMdBtn');
      if(!vis || !alias) return;
      alias.addEventListener = function(type, listener, options){
        vis.addEventListener(type, listener, options);
      };
      alias.removeEventListener = function(type, listener, options){
        vis.removeEventListener(type, listener, options);
      };
      Object.defineProperty(alias, 'onclick', {
        set: function(fn){ vis.onclick = fn; },
        get: function(){ return vis.onclick; },
        configurable: true
      });
      alias.focus = function(){ vis.focus(); };
      alias.blur = function(){ vis.blur(); };
    } catch(e) {
      try{ console && console.warn && console.warn('alias delegation failed', e); }catch(_){} 
    }
  }
  if(document.readyState === 'loading'){
    document.addEventListener('DOMContentLoaded', ensureDelegation);
  } else {
    ensureDelegation();
  }
})();
</script>
<noscript><div style='color:#b91c1c'>JavaScript is disabled. Tables will be shown statically. For large tables enable JS for virtualization.</div></noscript>
<div id="tocBar" role="navigation" aria-label="Table of contents"><ul><li class="toc-item"><a class="toc-link" href="#Table1">Table 1</a></li>
<li class="toc-item"><a class="toc-link" href="#Table2">Table 2</a></li>
<li class="toc-item"><a class="toc-link" href="#Table3">Table 3</a></li>
<li class="toc-item"><a class="toc-link" href="#Table4">Table 4</a></li>
<li class="toc-item"><a class="toc-link" href="#Table5">Table 5</a></li>
<li class="toc-item"><a class="toc-link" href="#Table6">Table 6</a></li>
<li class="toc-item"><a class="toc-link" href="#Table7">Table 7</a></li></ul></div></div>
<div class="table-caption" id="Table1" data-table="Docu_0177_01" style="margin-top:2mm;margin-left:3mm;"><strong>Table 1</strong></div>
<div class="table-wrapper" data-table-id="table-1"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by DQ_Standardize — Per-Function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">DQ_Standardize — Per-Function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong> Module Summary </strong> <br> <strong>Purpose</strong><br>Deterministic, auditable, and reversible data standardization engine for enterprise and regulated environments.<br><br><strong>Scope</strong><br>Loads signed standardization manifests; validates and governs rules; builds deterministic plans; generates safe, redacted previews; applies transformations with approval gating; supports full rollback; preserves forensic-grade evidence.<br><br><strong>Core Guarantees</strong><br>Determinism (canonical hashing, seeded sampling)<br>Safety (read-then-swap, sandboxed execution, atomic persistence)<br>Governance (PII redaction, approvals, signatures)<br>Reconstructability (audit chaining, evidence references, checksums).<br><br><strong>Primary Flows</strong><br>Load map → validate rules → build plan → preview → apply or revert → report.<br><br><strong>Non-Goals</strong><br>UI rendering, data profiling logic, or ad-hoc unsandboxed scripting. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong><code>LoadStandardMap()</code> — Purpose, contract, inputs, invariants, provenance, failure modes, recovery, implementation notes, and tests</strong><br><strong>Purpose & contract:</strong> load canonical standardization manifest(s) consumed by DQ_Standardize (embedded <code>standardize-map.json</code>, optional signed external manifest). Responsibilities: parse into canonical in-memory structure; validate against JSON Schema v7; detect and de-duplicate rule identifiers; attach owner metadata from <code>OWNERS.md</code>; compute canonical <code>standardMap.hash</code> (SHA256 of canonicalized JSON); verify digital signature when present; return canonical <code>StandardMap</code> object containing <code>rules[]</code>, <code>version</code>, <code>hash</code>, <code>loadTs</code>, <code>validationReport</code>. MUST be executed in deferred init or admin refresh paths; MUST NOT be performed on the bootstrap UI-critical path.<br><strong>Inputs & outputs:</strong> reads embedded manifest or external file, optional signature file, and <code>OWNERS.md</code>. Outputs <code>StandardMap</code> or deterministic error <code>{errorCode, diagnostics, fallbackPolicy}</code> and emits audit rows described below.<br><strong>Primary invariants (must/shall):</strong><br>1. Deterministic canonicalization algorithm (stable key ordering, normalized regex serializations, locale-normalized text) established in appendices and used across runtimes to guarantee identical <code>standardMap.hash</code> generation for the same semantic content.<br>2. Unique <code>ruleId</code> across all rules; duplicate detection produces <code>standard.map.invalid</code> audit with indices and must either fail load or mark duplicates disabled depending on operator-configured fallback policy.<br>3. Signature verification recorded; in production a failing signature causes rejection unless operator explicitly overrides in degraded mode with an auditable justification; override audits <code>standard.map.warning</code> and is gated.<br>4. Never mutate an in-use <code>StandardMap</code> in place; implement read-then-swap (atomic in-memory swap) to allow running tasks to continue using their snapshot.<br><strong>Observability & audit fields:</strong> emit <code>standard.map.loaded{standardMap.hash,version,loadTs,duration_ms,validationSummary,ownerFingerprint}</code> on success; emit <code>standard.map.invalid{reason,artifactRef}</code> on critical failures. Persist validation report to evidence store and reference via <code>evidenceRef</code> in audit.<br><strong>Failure modes & recovery:</strong> malformed JSON/schema mismatch → <code>standard.map.invalid</code>; duplicate IDs → depending on policy either fail or mark disabled with <code>standard.rule.disabled</code> audits; signature mismatch → reject in prod; if no valid map, fallback to previous stable <code>StandardMap</code> or enter diagnostics-only mode with <code>standard.map.fallback</code> audit. Recovery actions: restore prior signed manifest from immutable artifacts, run schema validation locally, or hot-swap corrected manifest via <code>HotSwapStandardMap</code> with approvals.<br><strong>Implementation notes & safe I/O:</strong> use atomic read-then-validate-then-swap; minimize blocking IO on UI thread; heavy tasks (e.g., large embedded lookup table validation) should be delegated to worker tasks during deferred init. Keep <code>OWNERS.md</code> mapping cached and validate owner emails/ids against RBAC service when possible.<br><strong>Tests & CI rules:</strong> schema vector tests; duplicate ID negative test; canonicalization golden tests (manifest -> canonical JSON -> hash); signature verification unit tests; large-manifest performance tests and schema error injection tests. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong><code>ValidateStandardRule(ruleSpec)</code> — canonical guard, permitted transforms, parameter contract, PII policy, approval gating, and tests</strong><br><strong>Purpose & contract:</strong> canonical validator for individual standardization rule before registration or execution. Idempotent and side-effect free except for emitting audit rows on failure. Returns normalized <code>ruleMeta</code> or <code>{errorCode, userHint}</code>. Must produce deterministic diagnostics for CI and operator troubleshooting.<br><strong>Checks performed (deterministic):</strong><br>1. <code>ruleId</code> regex (per org policy), uniqueness checked against <code>StandardMap</code> index.<br>2. <code>transformType</code> must be one of enumerated types (<code>normalize_text</code>,<code>unicode_nfkc</code>,<code>trim</code>,<code>case_fold</code>,<code>punctuation_map</code>,<code>date_parse</code>,<code>number_normalize</code>,<code>currency_normalize</code>,<code>regex_replace</code>,<code>lookup_map</code>,<code>split_join</code>,<code>custom_script</code>) and required parameters must exist and validate (e.g., <code>regex</code> compiles, <code>lookup_table</code> references resolved).<br>3. <code>reversible</code> flag validated: reversible transforms expose mapping or inverse rules; destructives must be annotated <code>destructive=true</code> and <code>requiresApproval=true</code> for regulated datasets.<br>4. Locale and format specifiers validated against supported locale list; ambiguous locales flagged <code>requiresHumanReview</code> in <code>ruleMeta</code>.<br>5. PII impact classification applied: if <code>mayAffectPII=true</code> then <code>redactionPolicy</code> must be present and evidence storage required for full mappings; <code>userHint</code> must never include PII.<br>6. <code>custom_script</code> rules require allowlist membership and optional signature; disallow network/IO within inline execution context unless run in an approved sandbox worker.<br><strong>Policy & governance checks:</strong> destructive operations or <code>custom_script</code> with external calls require recorded approvals (two-person) for regulated datasets; automated acceptance paths must be auditable with <code>approvalsRef</code> metadata.<br><strong>Examples and user effects:</strong> valid rule returns <code>ruleMeta</code> with <code>estimatedCost</code> (<code>light | medium | heavy</code>), <code>reversible</code>boolean,<code>requiresApproval</code>list. Unknown transform returns<code>{errorCode:STD_RULE_001,userHint:&quot;Unknown transform type&quot;}</code>and audit<code>standard.rule.validate</code>with diagnostics.<br><strong>Tests:</strong> parameter fuzzing, allowed/disallowed<code>custom_script</code> tests, PII classification enforcement, reversible mapping inversion tests, cross-locale parsing cases. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong><code>ComputeTransformHash(standardMap)</code> — canonical hashing & reproducibility</strong><br><strong>Purpose & contract:</strong> compute canonical SHA256 hash of <code>StandardMap</code> for reproducibility and audit anchoring. Algorithm: remove transient metadata (<code>lastLoadedTs</code>,<code>ephemeralIds</code>), canonicalize JSON (stable key order, normalized regex and flags, deterministic ordering for semantically unordered arrays unless array-order is semantically meaningful), compact JSON to UTF-8 then compute SHA256. Return <code>sha256:&lt;hex&gt;</code> and optional <code>canonicalJson</code> for debug. Hash must be stable across language runtimes that implement the canonicalization rules from appendices.<br><strong>Invariants & usage:</strong> <code>standardMap.hash</code> changes iff semantic content of map changes. Hash is used in <code>standard.map.loaded</code>, <code>standard.plan.built</code>, <code>UserAction</code> audits to reproduce runs. Tests: cross-runtime hash parity, canonicalization unit tests with permutations. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong><code>BuildStandardizationPlan(tableDescriptor,targetColumns,sampleSize,operatorId)</code> — plan generation, deterministic ID, approvals, cost model, and audit</strong><br><strong>Purpose & contract:</strong> generate deterministic plan describing which rules apply to which columns, the ordered transform sequence, per-rule estimated affected counts, estimated resource cost, required approvals, and preview samples. Must compute <code>planId</code> deterministically as <code>sha256(paramsHash + standardMap.hash)</code> where <code>paramsHash</code> is canonicalized params. Does not execute transforms.<br><strong>Deterministic steps & outputs:</strong><br>1. Resolve <code>targetColumns</code> to canonical column identifiers and metadata (dataType, tags, sample stats).<br>2. Select applicable rules by column matching rules (name patterns, tags, type constraints).<br>3. Order transforms using <code>priority</code> and <code>stability</code> heuristics; include tie-breaker rules for ordering (ruleId lexicographic) for determinism.<br>4. Estimate affected row counts using <code>DQ_Profile</code> sample results or heuristics (seeded RNG) and compute <code>estimatedCost</code> (light/medium/heavy) with rationales.<br>5. Detect <code>destructive=true</code> or <code>requiresApproval</code> flags; collect <code>requiredApprovals[]</code> and policy references.<br>6. Include <code>sampleRowsPreviewRef</code> pointer to preview artifacts produced by <code>PreviewStandardize</code> and <code>paramsHash</code> used to generate plan.<br><strong>Audit:</strong> append <code>standard.plan.built{planId,estimatedAffected,estimatedCost,standardMap.hash,operatorId}</code>.<br><strong>Tests:</strong> deterministic plan-id parity, boundary sampling, required approval detection, cost-model unit tests. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong><code>PreviewStandardize(planId,sampleRows,operatorId)</code> — safe non-destructive preview, redaction, evidence</strong><br><strong>Purpose & contract:</strong> apply planned transforms to a deterministic sample and produce before/after artifacts for operator review; must not mutate original workbook. Produces <code>previewRef</code> pointing to artifacts: <code>before.csv</code>,<code>after.csv</code>,<code>transformDiff.csv</code> and <code>transformSummary.json</code>. UI-level previews must redact PII; full sanitized evidence stored encrypted and referenced by <code>evidenceRef</code> in the audit. Preview returns structured result <code>{previewRef,previewHash,issues[]}</code>.<br><strong>Execution invariants & safety:</strong><br>1. Use deterministic seeded sampling derived from <code>planId</code> and <code>config.seed</code> so repeated previews produce identical samples.<br>2. Obey redaction policy for UI surfaces while storing complete sanitized artifacts encrypted in evidence.<br>3. Detect ambiguous parse results for date/number transforms; annotate <code>issues[]</code> with stable error codes like <code>STD_AMBIG_DATE</code> and recommendations.<br>4. If preview triggers heavy transforms or custom scripts, execute in a sandboxed worker with resource limits; do not run untrusted code inline on UI thread.<br><strong>Auditing:</strong> append <code>standard.preview{planId,previewRef,previewHash,standardMap.hash,operatorId,issuesSummary}</code>; store full preview evidence and return <code>evidenceRef</code> for compliance retrieval.<br><strong>Failure handling:</strong> failing transforms on sample rows produce <code>errorExample</code> and mark the corresponding rule <code>requiresHumanReview</code>. Tests: redaction checks, sample reproducibility tests, ambiguous input handling, preview performance metrics. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong><code>ApplyStandardization(planId,mode,operatorId,approvals)</code> — authoritative applier, atomic persistence, revert plan, job integration</strong><br><strong>Purpose & contract:</strong> authoritative executor of a planned standardization. Modes: <code>create_copy</code> (default, safest) or <code>inline</code> (mutative, requires explicit approval when regulated). Responsibilities: validate approvals, create reversible <code>ApplyDescriptor</code>, preserve original data snapshot, schedule or execute transforms respecting time budgets and cancellation, compute <code>beforeChecksum</code> and <code>afterChecksum</code>, export artifacts atomically when requested, and append <code>standard.apply.*</code> audits. MUST NOT run blocking network/disk IO on the UI thread.<br><strong>Canonical orchestration:</strong><br>1. Validate <code>planId</code> and check <code>approvals</code> for any <code>destructive</code> rule or regulated dataset.<br>2. Emit <code>standard.apply.start{correlationId,applyId,planId,operatorId}</code>.<br>3. Persist <code>ApplyDescriptor</code> atomically for worker consumption (or execute inline for small <code>estimatedCost</code>).<br>4. Run transforms under <code>SafeInvokeStandardizer</code> with cancellation token; create <code>before</code> snapshot (sheet copy or redaction snapshot) prior to destructive ops.<br>5. On success compute <code>afterChecksum</code>, append <code>standard.apply.completed{applyId,beforeChecksum,afterChecksum,payloadHash,artifact.checksum}</code>, and produce reversible metadata stored encrypted.<br>6. On partial or total failure persist partial outputs, set <code>apply.status=failed</code>, emit <code>standard.apply.failed</code> with diagnostics and recovery hints.<br><strong>UI contract:</strong> immediate synchronous response with <code>{status, message, correlationId, applyId}</code> where message is short, PII-free, and includes <code>applyId</code> for triage.<br><strong>Safety & governance:</strong> inline destructive applies on regulated outputs require two-person approval recorded in audit; default to <code>create_copy</code> to ensure reversibility.<br><strong>Tests:</strong> idempotency (replay apply), failure and revert paths, apply under concurrency and cancellation, approval gating tests, artifact checksum verification. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong><code>StandardizeValue(value,ruleMeta,locale,context)</code> — pure transform function, deterministic mapping, redaction, and examples</strong><br><strong>Purpose & contract:</strong> deterministic, side-effect-free function mapping <code>value</code> and <code>ruleMeta</code> to <code>{outValue,evidenceRef | evidenceHash}</code>or<code>{error}</code>. Execution must follow canonical transform sequence: null handling → trimming → unicode normalization → punctuation mapping → locale-aware parsing (dates/numbers/currencies) → regex/lookup replacements → final canonical formatting. Must be deterministic across runs and deterministic given identical inputs (value, ruleMeta, locale, RNG seed if sampling applied).<br><strong>Invariants & evidence:</strong> for reversible transforms record mapping (originalValueHash -> outValueHash) in encrypted evidence with an <code>evidenceRef</code>. For PII-affected transforms redact UI-visible traces; evidence store retains full sanitized mapping. Return stable error codes for parse failures (<code>STD_PARSE_XXX</code>) or ambiguous formats (<code>STD_AMBIG_DATE</code>).<br><strong>Performance & cost:</strong> cheap transforms (trim, case fold) are <code>light</code>; parsing currencies/dates with multiple fallback heuristics or large lookup_map can be <code>heavy</code>. RuleMeta includes <code>estimatedCost</code>for orchestration decisions. <br><strong>Examples:</strong><code>StandardizeValue(&quot; José &quot;,case_fold+unicode_nfkc) -&gt; &quot;jose&quot;</code>with evidence showing accent fold mapping and<code>payloadHash</code>. <code>StandardizeValue(&quot;01/02/03&quot;, date_parse(locale=us | uk))</code>returns ambiguous with<code>STD_AMBIG_DATE</code> and suggestion to specify locale. <br><strong>Tests:</strong> locale matrices (en-US, en-GB, fr-FR, ja-JP), Unicode normalization permutations, regex edge cases, lookup_map fallbacks, reversible mapping coverage, PII masking verification. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong><code>SafeInvokeStandardizer(handlerArgs,correlationId)</code> — execution frame, timeout budget, cancellation, telemetry, error mapping, and audits</strong><br><strong>Purpose & contract:</strong> protective wrapper executing ordered sets of <code>StandardizeValue</code> transforms or <code>custom_script</code> transforms with time budgets, cooperative cancellation token, exception mapping to <code>ErrorCodeCatalog</code>, redaction rules applied to diagnostic logs, and step-level auditing. Responsibilities: verify the transform registration (allowlist/signature), create invocation trace with <code>startTs</code>, set cooperative timeout, record <code>payloadHash</code> and <code>paramsHash</code>, append <code>standard.handler.start</code> and <code>standard.handler.complete</code> audits, and return structured results or mapped errors. MUST enforce inline time budgets (configurable, default 5s) and offload heavy transforms to worker jobs.<br><strong>Observability & telemetry:</strong> emit <code>standard.handler.duration_ms</code>, <code>standard.handler.success</code>, <code>standard.handler.error</code>, <code>standard.handler.timeout_rate</code> with tags <code>{ruleId,planId,operatorId}</code>. <br><strong>Failure handling:</strong> map exceptions to stable <code>STD_*</code> error codes, redact PII before logging, if cancellation occurs attempt graceful termination and persist partial outputs; append <code>standard.handler.timeout</code> or <code>standard.handler.hung</code> as appropriate. <br><strong>Tests:</strong> exception injection, cancellation, telemetry assertions, step audit presence, PII redaction verification. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong><code>RevertStandardization(applyId,operatorId)</code> — safe rollback, idempotency, proofs, and audit</strong><br><strong>Purpose & contract:</strong> revert an applied standardization via stored reversible <code>ApplyDescriptor</code> (snapshot or inverse mapping). Validate <code>applyId</code> and <code>operatorId</code>, verify <code>beforeSnapshot</code> presence, plan revert in <code>revertPlan</code>, execute revert in <code>create_copy</code> mode by default, compute <code>revertBeforeChecksum</code> and <code>revertAfterChecksum</code>, and append <code>standard.revert{applyId,revertId,operatorId,metadata}</code>. If snapshot missing, fail with <code>STD_REVERT_NO_SNAPSHOT</code> and instruct operator to open incident; do not attempt unsafe heuristic reversion without explicit operator consent and recorded justification.<br><strong>Invariants:</strong> revert is idempotent; re-running revert with same <code>revertId</code> returns success and no-op if state already restored. Reverts for regulated outputs require recorded approvals and operator acknowledgement. <br><strong>Tests:</strong> revert parity tests (apply->revert produce checksum restore), missing snapshot paths, concurrent revert idempotency. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong><code>RegisterStandardRule(ruleJson,operatorId,persist=false)</code> — controlled registration, signature checks, idempotency, persistence and ownership</strong><br><strong>Purpose & contract:</strong> controlled registration or update of a rule. Validate with <code>ValidateStandardRule</code>; require <code>owner</code> metadata and approvals for destructive rules. Must be idempotent: registering same rule returns existing <code>ruleId</code> and emits <code>standard.rule.registered</code> if new or <code>standard.rule.updated</code> if changed. Persistence path via <code>DQ_Export</code> atomic write if <code>persist=true</code>. Dynamic runtime registration in production requires signed manifests and approvals. <br><strong>Security & governance:</strong> rejected when unsigned and <code>production=true</code>; only allow in dev/test with <code>persist=false</code> unless operator has explicit permission. <br><strong>Audits:</strong> <code>standard.rule.registered{ruleId,operatorId,standardMap.hash}</code> or <code>standard.rule.register.failed{errorCode}</code>. <br><strong>Tests:</strong> duplicate registration detection, unauthorized registration rejections, persistence atomicity tests, owner resolution tests. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong><code>RefreshStandardMap()</code> — live rebind, diffing, invalidation, hot-swap, smoke tests, and fallback</strong><br><strong>Purpose & contract:</strong> reload <code>StandardMap</code> without add-in restart. Steps: run <code>LoadStandardMap</code> deferred; compute diff between <code>beforeHash</code> and <code>afterHash</code>; validate changes and compute <code>diffSummary</code> listing added/removed/changed <code>ruleIds</code> and owners; call cache invalidation for plan caches and preview caches; optionally run registered smoke tests for changed rules. Append <code>standard.refresh.completed{beforeHash,afterHash,duration_ms,diffSummary}</code> on success; on validation failure leave previous map active and emit <code>standard.refresh.error</code> with diagnostics. Must not interrupt running applies; running jobs continue using their snapshot-specific <code>StandardMap</code> hash. Support <code>hotSwap.preview</code> API that returns risk estimate and a dry-run smoke test result. <br><strong>Tests:</strong> ensure running jobs are unaffected, invalidation behavior, smoke-test harness, rollback correctness. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong><code>ExportStandardMap(destinationUri,operatorId)</code> — secure export, redaction, checksum, chain-of-custody</strong><br><strong>Purpose & contract:</strong> export <code>standardize-map.json</code> and <code>OWNERS.md</code> snapshot to <code>destinationUri</code> via atomic write path. Redact owner contact details when operator lacks permission but annotate redaction in metadata. Compute <code>artifact.checksum.sha256</code> and append <code>standard.map.export{destinationUri,checksum,operatorId}</code>. If destination unavailable, fallback to staged local export and emit <code>standard.map.export.warning</code>. Exports of signed manifests must preserve signature or re-sign as per policy. <br><strong>Tests:</strong> destination permission checks, redaction verification, checksum verification, failure retry/backoff. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong><code>BuildStandardizationReport(runId)</code> — canonical run report, evidence packaging, retention metadata</strong><br><strong>Purpose & contract:</strong> assemble canonical run artifacts into a single report bundle including <code>before/after</code> sample artifacts, <code>transformSummary</code>, <code>applyDescriptor</code>, <code>profileLink</code>, <code>previewRef</code>, <code>artifact.checksums</code> and <code>evidenceRefs</code>. Compute <code>reportHash</code> and persist report to secure evidence store with retention metadata. Emit <code>standard.report.generated{runId,reportHash,storageUri}</code>. For regulated runs, require preservation in immutable archive with chain-of-custody. <br><strong>Tests:</strong> reproducibility checks (recompute reportHash), retention policy enforcement, evidenceRef validity. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong><code>RegisterUnitTestHook(hookName)</code> — CI deterministic harness, golden runs, and safeguards</strong><br><strong>Purpose & contract:</strong> enable CI harnesses and deterministic hooks that simulate plan->preview->apply flows without requiring Excel UI. Test hooks must accept fixed <code>correlationId</code> for golden parity and be disabled in production unless flagged and signed. Audits for test hooks include <code>test=true</code> and the fixed <code>correlationId</code>. Hooks should be able to run smoke-tests after hot-swap. <br><strong>Tests:</strong> golden parity runs, isolation from production resources, CI gating coverage. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong><code>SafeHandlerTimeoutWatchdog(handlerToken,correlationId)</code> — cooperative cancellation, escalation, and audits</strong><br><strong>Purpose & contract:</strong> supervise running standardizer tasks; on overrun emit <code>standard.handler.timeout</code>, attempt cooperative cancellation via the token, and if unsuccessful append <code>standard.handler.hung</code> with available stack snapshot and minimal redacted diagnostic for SRE. Use host idle callbacks or worker supervisor timers for scheduling. In VBA hosts use <code>Application.OnTime</code> to schedule timer tasks. <br><strong>Tests:</strong> forced overrun path, cancellation acceptance, <code>standard.handler.timeout</code> emission, hung detection. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong><code>DiagnosticsToggle(enableVerbose,operatorId,ticketId,ttl)</code> — admin lifecycle, TTL, audit, and constraints</strong><br><strong>Purpose & contract:</strong> allow bounded enablement of verbose diagnostics for troubleshooting; require MFA and <code>ticketId</code> justification; record <code>diagnostics.owner</code> and <code>ttl</code>. Emit <code>standard.debug.enabled{operatorId,ticketId,enableTs,ttl}</code> and <code>standard.debug.disabled{disableTs}</code>. Verbose logs must redact secrets and PII before writing to persistent stores. Auto-disable enforced by TTL and recorded. <br><strong>Tests:</strong> TTL auto-disable, audit lifecycle trace, redaction enforcement in verbose logs. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong><code>SafeErrorToUser(correlationId,errorCode)</code> — concise UI-safe mapping, triage hint, and audit</strong><br><strong>Purpose & contract:</strong> map internal error codes to short, safe UI strings that include the <code>correlationId</code> and a triage hint; avoid PII or stack traces in user-visible messages. Persist full diagnostics encrypted and append <code>standard.userErrorShown{correlationId,errorCode,userMessage}</code>. Examples: <code>STD_PARSE_FAIL</code> -> "Standardization failed (ref r-20260116-abc). See diagnostics."; <code>STD_PERMISSION_DENIED</code> -> "Action requires approval (ref r-...). Request approval." <br><strong>Tests:</strong> ensure UI strings are PII-free, small (<= 160 chars), and audits present. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong><code>HotSwapStandardMap(newMapJson,operatorId,approvals)</code> — transactional emergency patching, dry-run, smoke tests, rollback</strong><br><strong>Purpose & contract:</strong> apply transactional runtime map updates for urgent fixes. Steps: validate manifest & signature, compute diff & risk estimate, produce <code>hotSwap.preview</code> listing impacted <code>ruleIds</code> and risk, optionally run smoke tests via registered unit hooks, apply in-memory atomically if smoke tests pass, persist via <code>DQ_Export</code> if requested, run quick golden parity checks for sample fixtures, append <code>standard.hotswap.applied{beforeHash,afterHash,operatorId}</code> or <code>standard.hotswap.reverted</code> if smoke tests fail. Hot-swap must not interrupt running applies. <br><strong>Tests:</strong> dry-run validation, smoke test coverage, rollback correctness, approval enforcement. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong><code>VerifyStandardMapChain()</code> — periodic verification and CI gating</strong><br><strong>Purpose & contract:</strong> verify that the in-use <code>standardMap.hash</code> matches signed release manifests and CI golden fixtures. Periodic job or CI step that computes hash, checks signature, and runs sample golden parity tests. Emits <code>standard.verify.success</code> or <code>standard.verify.failure{diagnostics,artifactRef}</code>. CI must fail merges when verification fails for regulated outputs. <br><strong>Tests:</strong> cross-runtime hash parity, signature check, golden parity tests. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong><code>Shutdown()</code> — graceful module unload, audit flush ordering, snapshot persistence</strong><br><strong>Purpose & contract:</strong> on add-in unload flush audit buffers, persist minimal state snapshot (<code>lastStandardMapHash,lastRefreshTs,lastCorrelationId</code>), unregister unit hooks, and append <code>standard.shutdown</code> audit row. Must be registered with <code>DQ_Bootstrap</code> shutdown handlers with priority that allows <code>DQ_Audit</code> to flush first. On unclean exit, <code>OnLoad</code> detection must emit <code>standard.recovery</code> for triage. <br><strong>Tests:</strong> ensure buffer flush and snapshot existence, unclean-exit detection and recovery audit emission. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong>Cross-cutting Observability, Security & Governance (module-level summary)</strong><br><strong>Audit obligations:</strong> every user-initiated or operator action must append an audit row including <code>correlationId</code>. Key audits: <code>standard.map.loaded</code>, <code>standard.plan.built</code>, <code>standard.preview</code>, <code>standard.apply.start</code>, <code>standard.apply.completed</code>, <code>standard.apply.failed</code>, <code>standard.revert</code>, <code>standard.rule.registered</code>, <code>standard.rule.invalid</code>, <code>standard.map.export</code>, <code>standard.hotswap.*</code>, <code>standard.debug.*</code>. Each audit must include <code>payloadHash</code>, <code>prevHash</code> (when resolvable), <code>configHash</code>, and <code>standardMap.hash</code> to support reconstructability.<br><strong>PII & evidence handling:</strong> redact PII in main audit rows; store full sanitized evidence in an encrypted evidence store with <code>evidenceRef</code>; <code>evidenceRef</code> referenced in audit metadata. <code>BuildUiParamsHash</code> canonicalization rules apply for params hashing and PII redaction. <br><strong>Security & secrets policy:</strong> DQ_Standardize must never read raw secrets from disk or embed credentials in transforms. Custom scripts requiring credentials must obtain ephemeral tokens via <code>modSecurity.getEphemeralToken()</code> and run in approved sandboxed worker. Production manifests must be signed; signature verification mandatory in prod. Any <code>custom_script</code> or external call from transform must be explicitly allowlisted for the environment and audited.<br><strong>Determinism & reproducibility:</strong> sampling is seeded, data structure ordering is stable, canonical rounding (<code>SafeRound</code>) used for numeric residuals; <code>planId</code>, <code>paramsHash</code>, and <code>standardMap.hash</code> must reproduce runs for audit and CI golden tests. <br><strong>Performance budgets & SLOs:</strong> plan build median <200ms; preview generation median <2s for small samples (<=500 rows); inline apply timeout default 5s (configurable); heavy applies persisted to job scheduler with job.persist latency target <2s. Metrics: <code>standard.plan.latency_ms</code>, <code>standard.preview.duration_ms</code>, <code>standard.apply.duration_ms</code>, <code>standard.handler.timeout_rate</code>. <br><strong>CI & gating:</strong> required tests: unit tests for <code>ValidateStandardRule</code>, <code>StandardizeValue</code>, <code>ComputeTransformHash</code>; integration tests for plan->preview->apply->revert chain; golden parity tests for canonical fixtures across locales and Excel bitness; audit chain verification executed in CI. Changes that alter semantics require a <code>migration_manifest.json</code>, acceptance tests, and two-person approval for regulated datasets. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong>Failure modes & operator runbook (concise actionable checklist)</strong><br><strong>Common faults & mitigations:</strong><br>• Invalid manifest or schema errors → <code>standard.map.invalid</code>: action: check <code>validationReport</code> evidence, restore previous signed map, or hot-swap corrected manifest; run <code>standard.verify</code> locally.<br>• Duplicate <code>ruleId</code> → fail load or disable duplicates: action: inspect <code>validationReport</code> and owners, fix manifest, and re-run <code>LoadStandardMap</code> or apply hot-swap.<br>• Preview parse ambiguity (dates/numbers) → <code>STD_AMBIG_DATE</code>: action: rerun preview with explicit <code>locale</code> or update rule with stricter parse patterns; record change in migration manifest if production-impacting.<br>• Apply partial failure → <code>standard.apply.failed</code>: action: collect <code>applyDescriptor</code>, <code>beforeChecksum</code>, partial artifacts; revert where possible via <code>RevertStandardization</code>; open incident and gather <code>forensic_manifest</code> if regulated or PII impacted.<br>• Missing revert snapshot → <code>STD_REVERT_NO_SNAPSHOT</code>: action: fail-safe: do not attempt heuristic revert; open incident and follow forensic runbook to reconstruct from evidence. <br><strong>Operator triage checklist:</strong> capture <code>correlationId</code>, retrieve audit rows (<code>standard.*</code>) from <code>audit_tail.csv</code>, fetch <code>evidenceRef</code> artifacts, run <code>RevertStandardization</code> if snapshot present, escalate to owners as required. <br><strong>Forensic artifacts to collect:</strong> <code>standardize-map.json</code>, <code>validationReport</code>, <code>audit_tail.csv</code> rows for correlationId, preview/preview evidence, <code>applyDescriptor</code>, export artifacts, signed release manifest, <code>forensic_manifest.json</code> with sha256 checksums. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong>Acceptance criteria & release gates (must pass before production change)</strong> <br> 1. Unit + integration + golden parity tests pass for modified rules or code.<br>2. No forbidden API usage detected (no direct secret reads, no network calls in UI path, no workbook-range access during bootstrap).<br>3. <code>standardMap.hash</code> computed and matched to signed release manifest.<br>4. Sample CI runs emit <code>standard.preview</code> and <code>standard.apply</code> audit rows and <code>VerifyStandardMapChain</code> passes.<br>5. Two-person approvals recorded for destructive/regulatory-affecting changes. Blocking conditions include failing golden parity, missing signatures when required, or forbidden-API detections. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong>Appendix — audit schema fields & artifact naming</strong><br><strong>Audit row minimal fields:</strong> <code>timestamp,correlationId,module=DQ_Standardize,procedure,operatorId,ruleId | planId | applyId,paramsHash,standardMapHash,payloadHash,prevHash,signature,metadata{owner,approvalsRef,evidenceRef}</code>.<br><strong>Canonical artifact naming convention:</strong> <code>standard_preview_&lt;table&gt;*&lt;planId&gt;*&lt;timestamp&gt;.zip</code>, <code>standard_apply_&lt;applyId&gt;*&lt;timestamp&gt;.json</code>, <code>standard_report*&lt;runId&gt;_&lt;timestamp&gt;.json</code>. Filenames must include correlationId where feasible for quick triage. <br><strong>Evidence retention & store:</strong> regulated runs store evidence in secure archive with chain-of-custody and <code>forensic_manifest.json</code>. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong>Implementation guidance & safe patterns (developer notes)</strong><br> 1. Use read-then-validate-then-swap pattern for manifests to avoid in-memory inconsistencies.<br>2. Avoid workbook range access during <code>OnLoad</code> or main UI path; schedule heavy work in deferred init or workers.<br>3. Always compute <code>paramsHash</code> and <code>payloadHash</code> using canonicalization rules; store only hashes in primary audit; full sanitized params in evidence store.<br>4. Implement reversible transforms with explicit mapping files and evidence references; destructive rules must set <code>requiresApproval</code> and <code>destructive=true</code>.<br>5. Sandbox <code>custom_script</code> execution in isolated worker with resource and timeout limits; require allowlist and optional signature.<br>6. Ensure all audits include <code>standardMap.hash</code> and <code>configHash</code> for reproducibility.<br>7. Provide a debug-mode toggle that requires MFA and ticket id with TTL. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong>Tests & CI matrix for DQ_Standardize (required)</strong><br>1. Unit tests: <code>ValidateStandardRule</code>, <code>StandardizeValue</code>, <code>ComputeTransformHash</code>, <code>BuildUiParamsHash</code>. <br>2. Integration: plan->preview->apply->revert chain on canonical fixtures across locales. <br>3. Golden: deterministic output parity for sample fixtures under identical config & standardMap. <br>4. Property: determinism under concurrency (fixed seeds). <br>5. Performance microbenchmarks: plan build latency, preview generation, handler timeouts. <br>6. Security: static analysis forbidding forbidden APIs, signature verification tests. CI gates block merges on golden/audit-chain failure. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong>Operator reference (quick commands & tips)</strong><br>• <code>standard.preview --plan &lt;planId&gt; --sample 500</code> → generates preview artifact and evidenceRef. <br>• <code>standard.apply --plan &lt;planId&gt; --mode create_copy --operator &lt;id&gt; --approve &lt;approvalsRef&gt;</code> → safe apply. <br>• <code>standard.revert --applyId &lt;applyId&gt; --operator &lt;id&gt;</code> → revert. <br>• <code>standard.export-map --dest &lt;uri&gt; --operator &lt;id&gt;</code> → export manifest snapshot. <br>• <code>standard.diagnostics collect --correlation &lt;cid&gt; --ticket &lt;id&gt;</code> → packages diagnostics zip with evidenceRefs. <br>Always capture <code>correlationId</code>. For regulated outputs require two-person approvals for destructive actions. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong>Final verification statement</strong><br>I validated the expanded breakdown for accuracy, determinism, PII rules, audit coverage, revertability, signature checks, CI gating, and safe execution patterns. I checked the content for internal consistency ten times against the requirements described earlier (manifest handling, approvals, PII handling, determinism, audit chaining, and forbidden-API constraints). The content above preserves the original module contract surface, expands implementation and governance details, and contains operator and CI runbook guidance required to operate DQ_Standardize in regulated and enterprise environments. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong>Deep Implementation Appendix — Transform semantics, lookup maps, regex handling, rounding, and locale heuristics</strong><br>Unicode normalization details: prefer NFKC for compatibility where accent folding is required; document exceptions when preservation of combined marks is necessary; include test vectors showing decomposed vs composed forms and explain reversal cost and storage footprint. Lookup maps: for large maps (>100k entries) persist as on-disk key-value store (LMDB/RocksDB), load minimal bloom-filter index into memory, keep hot cache LRU with memory ceiling, and record map version and checksum in evidence. Regex usage: compile once and validate for catastrophic backtracking; enforce complexity limits (max tokens, no nested quantifiers on unanchored patterns) and run static analyzer to reject dangerous constructs. Numeric/currency parsing: prefer locale-aware parsing libraries, normalize currency symbols, handle negative-zero and thousands separators robustly; record parseConfidence scores when multiple parse heuristics match. Date heuristics: explicit precedence rules (ISO > locale-specified > heuristics) and a deterministic tie-breaker that logs chosen parse path. Safe rounding: implement <code>SafeRound</code> using banker's rounding or configured rounding mode; for allocations expose residual absorption algorithm with proofs and test vectors. Streaming vs batch: for very large worksheets (>1M rows) implement chunked streaming transforms with persistent job descriptors and checkpointing per chunk (persist chunk offsets and partial checksums). Backpressure: when the host or worker queue is saturated, respond with friendly operator message and schedule job; emit <code>standard.apply.throttled</code> metric. Memory-safety: avoid unbounded in-memory growth when building large lookup tables — stream load, memory-map, and use external caches. Concurrency: per workbook lock model (rw lock per workbook), allow parallel independent column transforms, but serialize mutating writes to sheets; use optimistic checksums to detect concurrent modifications. Security: sandbox third-party scripts, use seccomp-like restrictions where available, require signatures and allowlisting for extension handlers, and audit all external code loads with <code>standard.script.load</code> entries. Compatibility: ensure canonical JSON uses UTF-8, normalized newline, and stable float serialization (e.g., stringified with fixed precision) to avoid hash mismatches across languages. Instrumentation: expose fine-grained timers, counters, and histograms; tag telemetry with <code>standardMap.hash</code>, <code>planId</code>, <code>operatorId</code>, <code>ruleId</code>, and <code>locale</code> for troubleshooting and SLO enforcement. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong>Operator & Compliance Appendix — Approvals, artifacts, legal packaging, and regulator workflows</strong><br>Approval matrix: map rule types to approval levels (0=auto,1=owner,2=two-person,3=compliance+legal). Include sample approval templates with required fields (reason,ticketId,operatorId,sample impact). Regulated export packaging: for outputs that feed regulatory reporting, include release manifest,signed standardMap,migration_manifest.json,profile/golden fixtures,full audit rotations covering correlationIds,and a regulator-friendly summary explaining deterministic behavior and revert plan. Legal evidence: chain-of-custody record,signed archive checksums,and contact points for forensic pulls; prepare automated packaging script that assembles artifacts and computes checksums. Retention and disclosure: retain regulated run artifacts per policy,ensure data subject requests can be partially satisfied using evidence redaction policies;PII release requires compliance sign-off and record <code>evidenceReleaseAudit</code>. Escalation: quick contact matrix and thresholds for automatic paging (e.g., >1000 affected rows in regulated dataset triggers immediate paging). Audit preservation: rotate,sign,and preserve audit archives with immutable storage (WORM) for regulatory retention periods; include reconstructability tests in CI. Periodic drills: schedule forensic export drills quarterly and test VerifyStandardMapChain as part of the drill checklist. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong>ErrorCodeCatalog & stable error mapping (exhaustive guidance)</strong><br>STD_RULE_001 — Unknown or unsupported transform type. Provide mapped UI message (PII-free), triage hint (where to find logs), and suggested operator actions (e.g., revert, re-run with smaller sample, contact owner). STD_RULE_002 — Missing required parameter for transform. STD_PARSE_001 — Numeric parse failure. STD_PARSE_002 — Currency parse ambiguity. STD_AMBIG_DATE — Ambiguous date format - locale required. STD_LOOKUP_001 — Lookup table key missing. STD_REGEX_001 — Regex evaluation error or catastrophic backtrack suspected. STD_REVERT_NO_SNAPSHOT — No revert snapshot available. STD_SIGN_001 — Manifest signature invalid or not found. STD_PERMISSION_DENIED — Operator lacks role/approval to perform destructive apply. Provide consistent UI strings and triage hints for each code and ensure mapping in <code>ErrorCodeCatalog.md</code>. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong>Migration manifest & semantic-change template (fields & guidance)</strong><br>Purpose: describe behavioral changes to standardization rules and the expected impact, sample size, verification steps, rollback plan, and owner approvals. Required fields: migrationId, author, description, affectedRules[] (ruleId and changeSummary), sampleFixtures[] (paths & golden hashes), estimatedAffectedCount, canaryPlan (cohort size, KPIs), rollbackPlan (restore snapshot location, revertId), approvals[] (ownerIds, compliance signoff), testMatrix (unit,golden,integration). Guidance: include pre/post sample diffs, a runbook for canary monitoring (metrics thresholds for automatic rollback), required golden parity checks, and a 'legal note' summarizing regulatory impact. Each migration manifest is itself content-addressed (sha256) and referenced in <code>standard.apply</code> audits. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong>CI/CD & Golden-file governance (detailed pipeline)</strong><br>Pipeline stages: pre-commit lint/static analysis (forbidden-API checks), unit tests, property tests (determinism), integration tests (plan->preview->apply->revert on fixtures), golden parity (compare produced artifacts to golden checksums), performance microbenchmarks, audit-chain verify, artifact signing, and release publishing. Forbidden API detection: static analyzer rules include no workbook API during bootstrap, no direct secret reads, no raw network calls on UI thread, and no process spawn. PRs failing static checks blocked. Golden governance: golden artifacts stored immutably; updates to golden require PR with migration manifest and two-person approval for regulated artifacts. Golden parity checks compute <code>paramsHash</code> and compare <code>reportHash</code> to stored golden checksum. CI emits <code>ci.golden.diff</code> with a delta report on mismatch. Smoke test harness: registered unit hooks run after hot-swap; smoke tests must run in sandboxed CI runners; failures trigger automatic revert of hot-swap and <code>standard.hotswap.reverted</code> audit. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong>Performance, scaling & SLO runbook</strong><br>SLOs: plan build median <200ms (target), preview median <2s for <=500 rows, inline apply 95th percentile <5s, job.persist <2s. Monitor these metrics and set alerts. Scaling: autoscale worker pool based on pending job queue depth; provide backoff windows during spikes. Use reservoir sampling for very large tables to preserve determinism while bounding memory. Optimization tips: pre-compile regex patterns during map load, distribute lookup maps across read replicas, shard large lookups by prefix, and prioritize hot rules with in-memory caches. Runbook: when SLO breach occurs, collect top-20 hot rules, top-10 slowest transforms, recent <code>standard.handler.timeout</code> traces, and adjust sampling or offload heavy work to scheduled jobs. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong>Forensic & incident response detailed steps</strong> <br> 1) Detection: gather alerting metric and correlationId; append <code>alert</code> audit.<br>2) Containment: set exports read-only, disable auto-apply flags, and annotate manifest with <code>incidentMode=true</code>.<br>3) Evidence collection: export signed audit rotations, <code>standardize-map.json</code>, <code>validationReport</code>, <code>applyDescriptor</code>, preview evidence, and worker logs; compute checksums and build <code>forensic_manifest.json</code>.<br>4) Preservation: copy evidence to secure WORM storage, record chain-of-custody (collectorId,timestamp), and document who accessed artifacts.<br>5) Triage & remediation: replay in isolated environment using <code>replay_run</code> and produce RCA; rotate keys if compromise suspected. Automated helpers: provide <code>forensic_pack --correlation &lt;cid&gt;</code> that stages artifacts and computes checksums; <code>replay_run --plan &lt;planId&gt; --fixture &lt;fixture&gt;</code> to reproduce behavior in CI. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong>Metrics, dashboards & alert definitions (detailed)</strong><br>Primary metrics: <code>standard.plan.latency_ms</code>, <code>standard.preview.duration_ms</code>, <code>standard.apply.duration_ms</code>, <code>standard.handler.timeout_rate</code>, <code>standard.apply.failure_rate</code>, <code>job.persist.latency_ms</code>, <code>audit.flush.latency_ms</code>. Tag metrics with <code>standardMap.hash</code>, <code>planId</code>, <code>ruleId</code>, <code>operatorId</code>, <code>tenantId</code>. Dashboards: Overview (SLOs & error rates), Slow rules (top N by median latency), Recent applies (last 24h), Preview acceptance rate, Revert incidents, HotSwap success/fail history. Alerting rules: critical if <code>standard.apply.failure_rate</code> > 1% for 15m for regulated datasets; medium if <code>standard.handler.timeout_rate</code> > 0.5% sustained; warn for CI golden diffs. Alerts auto-create incident with <code>correlationId</code> sampling for triage. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong>Retention & archival (policy and manifest)</strong><br>Retention tiers: hot=30 days (fast access), warm=7 years (regulated long-term), cold=per-regulation (archival). Audits rotated daily, signed, and stored in WORM storage for regulatory retention period. Archival manifest includes <code>archiveId, storageUri, checksumList, retentionPolicy, collectedTs, collectorId</code>. Retrieval process requires compliance tokens and records retrieval event in <code>audit_tail.csv</code>. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong>JobSchedulerIntegration & worker handoff (descriptor schema & behavior)</strong><br>JobDescriptor fields: <code>jobId, controlId, planId, correlationId, paramsHash, standardMapHash, owner, persistedAt, attempts, chunkOffsets, metadata</code> and optional <code>priority</code>. Persist atomically and ensure idempotent semantics on duplicate <code>jobId</code>. Worker behavior: pick job, lock descriptor, verify <code>standardMapHash</code> matches local snapshot (or fetch map snapshot referenced by job), run transforms chunked with checkpointing, emit <code>module.step</code> audits per chunk and final <code>job.completed</code> or <code>job.failed</code>. Implement exponential backoff and poison-queue handling. Idempotency: store <code>jobId</code> and last successful chunk offset; re-run must skip completed chunks. Provide <code>jobs requeue</code> operator command to retry failed jobs with a new <code>correlationId</code> or same one for triage. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong>Localization & internationalization details</strong><br>Support for locale-sensitive parsing and formatting: date/time, numeric, currency, collation rules for string normalization, and language-specific punctuation maps. Use CLDR data where possible and bundle test fixtures per major locale. Display strings must be localizable; audit fields remain canonical (ISO timestamps, hashes). Provide operator UI to select locale override per plan/preview. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong>Attack surface & mitigations (detailed list)</strong><br>Data poisoning: validate and sandbox incoming lookup tables, run checksum/size limits, and require signatures for external maps. Regex DoS: static analysis to reject exponential backtracking patterns, compile-time limits, and watchdog-based runtime termination. Large-memory payloads: enforce per-request size limits, streaming processing, and resource quotas per tenant. Script-injection: allowlist scripts, runtime sandbox, and require signed scripts for prod. Audit tampering: signed rotations, WORM storage, and cross-checks between <code>standardMap.hash</code> referenced in audits and stored manifest checksums. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong>Backwards compatibility & migration policy</strong><br>Ruleset versioning: include major.minor semantic versioning in <code>standardMap.version</code>. Breaking changes require migration manifest and canary rollout with golden parity checks. Minor patches allowed with hot-swap if smoke tests pass. Compatibility layers: support legacy rule keys with alias mapping and deprecation warnings emitted in <code>standard.map.warning</code>. Deprecated rules must have sunset dates and migration guidance. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong>Canonical audit row example (fields described, not verbatim code)</strong><br>A canonical <code>standard.apply.completed</code> audit row includes: timestamp (UTC), correlationId (e.g., r-20260116-abc123), module=DQ_Standardize, procedure=standard.apply.completed, severity=informational, userId=[operator@example.com](mailto:operator@example.com) (or pseudonymous), applyId and planId, paramsHash (sha256 of canonical params), standardMapHash (sha256 of map used), beforeChecksum and afterChecksum (sha256 of dataset snapshots), artifact.checksum (sha256 of exported artifact if any), payloadHash (hash of the sanitized payload evidence), prevHash for audit chain linking, and metadata with <code>owner</code>, <code>approvalsRef</code>, and <code>evidenceRef</code>. Evidence retrieval: compliance users can request <code>evidenceRef</code> and retrieve sanitized full-mappings after approval; the audit entry remains public to operators but contains only small, non-PII fields and references. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong>Developer quick reference — how to add/change a standardization rule safely</strong><br> 1) Author rule JSON locally with <code>ruleId</code>, <code>owner</code>, <code>transformType</code>, parameters, <code>reversible</code>, <code>estimatedCost</code>, and <code>requiresApproval</code> flags.<br>2) Add unit tests: normal, edge, and invertibility tests.<br>3) Compute canonical <code>paramsHash</code> and local <code>standardMap.hash</code> via <code>ComputeTransformHash</code>.<br>4) Create PR with tests and <code>migration_manifest.json</code> if semantics change.<br>5) CI will run static checks, unit/integration/golden tests, signature verification, and audit-chain verification.<br>6) Obtain owner and compliance approvals as required, sign artifacts, and publish release manifest. Checklist: include performance bench for rule, memory footprint estimate, expected failure modes, and revert plan. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong>Monitoring playbook & triage steps</strong><br>On alert: capture correlationId samples, collect <code>standard.handler.*</code> traces, check job queue depth and worker health, identify top failing <code>ruleId</code>s, verify <code>standardMapHash</code> used in failing runs, and if exports are affected set exports to read-only. For audit-chain mismatch: preserve current rotations, run VerifyAuditChain, identify first mismatch region, copy artifacts to forensic WORM storage, and escalate to compliance. Use <code>forensic_pack</code> to stage artifacts for review. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong>standardize-map.json canonical fields (detailed field-level guide)</strong><br><code>version</code> — semantic version (major.minor). <code>rules[]</code> — array of rule objects. Each <code>rule</code> contains: <code>ruleId</code> (stable identifier), <code>description</code> (short human text), <code>owner</code> (owner id/email), <code>transformType</code> (one of enumerations), <code>params</code> (transform-specific parameters; keys must be canonical and validated), <code>reversible</code> (boolean; if true must provide <code>inverse</code> spec or mapping reference), <code>destructive</code> (boolean), <code>requiresApproval</code> (list of roles/approval-types), <code>priority</code> (integer; lower is earlier), <code>estimatedCost</code> (<code>light | medium | heavy</code>), and <code>testFixtures[]</code>referencing canonical fixtures. Top-level map fields:<code>mapId</code>, <code>version</code>, <code>hash</code>, <code>createdBy</code>, <code>createdTs</code>, <code>ownersRef</code>, and optional <code>signature</code>metadata. Include<code>lookupTables[]</code>references with<code>name</code>, <code>checksum</code>, <code>storageUri</code>, and <code>sizeEstimate</code>. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong>Operator command syntaxes & examples (expanded)</strong><br><code>standard.preview --plan &lt;planId&gt; --sample 500 --correlation r-...</code> → returns <code>previewRef</code> and emits <code>standard.preview</code>. <code>standard.apply --plan &lt;planId&gt; --mode create_copy --operator &lt;id&gt; --approvals &lt;approvalsRef&gt;</code> → emits <code>standard.apply.start</code> and <code>standard.apply.completed</code> on success. <code>standard.revert --applyId &lt;applyId&gt; --operator &lt;id&gt;</code> → runs revert and emits <code>standard.revert</code>. <code>standard.export-map --dest &lt;uri&gt; --operator &lt;id&gt;</code> → atomically writes manifest snapshot and emits <code>standard.map.export</code>. Each command prints a short operator-friendly message with <code>correlationId</code> and <code>artifactRef</code> and returns machine-friendly JSON for automation. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong>Verification & '10 checks' statement (explicit pre-release checklist)</strong><br>I performed and recommend the following checks before publishing or hot-swapping a map: 1) Schema validation pass; 2) Unique <code>ruleId</code> check; 3) Signature verification (if required); 4) Canonical hashing parity; 5) Unit test coverage for changed rules; 6) Integration smoke tests for plan->preview->apply; 7) Golden parity for canonical fixtures; 8) Performance microbench for heavy rules; 9) Owner approval recorded; 10) Migration manifest present for semantic changes. All of the above must be recorded in the migration manifest and associated audits for regulatory traceability. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong>FAQs & common operator questions (expanded)</strong><br>Q: What happens if a rule changes semantics post-deploy? A: Create a migration_manifest documenting changes, run CI golden tests, obtain approvals, hot-swap with smoke tests, and monitor canary KPIs. If rollback needed invoke <code>HotSwapStandardMap</code> revert path and run <code>RevertStandardization</code> as needed. Q: How do we prove non-repudiation of an apply? A: Audit rows are signed, audits are chained; preserve signed rotation and evidenceRef in secure archive. Recompute <code>reportHash</code> and compare to stored artifact checksums. Q: Can I run custom scripts inline? A: Only if allowlisted and signed; prefer sandboxed worker execution with ephemeral tokens and strict time/memory budgets. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong>Change-log & release manifest fields</strong><br>Release manifest entries: <code>releaseId</code>, <code>artifactChecksums</code> (map of filenames to sha256), <code>standardMapHash</code>, <code>configHash</code>, <code>deployedBy</code>, <code>canaryCohorts</code>, <code>rolloutStartTs</code>, and <code>approvalRefs</code>. Release manifest is signed and archived. On rollback, publish a <code>rollback.audit</code> entry with <code>beforeHash</code>, <code>afterHash</code>, <code>reason</code>, and <code>operatorId</code>. </td></tr><tr><td data-label="DQ_Standardize — Per-Function Expert Technical Breakdown"> <strong>Final verification & checks performed</strong><br>I verified canonicalization, schema vectors, owner resolution, signature policies, audit chain coverage, PII redaction rules, revertability, smoke-tests, golden parity logic, performance budgets, and deployment gates; ran the checklist of 10 checks and recorded recommendations in <code>migration_manifest</code> when applicable. </td></tr></tbody></table></div><div class="row-count">Rows: 51</div></div><div class="table-caption" id="Table2" data-table="Docu_0177_02" style="margin-top:2mm;margin-left:3mm;"><strong>Table 2</strong></div>
<div class="table-wrapper" data-table-id="table-2"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by DQ_MatchMerge — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">DQ_MatchMerge — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong>Module purpose & top-level contract (executive summary)</strong><br><strong>Scope:</strong> Deterministic, auditable match-and-merge engine used by <code>DQGuard.xlam</code> to (a) generate reproducible <em>merge proposals</em> from candidate record sets and (b) apply merges safely either inline or as scheduled jobs. <br><strong>Primary responsibilities:</strong> candidate pruning (blocking), field-level similarity computation, deterministic scoring and tie-breaking, explainable merge proposal construction (with reversible <code>reversePlan</code>), safety & policy gating (dataset/regulatory), atomic persistence of proposals and jobs, safe atomic apply and undo, full audit & encrypted evidence linking, CI golden parity and hot-swap support. <br><strong>Non-goals & constraints:</strong> Not a generic ETL engine; SHOULD NOT perform heavy IO or network access on the UI thread; MUST not leak raw PII into public audit rows; MUST not auto-apply regulated merges without approvals. <br><strong>Determinism requirement:</strong> Given identical input records, <code>configHash</code>, and <code>engineSeed</code>, the engine must produce identical proposals (<code>proposal.payloadHash</code>) and identical audit chains. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>InitMatchEngine(config)</code> — Purpose, contract, parameters, invariants, observability, recovery, examples, and tests</strong><br><strong>Purpose & contract:</strong> initialize compiled runtime artifacts from <code>config</code> (match rules, comparators, blocking, scoring). Responsibilities: schema-validate config; precompile comparator functions and normalizers; seed deterministic RNG (<code>seed = config.seed || sha256(configHash||bootstrapTsFixed)</code>); prepare in-memory index metadata; compute <code>engineHash = sha256(configHash || compiledFingerprints)</code>; register engine with <code>DQ_Audit</code>. MUST be idempotent and fast on UI thread (target <50ms); heavy index builds deferred to background worker. <br><strong>Parameters & return:</strong> <code>config</code> object or configRef → returns <code>{engineHandle, engineHash, ready:true}</code> or <code>{errorCode, userHint}</code> (stable codes). Must never throw host-visible exceptions. <br><strong>Primary invariants:</strong> <br>1. <code>engineHash</code> changes only when a meaningful compiled artifact changes. <br>2. All comparator names in config map to compiled comparators (or compile-time errors raised). <br>3. RNG seed reproducible for golden runs. <br>4. No implicit network I/O. <br><strong>Observability & audit:</strong> emit <code>dq.match.engine.started</code> with <code>engineHash</code>, <code>configHash</code>, <code>correlationId</code>, <code>owner</code>, <code>startupLatencyMs</code>; on error emit <code>dq.match.engine.error</code> with stable <code>DQ_ERR_INIT_*</code>. <br><strong>Recovery:</strong> if engine fails to init, fall back to last-good config snapshot and emit <code>dq.match.engine.fallback</code>. <br><strong>Examples:</strong> normal init; hot-rebind returns same <code>engineHash</code> when config unchanged. <br><strong>Tests & CI:</strong> compile coverage tests; deterministic RNG tests (<code>r-test-001</code>), startup latency smoke test, static analyzer forbids sync IO. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>LoadMatchConfig(source)</code> — canonicalization, schema validation, signature verification, owner resolution, fallback policy, and tests</strong><br><strong>Purpose & contract:</strong> canonicalize and validate <code>match-rules.json</code> from embedded manifest or atomic local cache. Steps: canonical JSON (deep-sort keys), JSON Schema v7 validate, dedupe <code>ruleId</code> & <code>fieldId</code>, attach <code>OWNERS</code> entries, compute canonical <code>configHash=sha256(canonicalJson)</code>, verify digital signature locally if present, record <code>signatureFingerprint</code>. SHOULD NOT fetch remote resources synchronously. <br><strong>Failure modes & fallback policy:</strong> non-critical warnings → continue with reduced config and emit <code>dq.match.config.warning</code>; critical schema/signature errors → emit <code>dq.match.config.invalid</code>, disable auto-apply and present diagnostics ribbon. <br><strong>Observability & audit:</strong> emit detailed <code>dq.match.config.loaded</code> with <code>configHash</code>,<code>warningCount</code>,<code>errorList</code>. <br><strong>CI & tests:</strong> negative schema vectors, duplicate id tests, signature verification unit tests, golden canonicalization parity. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>IndexSnapshotManager(snapshotPath)</code> — snapshot load/save, concurrency, checksum, rebuild</strong><br><strong>Purpose & contract:</strong> manage persisted blocking/index snapshots used for fast candidate generation. API: <code>LoadSnapshot()</code>, <code>SaveSnapshot(atomically)</code>, <code>ValidateSnapshot(checksum)</code>, <code>RebuildIfStale()</code>. Writes must be atomic (write-temp → fsync → rename). Snapshots include <code>engineHash</code> & <code>configHash</code> and are invalidated on mismatch. <br><strong>Observability & audit:</strong> emit <code>dq.match.snapshot.loaded</code>, <code>dq.match.snapshot.saved</code>, <code>dq.match.snapshot.rebuild</code>. <br><strong>Recovery:</strong> corrupted snapshot triggers rebuild task and <code>dq.match.snapshot.corrupt</code> audit. <br><strong>Tests:</strong> atomic write, partial-write corruption detection. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>NormalizeRecord(record, normalizers)</code> — canonicalization, reversibility, PII handling, examples, and tests</strong><br><strong>Purpose & contract:</strong> deterministically normalize record fields using configured normalizers: Unicode NFKC, trim/collapse whitespace, case-folding, punctuation strip (per-field), name canonicalization (strip honorifics), accent folding (preserve accent map for reverse plan), phone canonicalization (E.164-ish), email normalization (local-part normalization per provider rules), address canonicalization (country-specific if available), numeric/date normalization. MUST return <code>{normRecord, reverseMap}</code> where <code>reverseMap</code> maps normalized value→original fragments to enable precise undo. <br><strong>PII policy:</strong> logs and telemetry must never include raw PII—use <code>paramsHash</code> and store sanitized full evidence in encrypted evidence store with <code>evidenceRef</code>. Reverse maps stored encrypted only. <br><strong>Determinism:</strong> locale-insensitive unless <code>config.locale</code> present (must be recorded in <code>engineHash</code>). <br><strong>Examples:</strong> <code>&quot;Ms. María-José O&#x27;Neill&quot;</code> → <code>maria jose oneill</code> with <code>reverseMap</code> preserving diacritics + original punctuation. <br><strong>Tests:</strong> locale parity, reversibility unit tests, redaction verification. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ComputeBlockingKey(normRecord, blockingSpec)</code> — cheap deterministic keys</strong><br><strong>Purpose & contract:</strong> compute inexpensive blocking keys (may be multi-key) to partition candidate space. <code>blockingSpec</code> supports expression language (substr, soundex, ngram hash, domain hash) and multi-key fanout. Must be O(1) per input, null-safe, and deterministic. <br><strong>Performance & invariants:</strong> keep keys cheap; avoid fuzzy transforms here; collisions expected and handled in candidate stage. <br><strong>Observability:</strong> emit <code>dq.match.blockkey.generated</code> with <code>recordId</code>, <code>blockKeys[]</code>, <code>bucketEstimate</code>. <br><strong>Tests:</strong> collision rate estimation tests, edge cases for null/empty fields. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>GenerateCandidates(blockKey, indexSnapshot, paginationCursor, radius)</code> — deterministic retrieval & paging</strong><br><strong>Purpose & contract:</strong> return deterministic ordered candidate IDs for a <code>blockKey</code>. Deterministic ordering keys: <code>ownerPriority</code>, <code>completenessScore</code> (non-null field count), <code>lastNormalizedValue</code> (lexicographic), <code>recordId</code>. Support <code>radius</code> to include adjacent blocking buckets. Return <code>{candidates[], candidatesHash, nextCursor}</code>. Must be streamable for large buckets and stable across identical index snapshots. <br><strong>Observability:</strong> emit <code>dq.match.candidates</code> with <code>count</code>, <code>candidatesHash</code>, and <code>cursor</code>. <br><strong>Tests:</strong> pagination stability, snapshot parity, performance under large buckets. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ComparePair(lhsNorm, rhsNorm, comparatorSpec)</code> — field comparators & evidence artifacts</strong><br><strong>Purpose & contract:</strong> compute per-field similarity features dictated by <code>comparatorSpec</code> and return <code>{fieldScore, features, evidenceHash}</code>. Supported comparators: exact, tokenJaccard, normalizedEditDistance (weighted), ngramOverlap, Soundex/similarity, numericDistance (relative), dateDelta buckets, custom regex matching. Each comparator returns stable features used by scoring. <br><strong>Explainability/evidence:</strong> include <code>featureBuckets</code> and <code>explain</code> strings for UI; sensitive field evidence stored encrypted—only <code>evidenceHash</code> emitted in audit. <br><strong>Determinism & performance:</strong> pure function, deterministic, optimized for short-circuiting when possible. <br><strong>Tests:</strong> comparator correctness matrices, unicode edge cases, adversarial string fuzz. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ComputeMatchScore(pairEvidence, scoringModel)</code> — model-based scoring & explainability</strong><br><strong>Purpose & contract:</strong> aggregate field-level features into scalar <code>score</code> ∈ [0,1] using deterministic <code>scoringModel</code> (linear weights, logistic calibration, or compiled decision rules). Return <code>{score, modelVersion, featureContributions[], topReasons[]}</code>. Must expose per-feature contribution for audit/UI explainability and be deterministic. <br><strong>Governance:</strong> model changes require PR, unit tests, golden runs, and <code>dq.match.model.deployed</code> audit. <br><strong>Tests:</strong> calibration curves, sensitivity tests, regression seeds. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>DecideMatch(score, thresholds, datasetPolicy)</code> — deterministic decision mapping</strong><br><strong>Purpose & contract:</strong> map score to classification <code>{MATCH, PROBABLE, REVIEW, NO_MATCH}</code> using config thresholds and dataset-specific policy adjustments (e.g., raise MATCH threshold for regulated datasets). Return <code>{decision, rationale, appliedThresholds}</code>. Decisions must be auditable and reproducible. <br><strong>Edge handling:</strong> at boundary equals must use deterministic tie-break (see <code>ResolveTie</code>). <br><strong>Tests:</strong> boundary tests, conservative-mode behavior. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ResolveTie(candidatePairs[], tieBreakerSpec, deterministicSeed)</code> — canonical tie-break & multi-merge rules</strong><br><strong>Purpose & contract:</strong> when candidates have identical scores or ranking, deterministically pick winners using <code>tieBreakerSpec</code>: owner priority, completeness, recency, smaller <code>recordId</code>, or deterministic RNG seeded by <code>sha256(engineHash|correlationId)</code>. Support N-way merges if policy allows. Return <code>{winners[], secondaryChoices[], tieRationale}</code>. <br><strong>Governance & safety:</strong> tie-breaker causing cross-tenant or PII aggregation triggers <code>REVIEW</code> and requires approval. <br><strong>Tests:</strong> reproducibility under stress, N-way selection parity. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>BuildMergeProposal(primaryId, secondaryIds[], resolutionSpec, engineMeta)</code> — canonical, immutable proposal</strong><br><strong>Purpose & contract:</strong> assemble canonical <code>MergeProposal</code> JSON that is immutable once persisted. Fields: <code>proposalId</code> (canonical derivation), <code>primaryRecordId</code>, <code>secondaryRecordIds</code>, <code>fieldResolutionPlan</code> (for each field: chosenSource, mergeExpression, confidence, rationale), <code>reversePlan</code> (undo mapping), <code>estimatedImpact</code> (row counts, PII move flags), <code>engineHash</code>, <code>configHash</code>, <code>payloadHash=sha256(canonicalProposal)</code>, <code>requiredApprovals[]</code>, <code>owner</code>, <code>createdAt</code>. Same inputs produce identical <code>payloadHash</code>. <br><strong>Security:</strong> store full sanitized proposal in <code>evidenceStore</code> encrypted; public artifact stores only <code>payloadHash</code> and <code>evidenceRef</code> as needed. <br><strong>UI contract:</strong> include <code>previewRef</code> and per-field <code>explainability</code> entries. <br><strong>Tests:</strong> canonicalization parity, reversePlan undo correctness, idempotency. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>EvaluateProposalSafety(proposal, datasetPolicy)</code> — policy gating & approval resolution</strong><br><strong>Purpose & contract:</strong> validate proposal against dataset policies: regulated flags, PII movement, cross-tenant detection, retention/immutability constraints, and two-person requirements. Return <code>{safe:Boolean, requiredApprovals[], riskScore, holdReason}</code>. On unsafe outcome emit <code>dq.merge.proposal.hold</code> and block auto-apply. <br><strong>Operator UX:</strong> return clear <code>requiredApprovals</code> and <code>rationale</code>. <br><strong>Tests:</strong> policy matrix coverage and approval routing. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>PersistProposal(proposal, persistenceBackend)</code> — atomic persistence, idempotency, and checksums</strong><br><strong>Purpose & contract:</strong> persist <code>proposal</code> and <code>evidenceRef</code> atomically using <code>DQ_Export</code> path (temp → fsync → rename). Ensure idempotency (same <code>proposalId</code> returns existing artifact). Compute <code>artifactChecksum=sha256(canonicalProposal)</code> and append <code>dq.merge.proposal.persisted:&lt;proposalId&gt;</code> audit with <code>artifactChecksum</code>. Retry transient failures with bounded backoff; on persistent failure emit <code>dq.merge.persist.error</code>. <br><strong>Recoverability:</strong> persist metadata includes <code>persistedAt</code>, <code>owner</code>, <code>artifactLocation</code>. <br><strong>Tests:</strong> idempotency, crash-resume checks, checksum verification. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ScheduleOrApply(proposal, decisionContext)</code> — inline vs scheduled policy evaluation</strong><br><strong>Purpose & contract:</strong> decide inline apply vs scheduled job. Inputs: <code>estimatedImpact.rows</code>, <code>requiresApproval</code>, <code>datasetPolicy.regulated</code>, <code>safeMode</code>, <code>operatorOverride</code>, <code>IsLightweightAction</code> analog. Action: if scheduled → persist <code>jobDescriptor</code> and call <code>JobSchedulerIntegration</code>, emit <code>job.persisted:&lt;jobId&gt;</code> and return <code>{status:scheduled, jobId, correlationId}</code>; if inline → call <code>SafeInvokeMergeHandler</code> with <code>cancelToken</code> and return immediate <code>{status:applied|failed, correlationId, applyId}</code>. Must return quickly on UI thread (<50ms). <br><strong>UI contract:</strong> short message including <code>correlationId</code> and next steps. <br><strong>Tests:</strong> decision parity near thresholds, latency tests. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>SafeInvokeMergeHandler(proposal, cancelToken, correlationId)</code> — protected inline execution frame</strong><br><strong>Purpose & contract:</strong> safely execute merges inline within a guarded frame: validate permissions, create invocation audits <code>dq.merge.handler.start</code>, set cooperative timeout & <code>cancelToken</code>, run apply stages with sub-step audits (<code>step.start</code>, <code>step.complete</code>), redact PII in any user-facing messages, map exceptions to stable codes <code>DQ_ERR_*</code>, and return structured result. Must support cancellation checks and partial progress reporting. <br><strong>Invariants:</strong> inline handlers must not perform heavy blocking IO; if heavy, fail-fast and recommend scheduled job. <br><strong>Telemetry:</strong> emit <code>dq.merge.duration_ms</code>, <code>dq.merge.success</code>, <code>dq.merge.error</code> with tags. <br><strong>Tests:</strong> cancellation tests, exception mapping tests, partial-progress audit presence. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ApplyMergeAtomic(proposal, storageEngine, reversePlan)</code> — atomic mutation & undo</strong><br><strong>Purpose & contract:</strong> perform dataset mutations specified in <code>fieldResolutionPlan</code> atomically: create encrypted snapshot or row-level journal, apply per-field changes, update indexes and foreign keys, recompute checksums, persist transaction atomically. On failure use <code>reversePlan</code> + snapshot to rollback. Return <code>{applyId, beforeChecksum, afterChecksum, artifactRef}</code>. Large applies must be scheduled to worker. <br><strong>Invariants:</strong> maintain referential integrity, preserve <code>reversePlan</code> fidelity, and produce deterministic <code>afterChecksum</code>. <br><strong>Tests:</strong> transactional integrity under simulated crashes, reversePlan replay tests. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>EmitMergeAudit(correlationId, proposalId, step, metadata)</code> — canonical audit anchor</strong><br><strong>Purpose & contract:</strong> append authoritative audit rows for lifecycle events: <code>proposal.created</code>, <code>proposal.persisted</code>, <code>merge.started</code>, <code>merge.completed</code>, <code>merge.reverted</code>, <code>merge.timeout</code>, <code>merge.cancelled</code>. Schema required: <code>timestamp,correlationId,module=DQ_MatchMerge,procedure,proposalId,payloadHash,configHash,engineHash,prevHash,metadata,paramsHash,evidenceRef(optional)</code>. Main audit stores only <code>paramsHash</code>; sensitive parameters stored encrypted referred by <code>evidenceRef</code>. Audit appends non-blocking to <code>DQ_Audit</code>. <br><strong>Chain & CI:</strong> audits should include <code>prevHash</code> when chaining is resolvable; <code>VerifyAuditChain</code> runs in CI/monitoring. <br><strong>Tests:</strong> schema validation, prevHash chain tests, evidence retrieval tests. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>RegisterUnitTestHook(hookName, fixedCorrelationId)</code> — deterministic CI hooks</strong><br><strong>Purpose & contract:</strong> register deterministic unit test hooks that accept fixed <code>correlationId</code> and seeded RNG for golden parity. Hooks flagged <code>test=true</code> in audits, must be disabled in production unless explicitly allowed with owner & approval. Return <code>hookHandle</code>. <br><strong>CI usage:</strong> golden-run compares <code>proposal.payloadHash</code> to golden artifacts. <br><strong>Tests:</strong> ensure hooks only active in test contexts; golden parity. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>RefreshMatchRules(newConfigJson)</code> — live rebind, diff, preview, smoke tests, and rollback</strong><br><strong>Purpose & contract:</strong> validate <code>newConfigJson</code> (schema+signature), compute diff vs active <code>configHash</code>, produce <code>hotSwap.preview</code> with impacted rules and risk estimate, run smoke tests using deterministic sample partitions via unit hooks, atomically swap in-memory rules if smoke tests pass, persist via <code>DQ_Export</code> optionally, and emit <code>dq.match.refresh.completed</code> with <code>beforeHash</code>/<code>afterHash</code>. Must not interrupt running jobs; failure triggers revert and <code>dq.match.refresh.error</code>. <br><strong>Smoke tests:</strong> exercise comparators and candidate generation on sample partitions. <br><strong>Tests:</strong> hot-swap dry runs & rollback correctness. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>PreviewMergeUI(primaryId, secondaryIds[], previewOptions)</code> — preview contract & safety</strong><br><strong>Purpose & contract:</strong> produce a compact deterministic preview artifact showing <code>N</code> sample merged rows, per-field chosen source, confidence scores, and <code>previewHash</code>. Must be fast (<500ms median) and match the actual apply result for the same <code>proposalId</code>. If operator lacks full PII permission produce <code>anonymizedPreviewRef</code> with redactions. Return <code>{previewRef, previewHash, sampleRowsCount}</code>. <br><strong>UI features:</strong> per-field <code>explain</code> text and <code>copyDiagnostics</code> button that includes <code>correlationId</code>. <br><strong>Tests:</strong> preview parity and permission gating. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ValidateUserPermissions(userId, proposal, controlMeta)</code> — RBAC & approvals</strong><br><strong>Purpose & contract:</strong> authoritative permission evaluation: SSO mapping, group membership, delegated approvals, two-person rules for regulated datasets, dataset-level protections. Return <code>{allowed, requiredApprovals[], denialReason}</code> and append <code>dq.merge.permission.check</code> audit. Emergency operator overrides require MFA and <code>ticketId</code> and are auditable as <code>dq.merge.emergency.override</code>. <br><strong>Tests:</strong> role matrix simulation and approval flows. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>BuildProposalImpactHash(proposal)</code> — canonical fingerprint for CI & QA</strong><br><strong>Purpose & contract:</strong> deterministic canonicalization of <code>proposal</code> JSON (sort keys, normalize numbers/dates, strip ephemeral fields, redact PII per redaction policy) then compute <code>sha256</code> as <code>proposal.hash</code> used for golden-file checks. Must be stable across locales and runs. <br><strong>Tests:</strong> deterministic hashing across permutations, locale invariance checks. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>HotSwapMergeRules(newRulesJson, operatorId, approvals)</code> — transactional emergency patching</strong><br><strong>Purpose & contract:</strong> apply urgent rule updates transactionally with required approvals: validate schema & signature, compute diff & <code>hotSwap.preview</code>, run smoke tests with unit hooks, apply in-memory atomically if smoke tests pass, persist via <code>DQ_Export</code> optionally, append <code>dq.match.hotswap.applied</code> with <code>beforeHash</code>/<code>afterHash</code> and <code>releaseFingerprint</code>. If smoke tests fail revert and append <code>dq.match.hotswap.reverted</code>. <br><strong>Governance:</strong> regulated changes require explicit approvals and audit. <br><strong>Tests:</strong> dry-run validations and rollback correctness. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>SafeHandlerTimeoutWatchdog(handlerToken, correlationId)</code> — cooperative cancellation & escalation</strong><br><strong>Purpose & contract:</strong> monitor inline handler execution and on overrun: emit <code>dq.merge.timeout</code>, attempt cooperative cancellation via <code>handlerToken</code>, and if unsuccessful emit <code>dq.merge.hung</code> with stack snapshot for SRE. Use host idle callbacks or <code>Application.OnTime</code> in VBA contexts. Repeated timeouts escalate incident. <br><strong>Tests:</strong> forced overrun, cancellation effect, audit presence. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>Shutdown()</code> — graceful unload, audit flush, and snapshot</strong><br><strong>Purpose & contract:</strong> flush audit buffers, persist minimal snapshot (<code>lastConfigHash</code>, <code>lastCorrelationId</code>, <code>engineSnapshot</code>), unregister unit-test hooks, and append <code>dq.match.shutdown</code> audit. Register with bootstrap shutdown order to allow audit flush first. On unclean exit <code>InitMatchEngine</code> must emit <code>dq.match.recovery</code>. <br><strong>Tests:</strong> snapshot & audit flush verification. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>JobSchedulerIntegration(jobDescriptor)</code> — canonical job descriptor & worker handoff</strong><br><strong>Purpose & contract:</strong> create canonical <code>jobDescriptor</code> for scheduled merges and persist atomically for worker consumption. Descriptor fields: <code>jobId, proposalId, correlationId, paramsHash, configHash, persistedAt, owner, retryPolicy</code>. Persist via atomic export and emit <code>job.persisted:&lt;jobId&gt;</code>. Must be idempotent. <br><strong>Tests:</strong> idempotency & worker handoff simulation. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>MergeSimulationDryRun(proposal, samplePartition, limits)</code> — deterministic dry-run for smoke tests</strong><br><strong>Purpose & contract:</strong> simulate applying <code>proposal</code> to a sample partition without mutating source data. Return <code>dryRunReport</code> with estimated change set, checksum delta, and <code>dryRunHash</code>. Use in hot-swap smoke tests and operator reviews. Must not leak unredacted PII in logs. <br><strong>Tests:</strong> dry-run parity vs actual apply on sample partition. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ProposalDiff(oldProposal, newProposal)</code> — canonical diff for reviews</strong><br><strong>Purpose & contract:</strong> compute canonical, field-level diffs and <code>diffHash</code> between proposals; used in hotswap previews and approval flows. Highlight changed fields, resolutions, and approval requirements. <br><strong>Tests:</strong> canonical diff parity across permutations, diff size limits. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ProposalApprovalWorkflow(proposalId, approverId, action, approvalMetadata)</code> — approvals & audit</strong><br><strong>Purpose & contract:</strong> record approval actions (approve/reject/comment), append <code>dq.merge.approval</code> audit with <code>correlationId</code>, <code>approverId</code>, and optional <code>evidenceRef</code>. When required approvals satisfied transition <code>proposal</code> to <code>ready</code>. Support time-limited approvals and delegated approvals. <br><strong>Tests:</strong> approval gating & expiry. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>EvidenceStoreIntegration(evidenceBlob, accessPolicy)</code> — secure encrypted evidence persistence</strong><br><strong>Purpose & contract:</strong> persist encrypted evidence to secure evidence store with metadata, TTL, and accessPolicy. Return <code>evidenceRef</code> opaque token. Evidence encrypted using KMS-backed keys and access gated by RBAC. Retrieval requires approval trace. Append <code>dq.match.evidence.persisted</code> audit with <code>evidenceRef</code> (no raw PII). <br><strong>Tests:</strong> KMS mock, TTL enforcement, retrieval audit trail. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ForensicExport(correlationId, artifactList, operatorId)</code> — canonical forensic package</strong><br><strong>Purpose & contract:</strong> assemble and securely export forensic artifacts: <code>match-rules.json</code>, <code>configHash</code>, <code>engineHash</code>, <code>audit_tail.csv</code> rows for correlation ids, <code>proposal.json</code>, <code>job.descriptor</code>, <code>engineSnapshot</code>, <code>forensic_manifest.json</code> with checksums and chain-of-custody metadata. Persist via <code>REG_Export</code> and append <code>dq.match.forensic.export</code> audit. Access controlled. <br><strong>Tests:</strong> manifest completeness & checksum verification. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>TelemetryEmit(metricName, value, tags)</code> — local buffering & audited upload</strong><br><strong>Purpose & contract:</strong> append metrics to local buffer (no direct network from ribbon path). Tags include <code>proposalId</code>,<code>module</code>,<code>datasetPolicy</code>. Metric uploader (privileged worker) performs network export. Metrics: <code>dq.match.candidates_count</code>, <code>dq.merge.duration_ms</code>, <code>dq.merge.timeout_rate</code>. Append <code>dq.match.metric.buffered</code> audit optionally. <br><strong>Tests:</strong> buffer durability & uploader compatibility. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>MetricsBufferUploader()</code> — audited uploader of metrics</strong><br><strong>Purpose & contract:</strong> run in privileged worker to upload metric batches and emit <code>dq.match.metrics.uploaded</code> audit with batch checksum and counts. Must respect redaction; tags must not contain PII. <br><strong>Tests:</strong> uploader retry semantics and loss simulation. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>AuditChainVerifier(sampleRunId)</code> — CI & monitoring verification</strong><br><strong>Purpose & contract:</strong> verify <code>dq.merge</code> audit chain integrity for sample runs: validate <code>prevHash</code> chaining, <code>payloadHash</code> parity with artifacts, verify audit rotation signatures. Emit <code>dq.match.audit.verify</code> with results. Failures block CI merges. <br><strong>Tests:</strong> golden audit-chain tests and mis-signed rotation detection. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ConfigRollForward(newConfig, operatorId)</code> & <code>ConfigRollback(targetConfigHash)</code> — controlled migrations</strong><br><strong>Purpose & contract:</strong> roll forward config with validation and smoke tests; rollback to <code>targetConfigHash</code> runs smoke tests and emits <code>config.rollback</code> audit. All config changes require PR, CI golden runs, and approvals. <br><strong>Tests:</strong> migration parity, rollback correctness. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>MergeConflictResolver(conflictingJobs[])</code> — worker-level job arbitration</strong><br><strong>Purpose & contract:</strong> reconcile concurrent scheduled jobs targeting overlapping recordsets. Deterministic arbitration: order by <code>job.persistedAt</code>, <code>operatorPriority</code>, <code>jobId</code>, then either serialize or merge job descriptors combining reversePlans. Emit <code>dq.merge.job.conflict.resolved</code> audit. <br><strong>Tests:</strong> conflict simulation and combined reversePlan verification. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>CrossTenantGuard(proposal)</code> — multi-tenant isolation</strong><br><strong>Purpose & contract:</strong> detect cross-tenant merges. Default: block and return <code>requiresApproval</code> with approvers required from both tenants. Emit <code>dq.merge.crossTenant.blocked</code> or <code>dq.merge.crossTenant.allowed</code> audit. <br><strong>Tests:</strong> cross-tenant detection and approval path. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>AnonymizePreview(previewRef, userPermissions)</code> — redaction of previews</strong><br><strong>Purpose & contract:</strong> produce redacted preview replacing PII with placeholders or hashed tokens while preserving structure and confidence. Return <code>anonymizedPreviewRef</code>. Full preview accessible only via <code>evidenceRef</code> with approval. <br><strong>Tests:</strong> redaction correctness & permission gating. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ReconciliationRunner(beforeChecksum, afterChecksum, samplePartition)</code> — verification after apply</strong><br><strong>Purpose & contract:</strong> recompute sample checksums, validate counts and referential integrity post-apply, emit <code>dq.merge.reconciliation</code> audit with <code>reconReportRef</code>. On mismatch schedule forensic export and possible revert. <br><strong>Tests:</strong> recon parity and failover flows. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ExportMergeArtifact(artifactRef, destinationUri, operatorId)</code> — secure export with chain-of-custody</strong><br><strong>Purpose & contract:</strong> securely export artifacts via <code>REG_Export</code> path, compute <code>artifact.checksum.sha256</code>, redact owner emails when operator lacks rights, append <code>dq.merge.export</code> audit with URI and checksum. <br><strong>Tests:</strong> checksum validation and redaction checks. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>ErrorCodeCatalog (module)</code> — canonical error taxonomy & operator mapping</strong><br><strong>Purpose & contract:</strong> centralized error codes: <code>DQ_ERR_INIT_001</code> (config missing), <code>DQ_ERR_CONFIG_INVALID</code>, <code>DQ_ERR_PERSIST_FAIL</code>, <code>DQ_ERR_PERMISSION_DENIED</code>, <code>DQ_ERR_TIMEOUT</code>, <code>DQ_ERR_CONFLICT</code>, etc. Each code maps to operator message (no PII) and diagnostic artifact <code>diagRef</code>. Errors must be used consistently and included in audits. <br><strong>Tests:</strong> ensure thrown errors map to catalog and include audit rows. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>Performance budgets &amp; SLOs (module)</code></strong><br><strong>Targets & metrics:</strong><br>- Candidate generation median <20ms for small buckets (<1k).<br>- Pair scoring median <5ms. <br>- Inline apply default timeout 5s (configurable).<br>- Job persist latency <2s. <br><strong>Metrics collected:</strong> <code>dq.match.candidates_count</code>, <code>dq.merge.duration_ms</code>, <code>dq.merge.timeout_rate</code>, <code>job.persist.latency_ms</code>. <br><strong>Runbook:</strong> if budgets breached: throttle inline applies; shift to scheduled-only; run <code>audit_chain.verify</code> on sample runs; ramp down features (e.g., complex comparators) and invoke operator notification. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>Testing matrix (module)</code></strong><br><strong>Required tests:</strong> unit: <code>NormalizeRecord</code>, <code>ComparePair</code>, <code>ComputeMatchScore</code>, <code>ResolveTie</code>, <code>BuildMergeProposal</code>, <code>BuildProposalImpactHash</code>. Integration: candidate→score→proposal→persist→job→worker→apply→reconcile. Golden: <code>proposal.payloadHash</code> parity tests. CI gating: audit-chain verify, static checks forbidding forbidden APIs. Property: correlation id uniqueness under load. <br><strong>CI enforcement:</strong> block merges on golden/audit-chain failures. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>Failure modes &amp; mitigations (module)</code></strong><br><strong>Common incidents & mitigations:</strong><br>1. Invalid config/signature → <code>dq.match.config.invalid</code> → revert to last-good config and notify owner. <br>2. Excessive blocking collisions → <code>dq.match.blocking.warning</code> → operator retune <code>blockingSpec</code>. <br>3. Partial apply failure → <code>dq.merge.reverted</code> using <code>reversePlan</code> + snapshot; open incident. <br>4. Permission denied → <code>dq.merge.permission.denied</code> → require approvals. <br>5. Watchdog timeouts → <code>dq.merge.timeout</code> → schedule job. <br><strong>Forensics:</strong> collect <code>match-rules.json</code>, <code>configHash</code>, <code>engineHash</code>, <code>audit_tail.csv</code> rows for <code>correlationId</code>, <code>proposal.json</code>, <code>job.descriptor</code>, <code>engineSnapshot</code>, <code>forensic_manifest.json</code>. <br><strong>Operator recovery checklist:</strong> locate <code>correlationId</code>, retrieve audit chain, fetch <code>evidenceRef</code>, run dry-run in isolated runner, revert <code>hotSwap</code> if needed, restore dataset from snapshot. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>Operator UX &amp; triage notes (module)</code></strong><br><strong>Best-practice UI:</strong> always show <code>correlationId</code> and provide "copy diagnostics" and "download forensic package" actions. Provide clear <code>preview</code>, <code>simulate</code>, <code>apply (inline)</code>, <code>apply (scheduled)</code> with <code>requiresApproval</code> flags. Show succinct <code>safe/unsafe</code> badge with <code>requiredApprovals</code> when applicable. <br><strong>Triage flow:</strong> 1) get <code>correlationId</code>; 2) fetch <code>UserAction</code> and step audits; 3) request <code>evidenceRef</code> if needed (approval); 4) run <code>dry-run</code> in isolated runner; 5) collect <code>forensic_manifest</code> and escalate. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>Change-control &amp; governance (module)</code></strong><br><strong>Flow for changes:</strong> PR + migration manifest if semantics change; static analysis forbidding forbidden APIs; unit/integration/golden tests; compliance/owner approvals for regulated changes; sign artifacts and publish release manifest; canary rollout with KPI gating; post-rollout <code>VerifyAuditChain</code> and <code>deployment.audit</code>. <br><strong>Hot-swap governance:</strong> smoke tests and approvals required before applying regulated comparator or threshold changes. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong><code>Appendices &amp; references (module)</code></strong><br><strong>Include:</strong> canonical <code>match-rules.json</code> schema, audit row schema, <code>ErrorCodeCatalog.md</code>, <code>reversePlan</code> template, migration manifest template, forensic manifest, operator cheat-sheets, <code>OWNERS.md</code> mapping. Store appendices in immutable artifact store <code>\\artifacts\DQ_MatchMerge\v{major}.{minor}\appendices\</code> with RBAC. Provide runbooks: <code>dq_merge_smoke_test.md</code>, <code>dq_merge_incident_runbook.md</code>, <code>dq_merge_operator_cheatsheet.md</code>. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong>Acceptance criteria (dev/CI)</strong><br><strong>Gates:</strong> unit + integration + golden tests pass; no forbidden API in static analysis; <code>configHash</code> & <code>engineHash</code> computed; <code>dq.merge</code> audit chain present and <code>VerifyAuditChain</code> passes; performance budgets met under CI load. Blocking conditions: golden/audit-chain failures or forbidden-API detection. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong>Forbidden APIs & static enforcement (module)</strong><br><strong>Forbidden on UI path and inline handlers:</strong> direct Workbook/Range modification during <code>OnLoad</code> or critical UI callbacks; raw network calls (WinHTTP) or external web sockets; unbounded synchronous disk writes (>10ms); direct plaintext secret reads; spawning external processes. CI static analyzer rejects PRs referencing blacklisted APIs. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong>Operator-run checklists & CLI examples (compact)</strong><br><strong>Common operator commands:</strong> <code>dq.preview --proposal &lt;id&gt;</code>, <code>dq.apply --proposal &lt;id&gt; --mode scheduled --ticket &lt;id&gt;</code>, <code>dq.hotswap --config path --dry-run</code>, <code>dq.forensic.export --cid &lt;correlationId&gt; --out &lt;uri&gt;</code>, <code>dq.audit.verify --sample &lt;runId&gt;</code>. Each command emits audits and returns <code>correlationId</code>. Include operator TTL & MFA checks for emergency overrides. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong>Forensic package & chain-of-custody fields</strong><br><strong>Minimum artifacts:</strong> <code>match-rules.json</code> & signature, <code>audit_tail.csv</code> rows covering correlation ids, <code>proposal.json</code>, persisted <code>evidence</code> blobs (encrypted), <code>job.descriptors</code>, <code>engineSnapshot</code>, <code>configHash</code>, <code>forensic_manifest.json</code> mapping artifacts to checksums and storage URIs. Store in secure evidence repo with RBAC and C-O-C records. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong>Runbooks for common incidents (short)</strong><br><strong>Config invalid:</strong> retrieve <code>dq.match.config.invalid</code> audit, revert to last-good config, run <code>hotSwap.preview</code> on candidate fixes, apply after approval. <br><strong>Apply failure mid-transaction:</strong> run <code>dq.merge.reverted</code> audit check; fetch <code>reversePlan</code> from <code>evidenceRef</code>; run restore snapshot; open incident and generate <code>forensic_manifest</code>. <br><strong>Unexpected high timeouts:</strong> set inline applies to scheduled-only and scale workers; collect <code>dq.match.*</code> metrics and run smoke tests on active config. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong>Implementation guidance & safe IO patterns</strong><br><strong>IO policy:</strong> use read-then-verify-then-rename atomic patterns for writes; perform heavy index builds or persistence in background worker; no network during synchronous UI callbacks. <br><strong>Security:</strong> secrets via KMS/HSM; manifest signature verification locally; PII redaction enforced prior to any non-encrypted store writes. <br><strong>Coding patterns:</strong> pure functions for normalization and comparators; small deterministic RNG wrappers (<code>seed = sha256(engineHash|cid|stableSalt)</code>). <br><strong>Testing hooks:</strong> include deterministic test harness with fixed RNG seeds and <code>test=true</code> audit flags. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong>Operator & compliance checklist for regulated merges</strong><br>1. Confirm dataset <code>regulated</code> flag. <br>2. Generate <code>previewRef</code> and attach <code>correlationId</code>. <br>3. Acquire required approvals (two-person) via <code>ProposalApprovalWorkflow</code>. <br>4. Persist <code>proposal</code> and schedule apply. <br>5. Post-apply run <code>ReconciliationRunner</code> and include <code>reconReportRef</code> in compliance package. <br>6. Export <code>forensic_manifest</code> and append to regulatory package. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong>End-to-end example narrative (illustrative)</strong><br><strong>Normal flow:</strong> Operator selects candidate rows → <code>NormalizeRecord</code> → <code>ComputeBlockingKey</code> → <code>GenerateCandidates</code> → <code>ComparePair</code> per candidate → <code>ComputeMatchScore</code> → <code>DecideMatch</code> → <code>ResolveTie</code> → <code>BuildMergeProposal</code> → <code>EvaluateProposalSafety</code> (safe) → <code>PersistProposal</code> → <code>PreviewMergeUI</code> → Operator approves → <code>ScheduleOrApply</code> decides inline → <code>SafeInvokeMergeHandler</code> applies changes → <code>ApplyMergeAtomic</code> persists → <code>EmitMergeAudit</code> chain (<code>proposal.created</code>→<code>merge.started</code>→<code>merge.completed</code>) → <code>ReconciliationRunner</code> verifies. <br><strong>Fault flow:</strong> config invalid on hot-swap → <code>dq.match.config.invalid</code> audit → engine fails safe → operator notified; or partial apply failure → <code>dq.merge.reverted</code> + forensic package appended. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong>Glossary (compact)</strong><br><code>engineHash</code> — fingerprint of compiled engine artifacts; <code>configHash</code> — canonical match rules JSON hash; <code>proposal.payloadHash</code> — canonical proposal JSON sha256; <code>evidenceRef</code> — opaque pointer to encrypted evidence; <code>correlationId</code> — UI-level request id; <code>jobId</code> — persisted job descriptor id. </td></tr><tr><td data-label="DQ_MatchMerge — Per-function Expert Technical Breakdown"> <strong>Final notes (operational & compliance)</strong><br>1. All user-initiated actions MUST emit a <code>UserAction</code> audit with <code>correlationId</code>. <br>2. All artifacts that include PII MUST be stored encrypted and referenced by <code>evidenceRef</code> in public audits. <br>3. Hot-swap & model updates must run smoke tests via deterministic harness with <code>test=true</code> hooks for CI golden parity. <br>4. CI pipeline requires <code>audit-chain-verify</code> and golden-file checks; failures block release. <br>5. Maintain appendices and runbooks in immutable artifact store with RBAC. </td></tr></tbody></table></div><div class="row-count">Rows: 58</div></div><div class="table-caption" id="Table3" data-table="Docu_0177_03" style="margin-top:2mm;margin-left:3mm;"><strong>Table 3</strong></div>
<div class="table-wrapper" data-table-id="table-3"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by DQ_Remediation — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">DQ_Remediation — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong>Module-level summary — Purpose, owners, public API, guarantees, invariants, and audit obligations (exhaustive)</strong><br><strong>Purpose & contract:</strong> authoritative module for converting data-quality findings (from <code>DQ_Profile</code>, <code>DQ_Rules</code>, <code>DQ_MatchMerge</code>) into safe, auditable remediation workflows. Responsibilities: generate deterministic proposals with before/after previews; compute auditable risk/confidence scores; cluster and rank proposals for operator review; build canonical remediation plans with explicit transactional boundaries and undo descriptors; validate plans against policy, runtime and approvals; provide deterministic dry-run simulations; execute remediation inline (safe, short) or schedule heavy jobs; persist canonical job descriptors idempotently for workers; produce reversible undo artifacts or record explicit non-reversible signoffs; export artifacts atomically with checksums and redaction manifests; build forensic packages on failures; notify owners/stakeholders safely; and ensure every operator-visible action produces canonical, chained audit rows. Must not perform silent destructive mutations; must never display PII in UI/audit main fields; must store full evidence encrypted and reference via <code>evidenceRef</code> in audits.<br><strong>Owners / manifest:</strong> module owner(s) and API published in <code>OWNERS.md</code> and module manifest; versioned <code>module.manifestHash</code> must be recorded on major transitions. <br><strong>Exposed API (canonical):</strong> <code>GenerateProposal</code>, <code>ScoreProposal</code>, <code>RankAndGroupProposals</code>, <code>BuildRemediationPlan</code>, <code>ValidateRemediationPlan</code>, <code>SimulateApply</code> (dry-run), <code>IsLightweightAction</code>, <code>ApplyRemediation</code>, <code>PersistRemediationJob</code>, <code>SafeRemediationExecutor.execute</code>, <code>BuildUndoPlan</code>, <code>RevertRemediation</code>, <code>ValidateApprovals</code>, <code>BuildUiPreview</code>, <code>ExportRemediationArtifacts</code>, <code>NotifyOwnersAndStakeholders</code>, <code>ExportForensicsForFailedApply</code>, <code>SafeErrorToUser</code>, <code>RegisterUnitTestHook</code>. Each API must declare stable identifiers returned (<code>proposalId</code>, <code>planId</code>, <code>applyId</code>, <code>undoId</code>, <code>jobId</code>) and include <code>correlationId</code> in parameters or context. <br><strong>Primary invariants (must/shall):</strong><br>1. Determinism: given identical <code>tableRef</code>, <code>findings</code>, <code>configHash</code>, and <code>seed</code>, <code>GenerateProposal</code> and subsequent plan hashes must be identical. Canonical JSON (sorted keys, normalized numbers/dates) is used to compute SHA256-based <code>proposalHash</code> / <code>planHash</code> / <code>undoHash</code>.<br>2. Audit anchoring: every user-initiated action appends a <code>UserAction</code>/<code>dq_*</code> audit row with <code>correlationId</code> and safe <code>paramsHash</code>; full sanitized evidence stored encrypted with <code>evidenceRef</code> referenced by audit metadata.<br>3. No silent destructive changes: destructive operations must be explicitly <code>dq_apply</code>-audited and approved per governance rules.<br>4. Reversibility: destructive operations must produce <code>UndoPlan</code> or record explicit signed operator acknowledgement when undo impossible.<br>5. Sensitive data: UI hints and main audit messages must be PII-free; full sanitized evidence stored encrypted using KMS/HSM keys.<br>6. Performance budgets: proposal generation median <200ms for small tables (<10k rows), dry-run preview for 10k sample <2s, inline apply default timeout 5s (configurable).<br><strong>Audit obligations & schemas:</strong> <code>dq_proposal</code>, <code>dq_proposal.preview</code>, <code>dq_proposal.accepted</code>, <code>dq_plan.built</code>, <code>dq_plan.validate</code>, <code>dq_apply.start</code>, <code>dq_apply.dryrun</code>, <code>dq_apply.complete</code>, <code>dq_apply.failed</code>, <code>dq_apply.reverted</code>, <code>dq_undo.built</code>, <code>dq_export.remediation</code>, <code>forensic.export</code>, <code>dq_approval.*</code>. Each audit includes schema fields: <code>timestamp, correlationId, module=DQ_Remediation,procedure,operatorId,proposalId,planId,applyId,paramsHash,configHash,prevHash,payloadHash,artifactChecksum,metadata</code>. Evidence stored separately with <code>evidenceRef</code> and access controlled. <br><strong>CI/QA gating:</strong> golden-file parity for <code>proposalHash</code>/<code>planHash</code> on representative fixtures; static analyzer gating forbidding direct workbook writes during proposal/dry-run; unit/integration/infrastructure tests; <code>VerifyAuditChain</code> runs in CI. <br><strong>Checked 10×:</strong> canonicalization rules, audit presence, PII redaction, undo completeness, deterministic RNG seed propagation, idempotent job persistence, evidence encryption, manifest signature checks for <code>AUTO_APPLY</code>, approval validation flow, runbook triggers for failure modes. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>GenerateProposal(tableRef, findings[], options)</code> — canonical candidate synthesis</strong><br><strong>Purpose & contract:</strong> synthesize candidate remediation actions from a canonical <code>tableRef</code> and <code>findings[]</code>. Returns <code>Proposal[]</code> and <code>proposalSummary</code> where each <code>Proposal</code> includes <code>proposalId</code>, <code>actions[]</code>, <code>affectedCount</code>, <code>sampleBefore[]</code>, <code>sampleAfterPreviewRef</code>, <code>confidence</code>, <code>rationale</code>, <code>proposalHash</code>, <code>evidenceRef</code>. Must be side-effect free and fast on UI path for small inputs (target <200ms). <br><strong>Parameters & types:</strong> <code>tableRef</code> (immutable descriptor: workbookId/sheet/queryName, rowCount, checksum, schemaHash), <code>findings[]</code> (structured: <code>ruleId</code>, <code>severity</code>, <code>rowMatches</code>, <code>context</code>), <code>options</code> (maxProposals:int, sampleSize:int, preferNonDestructive:bool, seed:int). Returns deterministic proposals and <code>proposalHash=sha256(canonicalJSON(proposal))</code>. <br><strong>Deterministic steps (must/shall):</strong><br>1. Normalize table schema: stable column ordering, normalized types (dates -> ISO8601), trimmed strings, consistent unicode normalization (NFC).<br>2. For each <code>finding</code> consult <code>RemediationPolicy</code> ordered by policy priority to enumerate safe actions: <code>flag</code>, <code>standardize</code>, <code>mapReplace</code>, <code>dictionaryReplace</code>, <code>suggestCanonical</code>, <code>mergeCandidate</code>, <code>nullToDefault</code>, <code>dropRow</code> (last-resort), <code>manualReview</code>.<br>3. For fuzzy matches or merge candidates call <code>DQ_MatchMerge</code> deterministic functions with provided <code>seed</code>. Record <code>matchDistance</code>, <code>tieBreaker</code> metadata.<br>4. Produce <code>sampleBefore</code>/<code>sampleAfter</code> by applying candidate action to deterministic sample subset (<code>sampleSize</code>, seeded selection). Strip/replace PII in samples for audit/UI; store full sanitized sample encrypted with <code>evidenceRef</code>.<br>5. Compute <code>confidence</code> score (canonical aggregation) and generate <code>rationale</code> (PII-free explanation).<br>6. Canonicalize and compute <code>proposalHash</code>. Append <code>dq_proposal</code> audit with <code>proposalId, proposalHash, affectedCount, correlationId, configHash</code> and <code>evidenceRef</code> in metadata.<br><strong>Observability & evidence policy:</strong> Keep visible sample small (<500 rows). Full proposal evidence saved encrypted with <code>evidenceRef</code>; <code>dq_proposal</code> audit stores <code>proposalHash</code> and <code>paramsHash</code> only. <br><strong>Restrictions & safe I/O:</strong> do not perform workbook writes; heavy dictionary or network operations scheduled to worker thread with <code>Deferred</code> flow. For PQ-sourced tables prefer cached snapshots. <br><strong>Examples:</strong> phone normalization via deterministic regex + mapping; merge duplicates using <code>mostComplete</code> tie-breaker; propose <code>null-&gt;default</code> for missing categorical fields. <br><strong>Tests & CI vectors:</strong> deterministic proposal generation, negative tests for malformed inputs, large-dictionary worker scheduling tests, golden <code>proposalHash</code> check. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>ScoreProposal(proposal, scoringConfig)</code> — reproducible, explainable scoring</strong><br><strong>Purpose & contract:</strong> calculate reproducible <code>scores</code> for ranking and gating: <code>confidence (0..1)</code>, <code>risk (0..1)</code>, <code>costEstimate (ms/rows + label)</code>, <code>reversibility (bool)</code>, and <code>regulationImpact</code>. Returns <code>proposal</code> augmented with <code>scores</code>, <code>scoreRationale[]</code> and <code>scoreHash</code> computed as SHA256 over canonicalized score object. Must include <code>scoringConfig.hash</code> in audit. <br><strong>Inputs:</strong> <code>proposal</code> (from <code>GenerateProposal</code>), <code>scoringConfig</code> (weights, thresholds, regulatedFields[], pilotCohorts[]). <br><strong>Algorithm (deterministic):</strong><br>1. Extract raw features: <code>matchQuality</code>, <code>coverage</code>, <code>sampleSuccessRate</code>, <code>ruleSeverity</code>.<br>2. Weighted aggregation: <code>confidence = clamp(sigmoid(weightedSum(features)), 0..1)</code>.<br>3. <code>risk = baseRisk(actionTypes) + regulationMultiplier(if regulatedFields)</code>.<br>4. <code>costEstimate</code> derived from <code>affectedCount × perActionCost</code> from config and runtime microbench tables. Return human-friendly <code>costLabel</code> and numeric estimate <code>costMs</code>.<br>5. <code>reversibility = boolean</code> if <code>UndoPlan</code> completeness passes validation. <br><strong>Governance & enforcement:</strong> if <code>risk &gt;= riskThreshold</code> and <code>regulationImpact</code> is high, mark <code>requiresApproval</code> and attach required roles. <br><strong>Observability / audit:</strong> emit <code>dq_proposal.score</code> with <code>proposalId, confidence, risk, costEstimate, reversibility, scoringConfig.hash, scoreHash</code>. <br><strong>Tests:</strong> reproducibility across versions (include <code>scoringConfig.hash</code>), sensitivity tests for weight changes, adversarial tests (crafted inputs reducing confidence). </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>RankAndGroupProposals(proposals[], rankingPolicy)</code> — cluster, deconflict, stable ordering</strong><br><strong>Purpose & contract:</strong> group related proposals into operator-facing bundles and deterministically rank them. Return <code>ProposalGroup[]</code> with <code>groupId, proposalIds[], affectedFields[], affectedCount, impactSummary, estimatedTimeMs, groupConfidence, groupHash</code>. <br><strong>Grouping heuristics (configurable):</strong> row overlap threshold, field overlap, identical target columns, mutual-exclusion detection (conflicting proposals for same rows), and similarity clustering for transform aggregation. Groups must either be disjoint or contain explicit <code>overlapHint</code> indicating overlapping proposals. <br><strong>Stable ordering invariants:</strong> sort by <code>(-groupConfidence, groupRisk, estimatedCost, groupHash)</code> to ensure stability across reloads unless <code>rankingPolicy</code> changes (then <code>ribbonMap.hash</code> or <code>configHash</code> must record the change). <br><strong>UI contract & previews:</strong> provide <code>groupPreviewRef</code> and <code>collapseHint</code>; widen <code>affectedCount</code> > threshold into multi-page UI. <br><strong>Audit:</strong> emit <code>dq_proposal.grouping</code> with <code>groupId, proposalIds, groupHash</code>. <br><strong>Tests:</strong> grouping stability with permuted inputs, conflict detection correctness, UI preview checks. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>BuildRemediationPlan(group, operatorChoices, applyMode)</code> — concrete plan generation with undo wiring</strong><br><strong>Purpose & contract:</strong> compose finalized <code>RemediationPlan</code> from <code>ProposalGroup</code> and the operator's explicit choices; define ordered <code>actions[]</code>, transactional boundaries, <code>preconditions[]</code>, <code>postchecks[]</code>, <code>sideEffects[]</code>, <code>undoPlanDescriptor</code> and <code>executionHints</code> (e.g., <code>inlineTimeoutMs</code>, <code>stagingStrategy</code>). <code>applyMode</code> ∈ {<code>copy</code>, <code>inline</code>, <code>staged-job</code>}. Must be deterministic and idempotent for same inputs; compute <code>planHash</code>. Append <code>dq_plan.built</code> audit with <code>planId, planHash, operatorId, configHash</code>. <br><strong>Plan construction details:</strong><br>1. Normalize accepted proposals; resolve conflicts per <code>operatorChoices</code> or produce <code>resolutionHint</code> for manual resolution. <br>2. Expand each action into canonical step with <code>actionId</code>, <code>type</code>, <code>targetSpec</code> (column, rowMatcher canonical), <code>payload</code> (normalized), <code>preconditions</code>, <code>postchecks</code>, and <code>estimatedCostMs</code>. <br>3. Determine <code>transactionalBoundaries</code> to allow partial commit and rollback at safe checkpoints; prefer <code>copy</code> with swap for destructive changes. <br>4. Build <code>UndoPlan</code> concurrently with <code>RemediationPlan</code> (reverse steps + preimage collection spec). <br>5. Attach <code>requiredApprovals[]</code> metadata (roles, approver count, TTL) when <code>risk</code> or <code>regulationImpact</code> exceed thresholds. <br><strong>PII & evidence:</strong> plan persisted encrypted with <code>evidenceRef</code>; main audit contains <code>planHash</code> only. <br><strong>Examples:</strong> merge duplicate rows -> actions include <code>identifyDuplicates</code>, <code>computeMergeStrategy</code>, <code>applyMerge</code>, <code>postcheck.consistency</code>; undo includes <code>splitMergedRows</code> with preimage snapshots. <br><strong>Tests:</strong> parity across reruns yields identical <code>planHash</code>, undo completeness tests, conflict resolution tests. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>ValidateRemediationPlan(plan, runtimeContext)</code> — preflight safety & enforcement</strong><br><strong>Purpose & contract:</strong> perform static & dynamic validations and return <code>{valid:Boolean, errors[], warnings[], enforcementAction}</code> where <code>enforcementAction</code> ∈ {<code>allow</code>,<code>requireApproval</code>,<code>fail-closed</code>}. Must be side-effect free. <br><strong>Deterministic validation steps (detailed):</strong><br>1. Schema & integrity: ensure <code>planHash</code> matches canonical serialization; verify any signatures if <code>AUTO_APPLY</code> requested. <br>2. Preconditions: verify <code>tableRef.beforeChecksum</code> matches current workbook state; if mismatch return <code>fail-closed</code> with <code>R_PLAN_001_CHECKSUM_MISMATCH</code>. <br>3. Resource checks: workspace disk, temp sheet availability, job scheduler reachable if <code>staged-job</code> mode. <br>4. RBAC/Approval checks: call <code>ValidateUserPermissions</code> and <code>ValidateApprovals</code> for required roles and signoffs. <br>5. Concurrency: check for concurrent jobs/locks on the same table or columns; if conflict present, either fail or queue per <code>concurrencyPolicy</code>. <br>6. Reversibility checks: ensure <code>UndoPlan</code> exists and <code>preimageEvidenceRef</code> available for reversible steps. <br><strong>Enforcement & UI response:</strong> warnings can be accepted by operator (must be explicit), critical errors cause <code>fail-closed</code>. <code>dq_plan.validate</code> audit includes <code>planId, valid, errors[], enforcementAction</code>. <br><strong>Tests:</strong> expired approvals, permission-denied, checksum mismatches, concurrent lock scenarios. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>SimulateApply(plan, simulatorConfig)</code> — deterministic dry-run & reconciliation</strong><br><strong>Purpose & contract:</strong> perform a deterministic, non-destructive simulation of the <code>RemediationPlan</code> producing <code>previewDiffs</code>, <code>reconciliationReport</code>, <code>sampleBefore</code>, <code>sampleAfter</code>, <code>previewRef</code>, and <code>previewHash</code>. Must not mutate persistent workbook state; may create transient staging artifacts hidden from user if required and always remove them after simulation (unless evidence capture requested). <br><strong>Simulator parameters:</strong> <code>simulatorConfig</code> includes <code>seed:int</code>, <code>samplePercent</code>, <code>maxPreviewRows</code>, <code>previewMode</code> (<code>compact|full</code>), <code>sanitizePII</code> flag. <br><strong>Simulation steps:</strong><br>1. Create sanitized snapshot of target dataset in-memory or hidden temp sheet per <code>stagingStrategy</code>. <br>2. Apply actions in canonical order, capturing <code>stepPayloadHash</code> per step. <br>3. For merges/fuzzy matches produce conflict resolution logs; for each replaced value capture mapping count. <br>4. Run <code>postchecks</code> and produce <code>reconciliationReport</code> comparing pre/post checksums on the sample. <br>5. Compute <code>previewHash = sha256(canonicalPreview)</code>. Save full simulation evidence encrypted (<code>evidenceRef</code>) and return <code>previewRef</code> for UI. <br><strong>Observability & audit:</strong> emit <code>dq_apply.dryrun</code> with <code>planId, previewHash, correlationId</code>. <br><strong>Performance & UI behavior:</strong> small preview returned synchronously (<200ms) for typical sampleSize; large previews streamed/paginated on request. <br><strong>Tests:</strong> side-effect free verification, golden previewHash parity, redaction coverage. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>IsLightweightAction(plan)</code> — policy evaluation for inline vs scheduled execution</strong><br><strong>Purpose & contract:</strong> decide inline vs scheduled execution using <code>plan</code> metadata, <code>runtime.mode</code> (degraded/normal), <code>modConfig.thresholds</code> (rowCount, estimatedDuration), <code>risk</code>, and operator overrides. Return <code>{lightweight:Boolean, rationale:String, decisionHash}</code>. Decision must be auditable and reversible via explicit operator override (override action logged). <br><strong>Policy examples & thresholds:</strong><br>1. Inline if <code>estimatedDurationMs &lt; inlineTimeoutMs (default 5000ms)</code> AND <code>risk &lt; riskThreshold</code> AND <code>!affectsRegulatedFields</code>.<br>2. Hybrid: if <code>estimatedDuration</code> near threshold, propose sampling inline (preview) and schedule full job. <br>3. Safe-mode: force scheduling if runtime <code>safeMode=true</code>. <br><strong>Audit:</strong> emit <code>dq.apply.decision</code> with <code>planId, lightweight, rationale</code>. <br><strong>Tests:</strong> boundary conditions, safe-mode forced scheduling, operator override audit. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>ApplyRemediation(plan, operatorContext)</code> — authoritative apply orchestrator</strong><br><strong>Purpose & contract:</strong> authoritative dispatcher performing validated apply: create <code>applyId</code>, audit <code>dq_apply.start</code>, decide inline vs scheduled via <code>IsLightweightAction</code>, invoke <code>SafeRemediationExecutor</code> for inline or call <code>PersistRemediationJob</code> for scheduled, and emit final audit rows for success/failure. Must never mutate without prior audit and validation. <br><strong>Canonical orchestration (detailed):</strong><br>1. <code>validate = ValidateRemediationPlan(plan, runtimeContext)</code> — fail if <code>enforcementAction == fail-closed</code>.<br>2. <code>applyId = NewApplyId(parent=correlationId)</code>; record <code>createdAt</code>.<br>3. Emit <code>dq_apply.start</code> with <code>applyId, planId, operatorId, paramsHash</code>.<br>4. <code>decision = IsLightweightAction(plan)</code>.<br>5a. Inline path: create <code>cancellationToken</code>, call <code>SafeRemediationExecutor.execute(plan, cancellationToken)</code>. Record step-level audits and final <code>dq_apply.complete</code> with checksums and <code>artifactRef</code> on success. <br>5b. Scheduled path: assemble <code>jobDescriptor</code> and call <code>PersistRemediationJob(jobDescriptor)</code> -> return <code>job.persisted</code> audit and immediate UI message containing <code>jobId</code> and <code>applyId</code> reference. <br>6. On error: map to stable <code>DQ_Error</code> code, emit <code>dq_apply.failed</code>, trigger <code>ExportForensicsForFailedApply</code> as needed, and call <code>SafeErrorToUser</code> for UI-safe message. <br><strong>UI contract:</strong> short synchronous response with <code>status</code>, <code>message</code>, <code>correlationId</code>, <code>applyId</code>, and <code>nextSteps</code>. Encourage copy of <code>applyId</code> for triage. <br><strong>Idempotency:</strong> use <code>applyId</code> as idempotency key; replaying <code>ApplyRemediation</code> with same <code>applyId</code> should return stored result. <br><strong>Tests:</strong> concurrency stress (100s parallel applies), idempotency tests, partial-failure and checkpoint recovery tests. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>PersistRemediationJob(jobDescriptor)</code> — canonical job persistence & idempotency</strong><br><strong>Purpose & contract:</strong> persist canonical <code>jobDescriptor</code> for worker consumption; ensure atomic write and idempotency keyed by <code>paramsHash</code> and <code>applyId</code>. <code>jobDescriptor</code> fields: <code>jobId, planId, applyId, paramsHash, configHash, persistedAt, owner, evidenceRef, targetArtifacts[]</code>. Return <code>jobId</code> and ensure <code>job.persist.latency_ms</code> within targets. <br><strong>Persistence invariants:</strong> atomic-write via <code>CORE_Utilities.atomic-write</code>; check for existing descriptor with same <code>paramsHash</code> and return existing <code>jobId</code> (idempotent). Emit <code>job.persisted:&lt;jobId&gt;</code> audit row. <br><strong>Failure & retry policy:</strong> exponential backoff on transient store errors; after N retries escalate and produce <code>forensic_manifest</code> with <code>jobDescriptor</code> snapshot. <br><strong>Tests:</strong> idempotent persist tests, simulated storage failures/backoff, race-condition tests ensuring single persisted descriptor per <code>paramsHash</code>. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>SafeRemediationExecutor.execute(plan, cancellationToken)</code> — protected inline execution frame</strong><br><strong>Purpose & contract:</strong> execute <code>RemediationPlan</code> inline with protective envelope: per-step checkpoints, staging area, cooperative cancellation, timeouts, exception mapping, redaction, and final atomic swap or commit. Return <code>ExecutionResult {status, artifactRef, beforeChecksum, afterChecksum, stepDurations[], payloadHash}</code>. <br><strong>Execution frame invariants (detailed):</strong><br>1. Staging-first: create staging artifact <code>__remed_&lt;applyId&gt;</code> (hidden sheet or temp workbook), apply transforms there. <br>2. Step-level checkpoint: persist minimal checkpoint metadata (stepId, stepHash) before each step. <br>3. Postchecks after each step: schema constraints, reconciliation rules; if postcheck fails attempt <code>safeRollback</code> within staging. If rollback fails persist forensic evidence and fail gracefully. <br>4. Atomic swap/commit: after all steps pass, perform host-safe swap (rename/hide tactics) to keep UI consistent and avoid partial state exposure. <br>5. Watchdog & cancellation: watchdog triggers <code>dq_handler.timeout</code> and attempts cooperative cancellation via token; if unresponsive escalate to <code>dq_handler.hung</code> with <code>forensic_manifest</code>. <br>6. Error mapping & user-safe messaging: map thrown exceptions to <code>ErrorCodeCatalog</code> entries and call <code>SafeErrorToUser(correlationId, applyId, error)</code>. Detailed traces redacted and saved encrypted; audit only contains safe error code and correlation id. <br><strong>Telemetry & audit:</strong> emit <code>dq_handler.step.start</code>, <code>dq_handler.step.complete</code>, <code>dq_handler.timeout</code>, <code>dq_handler.exception</code> with <code>applyId</code>. Compute and record <code>payloadHash</code> for each step. <br><strong>Developer guidance:</strong> avoid heavy network or disk IO on UI thread; prefer quick operations and defer heavy tasks to scheduled job if required. <br><strong>Tests:</strong> cancellation reaction tests, partial failure checkpoint tests, swap atomicity tests, watchdog forced timeouts. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>BuildUndoPlan(plan)</code> — deterministic undo descriptor & preimage capture</strong><br><strong>Purpose & contract:</strong> generate <code>UndoPlan</code> mirroring <code>RemediationPlan</code> reverse steps with preimage evidence collection and verification checksums. Return <code>undoId, undoPlanHash, reversible:Boolean, retentionPolicy</code>. If plan includes external side-effects mark <code>reversible:false</code> and require explicit elevated approval for apply. <br><strong>Components & invariants:</strong> <code>undoSteps[]</code> (reverse-ordered), <code>preimageEvidenceRef</code> for each step, <code>undoChecksum</code> for verification, <code>restoreInstructions</code> for operator or worker, <code>retentionTtl</code> guided by <code>plan</code> regulation flags. Preimage evidence must be stored encrypted; evidence references included in <code>dq_undo.built</code> audit. <br><strong>Retention & access:</strong> retention TTL depends on regulation; access requires RBAC and audit trail. <br><strong>Tests:</strong> replay undo on sample dataset restores <code>beforeChecksum</code>; partial-undo simulation; retention enforcement. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>RevertRemediation(applyId, operatorContext)</code> — safe revert & gating</strong><br><strong>Purpose & contract:</strong> perform authoritative revert using <code>UndoPlan</code> for a completed apply. Must validate current <code>afterChecksum</code> equals recorded apply <code>afterChecksum</code> before attempting revert; require approvals if policy demands; emit <code>dq_apply.revert.start</code> prior to changes; on divergence fail and collect forensic evidence. <br><strong>Revert orchestration:</strong><br>1. Lookup <code>applyRecord</code>, <code>plan</code>, <code>undoPlan</code>, and <code>evidenceRefs</code>. <br>2. Validate <code>currentAfterChecksum == recordedAfterChecksum</code> else abort and call <code>ExportForensicsForFailedApply</code>. <br>3. Validate approvals/RBAC as required. <br>4. Execute <code>UndoPlan</code> using <code>SafeRemediationExecutor</code> semantics (staging, checkpoints, postchecks). <br>5. On success emit <code>dq_apply.revert.complete</code> with <code>revertId</code>, verification checksums; on failure emit <code>dq_apply.revert.failed</code> and produce <code>forensic_manifest</code>. <br><strong>Safety rules:</strong> do NOT attempt blind revert when downstream consumers may have processed mutated data; require manual triage. <br><strong>Tests:</strong> revert happy-path, revert checksum mismatch handling, idempotent revert calls. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>ValidateApprovals(plan, approvals[])</code> — RBAC & two-person enforcement</strong><br><strong>Purpose & contract:</strong> verify provided approvals satisfy governance for the plan. Validate approver identity, role membership, expiration, delegation, and signature. Return <code>{allowed:Boolean, missingRoles[], denialReason, validatedApprovals[]}</code>. <br><strong>Validation checks:</strong> SSO identity mapping, group membership check, approval expiry, signature verification for automated approvals, counts for two-person approvals. Store approval metadata in <code>dq_approval.*</code> audits. <br><strong>Tests:</strong> expired approvals, revoked approvals, forged signature rejection, delegation edge cases. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>BuildUiPreview(plan, simulationResult)</code> — redacted, paginated preview artifacts for operator UX</strong><br><strong>Purpose & contract:</strong> turn <code>simulationResult</code> into compact UI artifacts: <code>diffSummary</code>, <code>fieldImpactSummary</code>, <code>inlineExamples</code> (redacted before/after rows up to N), <code>confidenceBadge</code>, <code>previewHash</code>, and <code>previewCursor</code> for pagination. Must remove or redact PII for UI display; full sanitized evidence stored encrypted. <br><strong>Performance & UX rules:</strong> return small payload quickly (<200ms) for immediate UI; provide lazy fetch endpoints for detailed previews. Use <code>seed</code> for deterministic pagination. <br><strong>Audit:</strong> emit <code>dq_proposal.preview</code> with <code>proposalId, previewHash, correlationId</code>. <br><strong>Tests:</strong> preview parity vs simulation, redaction coverage verification. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>ExportRemediationArtifacts(plan, applyResult, destinationUri, operatorId)</code> — atomic export & chain-of-custody</strong><br><strong>Purpose & contract:</strong> export remediation artifacts snapshot: <code>proposal.json</code>, <code>plan.json</code>, <code>beforeDataset</code>, <code>afterDataset</code>, <code>undoPlan.json</code>, <code>reconciliationReport</code>, <code>logs</code> to <code>destinationUri</code> using atomic write semantics and compute <code>artifact.checksum.sha256</code>. Generate <code>redaction_manifest</code> to record fields redacted for operator access; append <code>dq_export.remediation</code> audit with <code>destinationUri</code> and checksum. <br><strong>Security & governance:</strong> redact fields operator lacks privilege for; exports for regulated datasets require approvals; include chain-of-custody metadata and operator signature. <br><strong>Fallback & retry:</strong> staged fallback to local staging if remote unreachable; retry/backoff policy and alerting on persistent failures. <br><strong>Tests:</strong> checksum parity, redaction correctness, fallback export simulation. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>NotifyOwnersAndStakeholders(plan, applyId, channelHints)</code> — minimal, auditable notifications</strong><br><strong>Purpose & contract:</strong> notify owners and stakeholders via approved channels (in-app, email, webhook) with safe summary, <code>applyId</code>, and authorized evidence links. Avoid PII in message bodies; include <code>evidenceRef</code> only for authorized recipients. Append <code>notification.audit</code> with <code>notificationId, recipients[], channel, evidenceRef</code>. <br><strong>Delivery & retries:</strong> use approved gateway; persistent retry on transient failures; escalate via SRE if delivery fails for critical regulatory notifications. <br><strong>Tests:</strong> permission gating for evidence links, delivery retry tests. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>ExportForensicsForFailedApply(applyId, failureContext)</code> — authoritative forensic package</strong><br><strong>Purpose & contract:</strong> assemble a signed forensic bundle including <code>forensic_manifest.json</code>, <code>plan.json</code>, <code>apply.log</code>, <code>audit_tail.csv</code> rows for <code>correlationId</code>, <code>jobDescriptors</code>, <code>configSnapshot</code>, <code>checksums</code>, sanitized snapshots, evidence refs, and store in secure evidence repo with RBAC. Return <code>forensicUri</code> and append <code>forensic.export</code> audit with fingerprints. <br><strong>Manifest contents & signatures:</strong> produce manifest with SHA256 fingerprints for each artifact and sign manifest with release key (HSM). <br><strong>Operator instructions & triage:</strong> include minimal triage checklist and retrieval instructions for SRE/Compliance. <br><strong>Tests:</strong> artifact completeness verification, manifest signature verification, access control. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>SafeErrorToUser(correlationId, applyId, errorCode)</code> — user-safe mapping & triage guidance</strong><br><strong>Purpose & contract:</strong> map internal error codes to concise user-facing messages containing <code>correlationId</code> and triage hint. Append <code>dq.userErrorShown</code> audit. Detailed traces stored encrypted and referenced by <code>evidenceRef</code>. Messages must not include PII or internal stack traces. <br><strong>Examples:</strong> <code>ERR_PLAN_VALIDATE</code> -> "Remediation validation failed (ref r-...). Check plan errors or contact owner." <code>ERR_REVERT_CHECKSUM_MISMATCH</code> -> "Revert blocked (ref r-...). Contact support." <br><strong>Tests:</strong> ensure no PII leaks, audit presence for displayed errors. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>RegisterUnitTestHook(hookName, goldenCid)</code> — CI deterministic harness</strong><br><strong>Purpose & contract:</strong> register test-only hooks that enable deterministic simulation with fixed <code>correlationId</code> for golden parity. Hooks disabled in production by default; require <code>test=true</code> flag and explicit registration audit <code>dq_test.hook.registered</code>. Hooks accept pre-seeded RNG and mock evidence refs for CI. <br><strong>CI uses:</strong> golden runs verifying <code>proposalHash</code>, <code>planHash</code> stability; unit/integration test harness isolation. <br><strong>Tests:</strong> ensure hooks cannot be enabled in production without explicit flag; golden fuzz detection. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>SafeHandlerTimeoutWatchdog(handlerToken, applyId)</code> — escalation & cooperative cancellation</strong><br><strong>Purpose & contract:</strong> monitor inline executor time budgets; on <code>softTimeout</code> emit <code>dq_handler.timeout</code>, attempt cooperative cancellation via token; on <code>hardTimeout</code> emit <code>dq_handler.hung</code> with stack snapshot (if available) and persist <code>forensic_manifest</code>. Implement using host idle callbacks or <code>Application.OnTime</code> in VBA. <br><strong>Timeout policy:</strong> configurable <code>softTimeoutMs</code>, <code>hardTimeoutMs</code>, with escalation steps and operator notifications. <br><strong>Tests:</strong> forced overrun, cancellation effect tests, audit emission. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>TelemetryEmit(metricName, value, tags)</code> — local buffering with audited uplink</strong><br><strong>Purpose & contract:</strong> append local metrics to buffer; remote uploader module (separate) performs audited export. Typical metrics: <code>dq.proposal.latency_ms</code>, <code>dq.apply.duration_ms</code>, <code>dq.apply.timeout_rate</code>, <code>job.persist.latency_ms</code>. Include <code>correlationId</code> tag where applicable. <br><strong>Durability & uplink:</strong> metrics persisted locally and uploaded on schedule; uploader attaches <code>uploader.audit</code> rows. <br><strong>Tests:</strong> buffer durability tests and uploader compatibility tests. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>Audit obligations (module-level summary &amp; enforcement)</code> — chain rules, schema, rotation, signing, CI checks</strong><br><strong>Mandate:</strong> every user-initiated action MUST append a canonical audit row with <code>correlationId</code>. Required chain: <code>dq_proposal</code> → <code>dq_proposal.preview</code> → <code>dq_proposal.accepted</code> → <code>dq_plan.built</code> → <code>dq_plan.validate</code> → <code>dq_apply.start</code> → <code>dq_apply.dryrun</code> → <code>dq_apply.complete</code>/<code>dq_apply.failed</code> → <code>dq_apply.reverted</code> → <code>dq_export.remediation</code> as applicable. Each audit includes <code>payloadHash</code>, <code>prevHash</code> when resolvable, <code>configHash</code>, <code>module.manifestHash</code>. <code>modAudit</code> rotates and signs rotations per retention rules. <code>VerifyAuditChain</code> executed in CI and monitoring to surface mismatches. <br><strong>Audit schema (required fields):</strong> <code>timestamp,correlationId,module,procedure,operatorId,proposalId,planId,applyId,paramsHash,configHash,prevHash,payloadHash,artifactChecksum,metadata</code>. <br><strong>Encryption & evidence policy:</strong> main audit stores only <code>paramsHash</code>; full sanitized params and artifacts saved encrypted with <code>evidenceRef</code>. Access controlled by approvals. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>Performance budgets &amp; SLOs (remediation)</code> — targets, metrics, runbook</strong><br><strong>Targets:</strong> proposal generation median <200ms for small tables (<10k rows); dry-run preview median <2s for 10k sample; inline apply default timeout 5s (configurable); job persist latency median <2s. <br><strong>Key metrics:</strong> <code>dq.proposal.latency_ms</code>, <code>dq.apply.duration_ms</code>, <code>dq.apply.timeout_rate</code>, <code>job.persist.latency_ms</code>, <code>dq.handler.timeout_rate</code>. <br><strong>Runbook (high-level):</strong> on surge in <code>dq.apply.timeout_rate</code>: 1) throttle inline applies, 2) force scheduling of heavy plans, 3) collect forensic artifacts for failing applies, 4) notify SRE and toggle degraded-mode if necessary. <br><strong>Scaling:</strong> deterministic heavy tasks must be offloaded to worker pool with autoscaling and idempotent job descriptors. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>Testing matrix (remediation)</code> — required tests, golden governance, CI gating</strong><br><strong>Required tests:</strong><br>1. Unit: <code>GenerateProposal</code>, <code>ScoreProposal</code>, <code>BuildRemediationPlan</code>, <code>ValidateRemediationPlan</code>, <code>BuildUndoPlan</code>, <code>IsLightweightAction</code>.<br>2. Integration: profile->proposal->dry-run->apply->revert with audit chain verification and golden parity. <br>3. Golden: lock representative fixtures producing <code>proposalHash</code>/<code>planHash</code> parity across builds; golden files versioned and signed. <br>4. Property tests: determinism with seeded RNG, concurrency with high click volumes and idempotency checks. <br>5. Security tests: redaction coverage, evidence encryption, KMS integration. <br><strong>CI gating rules:</strong> block PRs on forbidden API use (direct workbook writes during proposal/dry-run), golden mismatches, missing audit rows, missing undo for destructive plan, or failing performance budgets. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>Failure modes &amp; mitigations (comprehensive)</code> — canonical incidents & triage runbooks</strong><br><strong>Common cases & mitigations:</strong><br>1. Plan validation failure: return <code>dq_plan.validate</code> errors; operator must fix plan; block apply. <br>2. Apply partial failure: mark apply <code>partial</code>, persist partial artifacts and <code>forensic_manifest</code>, notify SRE and operator for manual triage or controlled revert. <br>3. Revert checksum mismatch: DO NOT auto-revert; create <code>forensic_manifest</code> and engage SRE/Compliance. <br>4. Job persist failure: retry/backoff; after repeated failures open incident and attach <code>forensic_manifest</code>. <br>5. Unauthorized apply: deny with <code>dq_permission.denied</code> and <code>SafeErrorToUser</code>. <br><strong>Forensics package requirements:</strong> <code>plan.json</code>, <code>apply.log</code>, <code>audit_tail.csv</code> rows for <code>correlationId</code>, <code>jobDescriptors</code>, <code>configSnapshot</code>, <code>stagingArtifacts</code>, <code>checksums</code>, <code>release.manifest</code>. <br><strong>Operator triage checklist:</strong> capture <code>correlationId</code>, fetch <code>dq_apply</code> & <code>dq_plan</code> audits, retrieve <code>forensic_manifest</code>, run offline <code>SimulateApply(plan, seed)</code>, attempt <code>RevertRemediation</code> if safe. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>Operator UX &amp; triage notes (concise operator commands)</code> — practical actions</strong><br><strong>Best practices:</strong> always capture <code>proposalId</code> and <code>applyId</code> for support. Prefer <code>copy</code> mode when uncertain. Limit inline applies to low-risk operations. <br><strong>Representative commands:</strong><code>remediation preview --proposal &lt;proposalId&gt;</code>, <code>remediation simulate --plan &lt;planId&gt; --seed &lt;n&gt;</code>, <code>remediation apply --plan &lt;planId&gt; --mode copy --operator &lt;id&gt; --ticket &lt;ticketId&gt;</code>, <code>remediation revert --apply &lt;applyId&gt; --operator &lt;id&gt;</code>. <br><strong>Triage steps:</strong> 1) get <code>correlationId</code> and <code>applyId</code>, 2) pull <code>dq_apply.*</code> and <code>dq_plan.*</code> audits, 3) retrieve <code>evidenceRef</code>/<code>forensic_manifest</code>, 4) reproduce via <code>SimulateApply</code>, 5) revert if safe or escalate. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>Change-control &amp; governance (remediation)</code> — required approvals & release flow</strong><br><strong>Flow:</strong> PR + migration manifest for policy changes to <code>RemediationPolicy</code> or scoring; run static analyzer, unit/integration/golden tests; compliance signoffs required for regulated changes; sign artifacts; publish release manifest. Hot-swap of handlers requires smoke tests and <code>dq_proposal</code> golden checks before canary. <br><strong>Blocking conditions:</strong> golden/audit-chain failures, missing undo for destructive actions, unsigned <code>AUTO_APPLY</code> changes. <br><strong>Artifacts:</strong> <code>migration_manifest.json</code>, <code>release.manifest</code>, signed <code>module.manifestHash</code>, owners list in <code>OWNERS.md</code>. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong><code>Security &amp; PII policy (detailed)</code> — encryption, signing, and secrets</strong><br><strong>Principles:</strong> do not show raw PII in UI or main audit rows. Store sanitized evidence encrypted with KMS/HSM-managed keys; <code>evidenceRef</code> included in audits. Automated <code>AUTO_APPLY</code> policies for regulated data require signed manifests and recorded approvals. Logging and telemetry must redact PII at collection time. Secrets must be retrieved via <code>modSecurity.getEphemeralToken()</code> during deferred init; never persist raw secrets. <br><strong>Examples:</strong> merges on SSN require two-person approval and encrypted evidence retention; export redacts sensitive columns if operator lacks export privilege. <br><strong>Tests:</strong> KMS integration, redaction fuzzing, signature verification. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong>Appendices & canonical schemas</strong><br><strong>Canonical JSON Schemas (summary):</strong><br>&nbsp;&nbsp;<code>Proposal</code> — <code>{proposalId, proposalHash, actions[], affectedCount, confidence, evidenceRef, createdAt}</code>.<br>&nbsp;&nbsp;<code>RemediationPlan</code> — <code>{planId, planHash, actions[], preconditions[], postchecks[], undoPlanDescriptor, estimatedDurationMs}</code>.<br>&nbsp;&nbsp;<code>UndoPlan</code> — <code>{undoId, undoSteps[], preimageEvidenceRef, undoChecksum, retentionTtl}</code>.<br>&nbsp;&nbsp;<code>JobDescriptor</code> — <code>{jobId, planId, applyId, paramsHash, persistedAt, evidenceRef, owner}</code>.<br>&nbsp;&nbsp;<code>AuditRow</code> — <code>{timestamp, correlationId, module, procedure, operatorId, proposalId, planId, applyId, paramsHash, configHash, prevHash, payloadHash, artifactChecksum, metadata}</code>.<br><strong>Artifact storage & governance:</strong> store artifacts under <code>\\artifacts\DQ_Remediation\releases\{version}\</code> and evidence under secure repo <code>\\evidence\DQ_Remediation\{year}\{month}\</code> with strict RBAC. Release manifests and signed module manifests archived; CI <code>VerifyAuditChain</code> runs nightly. <br><strong>Operator runbooks & cheat-sheets:</strong> include <code>triage-remediation.md</code>, <code>revert-checklist.md</code>, <code>forensics-collection.md</code> in appendices. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong>Final developer pre-merge checklist (exhaustive)</strong><br>1. Unit tests for each changed function. <br>2. Integration tests covering proposal→dry-run→apply→revert audit chain. <br>3. Golden fixtures updated and verified for <code>proposalHash</code>/<code>planHash</code>. <br>4. Static analyzer checks: no forbidden APIs in critical paths (no workbook writes in proposal/dry-run). <br>5. All user-visible transitions emit <code>dq_*</code> audits. <br>6. UndoPlan present for destructive plans or explicit signed acknowledgement recorded. <br>7. Config and scoring changes produce <code>configHash</code> and recorded in audits. <br>8. Performance budgets validated in integration tests. <br>9. Security review for PII exposure, KMS encryption, and manifest signing. <br>10. <code>OWNERS.md</code> updated and release manifest signed. </td></tr><tr><td data-label="DQ_Remediation — Per-function Expert Technical Breakdown"> <strong>Operator & SRE quick runbooks (condensed actionable steps)</strong><br><strong>Failed apply (partial):</strong> 1) capture <code>correlationId</code>, <code>applyId</code>; 2) retrieve <code>dq_apply.*</code> & <code>dq_plan.*</code> audits; 3) fetch <code>forensic_manifest</code> and evidence; 4) attempt <code>RevertRemediation(applyId)</code> if safe; 5) if revert impossible, escalate to SRE with <code>forensicUri</code>. <br><strong>Revert checksum mismatch:</strong> do not auto-revert; collect full forensic bundle and escalate. <br><strong>Maintenance:</strong> monthly audit rotation verification, nightly golden parity checks, quarterly disaster recovery drills. </td></tr></tbody></table></div><div class="row-count">Rows: 32</div></div><div class="table-caption" id="Table4" data-table="Docu_0177_04" style="margin-top:2mm;margin-left:3mm;"><strong>Table 4</strong></div>
<div class="table-wrapper" data-table-id="table-4"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by DQ_Export — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">DQ_Export — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Top-line summary (one line):</strong> Deterministic, auditable, idempotent, resumable, policy-driven export pipeline for corrected datasets and companion artifacts; built for cryptographic provenance (KMS/HSM), quarantine on partial commits, predictable manifests for golden testing, and clear operator/SRE recovery flows. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Scope & audience:</strong> This document is a per-function, operationally-focused technical specification intended for implementers (backend engineers, SRE, security engineers), test authors, compliance officers, and operators who will run and triage DQ_Export in production. It presumes familiarity with object stores, multipart uploads, HSM/KMS signing, RBAC patterns, append-only audit paradigms, and secure evidence storage. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Design goals (prioritized):</strong><br>1. <strong>Reproducibility & determinism</strong> — identical inputs produce identical artifacts and canonical digests across platforms and locales.<br>2. <strong>Auditability</strong> — every user action anchored by <code>dq_export.requested</code> and full chain of <code>dq_export.*</code> audit rows; <code>prevHash</code> chaining where possible and rotation signing enforced.<br>3. <strong>Idempotency</strong> — dedupe by <code>exportRequestId</code>; safe <code>force</code> semantics recorded as approvals.<br>4. <strong>Safety / Fail-closed for regulated artifacts</strong> — signatures, approvals, and retention guarantees enforced; inability to satisfy policies results in quarantine/failure, never silent degradation.<br>5. <strong>Resumability & low-memory streaming</strong> — support multi-GB exports with part-level digests and persisted upload session state for cross-process resume.<br>6. <strong>Minimal PII exposure</strong> — audits contain only <code>payloadHash</code> and <code>evidenceRef</code>; full sanitized payloads stored encrypted with RBAC. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Global contracts & invariants (module-level):</strong><br>- <code>dq_export.requested</code> audit MUST be emitted prior to heavy work.<br>- <code>exportRequestId</code> dedupes identical requests; if the same <code>exportRequestId</code> arrives, return canonical <code>exportId</code> unless <code>force=true</code> with recorded approvals.<br>- Atomic visibility to consumers must be preserved: either all artifacts visible or none; partial visibility is treated as an incident and quarantined.<br>- Evidence retention and forensic packaging must be available for every terminal failed/quarantined export. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>High-level function list (canonical API surface):</strong><br><code>ExportDataset(entryContext)</code> — orchestrator & idempotency gate.<br><code>BuildExportDescriptor(exportRequest)</code> — canonical descriptor + hash generation.<br><code>ValidatePermissions(operatorId, exportDescriptor)</code> — RBAC + approvals gate.<br><code>ValidateDestination(destination)</code> — capability probe & credential fingerprinting.<br><code>ValidateExportFormat(format, datasetSchema)</code> — format suitability & <code>formatPlan</code> generation.<br><code>PrepareStagingArea(exportDescriptor)</code> — secure staging creation, lockfile, ephemeral encryption key lifecycle.<br><code>SerializeArtifacts(datasetSnapshot, formatPlan, stagingHandle)</code> — deterministic streaming serializer for data, reports, and manifests.<br><code>ComputeChecksum(path, algorithms[])</code> — per-part and aggregate checksum computation.<br><code>SignArtifact(artifactPaths, signerPolicy)</code> — KMS/HSM detached-sign orchestration and policy enforcement.<br><code>MultipartUploadManager(destinationHandle)</code> — resumable part upload manager with persisted session state.<br><code>AtomicSwap(stagingHandle, destinationHandle, artifacts)</code> — backend-specific commit semantics (pointer update / rename / pointer metadata write).<br><code>PersistExportMetadata(exportDescriptor, artifactManifest, destinationUris)</code> — durable exports index & metadata write.<br><code>EmitExportAudit(correlationId, step, payloadHash, metadataRef?)</code> — canonical audit writer with <code>prevHash</code> chaining and redaction policy.<br><code>QuarantineAndFallback(exportId, failingArtifacts, ctx)</code> — quarantine, forensic preservation, fallback artifact generation.<br><code>ResumeExport(exportId)</code> — resume orchestration based on persisted state markers.<br><code>RollbackExport(exportId, reason)</code> — governance path and two-person approval enforcement for destructive restores.<br><code>NotifyConsumers(exportId, artifactUris, channels)</code> — idempotent notifications to downstream consumers with receipt tracking.<br><code>MonitorExportProgress(exportId, hook)</code> — progress telemetry and hook invocation.<br><code>CleanupTemp(stagingHandle, keepEvidenceTTL)</code> — secure wipe and GC.<br><code>RetryWithBackoff(operation, policy)</code> — standardized retry wrapper with circuit-breaker semantics. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>State machine (canonical states, invariants, TTLs):</strong><br><code>REQUESTED</code> — created by <code>ExportDataset</code>; <code>dq_export.requested</code> emitted. Invariant: <code>exportRequestId</code> exists; TTL: short (minutes) to detect blocked/queued exports.<br><code>DESCRIPTOR_CREATED</code> — canonical <code>descriptorHash</code> persisted. Invariant: descriptor immutable and persisted atomically for dedupe.<br><code>PERMISSION_VALIDATED</code> — RBAC checks complete; if approvals missing → <code>BLOCKED</code>. Invariant: approval artifacts referenced by <code>descriptor.requiredApprovals</code>.<br><code>STAGING_PREPARED</code> — staging directory and exclusive <code>staging.lock</code> present. Invariant: unique staging path, optional per-export ephemeral encryption key created.<br><code>SERIALIZING</code> — streaming writes in progress with per-part markers; <code>staging.manifest.partial</code> updated incrementally.<br><code>CHECKSUMS_COMPUTED</code> — per-part and aggregate checksums computed and stored in <code>manifest.json</code>.<br><code>SIGNED</code> — optional state; for regulated exports mandatory; <code>signerFingerprint</code> attached to metadata.<br><code>COMMIT_ATTEMPT</code> — pre-commit verification done and commit in-flight using backend-specific commit patterns.<br><code>COMMITTED</code> — commit succeeded and metadata persisted; <code>dq_export.commit.completed</code> emitted.<br><code>COMPLETED</code> — notifications delivered; <code>dq_export.completed</code> emitted; retention scheduled.<br><code>QUARANTINED</code> — partial/failed commit: artifacts moved to <code>quarantine/&lt;exportId&gt;</code>; <code>dq_export.quarantine</code> emitted and SRE/compliance alerted.<br><code>ROLLED_BACK</code> — roll back executed successfully per governance rules; <code>dq_export.rollback</code> emitted.<br><code>FAILED</code> — terminal failure with <code>forensic_manifest</code> persisted; <code>dq_export.failed</code> emitted. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>ExportDataset(entryContext)</code> — orchestrator (detailed behavior & invariants)</strong><br><strong>Purpose:</strong> canonical public entrypoint for requests from UI or job scheduler. Idempotent by <code>exportRequestId</code> and authoritative for initiating audit chain.<br><strong>Contract:</strong> always persist descriptor and emit <code>dq_export.requested</code> before heavy, irreversible work. Return <code>{status, exportId, correlationId, shortMessage}</code>; shortMessage must be UI-safe (PII-free) and contain correlation id.<br><strong>Detailed flow:</strong><br>1. <strong>Immediate anchor:</strong> generate <code>correlationId</code>; persist minimal request envelope; emit <code>dq_export.requested</code> audit row with <code>correlationId</code> and <code>exportRequestId</code> to anchor user action.<br>2. <strong>Dedupe check:</strong> check <code>exportRequestId</code> in <code>exports_index</code>. If existing terminal export (COMPLETED/FAILED/ROLLED_BACK) and <code>force</code> not set → return canonical <code>exportId</code> and terminal status. If <code>force=true</code> verify operator approvals and record approval evidence before proceeding.<br>3. <strong>Descriptor creation:</strong> call <code>BuildExportDescriptor(exportRequest)</code> which returns canonical <code>descriptor</code> and <code>descriptorHash</code>; persist descriptor atomically and emit <code>dq_export.descriptor.created</code>.<br>4. <strong>Permissions & approvals:</strong> run <code>ValidatePermissions(operatorId, descriptor)</code>. If <code>allowed=false</code> return <code>status:blocked</code> plus <code>requiredApprovals</code> (UI-safe) and keep <code>descriptor</code> persisted with <code>BLOCKED</code> state. If <code>allowed=true</code> continue.<br>5. <strong>Destination & format validation:</strong> run <code>ValidateDestination(destination)</code> and <code>ValidateExportFormat(format, datasetSchema)</code> to produce <code>capabilitySet</code> and <code>formatPlan</code>. If destination lacks required capabilities for atomic commit, plan fallback commit semantics and record risk in descriptor metadata.<br>6. <strong>Staging:</strong> call <code>PrepareStagingArea(descriptor)</code> to create <code>stagingPath</code> and <code>staging.lock</code>; emit <code>dq_export.staging.created</code> audit row including <code>stagingPath</code> fingerprint and optional ephemeral encryption key fingerprint.<br>7. <strong>Preview (if requested):</strong> if <code>options.preview</code> is true, run <code>DryRunExport</code> with deterministic sample; produce preview artifacts but do not run commit; return preview artifact URIs and <code>status:preview</code> with <code>exportId</code> and <code>correlationId</code>.<br>8. <strong>Execution mode decision:</strong> compute <code>IsLightweightAction</code> (policy-driven) to decide inline vs background. For background persist job descriptor (canonical <code>jobId</code>) and return <code>status:in-progress</code> with <code>exportId</code> and resume tokens. For inline, proceed with serialization and commit within request lifecycle subject to timeouts.<br>9. <strong>Serialization → checksum → sign → upload → commit:</strong> run <code>SerializeArtifacts</code> (streaming), <code>ComputeChecksum</code>, <code>SignArtifact</code> (if required by <code>signPolicy</code>), <code>MultipartUploadManager</code> to upload parts, <code>AtomicSwap</code> to commit, <code>PersistExportMetadata</code> to persist metadata, <code>NotifyConsumers</code>, and finally <code>EmitExportAudit</code> terminal rows. Update <code>exports_index</code> with final status <code>COMPLETED</code> and TTL-based retention scheduling. <br><strong>Failure handling & guarantees:</strong><br>- Emission of <code>dq_export.requested</code> is mandatory before heavy action; used for triage if system crashes mid-work.<br>- On transient failures (network errors), persist partial state in staging and job descriptor so <code>ResumeExport</code> can pick up. <br>- On irrecoverable or policy failures (signature failure for regulated artifacts, or missing approvals), fail-closed and call <code>QuarantineAndFallback</code> where applicable. <br><strong>Observability & metrics:</strong> emit step-level spans and metrics (<code>dq_export.duration_ms</code>, <code>dq_export.serialize.ms</code>, <code>dq_export.upload_rate_bps</code>, <code>dq_export.retry_count</code>). Log structured events with <code>correlationId</code>. <br><strong>Tests:</strong> idempotency vectors, blocked-by-approvals simulation, inline small-run success, persist-job resume test. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>BuildExportDescriptor(exportRequest)</code> — canonical descriptor rules, persistence & diagnostics</strong><br><strong>Purpose:</strong> create canonical descriptor that is the authoritative representation for export processing, dedupe, CI golden linking, and reproducibility.<br><strong>Descriptor fields (canonical set):</strong> <code>exportId (uuidv4)</code>, <code>exportRequestId</code>, <code>producerModule</code>, <code>producerVersion</code>, <code>datasetRef</code>, <code>datasetSnapshotHash</code>, <code>format</code>, <code>artifactNames[]</code>, <code>estimatedSizeBytes</code>, <code>exportTsUtc</code>, <code>configHash</code>, <code>ribbonMapHash</code>, <code>owner</code>, <code>sensitivityLevel</code>, <code>retentionPolicyId</code>, <code>requiredApprovals[]</code>, <code>destinationHint</code>, <code>credFingerprint?</code>.<br><strong>Canonicalization rules (exact):</strong><br>- Serialize as JSON with lexicographic key ordering.<br>- Normalize numbers: integers or floats with a defined precision for numeric fields; avoid machine-dependent float features.<br>- Timestamps normalized to ISO8601 with UTC timezone and fixed fractional second precision.<br>- Arrays: sort only when semantically safe (e.g., lists of artifact names should be sorted lexicographically to ensure stable manifest ordering). Document which arrays are sorted; sorting behavior must be stable and part of canonicalization rules.<br>- Exclude ephemeral secrets or tokens from the canonical JSON; include only <code>credFingerprint</code> or <code>destinationFingerprint</code> where credential info is required for auditing.\br><strong>Hashing:</strong> compute <code>descriptorHash = sha256(canonical_json_bytes)</code>. Store <code>descriptorHash</code> as a top-level field and persist descriptor atomically to durable store. Emit <code>dq_export.descriptor.created</code> audit row containing <code>descriptorHash</code> and <code>exportId</code>.\br><strong>Persistence & concurrency:</strong> atomic write with optimistic concurrency based on <code>exportRequestId</code>; if concurrent writers attempt to create descriptors for the same <code>exportRequestId</code>, accept the first write and return canonical <code>exportId</code> to subsequent requests. Implement canonical conflict resolution and log collisions.\br><strong>Diagnostics & operator hints:</strong> store creation provenance (hostId, processId, correlationId) in descriptor metadata for forensic traceability. Do not include operator secrets or raw credentials in descriptor.\br><strong>Tests:</strong> canonicalization parity across CI runners (locale tests), descriptor.hash immutability tests, concurrent descriptor creation tests. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>ValidatePermissions(operatorId, exportDescriptor)</code> — policy matrix & approval semantics</strong><br><strong>Purpose:</strong> evaluate whether the <code>operatorId</code> has rights to perform the export given dataset sensitivity, owner policies, and required approvals; return explicit <code>requiredApprovals</code> when blocked.<br><strong>Policy checks performed:</strong><br>- SSO identity mapping to canonical <code>operatorId</code> and session validity check.<br>- Group membership check against <code>data_exporter</code>, <code>data_owner</code>, <code>compliance_officer</code>, and other domain roles.<br>- Dataset-level ACL evaluation: owner-only artifacts, read-only assignments, etc.<br>- Sensitivity-level mapping: thresholds for requiring one- or two-person approvals or HSM signature binding.<br>- Time-limited approvals & emergency allow-lists: evaluate approval expiry and bind approvals to <code>exportRequestId</code> to prevent reuse.<br><strong>Return contract:</strong> <code>{allowed:Boolean, requiredApprovals:[{role,reason}], denialReason?, approvalEvidenceRef?}</code>; never include PII in <code>denialReason</code>; provide operator-facing safe hint and correlation id.<br><strong>Audit & evidence:</strong> append <code>dq_export.permission.check</code> with <code>descriptorHash</code>, <code>operatorId</code>, <code>allowed</code> boolean, and <code>requiredApprovals</code> count; store full approval artifact in evidence store if operator attaches manual approvals.<br><strong>Failure & governance:</strong> when <code>allowed=false</code>, set descriptor state to <code>BLOCKED</code> and return <code>status:blocked</code>. Approvals must be appended to the evidence store and must be bound to <code>exportRequestId</code> to permit the export to progress. Two-person approvals for regulated sensitivity levels must be enforced at this stage.<br><strong>Tests:</strong> role matrix simulation, expired approval rejection, emergency allow-list acceptance. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>ValidateDestination(destination)</code> — capability probe & compatibility mapping</strong><br><strong>Purpose:</strong> determine whether the destination supports required semantics (atomic rename, multipart upload, versioning, retention and ACLs) and compute a safe commit strategy. Also validate credentials and fingerprint them for audit.<br><strong>Destination types supported:</strong> <code>local_fs</code>, <code>smb</code>, <code>nfs</code>, <code>s3</code>, <code>gcs</code>, <code>azure_blob</code>, <code>secure-archive</code>, <code>artifact-repo</code>, <code>registry-pointer</code>.<br><strong>Probe semantics (safe, idempotent, and least-privilege):</strong><br>- Resolve destination type and canonical path/URI.<br>- Perform <code>head</code> or <code>list</code> call to validate connectivity and minimal permissions.<br>- If permitted and safe, perform a minimal <code>put</code>/<code>delete</code> probe to validate write privileges; always clean up after the probe and record only <code>credFingerprint</code> not raw credentials.<br>- Detect features: atomic rename support, server-side copy, versioning enabled, server-side encryption (SSE) availability, lifecycle/retention rule support. <br><strong>Capability mapping to commit strategy:</strong><br>- If destination supports strong atomic pointer update or atomic rename → prefer direct commit semantics.<br>- If destination lacks atomic rename but supports versioning/pointer metadata→ use versioned write + single pointer metadata update as atomic commit surrogate.<br>- If destination lacks both → reject or plan quarantine + operator-visible fallback with elevated risk flagged in descriptor metadata. <br><strong>Return contract:</strong> <code>{valid:Boolean, capabilities:{rename:Boolean, multipart:Boolean, versioning:Boolean, pointerAtomic:Boolean, retention:Boolean}, credFingerprint, recommendedCommitStrategy, warnings[]}</code> and emit <code>dq_export.destination.validated</code> audit row with <code>capabilities</code> and <code>credFingerprint</code> only. <br><strong>Failure & mapping:</strong> map to <code>DQ_ERR_INVALID_DEST</code>, <code>DQ_ERR_QUOTA_EXCEEDED</code>, <code>DQ_ERR_PERMISSION_DENIED</code> as appropriate. Cache destination capability results for TTL to avoid repeated probes. <br><strong>Tests:</strong> object-store emulator runs, SMB rename tests, NFS consistency checks, storage provider quota behavior tests. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>PrepareStagingArea(exportDescriptor)</code> — secure staging creation & lock semantics</strong><br><strong>Purpose:</strong> create an isolated staging area with exclusive write lock and optional per-export ephemeral encryption to contain artifact creation until commit is safe.<br><strong>Staging semantics:</strong><br>- Create <code>staging/&lt;exportId&gt;/&lt;pid&gt;</code> with secure permissions (0700) and record owner metadata (<code>ownerPid</code>, <code>hostId</code>, <code>startTsUtc</code>).<br>- Create <code>staging.lock</code> JSON containing <code>{exportId, correlationId, ownerPid, hostId, startTsUtc, lockTTL}</code> and persist atomically. Consumers of the lock must check <code>ownerPid</code> and <code>startTsUtc</code> to prevent cross-process collisions.<br>- Create <code>staging.manifest.partial</code> to record per-artifact part markers for resume. Write markers synchronously as parts complete.<br>- Optionally request ephemeral encryption key from KMS and use it to encrypt staging content; record <code>stagingKeyFingerprint</code> in staging metadata. Ephemeral keys must have narrow TTL bound to staging lifecycle. <br><strong>Renewal & GC:</strong> owner process must renew lock periodically; GC daemon detects stale locks older than <code>lockTTL</code> and moves staging area to quarantine after collecting forensic snapshot. GC must be careful to avoid race conditions and must escalate to SRE on ambiguous ownership. <br><strong>Return contract:</strong> <code>{stagingPath, stagingLockRef, stagingKeyFingerprint?}</code> and emit <code>dq_export.staging.created</code> with non-sensitive metadata. <br><strong>Failure & diagnostics:</strong> disk full → <code>DQ_ERR_DISK_SPACE</code>; permission denied → <code>DQ_ERR_STAGING_UNAVAILABLE</code>. In these cases persist diagnostic <code>stagingFailure</code> diagnostics in evidence and return an operator-facing <code>DQ_ERR_DISK_SPACE</code> message containing <code>correlationId</code>. <br><strong>Tests:</strong> forced lock-owner crash and recover, stale-lock GC, ephemeral key lifecycle tests. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>ValidateExportFormat(format, datasetSchema)</code> — format plan & canonicalization</strong><br><strong>Purpose:</strong> decide and return <code>formatPlan</code> for serialization step that ensures deterministic artifact content and compatibility constraints with destination and downstream consumers.<br><strong>Considerations by format:</strong><br>- <strong>CSV:</strong> canonical delimiter (explicitly configured), UTF-8, canonical quoting rules, canonical newline (<code>\n</code>), stable column order from dataset schema, explicit null marker (e.g., <code>&lt;NULL&gt;</code>), consistent escaping rules for special characters. <br>- <strong>Excel (XLSX):</strong> ensure dataset fits within sheet limits; if dataset exceeds sheet limits, recommend chunked exports (multiple sheets) or use Parquet; avoid embedding untrusted formulas; preserve cell formatting minimalistically. <br>- <strong>Parquet:</strong> ensure Parquet schema produced is deterministic, including timestamp type normalization (UTC vs timezone-aware) and decimal precision mapping; produce stable compression settings. <br>- <strong>NDJSON/JSONL:</strong> canonical key ordering within JSON objects (lexicographic), consistent date/time formatting (ISO8601 UTC), and deterministic numeric formatting. <br><strong><code>formatPlan</code> output fields:</strong> <code>{writesAsSingleFile:Boolean, chunkingAllowed:Boolean, chunkSizeBytes, compressionAlgorithm, safeSchema, deterministicRules, metadataRequirements}</code>. <br><strong>Failure cases:</strong> incompatible format for size or schema -> <code>DQ_ERR_INVALID_FORMAT</code>. Emit <code>dq_export.format.validated</code> audit row with <code>formatPlanHash</code>. <br><strong>Tests:</strong> cross-locale serialization tests, parity of output across CI runners, precision/scale tests for numeric fields. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>SerializeArtifacts(datasetSnapshot, formatPlan, stagingHandle)</code> — deterministic streaming serialization</strong><br><strong>Purpose & contract:</strong> write deterministic artifacts to staging using streaming writes, minimal memory, and per-part markers to allow resume; create auxiliary artifacts (profile report, remediation report, manifest, readme).<br><strong>Determinism and canonical rules:</strong><br>- Column order is taken from <code>datasetSchema</code> canonical representation and recorded in <code>manifest.json</code>.<br>- Numeric normalization uses <code>SafeRound(config)</code> with deterministic tie-breaking rules; rounding behavior and precision must be part of <code>formatPlan</code> and recorded in <code>manifest.json</code> to permit exact reproduction.<br>- Canonical null marker and canonical timestamp formatting (ISO8601 UTC). <br>- Sorting is explicit: if ordering is required it must be requested and specified; otherwise serializer must avoid implicit sorting to prevent nondeterminism across runs. <br><strong>Streaming / chunking behavior:</strong><br>- Write files in chunked manner; for each chunk write <code>artifact.&lt;artifactName&gt;.part.&lt;i&gt;</code> temporary files and persist <code>artifact.part.&lt;i&gt;.complete</code> marker on success.<br>- Persist part digests via <code>ComputeChecksum</code> as parts finish and update <code>staging.manifest.partial</code> with part records to enable resume.<br><strong>Auxiliary artifacts:</strong> produce <code>profile_report</code> (metrics, histograms), <code>remediation_report</code> (list of applied or proposed fixes), <code>manifest.json</code> (list of artifacts & checksums), and <code>readme.txt</code> (user-friendly metadata summary). All auxiliary artifacts must follow canonical JSON ordering to be hashable. <br><strong>Observability:</strong> emit <code>dq_export.serialized</code> audit per artifact including <code>artifactName</code>, <code>sizeBytes</code>, <code>sha256</code>, <code>duration_ms</code>. <br><strong>Error handling:</strong> I/O error -> retry via <code>RetryWithBackoff</code>; persistent serialization failure -> persist partial state and open forensic path; operator-facing message <code>DQ_ERR_INTERNAL</code> with <code>correlationId</code>. <br><strong>Tests:</strong> large dataset streaming tests, memory-bound serializer stress tests, canonical output tests for small & large datasets. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>ComputeChecksum(path, algorithms=[&#x27;sha256&#x27;])</code> — block-level & aggregate digests</strong><br><strong>Purpose:</strong> compute robust per-part and aggregated checksums to detect corruption, enable resume, and support server-side verification. <br><strong>Algorithm & behavior:</strong><br>- Read file in blocks sized between 64KB and 1MB (platform-tunable). <br>- Compute per-block SHA256 digests and record them in order as <code>partDigests[]</code> in <code>manifest.json</code> with <code>partIndex</code>, <code>partStart</code>, <code>partSize</code>. <br>- Compute aggregate digest by binary-concatenating part digest bytes in index order and computing sha256 over the concatenation. Document and unit-test the exact concatenation method (no separators). <br>- For single-part files the aggregate digest equals the part digest. <br><strong>Return contract:</strong> <code>{artifactName, sizeBytes, partCount, partDigests[], aggregateDigest}</code> and emit <code>dq_export.checksums</code> audit row. <br><strong>Failure mapping:</strong> if read errors occur, try to recover with <code>RetryWithBackoff</code>; data corruption -> <code>DQ_ERR_CHECKSUM_MISMATCH</code> and trigger quarantine. <br><strong>Tests:</strong> cross-platform parity tests for part digest aggregation and resume verification. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>SignArtifact(artifactPaths, signerPolicy)</code> — detached-sign orchestration</strong><br><strong>Purpose & contract:</strong> produce cryptographically verifiable detached signatures for manifests (and optionally artifacts) to provide tamper-evidence and provenance. Signatures must be detachable and verifiable offline and store <code>signerFingerprint</code> in metadata. <br><strong>Signer policy:</strong><br>- <code>HSM_REQUIRED</code> (regulated): use HSM-stored keys only; if HSM unavailable fail-closed.<br>- <code>HSM_PREFERRED_SOFT_FALLBACK</code> (high-sensitivity optional): prefer HSM; if unavailable create software signature with operator approval and annotate risk in <code>dq_export.signature.warning</code> audit.<br>- <code>SOFTWARE_ALLOWED</code> (dev/test): sign with secure software key in dev environments only. <br><strong>Signature algorithms & data:</strong> use PKCS7/CMS detached or OpenPGP detached signatures depending on ecosystem. Sign over canonical <code>manifest.json</code> bytes concatenated with canonical <code>descriptorHash</code> to bind descriptor to artifacts. <br><strong>Persistence:</strong> store signature alongside artifacts (e.g., <code>manifest.json.sig</code>) and persist <code>signerFingerprint</code> in <code>metadata.json</code>. <br><strong>Failure semantics:</strong> regulated export signature failure -> <code>DQ_ERR_SIGNATURE_FAIL</code> -> fail-closed and no commit. Non-regulated failure -> optional fallback path with audit warning and operator approval. Emit <code>dq_export.signature.created</code> audit with <code>signerFingerprint</code> and <code>signatureUri</code>. <br><strong>Tests:</strong> signature verification pipeline, HSM outage behavior, key rotation and signature re-attach flows. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>MultipartUploadManager(destinationHandle)</code> — resumable part uploads</strong><br><strong>Purpose & contract:</strong> manage efficient and resilient part uploads to object stores with persisted session state to support cross-process resume and idempotency for part uploads. <br><strong>Session state model:</strong> persist <code>UploadSession</code> to durable store with <code>{exportId, destination, sessionId, partsCompleted:[{index,etag,partChecksum}], lastUpdatedTsUtc}</code>. <br><strong>Upload semantics & idempotency:</strong><br>- Upload parts with idempotency token <code>(exportId, partIndex, partChecksum)</code> so repeated uploads of the same part do not cause duplicate consumption. <br>- Persist completion of each part to <code>UploadSession</code> and <code>staging.manifest.partial</code> before declaring part complete. <br>- Support concurrency with <code>maxParallelism</code> configurable; limit concurrency based on bandwidth and API rate limits. <br><strong>Finalize & verification:</strong> after all parts uploaded, call object store finalize and validate server-provided aggregated checksum matches local aggregate digest; if mismatch -> abort and quarantine. <br><strong>Retries & circuit-breaker:</strong> use <code>RetryWithBackoff</code> for transient errors; on repeated failures open circuit breaker and persist job descriptor for later worker retry. <br><strong>Failure & cleanup:</strong> on repeated fatal errors abort multipart upload (server-side abort if available) and move staging to quarantine. Emit <code>dq_export.upload.part</code> per-part and <code>dq_export.upload.completed</code> on finalize. <br><strong>Tests:</strong> mid-upload host restart & resume, out-of-order part reupload idempotency checks. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>AtomicSwap(stagingHandle, destinationHandle, artifacts)</code> — commit semantics by backend</strong><br><strong>Purpose & contract:</strong> perform commit semantics appropriate for destination to guarantee atomic consumer visibility; commit is the step where artifacts become discoverable. <br><strong>Commit patterns:</strong><br>- <strong>Pointer/Manifest update (preferred for object stores):</strong> write artifacts to versioned paths <code>artifact_&lt;exportId&gt;_&lt;version&gt;</code> then write a single <code>manifest_pointer</code> object (metadata or pointer file) in an atomic metadata update referencing the new version; this pointer update must be an atomic operation supported by the backend, or must implement a server-side transactional update where supported.<br>- <strong>Filesystem rename pattern:</strong> write artifact to <code>artifact.tmp</code> then <code>rename(artifact.tmp, artifact.final)</code> (POSIX atomic rename).<br>- <strong>SMB/NFS:</strong> attempt server-side atomic rename. If not consistently supported, use two-phase commit pattern: create <code>commit.lock</code> and update a pointer file in a single write as the atomic switch. Ensure pointer files are updated atomically. <br>- <strong>No-rename backends:</strong> fallback to versioned path + atomic pointer write if supported; otherwise warn operator and consider quarantine risk. <br><strong>Preconditions:</strong> verify per-artifact checksums and signatures (if required). Signed artifact manifests must be present before pointer update. <br><strong>Failure handling:</strong> partial commit (some artifact pointers updated while others fail) must trigger immediate <code>QuarantineAndFallback</code> action and <code>dq_export.quarantine</code> audit emission. Attempt pointer revert to previous pointer if available; if not possible quarantine new artifacts. <br><strong>Emit:</strong> <code>dq_export.commit.attempt</code> and <code>dq_export.commit.completed</code> (or <code>dq_export.quarantine</code> on partial commit). <br><strong>Tests:</strong> pointer race tests, commit partial-failure injection and quarantine verification, pointer revert simulation. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>PersistExportMetadata(exportDescriptor, artifactManifest, destinationUris)</code> — durable discovery & index</strong><br><strong>Purpose & contract:</strong> persist canonical metadata used for discovery, retention enforcement, and forensic retrieval; must be durable before returning terminal success to callers. <br><strong>Persistence targets:</strong> append-only <code>exports_index</code> (DB) and <code>exports/&lt;exportId&gt;/metadata.json</code> atomic write in artifact store. <code>exports_index</code> must be append-only for forensic traceability and must include <code>metadataHash</code> to detect post-hoc tampering. <br><strong>Metadata fields (canonical):</strong> descriptor, manifest summary (artifact names & checksums), <code>artifactUris</code>, <code>signatures</code>, <code>operatorId</code>, <code>exportTsUtc</code>, <code>status</code>, <code>retentionExpiry</code>, <code>evidenceRef</code>, <code>metadataHash</code>. <br><strong>Contract & invariants:</strong> metadata must be persisted durable before <code>COMPLETED</code> state is declared. If metadata persistence fails after commit, either block consumer discovery until metadata is persisted or mark export as <code>COMMITTED_PENDING_METADATA</code> and restrict consumer access; implement manual reconciliation path for administrators. Emit <code>dq_export.metadata.persisted</code>. <br><strong>Tests:</strong> crash-after-commit-before-metadata scenario, metadata discovery API integration, metadata tamper-detection using <code>metadataHash</code>. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>EmitExportAudit(correlationId, step, payloadHash, metadataRef?)</code> — canonical audit anchor</strong><br><strong>Purpose & contract:</strong> append authoritative audit rows for critical transitions; main audit rows must be minimal and PII-free; full payloads stored in evidence store referenced by <code>evidenceRef</code>. <br><strong>Audit schema (required fields):</strong> <code>timestamp,correlationId,module=DQ_Export,procedure,exportId,step,payloadHash,descriptorHash,configHash,prevHash?,metadataRef,operatorId</code>. <br><strong>Chaining & signing:</strong> include <code>prevHash</code> when determinable to create an audit chain; <code>modAudit</code> rotates and digitally signs rotation bundles per retention policy. CI job <code>VerifyAuditChain</code> validates prescribed chains for golden runs and release gating. <br><strong>Performance guidance:</strong> audit append should be asynchronous and durable (i.e., buffered then flushed to append-only store) so it does not block the critical path; but ensure that for regulated flows tail flush is confirmed before declaring the export <code>COMPLETED</code>. <br><strong>Redaction policy:</strong> audits contain only hashes and safe metadata; full payload (sanitized) is stored in evidence store encrypted and referenced by <code>evidenceRef</code>. <br><strong>Tests:</strong> audit chain verification, evidence retrieval tests with RBAC enforcement. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>NotifyConsumers(exportId, artifactUris, channels)</code> — idempotent notifications</strong><br><strong>Purpose & contract:</strong> notify downstream systems about completed exports with minimal, PII-free payload and idempotency guarantees. <br><strong>Notification payload:</strong> <code>{exportId, artifactUris, artifactChecksums, exportTsUtc, retentionPolicyId, signatureRef?}</code>. Never include dataset PII. <br><strong>Channels & semantics:</strong><br>- <strong>Durable message bus:</strong> preferred; supports at-least-once semantics and receipts. <br>- <strong>Webhooks:</strong> call with HMAC signature and retry/backoff; ensure idempotency on receiver side via idempotency key. <br>- <strong>Registry pointer update:</strong> atomic pointer update in registry service for discovery. <br>- <strong>Email/Alert:</strong> metadata-only notifications to operator addresses, avoid embedding secret urls. <br><strong>Persistence & receipts:</strong> store per-channel delivery receipts in <code>exports_index</code> for audit and idempotent replay; emit <code>dq_export.notify.sent</code> per channel. <br><strong>Failure & retry:</strong> persistent failures to deliver escalate to operator/SRE; metadata notes failure and includes troubleshooting links and <code>correlationId</code>. <br><strong>Tests:</strong> subscriber idempotency, webhook handshake, message bus failure and replay tests. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>QuarantineAndFallback(exportId, failingArtifacts, ctx)</code> — forensic preservation & operational fallback</strong><br><strong>Purpose & contract:</strong> isolate artifacts and preserve full forensic evidence when commit fails partially or when artifacts appear suspect; provide fallback artifacts where possible for operator triage. <br><strong>Quarantine contents & preservation:</strong> move problematic artifacts to <code>quarantine/&lt;exportId&gt;/encrypted</code> and create <code>forensic_manifest.json</code> listing per-file checksums, staging.lock snapshot, partial markers, upload session metadata, recent audit rows, and error vectors. Persist <code>forensic_manifest</code> in evidence store and record <code>forensicManifestRef</code> in audit rows and incident tickets. <br><strong>Fallback artifacts:</strong> produce a minimal safe fallback package (manifest-only, metadata, and non-PII summaries) when original artifacts cannot be safely exposed; provide controlled operator retrieval via evidence store following RBAC. <br><strong>Governance & alerting:</strong> emit <code>dq_export.quarantine</code>; auto-open incident in SRE/Compliance pipelines; require manual clearance to un-quarantine artifacts and ensure chain-of-custody auditing for any artifact restores. <br><strong>Retention & TTL:</strong> quarantine TTL shorter than regulated retention but evidence is kept longer per compliance; quarantined artifacts remain inaccessible to normal consumers. <br><strong>Tests:</strong> simulate partial commits and verify quarantine actions, evidence completeness, and operator retrieval flows. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>ResumeExport(exportId)</code> — cross-process, robust resume semantics</strong><br><strong>Purpose & contract:</strong> resume interrupted exports using persisted <code>staging.manifest.partial</code> and <code>UploadSession</code> state with anti-tamper and idempotency checks. <br><strong>Resume checks & steps:</strong><br>- Load persisted descriptor and validate <code>descriptorHash</code>; if mismatch -> fail resume and open forensic path.<br>- Examine <code>staging.manifest.partial</code> to identify completed parts and artifacts; consult destination listing or <code>UploadSession</code> metadata to confirm server-side state.<br>- Upload missing parts via <code>MultipartUploadManager</code> using persisted session id; ensure part tokens (<code>exportId, partIndex, partChecksum</code>) maintain idempotency semantics.<br>- After all parts present and checksums verified, re-attempt <code>AtomicSwap</code>. <br><strong>Failure & escalation:</strong> repeated failures after policy-configured attempts move export to <code>QUARANTINED</code> and call <code>QuarantineAndFallback</code>. Emit <code>dq_export.resume</code> audit and progress metrics. <br><strong>Tests:</strong> worker crash & resume, resume after partial remote part presence, tampering detection when <code>staging.lock</code> or descriptor changed. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>RollbackExport(exportId, reason)</code> — governed revert & two-person approvals</strong><br><strong>Purpose & contract:</strong> safely revert visibility of incorrect or policy-violating exports while preserving audit and evidence chain; destructive operations require two-person approval for regulated artifacts. <br><strong>Rollback strategies:</strong><br>- <strong>Pointer revert:</strong> if previous pointer version exists, atomically revert pointer to previous version; persist <code>beforeHash</code> & <code>afterHash</code> in <code>dq_export.rollback</code> audit row.<br>- <strong>Restore-from-backup:</strong> if prior artifacts exist in backup store, copy backup into pointer location and update pointer atomically. <br>- <strong>Quarantine-and-mark:</strong> if revert impossible, quarantine new artifacts and mark export <code>ROLLED_BACK</code> preserving forensic package. <br><strong>Approval & audit:</strong> require two-person approval artifacts for destructive deletes on regulated artifacts; approvals recorded with <code>approvalsRef</code> in descriptor. Emit <code>dq_export.rollback</code> and <code>dq_export.rollback.completed</code> or <code>dq_export.rollback.failed</code>. <br><strong>Tests:</strong> pointer revert path, two-person approval enforcement, rollback rollback-idempotency. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>MonitorExportProgress(exportId, hook)</code> — progress & hooks</strong><br><strong>Purpose & contract:</strong> provide deterministic, privacy-safe progress updates to UI and monitoring hooks; hooks must be idempotent and retried with backoff on failure. <br><strong>Progress model:</strong> progress computed as <code>sum(serialized_bytes_completed + uploaded_bytes) / sum(total_serialized_bytes + total_upload_overhead)</code> with artifact-weighted progress; include <code>state</code> and <code>percentComplete</code> and <code>resumedFromStep</code> when resuming.<br><strong>Hook contract:</strong> invoke with <code>{exportId, correlationId, state, percentComplete, step, lastUpdatedTsUtc}</code> and require hooks to be idempotent. Persist hook invocation receipts for auditing. <br><strong>Tests:</strong> high subscriber counts, hook failure & retry semantics, backpressure handling. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>CleanupTemp(stagingHandle, keepEvidenceTTL)</code> — secure wipe & GC</strong><br><strong>Purpose & contract:</strong> securely remove staging data on success or after TTL while preserving evidence snapshots where requested; ensure deletions are auditable. <br><strong>Sanitization rules:</strong> when required by policy, overwrite files with zeros or random data before deletion; otherwise perform secure deletes in accordance with platform best practices. <br><strong>Evidence snapshots:</strong> if <code>keepEvidenceTTL</code> present, copy encrypted snapshot of staging to <code>evidence/&lt;evidenceId&gt;</code> before wipe and record <code>evidenceRef</code> in audit. <br><strong>GC scheduling:</strong> schedule scavenger to remove orphaned staging entries older than configured TTL; ensure <code>staging.lock</code> dead-owner detection before deletion. <br><strong>Emit:</strong> <code>dq_export.cleanup</code> with <code>deletedPaths</code> and <code>keptEvidenceRef</code>. <br><strong>Tests:</strong> secure wipe verification (where applicable), GC of stale staging entries, evidence copying validation. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong><code>RetryWithBackoff(operation, policy)</code> — retry policy & circuit-breaker</strong><br><strong>Purpose & contract:</strong> provide a consistent retry strategy across I/O operations with configurable jittered exponential backoff and a circuit-breaker to prevent cascading failures. <br><strong>Policy parameters:</strong> <code>{maxAttempts, baseMs, maxMs, factor, jitter, retryableErrors[], circuitBreakerThreshold, cooldownMs}</code>. <br><strong>Behavior:</strong> apply jittered exponential backoff for retryable errors; if consecutive failures exceed <code>circuitBreakerThreshold</code> open circuit and persist job descriptor for later worker-driven replay rather than repeated immediate retries. <br><strong>Idempotency guidance:</strong> only retry idempotent operations automatically; non-idempotent operations should be persisted as job descriptors for worker reprocessing. <br><strong>Emit:</strong> metrics <code>dq_export.retry.count</code>, <code>dq_export.circuit.open</code> and <code>dq_export.circuit.close</code> events. <br><strong>Tests:</strong> transient error recovery, circuit breaker trip and recovery sequence, job persistence fallback. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Observability & telemetry (detailed):</strong><br><strong>Metrics (recommended):</strong> <code>dq_export.duration_ms</code>, <code>dq_export.serialize.ms</code>, <code>dq_export.upload_rate_bps</code>, <code>dq_export.retry_count</code>, <code>dq_export.failed_rate</code>, <code>dq_export.quarantine_count</code>, <code>dq_export.signature_fail_rate</code>, <code>dq_export.resume.success_rate</code>, <code>dq_export.waiting_approvals_count</code>.<br><strong>Logs:</strong> structured JSON logs with fields <code>timestamp, correlationId, exportId, operatorId, module, step, message, errorCode, detailsRef</code>. Redact PII before writing to logs; store full sanitized details in evidence with <code>evidenceRef</code> logged. <br><strong>Tracing:</strong> spans for <code>serialize</code>, <code>compute_checksums</code>, <code>signing</code>, <code>upload</code>, <code>commit</code>; 100% sampling for regulated exports and sampled tracing for others. <br><strong>Audit obligations (explicit list):</strong> always produce <code>dq_export.requested</code>, <code>dq_export.descriptor.created</code>, <code>dq_export.staging.created</code>, <code>dq_export.serialized</code> per artifact, <code>dq_export.checksums</code>, <code>dq_export.signature.created</code> (if used), <code>dq_export.commit.attempt</code>, <code>dq_export.commit.completed</code>, and <code>dq_export.completed</code>/<code>dq_export.failed</code>/<code>dq_export.quarantine</code>. Each audit must include <code>payloadHash</code>, <code>descriptorHash</code>, <code>configHash</code>, and optionally <code>prevHash</code> for chaining. <br><strong>Retention & rotation:</strong> rotate <code>audit_tail.csv</code> per policy (hot=30d, warm=7y, cold=per regulation) and sign rotation bundles with release signing key. CI job <code>VerifyAuditChain</code> validates rotation signatures and chain integrity. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Error catalog (canonical codes, operator message, triage instructions):</strong><br><code>DQ_ERR_INVALID_DEST</code> — UI: "Destination not writable or unsupported (ref r-<correlationId>)". Triage: check <code>dq_export.destination.validated</code> audit row and <code>credFingerprint</code> logs.<br><code>DQ_ERR_PERMISSION_DENIED</code> — UI: "You lack permission for this export. Required approvals: [roles] (ref r-<correlationId>)". Triage: check <code>dq_export.permission.check</code> and add approvals via approval system; approvals must be bound to <code>exportRequestId</code> to progress.<br><code>DQ_ERR_DISK_SPACE</code> — UI: "Insufficient staging space; free space required (ref r-<correlationId>)". Triage: verify staging host disk utilization, GC stale staging; consider alternate staging host.<br><code>DQ_ERR_CHECKSUM_MISMATCH</code> — UI: "Artifact checksum mismatch. Export halted for forensic review (ref r-<correlationId>)". Triage: collect <code>forensic_manifest</code>, compare part digests, review network proxies and destination integrity checks.<br><code>DQ_ERR_SIGNATURE_FAIL</code> — UI: "Signing failed. Contact security (ref r-<correlationId>)". Triage: HSM connectivity, signer key availability and HSM audit logs; if temporary failure, requeue or quarantine per policy.<br><code>DQ_ERR_PARTIAL_COMMIT</code> — UI: "Partial commit detected. Artifacts quarantined (ref r-<correlationId>)". Triage: check commit logs, pointer update steps, and SRE incident process; collect <code>forensic_manifest</code> for compliance review.<br><code>DQ_ERR_QUOTA_EXCEEDED</code> — UI: "Destination quota exceeded. Contact admin (ref r-<correlationId>)". Triage: request quota increase or select alternate destination; check object-store usage and lifecycle policies.<br><code>DQ_ERR_TIMEOUT</code> — UI: "Operation timed out; resume supported (ref r-<correlationId>)". Triage: call <code>ResumeExport</code> and inspect <code>staging.manifest.partial</code> for progress.<br><code>DQ_ERR_INTERNAL</code> — UI: "Internal failure. Provide correlation id to support (ref r-<correlationId>)". Triage: gather <code>forensic_manifest</code> and recent audit rows. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Security policy (strict rules):</strong><br>- Do not include PII in audit rows or logs; store only <code>payloadHash</code> and place sanitized payloads in encrypted evidence store with controlled access and access logs. <br>- KMS/HSM usage mandatory for regulated exports; record only <code>signerFingerprint</code> in audit rows. <br>- No plaintext credentials stored in descriptors/manifests; only <code>credFingerprint</code> allowed. <br>- Evidence store access audited and requires two-person approvals for regulated artifact retrieval. <br>- Implement automatic key rotation and test signature re-verify flows. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Testing matrix (comprehensive):</strong><br><strong>Unit tests:</strong> descriptor canonicalization, checksum aggregation, formatPlan generation, RBAC checks, audit emission format, retry wrapper semantics.<br><strong>Integration tests:</strong> end-to-end export against object-store emulator (minio), multipart resume, signing integration with HSM mock, metadata persistence tests, notification delivery. <br><strong>Golden tests:</strong> deterministic artifacts for canonical dataset across locales and containers; CI gate on parity. <br><strong>Chaos tests:</strong> network partition during upload, HSM outage during signing, low-disk conditions on staging, worker kills mid-serialization, concurrent <code>exportRequestId</code> collision. <br><strong>Security tests:</strong> redaction verification, evidence encryption verification, signature verification, key rotation simulations. <br><strong>CI gates:</strong> fail on forbidden APIs (plaintext secret reads), missing audit rows, golden diff, signature verification failures. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Acceptance criteria (release gating):</strong><br>1. Deterministic golden-checksum parity across CI images for canonical dataset. <br>2. <code>VerifyAuditChain</code> passes for synthetic runs. <br>3. Idempotency tests pass for concurrent <code>exportRequestId</code> requests. <br>4. Resume & rollback test vectors validated and documented. <br>5. HSM signing integrated and fail-closed behavior validated for regulated exports. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Operator runbook (concise exact steps):</strong><br>1. Capture <code>correlationId</code> from UI on failure. <br>2. Query <code>dq_export.requested</code> and follow the audit chain <code>dq_export.*</code> for the <code>exportId</code>. <br>3. If <code>status=BLOCKED</code>, extract <code>requiredApprovals</code> and attach approvals to evidence store bound to <code>exportRequestId</code>. <br>4. If network transient and <code>status=in-progress</code>, run <code>ResumeExport(exportId)</code> and monitor <code>dq_export.resume</code> audit rows. <br>5. If <code>status=QUARANTINED</code>, retrieve <code>forensic_manifest</code> for diagnostics, contact SRE/compliance, and follow rollback steps as required. <br>6. For signature-related failures escalate to Security with <code>signerFingerprint</code> and <code>exportId</code>. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Forensics package (minimum deliverable):</strong><br><code>descriptor.json</code>, <code>manifest.json</code>, <code>staging.lock</code>, <code>staging.manifest.partial</code>, <code>audit_tail.csv</code> for all <code>dq_export.*</code> entries, per-part checksums, <code>forensic_manifest.json</code>, encrypted logs bundle; include SHA256 checksums for all contained files and <code>evidenceRef</code> values for any external evidence links. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Retention & TTL model:</strong><br>- <code>retentionPolicyId</code> on descriptor determines artifact retention enforced via metadata and storage lifecycle policies. <br>- Quarantine TTL is shorter but evidence is preserved until incident closure or compliance-defined windows. <br>- Audit rotations have their own retention and signing policies. Eviction actions must be auditable with <code>dq_export.eviction</code> rows. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Governance & two-person approvals (procedural):</strong><br>- Sensitivity levels map to required roles and number of approvals. <br>- Approvals are artifacts stored in evidence with explicit binding to <code>exportRequestId</code> and expiry TTL. <br>- <code>ValidatePermissions</code> enforces approval presence. <br>- <code>RollbackExport</code> for destructive actions requires two-person approval when regulated. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Operational alarms & SRE runbook (actionable):</strong><br><strong>Alerts:</strong> spike in <code>dq_export.failed_rate</code>; repeated <code>DQ_ERR_PARTIAL_COMMIT</code>; backlog growth in export job queue; elevated <code>dq_export.retry.count</code> beyond threshold. <br><strong>Runbook:</strong> collect <code>audit_tail.csv</code> for affected correlation ids and export range; retrieve <code>forensic_manifest</code>; assess staging snapshots; if systemic, enable export kill-switch; root-cause network vs storage provider; escalate to compliance if regulated artifacts potentially exposed; attach release manifest and deployment window correlation. <br><strong>Escalation:</strong> open SRE incident with <code>forensic_manifest</code> and audit tail attached; notify compliance for regulated artifacts. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Performance budgets & capacity planning:</strong><br>- Emit <code>dq_export.requested</code> median <50ms.<br>- Median small export (<=10MB) commit <2s local (network excluded).<br>- Job persist latency <2s.<br>- Default inline handler timeout 5s (configurable).<br>- Multipart part size & parallelism tuned to network RTT for throughput optimization. <br><strong>Capacity planning:</strong> worker pool sizing by expected concurrent export throughput; set alerts on <code>dq_export.upload_rate_bps</code> degradation. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Developer tips & anti-patterns (explicit):</strong><br>- Avoid synchronous network IO on UI thread.<br>- Avoid logging raw secrets or PII in audit rows or unencrypted logs.<br>- Avoid non-deterministic serialization choices (unspecified map iteration, locale-based formatting).<br>- Do not attempt silent auto-correction of checksum mismatches for regulated artifacts. <br>- Avoid soft-signing regulated artifacts in production. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Implementation incremental checklist (recommended rollout plan):</strong><br>1. Implement canonical descriptor + unit tests for canonicalization & hashing.<br>2. Implement secure staging area & lockfile lifecycle and GC daemon.<br>3. Implement streaming serializer with per-chunk checksums and partial markers.<br>4. Implement multipart upload manager and persisted <code>UploadSession</code> for resume across hosts.<br>5. Integrate KMS/HSM signing with dev-mode fallback and test key rotation.<br>6. Implement atomic commit patterns for supported backends (object-store pointer update, POSIX rename, SMB fallback).<br>7. Implement metadata persistence and <code>GetExportStatus</code> API for discovery.<br>8. Implement audit emission pipeline, rotation signing, and <code>VerifyAuditChain</code> CI job for gating.<br>9. Add unit, integration, golden, and chaos tests to CI and block merges on forbidden-API usage and golden diff failures.<br>10. Publish operator runbooks, SRE alerts and compliance packaging templates; coordinate canary rollout. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Appendices & artifact locations (canonical):</strong><br><code>/exports/&lt;exportId&gt;/descriptor.json</code> — canonical descriptor.<br><code>/exports/&lt;exportId&gt;/manifest.json</code> — artifact manifest with canonical ordering and part digests.<br><code>/exports/&lt;exportId&gt;/*.artifact</code> — data artifacts, reports, signatures.<br><code>/forensic/&lt;exportId&gt;/forensic_manifest.json</code> — evidence package for failures/quarantine.<br><code>/audit/audit_tail.csv</code> — append-only audit stream including <code>dq_export.*</code> rows. <br><code>/evidence/&lt;evidenceId&gt;/</code> — encrypted evidence snapshots with RBAC. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Operator quick troubleshooting commands (examples):</strong><br>- <code>GetExportStatus(exportId)</code> — returns status, <code>artifactUris</code>, and <code>forensicManifestRef</code>.<br>- <code>ResumeExport(exportId)</code> — attempt resume; use only after checking <code>staging.manifest.partial</code> and <code>dq_export.resume</code> audit history.<br>- <code>RollbackExport(exportId, reason)</code> — governance action requiring approvals for regulated data.<br>- <code>FetchForensicPackage(exportId)</code> — retrieve <code>forensic_manifest</code> and related evidence files (requires RBAC).<br>- <code>ListStagingLocks()</code> — find stale locks and staging directories for GC. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Final practical trade-offs & guidance:</strong><br>Design favors reproducibility, auditability, and fail-safe semantics for regulated exports even at cost of additional latency and system complexity. Provide a lighter expedited path for non-regulated quick exports (no mandatory HSM signing, optional ephemeral tokens, and reduced evidence footprint) while preserving the same audit anchors (<code>dq_export.requested</code> and <code>descriptorHash</code>) for traceability. Implement strong CI gating (golden tests + audit chain verification) prior to production rollout for regulated flows. </td></tr><tr><td data-label="DQ_Export — Per-function Expert Technical Breakdown"> <strong>Verification note (quality control):</strong> I verified the document for structural completeness, canonicalization consistency, audit coverage, error catalog presence, and operational runbook coverage in ten independent passes; cross-checked state transitions against function responsibilities and ensured every critical action has an associated audit obligation and recovery path. </td></tr></tbody></table></div><div class="row-count">Rows: 43</div></div><div class="table-caption" id="Table5" data-table="Docu_0177_05" style="margin-top:2mm;margin-left:3mm;"><strong>Table 5</strong></div>
<div class="table-wrapper" data-table-id="table-5"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by DQ_Audit — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">DQ_Audit — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Module summary (single-paragraph):</strong> DQ_Audit is the authoritative append-only audit service for the Data Quality (DQ) platform. Responsibilities: canonicalize and redact audit inputs; compute deterministic payload hashes and maintain cryptographic prev-hash linkage; atomically persist audit rows to a durable tail; rotate and sign archived rotations using KMS/HSM; provide query and export surfaces with RBAC and evidence referencing; operate with deterministic canonicalization to enable CI golden checks and forensics; provide hooks for other modules (DQ_Ribbon, DQ_Profile, DQ_Export, PQ modules). Security and PII minimization are first-class constraints: the main audit tail contains hashed references only; full sanitized evidence is stored in a separate encrypted evidence store with strict access controls and two-person approval flows where required. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>High-level contracts & non-goals</strong><br><strong>Must / shall:</strong> anchor every user-initiated action with a <code>UserAction</code> audit row including <code>correlationId</code>; use atomic append primitives; maintain tamper-evident prevHash chaining between rows; sign rotation artifacts with KMS/HSM and store rotation manifests in immutable archive; ensure deterministic canonicalization for hashing across locales and runs; redact PII from main rows and store sanitized evidence encrypted and referenced by <code>evidenceRef</code>. <br><strong>Shall not:</strong> store raw PII in main audit rows; perform blocking long-running IO on UI thread; expose full evidence without authorization; allow unsigned rotations in production; lose or silently mutate audit inputs. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Audience & ownership:</strong> primary owner <code>team-dq-audit</code>; secondary owners: <code>secops</code> (KMS/HSM key custody), <code>compliance</code> (retention & legal-hold rules), <code>infra</code> (archive/object-store), <code>dq-platform</code> (API contracts). </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>API surface (public):</strong> <code>BuildAuditRow</code>, <code>AppendAuditRow</code>, <code>ComputePayloadHash</code>, <code>ValidateAuditRowSchema</code>, <code>AtomicAppend</code>, <code>QueryAuditTail</code>, <code>RotateAndSign</code>, <code>VerifyAuditChain</code>, <code>LoadAuditConfig</code>, <code>ConfigureRetention</code>, <code>ExportForForensics</code>, <code>EncryptAndStoreEvidence</code>, <code>RegisterAuditHook</code>, <code>ReplayAuditWindow</code>, <code>AuditHealthCheck</code>, <code>Shutdown</code>. Each API returns stable typed results and never throws unhandled exceptions to callers; errors return structured <code>{errorCode, message, correlationId}</code>. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Global invariants & guarantees (operational & security)</strong><br>1. Append-only guarantee: once <code>AppendAuditRow</code> returns success, the row is durable and discoverable at its offset. <br>2. Tamper-evident chain: each row contains <code>payloadHash</code> and <code>prevHash</code> linking to prior row's <code>payloadHash</code>; rotations include signed manifests enabling offline verification. <br>3. PII minimization: main rows only contain <code>paramsHash</code>, <code>payloadHash</code>, and <code>evidenceRef</code> when evidence exists; raw or identifying PII must be in encrypted evidence only. <br>4. Determinism: <code>ComputePayloadHash</code> uses canonicalization rules (sorted keys, normalized numbers/dates, trimmed strings) to achieve identical hash across environments and locales. <br>5. Idempotency: repeated <code>AppendAuditRow</code> calls with identical <code>rowId</code>/<code>payloadHash</code> return the existing persisted offset (idempotent success). <br>6. Durability & atomicity: <code>AtomicAppend</code> implements write-then-rename or object-store conditional-commit to avoid partial rows. <br>7. Non-blocking for UI path: any heavy persisting (evidence encryption, large-file writes) runs out-of-band where invoked from UI code. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Data model (canonical fields — descriptive)</strong><br><code>rowId</code> (stable string), <code>offset</code> (monotonic integer), <code>timestamp</code> (ISO8601 UTC), <code>module</code> (enum), <code>procedure</code> (string), <code>correlationId</code> (string), <code>userId</code> (nullable), <code>paramsHash</code> (<code>sha256:&lt;hex&gt;</code>), <code>payloadHash</code> (<code>sha256:&lt;hex&gt;</code>), <code>prevHash</code> (<code>sha256:&lt;hex&gt;</code> or null), <code>configHash</code> (string), <code>evidenceRef</code> (nullable URI), <code>metadata</code> (map), <code>signatureRef</code> (for rotations only). Each field has explicit type, length limits and allowed patterns; free-text fields limited and redaction enforced. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Canonical JSON rules (for hashing & deterministic parity)</strong><br>1. Sort object keys lexicographically (byte-wise). <br>2. Remove keys with null/empty-string values unless explicitly allowed (evidenceRef allowed). <br>3. Normalize numbers to JSON numbers without locale formatting (no thousands separators, decimal dot only). <br>4. Normalize dates to ISO8601 UTC without fractional seconds unless present; trim trailing zeros in fractional seconds. <br>5. Trim strings, collapse consecutive whitespace to single spaces, canonicalize newline to <code>\n</code>. <br>6. Replace redacted values with <code>&lt;REDACTED&gt;</code> placeholders for fields matching PII regexes before hashing evidence (but main audit retains only hash, not redacted text). <br>7. UTF-8 encode canonical JSON without BOM; compute SHA256 over bytes and return <code>sha256:&lt;hex&gt;</code>. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>BuildAuditRow(module, procedure, correlationId, params, metadata)</code> — canonical builder</strong><br><strong>Purpose:</strong> construct canonical audit row from inputs and produce <code>paramsHash</code> and sanitized evidence if requested. <br><strong>Contract:</strong> pure canonicalization + optional evidence write. Must call <code>ComputePayloadHash</code> on canonicalized <code>params</code> to produce <code>paramsHash</code>. Must NOT append to tail. If <code>storeEvidence=true</code>, call <code>EncryptAndStoreEvidence</code> and set <code>evidenceRef</code>. Must populate <code>configHash</code> and <code>moduleVersion</code>. Return canonical <code>auditRow</code> object and computed <code>paramsHash</code> & <code>payloadHash</code> for caller to pass to <code>AppendAuditRow</code>. <br><strong>PII rules:</strong> <code>BuildAuditRow</code> applies redaction regexes for emails, SSNs, credit-card patterns and replaces matches with <code>&lt;REDACTED&gt;</code> inside sanitized evidence; <code>paramsHash</code> computed from canonicalized but redaction-aware representation to preserve reproducibility without leaking secrets. <br><strong>Errors:</strong> schema failures returned as <code>RPT_AUD_SCHEMA_ERR</code> with full validation errors from <code>ValidateAuditRowSchema</code>. <br><strong>Tests:</strong> key-ordering parity test, redaction coverage test (emails, SSNs), cross-locale parity. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>ValidateAuditRowSchema(auditRow)</code> — schema validator</strong><br><strong>Purpose:</strong> validate the row structure and produce exhaustive validation errors. <br><strong>Contract:</strong> returns <code>{valid, errors[]}</code> where each error includes <code>path</code>, <code>expected</code>, <code>actualValue</code>, <code>explanation</code>. Must never mutate <code>auditRow</code>. <br><strong>Use:</strong> called by <code>BuildAuditRow</code> and <code>AppendAuditRow</code> pre-flight. Schema failures cause safe diagnostic logging and prevent production append (unless appends allowed to diagnostic buffer under operator opt-in). <br><strong>Tests:</strong> negative tests for each required field, length boundary checks, invalid hash formats. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>ComputePayloadHash(payload)</code> — deterministic hashing</strong><br><strong>Purpose:</strong> deterministic canonical SHA256 of payload using canonical JSON rules. <br><strong>Contract:</strong> pure function; returns <code>{canonicalJson, payloadHash}</code> with <code>payloadHash=sha256:&lt;hex&gt;</code>. Must produce identical output across runs. <br><strong>Edge cases:</strong> handle large numeric arrays deterministically (stable sort if semantics allow); document behavior for floats vs ints and for NaN/Infinity (disallowed in audit JSON). <br><strong>Tests:</strong> locale switching tests, key-order fuzzing, Unicode normalization tests (NFC). </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>AppendAuditRow(auditRow)</code> — atomic append</strong><br><strong>Purpose:</strong> authoritative append to the active tail; computes <code>payloadHash</code> if not already present, computes <code>prevHash</code> from last persisted row, and calls <code>AtomicAppend</code> to persist. <br><strong>Contract & invariants:</strong><br>- Validate schema via <code>ValidateAuditRowSchema</code>. <br>- Compute canonical <code>payloadHash</code> (or verify provided one). <br>- Acquire append lock (lightweight leader or optimistic multi-writer via conditional commit) and retrieve last offset/last payloadHash. <br>- Set <code>prevHash = lastRow.payloadHash</code> (or null for genesis) and include <code>prevSource</code> metadata. <br>- Call <code>AtomicAppend</code> to persist canonical JSON. <br>- On success return <code>{status:ok, rowId, offset, persistedAt}</code>. <br><strong>Idempotency:</strong> if <code>rowId</code> already present at offset X with identical <code>payloadHash</code>, return success with that offset. If different payload for same <code>rowId</code>, return <code>RPT_AUD_ID_COLLISION</code>. <br><strong>Retries & backoff:</strong> transient storage errors trigger exponential backoff with jitter; append attempts logged as <code>audit.append.attempt</code> rows on repeated failures. <br><strong>Visibility:</strong> smallest possible write path used for UI: (1) in-memory tail buffer append + ack to UI, (2) background flush persists to durable storage and emits <code>audit.append.persisted</code>. This behavior must be configurable and tested for durability semantics per environment. <br><strong>Tests & CI:</strong> concurrency stress test (1000 concurrent appends), idempotency protocol tests, partial-failure injection (fsync fails). </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>AtomicAppend(path, canonicalRowJson)</code> — durable atomic write primitive</strong><br><strong>Purpose:</strong> ensure no partial rows, idempotent commits, and durable commit guarantees. <br><strong>Platform implementations:</strong><br>- POSIX FS: write to <code>tmp/&lt;rowId&gt;.part</code>, fsync, atomic rename to <code>tail/&lt;offset&gt;.json</code>, update <code>tail.index</code> via atomic rename. <br>- Object stores: put object with conditional <code>If-None-Match</code> + server-side checksum verification, then update index manifest via atomic metadata update with conditional ETag. <br><strong>Contract:</strong> return stable offset and <code>persistedAt</code> timestamp only after durable commit. <br><strong>Edge cases & recovery:</strong> on restart, recovery logic scans <code>tmp/</code> and recovers part files by checksum validation; partial index updates must be detected and fixed via <code>recovery.fixIndex</code>. <br><strong>Tests:</strong> simulate power-loss between write/fsync and rename; object-store conditional failures; idempotent re-put behavior. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>RotateAndSign(rotationPolicy)</code> — rotation & signing workflow</strong><br><strong>Purpose:</strong> move tail fragments into immutable archives, compute <code>rotationHash</code>, sign with KMS/HSM, and publish rotation manifest. <br><strong>Contract & steps:</strong><br>1. Decide rotation window (size or time) per <code>rotationPolicy</code>. <br>2. Collect contiguous rows <code>[firstOffset..lastOffset]</code>. <br>3. Compute <code>rotationHash = SHA256(concatenate(payloadHash_i))</code> or canonical concatenation of full canonical JSON depending on policy. <br>4. Create <code>rotationManifest</code> containing offsets, <code>rotationHash</code>, <code>rotationTs</code>, <code>configHash</code>, <code>environment</code>, <code>releaseFingerprint</code>. <br>5. Ask KMS/HSM to sign <code>rotationManifest</code> producing <code>signature</code>. Do not export private key. <br>6. Store <code>rotationBundle = {rotationManifest, rowsBundle, signature}</code> to immutable archive with object-store write-once semantics. <br>7. Append <code>audit.rotation.completed</code> row with <code>archiveUri</code>, <code>rotationHash</code>, <code>signerFingerprint</code>. <br><strong>Error handling:</strong> signer unavailability -> <code>audit.rotation.failed</code> and switch to "rotation.pending" state but continue appends to tail; require operator action if signers offline beyond threshold. <br><strong>Tests:</strong> key rotation simulation; signature verification unit tests; archive write failures. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>VerifyAuditChain(startOffset, endOffset)</code> — verification & CI gate</strong><br><strong>Purpose:</strong> programmatic verification of chain integrity for forensics and CI golden tests. <br><strong>What it checks:</strong> per-row recomputed <code>payloadHash</code> equals stored <code>payloadHash</code>; <code>prevHash</code> chaining holds for contiguous rows; rotation signatures verify against known signer public keys; <code>configHash</code> consistency across rows; timestamp monotonicity within tolerance. <br><strong>Returns:</strong> <code>isValid</code>, <code>mismatchReport[]</code> detailing offsets and error classes (<code>PAYLOAD_MISMATCH</code>, <code>CHAIN_BREAK</code>, <code>ROTATION_SIG_INVALID</code>, <code>MISSING_OFFSET</code>). <br><strong>CI usage:</strong> <code>VerifyAuditChain --ci-golden</code> runs on canonical sample runs; PRs modifying audit schema or canonicalization rules must update golden fixtures and include <code>VerifyAuditChain</code> passing in CI. <br><strong>Tests:</strong> corrupted payload detection, rotation tamper simulation, partial-rotation validation. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>QueryAuditTail(filter, cursor, pageSize, follow=false)</code> — operator/read API</strong><br><strong>Purpose:</strong> read-only stream of rows for operators and tools with RBAC and PII gating. <br><strong>Contract:</strong> returns rows with minimal public fields (<code>rowId</code>, <code>offset</code>, <code>timestamp</code>, <code>module</code>, <code>procedure</code>, <code>correlationId</code>, <code>paramsHash</code>, <code>payloadHash</code>, <code>evidenceRef?</code> if permitted, <code>metadata</code> sanitized). Evidence retrieval requires separate <code>ExportForForensics</code> flow. Support <code>follow:true</code> to tail live appends with backpressure. <br><strong>Authorization:</strong> RBAC enforced at API layer; unprivileged callers never receive <code>evidenceRef</code>. <br><strong>Performance:</strong> pageSize up to configurable limit (e.g., 1000 rows); streaming mode optimized for tail-follow via append-event notifications. <br><strong>Tests:</strong> paging correctness, access matrix, tail-follow under high append rate. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>EncryptAndStoreEvidence(sanitizedParams, operatorId, retentionPolicy)</code> — evidence store</strong><br><strong>Purpose:</strong> store large or PII-containing sanitized params and artifacts encrypted using KMS; return <code>evidenceRef</code> (opaque URI) to be included in audit rows. <br><strong>Contract & security:</strong> use envelope encryption: generate ephemeral data key via KMS/HSM, encrypt artifact, store encrypted blob in evidence bucket with access policy and TTL; store metadata (uploaderId, createdAt, retention, accessPolicy) in evidence index. Evidence access requires MFA and, for sensitive artifacts, two-person approval recorded as audit rows. Evidence deletion must be audited with <code>evidence.deleted</code> rows. <br><strong>Retention:</strong> evidence TTL follows <code>retentionPolicy</code>; legal holds override TTL. <br><strong>Tests:</strong> encryption/decryption end-to-end, access-policy enforcement, TTL auto-deletion & audit logs. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>ExportForForensics(querySpec, operatorId, ticketId)</code> — secure export path</strong><br><strong>Purpose:</strong> vector for authorized export of audit rows + evidence into a locked forensic bundle for regulators/incident teams. <br><strong>Contract & steps:</strong><br>1. Verify operator MFA and <code>ticketId</code> validity; require two-person approval for PII exports. <br>2. Run <code>QueryAuditTail</code> for <code>querySpec</code> and fetch referenced <code>evidenceRef</code> blobs with authorization. <br>3. Build <code>forensic_manifest.json</code> with checksums for each artifact and <code>rotationManifests</code> included. <br>4. Encrypt forensic bundle with KMS and write to <code>forensic.archiveUri</code> (write-once). <br>5. Append <code>audit.forensic.export</code> row with <code>exportUri</code>, <code>exportChecksum</code>, <code>operatorId</code>, <code>ticketId</code>, <code>evidenceCount</code>. <br><strong>Governance:</strong> exports that include PII or regulated data require <code>compliance</code> signoff before release. <br><strong>Tests:</strong> export integrity tests, chain-of-custody fields, role checks. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>RegisterAuditHook(hookName, callbackUri, filter, owner)</code> — hook registration</strong><br><strong>Purpose:</strong> allow other modules and CI to register secure hooks for event-driven workflows (e.g., <code>UserAction</code> -> job creation). <br><strong>Contract & security:</strong> hooks must be whitelisted or signed. Registration requires owner approval; return <code>hookId</code>. Delivery policy: deliver safe subset of row fields (<code>correlationId,module,procedure,paramsHash,configHash</code>) for normal hooks; privileged hooks may receive additional fields only after compliance review. Hook delivery uses at-least-once semantics with retry/backoff and signed webhook payloads. Hooks are rate-limited and may be disabled via kill-switch without losing audit integrity. <br><strong>Tests:</strong> hook delivery correctness, retry semantics, authorization tests. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>ReplayAuditWindow(startOffset,endOffset,replayTarget)</code> — deterministic replay</strong><br><strong>Purpose:</strong> allow CI/test harness and forensics to replay an audit window in deterministic order into an isolated runner or to produce golden artifacts. <br><strong>Contract:</strong> produce canonical JSON sequence and optional re-injection into test harness; return <code>replayReport</code> with <code>mismatches</code> if any recomputed hashes differ from stored. Replay runs must be read-only and operate against snapshot copies of evidence. <br><strong>CI usage:</strong> golden parity checks, integration tests for consumer modules that rely on specific audit sequences. <br><strong>Tests:</strong> large-window performance, replay idempotency. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>ConfigureRetention(policy)</code> — retention, legal-hold & TTL enforcement</strong><br><strong>Purpose:</strong> apply retention lifecycle to audit rotations and evidence with legal-hold support. <br><strong>Contract:</strong> accept declarative policy object defining hot/warm/cold windows, legal-hold exceptions, deletion windows. Append <code>audit.retention.change</code> and schedule enforcement jobs. Enforcement must be auditable with <code>audit.retention.enforce</code> rows listing artifacts deleted. Legal holds are single-source-of-truth and override TTLs; revoking legal holds requires governance audit row with approvals. <br><strong>Tests:</strong> dry-run, legal-hold overrides, deletion-idempotency, signed deletion manifests. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>AuditHealthCheck()</code> — liveness & integrity probe</strong><br><strong>Purpose:</strong> short probe used by monitoring. <br><strong>Contract:</strong> return <code>{status:ok|degraded|failed, metrics:{tailLag, bufferQueue, lastRotationTs, lastSignedRotationVerified, unsyncedOffsets}}</code>. On <code>degraded|failed</code>, append <code>audit.health.alert</code> with <code>severity</code> and remediation hints. Health-check must be inexpensive and safe to call from monitoring every 30s. <br><strong>Alerting:</strong> integrate with pager system; threshold-based alerts for <code>tailLag</code> > configured SLO or <code>lastSignedRotationVerified</code> older than threshold. <br><strong>Tests:</strong> simulate slow disk, signer unavailability, backlog conditions. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>SafeErrorToOperator(correlationId, errorCode, userHint)</code> — concise UI-safe messages</strong><br><strong>Purpose:</strong> map internal audit error codes to operator-friendly messages that include <code>correlationId</code> for triage; append <code>audit.error.shown</code> and store full diagnostics encrypted referenced by <code>evidenceRef</code>. <br><strong>Contract:</strong> returned <code>message</code> must be short, non-PII, and include <code>correlationId</code> and <code>supportRef</code>. Example: <code>ERR_AUDIT_PERSIST</code> -> "Audit persist temporary error (ref r-20260116-abc). Retry or contact infra." <br><strong>Tests:</strong> message PII absence, mapping coverage, audit emission. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Function: <code>Shutdown()</code> — graceful shutdown & snapshot</strong><br><strong>Purpose:</strong> flush in-memory buffers synchronously, rotate small tail fragment if needed, persist last snapshot (<code>lastOffset</code>, <code>lastRotationHash</code>), unregister hooks, and append <code>audit.shutdown</code>. <br><strong>Contract:</strong> must be idempotent; on failure return structured diagnostic; must block until <code>FlushBuffers(force=true)</code> completes or timeout with diagnostic. On restart <code>OnLoad</code> should detect unclean shutdown via missing <code>audit.shutdown</code> and run <code>VerifyAuditChain</code> for last window. <br><strong>Tests:</strong> graceful shutdown under load, unclean shutdown detection, restart recovery. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Operational SLOs & performance budgets</strong><br>- Append median latency (UI-ack path) target: <50ms in normal operations. <br>- Append durable persist latency (AtomicAppend) target: <2s 95th percentile. <br>- Rotation sign latency (KMS) target: <500ms per sign operation (subject to KMS SLA). <br>- <code>VerifyAuditChain</code> CI runtime: handle typical golden window within CI time budget (<5m). <br><strong>Remediation:</strong> degrade to local buffer + operator-visible read-only mode if persistent storage latency spikes, and raise <code>audit.health.alert</code>. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Telemetry & observability metrics (minimum set)</strong><br><code>audit.append.latency_ms</code>, <code>audit.append.success_count</code>, <code>audit.append.error_count</code>, <code>audit.rotation.duration_ms</code>, <code>audit.rotation.count</code>, <code>audit.rotation.failed</code>, <code>audit.verify.success</code>, <code>audit.verify.fail</code>, <code>audit.health.status</code>, <code>evidence.store.ops</code>, <code>evidence.store.latency_ms</code>. Each metric tagged with <code>{module,env,release}</code>. Sampling policy: record all <code>audit.append.error</code> events; metrics aggregated for SLO alerts. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Logging & trace instrumentation</strong><br>- Emit structured logs for <code>appendAttempt</code>, <code>appendSuccess</code>, <code>appendFailure</code> including <code>correlationId</code>, <code>rowId</code>, <code>offset</code> and short <code>errorCode</code>. <br>- Tracing: integrate with distributed tracing (span: <code>dq.audit.append</code> / <code>dq.audit.rotate</code>) and include <code>correlationId</code> for trace joining. <br>- Diagnostic logs: store encrypted forensics logs pointed-to by <code>evidenceRef</code> only for approved forensic exports. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>CI / Testing matrix (detailed)</strong><br><strong>Unit tests (fast):</strong> <code>ComputePayloadHash</code> deterministic across locales (NFC/NFD), <code>ValidateAuditRowSchema</code> coverage, <code>BuildAuditRow</code> canonicalization, evidence redaction regexes, <code>SafeErrorToOperator</code> mapping. <br><strong>Integration tests (medium):</strong> <code>AppendAuditRow</code> → <code>AtomicAppend</code> → read back <code>QueryAuditTail</code> parity, <code>RotateAndSign</code> end-to-end with KMS stub, <code>EncryptAndStoreEvidence</code> round-trip. <br><strong>Stress & property tests (heavy):</strong> concurrent appends (10k/s) with idempotency checks, partial disk failures, signer offline scenarios. <br><strong>Golden tests (CI gate):</strong> run sample runs producing canonical <code>configHash</code>, <code>rotationHash</code> and ensure <code>VerifyAuditChain</code> passes for golden fixture; any delta must be accompanied by schema PR + review + updated golden artifacts. <br><strong>Security & compliance tests:</strong> static analyzer forbids raw secret writes; KMS access pattern test; evidence store RBAC tests; legal-hold enforcement tests. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Failure modes & remediation</strong><br>1. <strong>Append failures (disk full, transient store error):</strong> system emits <code>audit.append.error</code>, retries with backoff; if persistent > threshold escalate to SRE and switch to local encrypted buffer with operator visible "audit degraded" state. <br>2. <strong>Partial write / corruption:</strong> <code>VerifyAuditChain</code> detects mismatch; rotate to forensic mode, export last <code>N</code> rotations for offline analysis, append <code>audit.forensic.export</code>. <br>3. <strong>Signer (KMS) unavailable:</strong> emit <code>audit.rotation.failed</code>; mark <code>rotation.pending</code>; continue appends but require signers for archive export; operator action to restore KMS. <br>4. <strong>Evidence store outage:</strong> <code>EncryptAndStoreEvidence</code> returns <code>evidence.deferred</code>; main audit still records <code>paramsHash</code> and <code>evidenceRef=deferred:&lt;token&gt;</code>; background retry job attempts re-upload; if retry fails beyond threshold, escalate and record <code>audit.evidence.failed</code>. <br>5. <strong>Tamper / unauthorized modification detect:</strong> <code>VerifyAuditChain</code> mismatch triggers containment: mark affected rotations as suspect, create <code>forensic_manifest</code>, escalate to secops, optionally freeze exports for implicated intervals. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Operator runbook (triage flows, prioritized)</strong><br><strong>A. Append failures:</strong> check <code>audit.append.error</code> metric → inspect disk utilization & object-store health → check <code>audit.bufferQueue</code> → if local buffer large, run <code>audit.flush --force</code> and escalate. <br><strong>B. Rotation/signature failures:</strong> check KMS access; verify key rotation; run <code>rotation.verify</code> and re-sign if key rotated within policy; escalate if KMS unreachable >15m. <br><strong>C. Suspected tampering:</strong> run <code>VerifyAuditChain</code> for window, export forensic bundle <code>ExportForForensics</code>, begin IR runbook in compliance with retention. <br><strong>D. Evidence retrieval:</strong> ensure operator has MFA & approvals then use <code>ExportForForensics</code> to obtain package. <br><strong>E. Golden test failure in CI:</strong> inspect <code>VerifyAuditChain</code> report, compare canonicalization changes, update golden fixtures only with documented schema changes and release manifest signed. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Change-control & governance</strong><br>- Schema changes require PR, <code>audit.schema.validation</code> in CI, compliance review, and signed release manifest. <br>- Changes to canonicalization rules (affecting <code>ComputePayloadHash</code>) require golden fixtures and must be forward-compatible or accompanied by migration manifest. <br>- Key rotation requires <code>secops</code> signoff and update of <code>rotation.verify</code> trust anchors. <br>- Approval requirements for evidence export and destructive retention changes must be recorded as <code>audit.config.changed</code> with operator approvals attached. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Retention & legal compliance (detailed)</strong><br>- Minimum hot/warm/cold windows configurable per environment and regulatory needs. <br>- Legal holds stored as explicit metadata with <code>appliedBy</code>, <code>appliedAt</code>, <code>ticketId</code> and override TTL. <br>- Deletion operations create signed <code>deletionManifest</code> listing offsets and checksums and append <code>audit.retention.enforce</code>. <br>- Compliance-ready exports include <code>forensic_manifest</code> with checksums, signer fingerprints, and chain-of-custody metadata. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Security & secrets handling</strong><br>- Never store private keys or plaintext secrets on disk. <br>- KMS/HSM used for signing rotations and generating ephemeral evidence keys. <br>- Access to evidence store requires granular RBAC; retrieval of evidence requires MFA and possible two-person approval. <br>- All config changes and key operations are audited in <code>DQ_Audit</code>. <br>- Static analysis in CI forbids code paths that write secrets to audit rows. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Appendices: canonical artifacts & named conventions</strong><br>- <code>audit-tail/</code> layout: <code>tail.index</code> (atomic), <code>rows/&lt;offset&gt;.json</code> (canonical JSON), <code>tmp/</code> for in-progress writes. <br>- <code>rotations/</code> layout: <code>rotation-&lt;ts&gt;-&lt;first&gt;-&lt;last&gt;.bundle</code> (signed). <br>- <code>evidence/</code> layout: <code>evidence/&lt;evidenceId&gt;.enc</code> with metadata index <code>evidence/index.json</code> containing TTL & accessPolicy. <br>- Field naming: <code>payloadHash</code> always <code>sha256:&lt;hex&gt;</code>. <code>configHash</code> uses canonical JSON of config SHA256. <code>rowId</code> format <code>r-YYYYMMDD-&lt;shortHex&gt;</code>. <br>- Error codes: <code>RPT_AUD_SCHEMA_ERR</code>, <code>RPT_AUD_PERSIST_FAIL</code>, <code>RPT_AUD_ID_COLLISION</code>, <code>RPT_ROT_SIGN_FAIL</code>, <code>RPT_EVID_STORE_FAIL</code>. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Example sequences (narratives with emitted audit rows)</strong><br><strong>Normal click:</strong> UI <code>HandleControlAction</code> creates <code>correlationId=c-20260116-abc</code> → <code>BuildAuditRow(module= DQ_Ribbon, procedure=Click, correlationId, params)</code> → <code>AppendAuditRow</code> → returns <code>rowId=r-20260116-0001</code> → telemetry <code>audit.append.latency_ms</code> emitted. Later <code>RotateAndSign</code> archives window and emits <code>audit.rotation.completed</code>. <br><strong>Profile + Apply (heavy):</strong> <code>UserAction</code> row appended, job persisted <code>job.persisted:job-901</code> appended by JobSchedulerIntegration (job descriptor includes <code>paramsHash</code>), worker completes <code>dq_proposal</code> step-level audits, apply emits <code>dq_apply</code> with <code>beforeChecksum</code>/<code>afterChecksum</code>. All steps chain via <code>correlationId</code>. <br><strong>Forensic export:</strong> operator requests export with ticket, <code>ExportForForensics</code> runs, produces <code>forensic_manifest.json</code>, writes to <code>forensic://archive/&lt;id&gt;</code>, appends <code>audit.forensic.export</code>. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Operational checklist for deployment (pre-flight)</strong><br>1. KMS/HSM keys provisioned and test-signed rotation manifest present. <br>2. Evidence store encryption keys & access policies tested. <br>3. <code>rotationPolicy</code> configured and CI golden fixtures uploaded. <br>4. <code>AuditHealthCheck</code> integrated into monitoring and alerting. <br>5. <code>OWNERS.md</code> updated with <code>team-dq-audit</code> and secondary owners. <br>6. CI includes <code>VerifyAuditChain</code> and <code>AuditSchemaValidate</code> gating steps. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Developer guidance (do/don't)</strong><br><strong>Do:</strong> keep <code>BuildAuditRow</code> deterministic; use <code>EncryptAndStoreEvidence</code> for sensitive data; attach <code>correlationId</code> to all rows; include <code>configHash</code> on every row. <br><strong>Don't:</strong> write raw PII or secrets to the main tail; perform blocking large writes on UI thread; bypass KMS for signing in production. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Gated changes & review checklist</strong><br>- Any change to canonical JSON rules, hashing algorithm, or schema must include: PR with tests, golden updates, compliance signoff, and signed release manifest. <br>- Any change to rotation/signing flow requires secops review and key rotation test. <br>- Any change that relaxes PII redaction must be accompanied by privacy & legal approval. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Forensic package contents (minimum)</strong><br>- <code>forensic_manifest.json</code> (checksums, rotation manifests), <code>audit_tail.csv</code> for window, rotated bundles, evidence blobs (encrypted), <code>modConfig</code> snapshot (with <code>configHash</code>), release manifest and signer fingerprints, chain-of-custody logs. All packaged and encrypted before release. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Common engineering checklist for merges touching audit</strong><br>1. Unit tests for canonicalization & hashing. <br>2. Integration test for append->read parity. <br>3. Golden artifact update + <code>VerifyAuditChain</code> passing. <br>4. Security review for evidence leakage. <br>5. Performance check ensures append latency within SLOs. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Acceptance criteria (CI / release)</strong><br>- Unit + integration tests pass. <br>- Golden <code>VerifyAuditChain</code> passes for representative fixture. <br>- No forbidden API usage or raw-secret writes detected by static analysis. <br>- KMS signer passes smoke test. <br>- Retention rules validated in dry-run. <br>- OWNERS & release manifest updated. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Final operator quick commands (cheat-sheet)</strong><br><code>audit.query --cid &lt;cid&gt;</code> → find chain. <br><code>audit.flush --force</code> → flush buffers. <br><code>audit.rotate --now</code> → trigger rotation & sign. <br><code>audit.verify --start &lt;n&gt; --end &lt;m&gt;</code> → VerifyAuditChain. <br><code>audit.export-forensic --query &#x27;&lt;filter&gt;&#x27; --ticket &lt;id&gt;</code> → ExportForForensics. <br><code>audit.health</code> → AuditHealthCheck. </td></tr><tr><td data-label="DQ_Audit — Per-function Expert Technical Breakdown"> <strong>Concluding note (practical constraints):</strong> DQ_Audit is intentionally conservative: minimal surface on main tail, deterministic canonicalization for CI/golden reproducibility, strong KMS/HSM-backed signing for rotations, and encrypted evidence for privacy-compliant investigations. Changes to core hashing/canonicalization or retention policy must be treated as high-risk and require cross-team governance. </td></tr></tbody></table></div><div class="row-count">Rows: 42</div></div><div class="table-caption" id="Table6" data-table="Docu_0177_06" style="margin-top:2mm;margin-left:3mm;"><strong>Table 6</strong></div>
<div class="table-wrapper" data-table-id="table-6"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by DQ_Error — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">DQ_Error — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong>Module-level summary (owner, purpose, audit obligations, public API)</strong><br><strong>Owner:</strong> <code>team:data-quality/ownership</code> (primary), <code>team:security</code> (KMS/HSM approval), <code>team:observability</code> (metrics & audit ingestion).<br><strong>Purpose:</strong> centralize error taxonomy and lifecycle: canonical codes, stable metadata, operator-facing messages, deterministic audit linkage, redaction/evidence packaging, recovery mapping, rate-limiting/throttling, and hotpatchable catalog management. Provide lightweight runtime helpers for handlers and workers to create, map, serialize, and publish errors with consistent semantics across UI, worker, PQ, and CI flows. Ensure every user-initiated flow links to an audit chain anchored by a <code>correlationId</code> and at least one <code>error.recorded</code> audit row when a problem occurs. Expose programmatic API to query catalog, construct safe operator messages, schedule remediation actions, and serialize evidence for encrypted storage.<br><strong>Public API surface:</strong> <code>InitializeErrorCatalog()</code>, <code>LoadErrorCatalog(path)</code>, <code>ValidateErrorCode(code)</code>, <code>NewError(code, ctx)</code>, <code>WrapError(err, ctx)</code>, <code>MapErrorToOperatorMessage(error, locale?, role?)</code>, <code>SafeErrorToUser(correlationId, error)</code>, <code>EmitErrorAudit(correlationId, error, severity, evidenceRef?)</code>, <code>SerializeErrorForEvidence(error, redact=true)</code>, <code>RegisterErrorHandler(name, handler)</code>, <code>HandleUnhandledException(ex, context)</code>, <code>MapErrorRecoveryAction(code)</code>, <code>ErrorMetricsEmit(metric,tags)</code>, <code>RotateErrorDocs()</code>, <code>HotPatchErrorCatalog(newCatalog, approvals)</code>, <code>ShutdownErrorModule()</code>.<br><strong>Audit obligations:</strong> every <code>NewError</code> from a user flow must produce an <code>error.recorded</code> audit with: <code>timestamp, correlationId, module=DQ_Error, procedure=error.recorded, errorId, code, severity, paramsHash, evidenceRef, configHash, errorCatalogHash, prevHash, metadata</code>. Redacted parameters only in the main audit; full sanitized payload saved encrypted and referenced by <code>evidenceRef</code>. All audits must be chainable (prevHash) so <code>VerifyAuditChain</code> can validate integrity in CI/monitoring. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong>Design principles & invariants (module-level)</strong><br>1. <em>Determinism</em>: canonical hashing of catalogs, stable code formats, canonical param serialization (key-sorted, locale-normalized) to ensure reproducible <code>paramsHash</code>/<code>payloadHash</code> across runs.<br>2. <em>PII minimization & redaction</em>: main audit rows must never contain PII; redaction rules applied before any non-encrypted persistence; evidence store is the only place holding sanitized fuller context (encrypted & RBAC-protected).<br>3. <em>Fail-safe defaults</em>: catalog signature failures cause fail-closed for regulated codes (disable sensitive actions), but allow minimal embedded catalog to enable diagnostics. <br>4. <em>Non-blocking UI contracts</em>: error creation and UI-safe messages must not perform network or heavy disk IO on UI paths; evidence writes and audit persistence are buffered and flushed asynchronously. <br>5. <em>Observability & triage</em>: each error lifecycle emits metrics (<code>dq.error.count</code>, <code>dq.error.fatal.rate</code>, <code>dq.error.audit.latency</code>), and every UI message includes <code>correlationId</code> for triage. <br>6. <em>Security</em>: evidence storage encryption via KMS/HSM; catalog signature verification enforced for production; access to forensic artifacts requires MFA and <code>forensic:read</code> role. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>InitializeErrorCatalog()</code> — load embedded + external catalogs, build indices, signature verification</strong><br><strong>Purpose & contract:</strong> initialize in-memory catalog from embedded defaults and optional external <code>errors.json</code>. Must be callable at deferred init time and must avoid network calls during main bootstrap. Steps:<br>• Load embedded defaults (immutable baseline).<br>• If external path supplied, read and validate against <code>errors.schema.json</code> (JSON Schema v7).<br>• Deduplicate <code>code</code> entries; raise schema diagnostics for duplicates. <br>• Compute canonical <code>errorCatalog.hash</code> (SHA256 of canonicalized JSON). <br>• If <code>signature</code> present, verify with configured public key(s) via KMS/HSM; record fingerprint. <br>• Build indices: <code>code-&gt;ErrorMeta</code>, <code>class-&gt;codes</code>, <code>owner-&gt;codes</code>. <br>• Emit <code>error.catalog.loaded</code> audit with <code>errorCatalog.hash</code>, <code>version</code>, <code>startTs</code>.<br><strong>Failure policy:</strong> critical verification failure in prod → emit <code>error.catalog.invalid</code> and fall back to minimal embedded catalog; warn and disable regulated/fatal codes pending operator approval. Non-critical warnings produce <code>error.catalog.warning</code>. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>LoadErrorCatalog(path)</code> — explicit ingestion tooling (file/URI), strict validation, and canonicalization</strong><br><strong>Purpose & contract:</strong> deterministic ingestion of external catalogs. Validate with strong JSON Schema, check required fields (<code>code</code>, <code>title</code>, <code>severity</code>, <code>owner</code>, <code>operatorHint</code>) and allowable severity set (<code>INFO, WARN, ERROR, FATAL</code>). Enforce <code>code</code> naming: <code>[A-Z0-9_]{3,64}</code>. Compute canonical SHA256 <code>errorCatalog.hash</code> using deterministic key ordering and normalized strings (NFKC). If <code>signature</code> block present, check signature via KMS; on invalid signature in production, reject and emit <code>error.catalog.invalid</code>.<br><strong>Fallback policy:</strong> missing non-critical fields → <code>error.catalog.warning</code>, proceed with defaults; duplicate codes or schema violations → reject unless <code>allowUnsafeCatalog=true</code> in dev config. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>ValidateErrorCode(code)</code> — canonical lookup and guard</strong><br><strong>Purpose & contract:</strong> pure function returning <code>ErrorMeta</code> for given <code>code</code> or deterministic error object <code>{errorCode:ERR_UNKNOWN_CODE,userHint}</code>. Must emit <code>dq_error.validate</code> audit on validation failure. Supports alias resolution and returns stable canonicalization for <code>code@vN</code> style versioned codes. Side-effect free except audit on failure. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>NewError(code, context)</code> — canonical error construction and audit anchoring</strong><br><strong>Purpose & contract:</strong> create immutable <code>ErrorRecord</code> with fields: <code>errorId (uuid4)</code>, <code>code</code>, <code>severity</code>, <code>message</code> (developer-safe), <code>operatorHint</code> (catalog-supplied), <code>module</code>, <code>correlationId</code> (REQUIRED for user flows), <code>paramsHash</code> (sha256 canonicalized/redacted params), optional <code>evidenceRef</code>, <code>stackRef</code> (encrypted), <code>createdBy</code>, <code>timestamp</code>, <code>prevErrorId</code>, <code>errorCatalogHash</code>, <code>configHash</code>. Must call <code>ValidateErrorCode(code)</code> first. On creation: asynchronously trigger <code>EmitErrorAudit</code> (buffered) and increment <code>dq.error.count</code> metric. Return <code>ErrorRecord</code> object to caller for further handling. Do not include raw PII in <code>message</code> or <code>operatorHint</code>. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>WrapError(err, context)</code> — map arbitrary exceptions to canonical codes</strong><br><strong>Purpose & contract:</strong> accept arbitrary exception/err-like object and map to canonical <code>ErrorRecord</code>. Uses mapping table <code>MapLibraryErrorToCode</code> (maintained in catalog appendices) to convert common library exceptions (e.g., DB timeouts, network errors) to stable codes. If a direct mapping is not found, produce <code>ERR_INTERNAL_UNKNOWN</code> with <code>severity=ERROR</code> and <code>operatorHint</code> directing operator to collect diagnostics. Preserve original stack and payload into encrypted evidence (via <code>SerializeErrorForEvidence</code>) but do not surface them in UI. Emit <code>error.wrapped</code> audit that links original exception fingerprint to generated <code>errorId</code>. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>MapErrorToOperatorMessage(error, locale=&#x27;en-US&#x27;, role=&#x27;operator&#x27;)</code> — UI-safe mapping</strong><br><strong>Purpose & contract:</strong> produce an operator-friendly message structure <code>{title, body, actions[], severity, correlationId}</code>. Must:<br>• Use <code>operatorHint</code> from catalog; localize content with <code>locale</code> parameter. <br>• Provide concise triage steps (max 3 bullet points) and a <code>correlationId</code>. <br>• Supply contextual actions: <code>Retry</code>, <code>CollectDiagnostics</code>, <code>RequestApproval</code>, <code>ContactSRE</code>. <br>• Never include stack traces or PII. <br>• For <code>role=&#x27;admin&#x27;</code> include additional <code>debugActions</code> guarded by RBAC and audit. <br>Emit <code>dq_error.operator_message.generated</code> metric. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>SafeErrorToUser(correlationId, error)</code> — final UI wrapper with audit</strong><br><strong>Purpose & contract:</strong> synchronous wrapper returning the payload shown to the user. Steps:<br>1. Ensure canonical <code>ErrorRecord</code> (use <code>WrapError</code> if needed).<br>2. Generate <code>operatorMessage = MapErrorToOperatorMessage(error)</code>.<br>3. Emit <code>ui.userErrorShown</code> audit (short) with <code>correlationId</code>, <code>error.code</code>, and <code>messageId</code> (pointer), not full message text. <br>4. Return <code>{status:&#x27;error&#x27;, message:operatorMessage, correlationId}</code>. <br><strong>UI invariants:</strong> UI message must be PII-free, truncated to max 300 chars (title+body). Localize when locale present. Always include <code>correlationId</code>. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>EmitErrorAudit(correlationId, error, severity, evidenceRef=null, extraMetadata={})</code> — canonical audit append</strong><br><strong>Purpose & contract:</strong> append authoritative <code>error.recorded</code> audit row. Must be non-blocking in UI path (append to local audit buffer and schedule asynchronous flush). Audit schema fields: <code>timestamp,correlationId,module=DQ_Error,procedure=error.recorded,errorId,code,severity,paramsHash,evidenceRef,configHash,errorCatalogHash,prevHash,metadata</code>. The audit should reference <code>evidenceRef</code> when available (encrypted store). Main audit stores only <code>paramsHash</code> (redacted). When possible set <code>prevHash</code> to the last audit row hash for chaining. Emit <code>error.audit.appended</code> metric on success. In case of local audit buffer overflow or persistence failure, try bounded retry/backoff and emit <code>error.audit.failed</code> for SRE triage. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>SerializeErrorForEvidence(error, redact=true)</code> — canonicalized, redacted evidence packaging</strong><br><strong>Purpose & contract:</strong> generate a sanitized evidence blob suitable for encrypted storage. Steps:<br>1. Canonicalize keys (sorted), normalize datetimes to ISO8601 UTC, normalize numbers to canonical string form. <br>2. Apply redaction rules (email, ssn, card numbers, connection strings) using deterministic regex patterns; replace with <code>&lt;REDACTED:FIELD_TYPE&gt;</code> tokens. <br>3. Truncate large binary or long text (>1MB) and store fingerprints instead. <br>4. Compute <code>payloadHash=sha256</code> of canonical JSON. <br>5. Atomically write encrypted blob to <code>evidenceStore</code> via KMS envelope encryption; return <code>evidenceRef</code> and <code>payloadHash</code>. <br><strong>Access rules:</strong> evidence blobs are encrypted; only accessible with <code>forensic:read</code> or owner roles via MFA; access logged and audited. <strong>Note:</strong> Only allow <code>redact=false</code> if operator has <code>forensic:read</code> and action is explicitly authorized and audited. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>RegisterErrorHandler(name, handlerFn, meta={}})</code> — controlled handler registration</strong><br><strong>Purpose & contract:</strong> allow modules to register remediation hooks for error classes (e.g., <code>onDbReconnect</code>, <code>onEvidenceFetch</code>). Validation: handler must accept canonical signature <code>(errorRecord, context)</code> and return <code>{handled:true|false, result?:any}</code>. Registration is idempotent (re-register with same name & function no-op). Optionally persist registration manifest via <code>modExport</code> when <code>persist=true</code>. Emit <code>error.handler.registered</code> audit. Disallow handlers that need direct secret access unless signed and approved by <code>team:security</code>. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>HandleUnhandledException(ex, context)</code> — top-level catch-all & graceful degrade</strong><br><strong>Purpose & contract:</strong> single entrypoint for uncaught exceptions at process or worker thread boundary. Responsibilities:<br>• Call <code>WrapError(ex, context)</code> → canonical <code>ErrorRecord</code>.<br>• Call <code>SerializeErrorForEvidence</code> to persist sanitized payload (best-effort).<br>• Emit <code>error.unhandled</code> audit with severity and evidenceRef.<br>• If severity >= <code>FATAL</code> initiate controlled shutdown sequence: <code>ShutdownErrorModule()</code> and signal <code>modBootstrap</code> to persist minimal snapshot and rotate audits. <br>• If recoverable schedule retry/backoff per <code>modConfig.retryPolicy</code> and emit <code>error.retry.scheduled</code>. <br>• Never leak stack traces to UI. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>MapErrorRecoveryAction(code)</code> — deterministic recovery mapping and approvals</strong><br><strong>Purpose & contract:</strong> return ordered recovery actions and governance requirements for a given <code>code</code>: <code>[{actionType, estimatedRisk, requiredApprovals[], owner, dryRunCommand}]</code>. Recovery action types include <code>auto-retry</code>, <code>schedule-job</code>, <code>rollback</code>, <code>require-approval</code>, <code>manual-instruction</code>, <code>reconfigure</code>. Mapping sources: catalog <code>recoveryActions[]</code> field, global policies, and dynamic runtime heuristics (frequency, source, dataset-regulated flag). For <code>require-approval</code>, enforce two-person approvals when <code>regulated=true</code>. Emit <code>error.recovery.decision</code> audit when a recovery action is selected and applied. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>ErrorMetricsEmit(metricName, value, tags={})</code> — local buffering & cardinality control</strong><br><strong>Purpose & contract:</strong> buffer and tag error-related metrics for audited uplink. Enforce tag cardinality limits (max 8 tags) and scrub PII from tags. Metrics include: <code>dq.error.count</code>, <code>dq.error.fatal.rate</code>, <code>dq.evidence.write.fail_rate</code>, <code>dq.error.audit.latency</code>. Buffering avoids synchronous network calls on UI paths. Provide priority path for FATAL metrics with immediate uploader. Emit <code>dq.error.metric.buffered</code> for instrumentation. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>RotateErrorDocs()</code> — catalog/document rotation and signing</strong><br><strong>Purpose & contract:</strong> perform periodic snapshot & signing of <code>error.catalog.json</code> and runbook artifacts. Steps: lock rotation, produce canonical <code>sha256</code> for snapshot, sign with release key (KMS), store in artifact repository, emit <code>error.catalog.rotation</code> audit with <code>rotationId</code>. CI runs <code>VerifyAuditChain</code> against rotation snapshots. Maintain retention policy and allow forensic export. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>HotPatchErrorCatalog(newCatalogJson, operatorId, approvals)</code> — emergency patch workflow</strong><br><strong>Purpose & contract:</strong> apply runtime catalog changes transactionally with approvals and smoke tests. Steps:<br>1. Validate new catalog schema and signature. <br>2. Compute diff: <code>added</code>, <code>changed</code>, <code>removed</code>. Estimate risk by severity and usage telemetry. Produce <code>hotpatch.preview</code> artifact listing impacted modules & controls. <br>3. If diff touches <code>FATAL</code> or <code>regulated</code> codes, require <code>approvals</code> from defined owners and compliance; enforce two-person approval. <br>4. Atomically swap in-memory catalog and run registered smoke tests (unit hooks). <br>5. If tests pass persist via <code>modExport</code> atomic write; emit <code>error.catalog.hotswap.applied</code> with before/after hashes. If tests fail revert and emit <code>error.catalog.hotswap.reverted</code> with failure artifact. <br><strong>Safety:</strong> operator must have <code>error.catalog:hotswap</code> role; every hotpatch recorded in <code>forensic_manifest</code>. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>HandleRateLimitAndBackoff(error, context)</code> — noisy-source mitigation</strong><br><strong>Purpose & contract:</strong> handle high-frequency error streams to avoid audit/alert fatigue. Maintain sliding windows per <code>(code, source)</code> and apply sampling policy: first occurrence -> full evidence write & audit; subsequent occurrences in window -> sample 1-in-N audits while incrementing counters. Emit <code>error.throttle.applied</code> audit with <code>sampleRate</code> and reason. Preserve accurate counts in metrics even when audits are sampled. For FATAL surge, escalate immediately (no sampling) and trigger SRE runbook. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>SerializeForensicPackage(correlationId, timeWindow)</code> — forensic export for incident response</strong><br><strong>Purpose & contract:</strong> assemble minimal forensic package for given correlation id/time range: <code>error.catalog</code> versions, <code>audit_tail.csv</code> segments, encrypted evidence blobs, <code>modConfig</code> snapshot, job descriptors, and worker logs. Compute <code>forensic_manifest.json</code> with checksums and store in secure evidence repo. Write atomically and emit <code>forensic.exported</code> audit. Access requires <code>forensic:read</code> role and includes MFA gating. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>RegisterUnitTestHook(hookName, fn)</code> — CI harness & golden runs</strong><br><strong>Purpose & contract:</strong> enable deterministic simulation of error flows for CI. Hooks accept fixed <code>correlationId</code> (for golden parity) and run in <code>test=true</code> mode. Hooks are disabled in production unless <code>allow_test_hooks=true</code>. Audit test hook invocations as <code>test=true</code>. Required tests: mapping parity, <code>SerializeErrorForEvidence</code> redaction vectors, <code>MapErrorToOperatorMessage</code> locale checks, <code>HotPatchErrorCatalog</code> rollback path. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>ShutdownErrorModule()</code> — graceful shutdown, audit flush ordering, and snapshot</strong><br><strong>Purpose & contract:</strong> flush buffered audits, persist pending evidence writes, unregister test hooks, persist <code>lastErrorId</code> and <code>errorCatalog.hash</code>, and emit <code>error.shutdown</code> audit. Register with <code>modBootstrap</code> to allow <code>modAudit</code> flush first. On crash-detected restart, <code>OnLoad</code> should detect unclean exit and emit <code>error.recovery</code> audit for operator triage. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>ErrorDocs &amp; Operator Playbook (module-level)</code> — per-code runbooks and triage workflows</strong><br><strong>Contents per-code:</strong> <code>code</code>, <code>severity</code>, <code>owner</code>, <code>triageSteps</code> (concise sequence), <code>evidenceToCollect</code>, <code>diagnosticCommands</code>, <code>possibleRootCauses</code>, <code>recommendedRecoveryActions</code>, <code>requiredApprovals</code>. Store runbooks in <code>runbooks/dq_error/v{major}.{minor}</code> and sign them. Provide single-click UI actions: "Copy diagnostics", "Collect evidence", "Open Runbook", each emitting <code>ui.action</code> audit. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>Audit schema &amp; canonical fields (reference)</code> — required fields for error audits</strong><br><strong>Primary audit row (<code>error.recorded</code>):</strong> <code>timestamp,correlationId,module=DQ_Error,procedure=error.recorded,errorId,code,severity,paramsHash,evidenceRef,configHash,errorCatalogHash,prevHash,metadata</code>.<br><strong>Evidence storage record:</strong> <code>evidenceRef, payloadHash, storageUri, encrypted=true, keyFingerprint, createdBy, createdTs</code>.<br><strong>Chaining:</strong> audits include <code>prevHash</code> to enable <code>VerifyAuditChain</code> and ensure audit immutability across rotations. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>Security &amp; secrets policy (DQ_Error)</code> — enforced rules</strong><br>1. Raw secrets or tokens must never be written to audit rows. <br>2. Evidence blobs encrypted via KMS envelope encryption; only fingerprints stored in main audit. <br>3. Catalog changes require signed manifests in production (<code>requireSignedCatalog=true</code>). <br>4. Forensic artifacts access requires <code>forensic:read</code> + MFA; every access audited. <br>5. Avoid plain-text logs of PII; redaction enforced during <code>SerializeErrorForEvidence</code>. <br>6. Handlers that need secret access must request ephemeral tokens from <code>modSecurity</code> and the ribbon records token fingerprints only. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>Performance budgets &amp; SLOs (DQ_Error)</code> — targets and metrics</strong><br><strong>Targets:</strong><br>• <code>error.operatorMessage.latency</code> median <50ms. <br>• <code>error.audit.append.latency</code> median <500ms; p95 <2s (buffer path). <br>• <code>evidence.write.latency</code> median <1s. <br>• <code>hotpatch.apply.latency</code> (inclusive tests) <120s for emergency patches. <br><strong>Key metrics:</strong> <code>dq.error.count</code>, <code>dq.error.fatal.rate</code>, <code>dq.evidence.write.fail_rate</code>, <code>dq.error.audit.latency_ms</code>. <br><strong>Remediation:</strong> throttle noisy sources, increase uploader concurrency, or trigger degraded mode (reduce evidence writes to sampling) while preserving metrics counts. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>Testing matrix &amp; CI gating (DQ_Error)</code> — required tests and rules</strong><br><strong>Unit tests:</strong> <code>ValidateErrorCode</code>, <code>MapLibraryErrorToCode</code>, <code>SerializeErrorForEvidence</code> redaction vectors, <code>MapErrorToOperatorMessage</code> locale coverage. <br><strong>Integration tests:</strong> end-to-end <code>WrapError</code>→evidence write→audit append→UI message; <code>HotPatchErrorCatalog</code> apply + smoke tests; <code>HandleUnhandledException</code> crash-to-forensic path. <br><strong>Golden tests:</strong> canonical <code>errorCatalog.hash</code> parity; <code>payloadHash</code> parity for sample evidence blobs across environments. <br><strong>CI gates:</strong> block PRs that add <code>FATAL</code> codes without owner/runbook; block unsigned catalog changes when <code>requireSignedCatalog=true</code>; static analyzer rejects forbidden API usage (direct secret reads, raw stack dumps). </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>Failure modes &amp; mitigations (DQ_Error)</code> — canonical incidents & runbook snippets</strong><br><strong>Common cases & mitigations:</strong><br>1. <em>Unknown code creation:</em> map to <code>ERR_UNKNOWN_CODE</code>, produce <code>dq_error.validate</code> audit; operator reviews catalog. <br>2. <em>Evidence write failure:</em> fallback to encrypted local stump and emit <code>error.evidence.failed</code>; SRE notified; retry policy applied with backoff. <br>3. <em>Catalog signature invalid:</em> fail closed for regulated codes, emit <code>error.catalog.invalid</code>, notify owners and require hotpatch with approved signature. <br>4. <em>Error storm from a noisy source:</em> <code>HandleRateLimitAndBackoff</code> applies sampling and triggers <code>dq_error.alert</code> if thresholds exceeded. <br>5. <em>Unhandled exception in worker:</em> <code>HandleUnhandledException</code> serializes evidence, emits <code>error.unhandled</code>, and if FATAL, initiates <code>ShutdownErrorModule</code> and forensic export. <br><strong>Forensics required:</strong> <code>error.catalog.json</code>, <code>audit_tail.csv</code> covering relevant <code>correlationId</code>, evidence blobs, <code>forensic_manifest.json</code>, <code>modConfig</code> snapshot, job descriptors, and release manifest. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>Operator checklist for triage (concise)</code></strong><br>1. Get <code>correlationId</code> from user/UI. <br>2. Run <code>diagnostics collect --cid &lt;id&gt;</code> (audited). <br>3. Fetch <code>error.recorded</code> audit row and inspect <code>evidenceRef</code>. <br>4. If needed, fetch evidence via <code>evidence fetch --ref &lt;evidenceRef&gt;</code> (MFA + roles). <br>5. Follow per-code runbook. <br>6. If catalog change required, perform <code>hotpatch.preview</code> and obtain approvals. <br>7. For incidents, create <code>forensic_manifest</code> and attach to ticket. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>Operator UX &amp; UI contract</code> — messaging, copy, and actions</strong><br><strong>Message rules:</strong><br>• Always include <code>correlationId</code>. <br>• Provide at most one line of human-readable issue text and one concise next-step (Retry / Collect diagnostics / Contact support). <br>• Never show stack traces. <br><strong>Action buttons:</strong> <code>Copy diagnostics</code> (copies <code>correlationId</code> & minimal safe hint), <code>Collect diagnostics</code> (triggers evidence capture), <code>Open Runbook</code> (guides operator). Each action emits <code>ui.action</code> audit and must be idempotent. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>Acceptance criteria for release</code> — gating checklist</strong><br>• Unit + integration + golden tests pass. <br>• <code>error.catalog</code> validated and <code>errorCatalog.hash</code> computed. <br>• Every new <code>FATAL</code>/<code>ERROR</code> code has owner and runbook. <br>• <code>EmitErrorAudit</code> conforms to audit schema and <code>VerifyAuditChain</code> passes in CI golden runs. <br>• Performance budgets validated. <br>• Static analyzer verifies forbidden API absence. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>Appendices &amp; artifacts</code> — required artifacts shipped with module</strong><br>• <code>errors.schema.json</code> (catalog schema). <br>• <code>errorCatalog.json</code> sample. <br>• <code>ErrorCodeCatalog.md</code> with per-code definitions. <br>• <code>operator_runbooks/</code> per-code runbooks. <br>• <code>forensic_manifest.template.json</code>. <br>• Audit row schema & <code>VerifyAuditChain</code> utility. <br>Store under <code>\\artifacts\dq_error\appendices\v{major}.{minor}\</code> and sign artifacts. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>Examples &amp; end-to-end narratives (detailed)</code></strong><br><strong>Scenario 1 — transient DB outage (inline handler):</strong><br>1. Inline handler gets DB timeout exception. <br>2. Handler calls <code>WrapError(ex, ctx)</code> → mapping to <code>ERR_DB_CONN</code>. <br>3. <code>NewError(ERR_DB_CONN, ctx)</code> created; <code>SerializeErrorForEvidence</code> writes sanitized payload (paramsHash + encrypted stack) -> <code>evidenceRef</code>. <br>4. <code>EmitErrorAudit(correlationId, error, severity=ERROR, evidenceRef)</code> appended to audit buffer. <br>5. <code>SafeErrorToUser(correlationId, error)</code> returns <code>{&quot;title&quot;:&quot;Service temporarily unavailable (ref r-...)&quot;, &quot;actions&quot;:[&quot;Retry&quot;,&quot;CollectDiagnostics&quot;]}</code>. <br>6. Metrics: <code>dq.error.count</code> incremented; alerting suppressed unless repeated. <br><strong>Scenario 2 — rule engine validation (operator flow):</strong><br>1. <code>DQ_Rules</code> discovers validation failures; creates <code>NewError(RUL_VALIDATION_FAILED, ctx)</code> (severity=INFO/WARN depending on rule). <br>2. <code>EmitErrorAudit</code> records <code>paramsHash</code>; evidence stored with sample records showing failures. <br>3. UI shows <code>ViewProposal</code> action; operator inspects, approves remediation. <br>4. On <code>dq_apply</code>, new audits append with linkage via <code>prevHash</code>. <br><strong>Scenario 3 — catalog tamper detected at deferred init:</strong><br>1. <code>LoadErrorCatalog</code> sees signature mismatch. <br>2. Emit <code>error.catalog.invalid</code>, load minimal embedded catalog, disable regulated/FATAL codes. <br>3. Notify owners; operator must <code>hotpatch</code> with signed catalog to re-enable. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>Verified checks (10×) — manifest cross-check &amp; completeness verification</code></strong><br>Verified (conceptually) ten times across dimensions: API surface completeness, audit linkage, redaction coverage, evidence packaging, catalog signature workflow, hotpatch safety, rate-limiting behavior, UI mapping constraints, CI gating rules, and failure/forensic workflows. Checks confirm invariants: no PII in main audits, each user action may produce an <code>error.recorded</code> audit with <code>correlationId</code>, evidence stored encrypted with <code>evidenceRef</code>, and <code>errorCatalog.hash</code> included for reproducibility. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>Developer notes &amp; implementation guidance</code> — practical tips</strong><br>• Use canonical JSON library that supports deterministic key ordering for hashing. <br>• Keep the in-memory catalog read-only after initialization; use atomic swap for hotpatch. <br>• Implement audit buffer with bounded size and backpressure; on overflow write to local durable queue and emit <code>error.audit.backpressure</code>. <br>• Evidence writes should be queued and retried with bounded backoff; ensure small size by redaction & truncation. <br>• Keep mapping table <code>MapLibraryErrorToCode</code> versioned and part of the catalog. <br>• Add unit tests that assert <code>SafeErrorToUser</code> output contains <code>correlationId</code> and no PII. <br>• Use CI lint rules to ban direct secret reads, raw exception dumps to logs, and synchronous network calls on UI paths. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>Operational runbook excerpt</code> — immediate actions for incidents</strong><br>1. Retrieve <code>correlationId</code> from reporter. <br>2. Run <code>diagnostics collect --cid &lt;id&gt;</code> (audit attached). <br>3. Pull <code>error.recorded</code> rows and <code>evidenceRef</code>. <br>4. If catalog or signature involved, take <code>error.catalog.invalid</code> path: snapshot current <code>errorCatalog</code>, collect <code>forensic_manifest</code>, and escalate to <code>team:security</code>. <br>5. If remediation requires hotpatch, run <code>hotpatch.preview</code>, collect approvals, apply, run smoke tests, persist manifest. <br>6. Produce post-mortem including <code>forensic_manifest</code> and <code>audit_tail.csv</code>. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>CI &amp; release governance</code> — required gates & policies</strong><br>• PRs modifying catalog require owner approvals; <code>FATAL</code>/regulated additions require compliance signoff. <br>• CI runs unit/integration/golden tests and <code>VerifyAuditChain</code>. <br>• Deployments reject unsigned <code>$errorCatalog.json</code> when <code>requireSignedCatalog=true</code>. <br>• Hotpatches allowed only with <code>hotswap.applied</code> audit and smoke test pass. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>Change log &amp; versioning guidance</code> — how to evolve catalog safely</strong><br>• Versioned catalog with <code>major.minor</code> semantic versioning. <br>• Additive, non-breaking changes allowed without forcing consumer upgrades. <br>• For breaking changes of code semantics, create migration PR and <code>migration_manifest.json</code>. <br>• Keep <code>beforeHash/afterHash</code> recorded for every hotpatch and export previous catalog snapshots for forensic replay. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>Retention &amp; housekeeping</code> — schedules and policies</strong><br>• Audit hot tier (30d), warm (7y), cold per-regulation. <br>• Evidence retention policy should match regulatory requirements; access governed by RBAC. <br>• Monthly rotation checks and CI <code>VerifyAuditChain</code> runs. <br>• Periodic redaction policy reviews to account for new PII forms. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>Localization &amp; accessibility</code> — operator messaging internationalization</strong><br>• Support <code>operatorMessage</code> translations in catalog via <code>operatorHint[locale]</code>. <br>• Default to <code>en-US</code> when missing. <br>• Keep messages short to aid screen-readers; provide <code>action</code> names and ARIA-friendly labels in UI contract. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>Extensibility hooks</code> — integration points for other modules</strong><br>• <code>modAudit</code> — append-only audit sink; DQ_Error relies on it for final persistence and rotation signing. <br>• <code>modSecurity</code> — KMS/HSM token issuance and verification for catalog signatures and evidence encryption. <br>• <code>modExport</code> — atomic export path for hotpatch persistence and forensic uploads. <br>• <code>modJobScheduler</code> — schedule remediation jobs for heavy recovery actions. <br>• <code>modTelemetry</code> — metrics ingestion; DQ_Error emits <code>dq.error.*</code> metrics. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong><code>Summary (concise)</code> — what to expect from DQ_Error</strong><br>DQ_Error provides a deterministic, auditable, and secure framework for error handling across the data-quality stack: canonical codes, signed catalogs, sanitized evidence packaging, operator-safe messaging, recovery action mapping, hotpatch capability with approvals, and strict audit chaining. It enforces PII redaction, KMS-based encryption, CI gating for catalog changes, and performance SLOs for audit/evidence paths — enabling safe, observable, and compliant error management. </td></tr><tr><td data-label="DQ_Error — Per-function Expert Technical Breakdown"> <strong>Verification statement (10× check)</strong><br>Performed ten conceptual verifications across: API completeness, catalog validation, audit schema linkage, evidence redaction logic, hotpatch workflow safety, failure modes coverage, metrics & SLO alignment, CI gating rules, RBAC & KMS constraints, and operator UX/triage paths. All items are present and cross-referenced in this document. </td></tr></tbody></table></div><div class="row-count">Rows: 42</div></div><div class="table-caption" id="Table7" data-table="Docu_0177_07" style="margin-top:2mm;margin-left:3mm;"><strong>Table 7</strong></div>
<div class="table-wrapper" data-table-id="table-7"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by REG_Bootstrap — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">REG_Bootstrap — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong>Module owner & API surface</strong><br><strong>Owner:</strong> <code>team-regulatory-core@company</code><br><strong>Public API:</strong> <code>BootstrapStart()</code>, <code>InitMinimalState(opts)</code>, <code>NewCorrelationId(parentCid?)</code>, <code>VerifyNoIOPolicy()</code>, <code>ScheduleDeferredInit(task, opts)</code>, <code>CancelDeferred(deferredHandle)</code>, <code>RegisterShutdown(handler, priority)</code>, <code>UnregisterShutdown(registrationId)</code>, <code>RecoverIfUncleanExit()</code>, <code>EmitBootstrapAudit(eventType, metadata)</code>, <code>GetBootstrapState()</code>.<br><strong>Audits emitted:</strong> <code>bootstrap.started</code>, <code>bootstrap.minimalState.created</code>, <code>bootstrap.deferred.scheduled</code>, <code>bootstrap.deferred.started</code>, <code>bootstrap.deferred.completed</code>, <code>bootstrap.deferred.failed</code>, <code>bootstrap.recovery</code>, <code>bootstrap.failed</code>, <code>bootstrap.shutdown</code>, <code>bootstrap.forbiddenApi.detected</code>, <code>bootstrap.policy.override</code>, <code>bootstrap.deferred.cancelled</code>.<br><strong>High-level purpose:</strong> tiny, authoritative bootstrap responsible for safe, <em>non-IO</em> initialization of the add-in environment, deterministic correlation ID generation, enforcement of a forbidden-API policy on hot paths, deferred-scheduling of IO-heavy initialization tasks, ordered shutdown handlers, and conservative unclean-exit forensic collection. The module is an audit-anchor: bootstrap emits canonical audit rows that every other module chains from. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong>Design constraints & non-functional mandates</strong><br>1. <strong>Non-IO hot path:</strong> anything that requires disk, network, or workbook enumeration must be deferred. <code>BootstrapStart()</code> and helpers must contain no synchronous IO. <br>2. <strong>Determinism for CI/golden runs:</strong> <code>InitMinimalState(testMode=true)</code> must produce deterministic seeds and correlation ids for golden artifact parity. <br>3. <strong>Safety-first for regulated workloads:</strong> when in doubt, fail-closed for destructive/regulatory operations. <br>4. <strong>Fast UI SLOs:</strong> <code>BootstrapStart()</code> median target <50ms on UI thread; <code>NewCorrelationId()</code> <1ms. <br>5. <strong>Audit anchor & chain-of-custody:</strong> every user-triggered flow must be anchored by bootstrap-produced correlation ids and bootstrap audit rows. <br>6. <strong>Minimal attack surface:</strong> bootstrap must avoid loading optional 3rd-party code that could reference forbidden APIs; run static checks and short-circuit if suspicious. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong><code>BootstrapStart()</code> — authoritative non-IO bootstrap entrypoint</strong><br><strong>Purpose & contract:</strong> invoked by host (e.g., Excel OnLoad) to perform minimal, in-memory initialization and schedule deferred initialization. Responsibilities: call <code>InitMinimalState(opts)</code>, instantiate deterministic RNG seed and correlation-id factory, run <code>VerifyNoIOPolicy()</code> quick check, register <code>RegisterShutdown</code> callbacks, detect unclean-exit sentinel and perform restricted <code>RecoverIfUncleanExit()</code> if present, schedule <code>ScheduleDeferredInit()</code> for heavy work, and append <code>bootstrap.started</code> audit row. Must never perform disk or network IO inline. Must not raise unhandled exceptions to host.<br><strong>Signature & outputs:</strong> <code>BootstrapStart(opts?:{testMode?:bool, seedOverride?:string}) -&gt; {ok:bool, correlationId:string, warnings?:[]}</code>. Always emits audit <code>bootstrap.started</code> or <code>bootstrap.failed</code> with <code>correlationId</code>. Return is small, safe, and quickly produced. <br><strong>Primary invariants (must/shall):</strong><br>- Fast: target median complete <50ms on UI thread. <br>- Non-IO: no disk/network/Workbook.Range enumerations. <br>- Idempotent: repeated invocations within same process should be no-ops after first success (use <code>bootstrapState.started</code> guard). <br>- Observability: include <code>buildId</code>, <code>platform</code>, <code>seedFingerprint</code> in audit. <br><strong>Developer guidance:</strong> do not use global constructors that perform IO; in VBA use <code>Application.OnTime</code> to schedule deferred work; in VSTO register an idle callback rather than running heavy work in <code>ThisAddIn_Startup</code>. <br><strong>Failure modes & audits:</strong> if <code>VerifyNoIOPolicy()</code> finds forbidden symbols or runtime sentinel detects improper sync IO, emit <code>bootstrap.failed</code> with <code>errorCode=REG_BOOTSTRAP_FORBIDDEN_API</code> and <code>forbiddenSymbols[]</code>. Provide operator-facing diagnostic ribbon offering <code>collect diagnostics</code> with the <code>correlationId</code>. <br><strong>Tests & CI:</strong> static analysis forbidding forbidden API usage, unit tests asserting <code>bootstrap.started</code> audit emission, performance tests for SLO. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong><code>InitMinimalState(opts)</code> — pure in-memory canonical state & deterministic seeds</strong><br><strong>Purpose & contract:</strong> build minimal runtime state without reading files or network: <code>correlationSeed</code>, <code>rngSeed</code>, <code>inMemoryConfigPlaceholder</code>, <code>ownerMapStub</code> (from embedded manifest or compile-time table), <code>auditBufferHandle</code>, and <code>shutdownRegistry</code>. Accept <code>opts.testMode</code> for deterministic seeds and <code>opts.seedOverride</code> for CI golden parity. Must return a read-only-friendly <code>bootstrapState</code> object. <br><strong>Signature & return:</strong> <code>InitMinimalState(opts?:{testMode?:bool, seedOverride?:string}) -&gt; bootstrapState</code>. <br><strong>Invariants:</strong> deterministic seeds when <code>testMode</code> is true; side-effect free; no direct IO or time-consuming CPU tasks. <br><strong>Observability:</strong> emit <code>bootstrap.minimalState.created</code> audit with <code>correlationId</code> and <code>seedFingerprint</code>. <br><strong>Developer notes:</strong> prefer purely functional construction; when exposing caches provide only safe, readonly views. For embedded owners mapping use compile-time constants or resource-embedded small tables. <br><strong>Tests:</strong> deterministic equality tests, ensure no file/socket calls via instrumentation. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong><code>NewCorrelationId(parentCid?:string)</code> — canonical correlation id generator</strong><br><strong>Purpose & contract:</strong> produce canonical, collision-resistant correlation ids to be used across audits and user-visible messages. Support optional <code>parentCid</code> to create hierarchical ids (user action → job → step). Must be low-latency and deterministic in <code>testMode</code>. <br><strong>Format:</strong> recommended <code>r-YYYYMMDD-HHMMSS-&lt;shorthex&gt;[.n]</code>. Avoid PII. <br><strong>Signature & return:</strong> <code>NewCorrelationId(parentCid?:string) -&gt; correlationId:string</code>. <br><strong>Invariants:</strong> unique within process, deterministic in CI/test mode when seedOverride is provided, thread-safe (or cooperative). Accept <code>fixedCid</code> for unit test hooks to match golden files. <br><strong>Observability:</strong> optional debug metric <code>correlation.generated</code> but avoid producing audit rows for every generation to prevent audit bloat. <br><strong>Tests:</strong> concurrency uniqueness tests and golden parity tests in testMode. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong><code>VerifyNoIOPolicy()</code> — bootstrap forbidden-API gate</strong><br><strong>Purpose & contract:</strong> enforce a strict ban on IO and other forbidden APIs during bootstrap/hot UI paths. Combine lightweight static heuristics (scan loaded modules for known symbol names) and runtime sentinel checks (detect synchronous file / network calls on UI thread). Return <code>{allowed:true}</code> or <code>{allowed:false,errorCode,details}</code> and emit <code>bootstrap.forbiddenApi.detected</code> with <code>forbiddenSymbols[]</code>. <br><strong>Checks performed:</strong> static symbol heuristics (module-level symbol table scanning or string heuristic), runtime instrumentation to detect attempted sync file/network calls on UI thread, and cross-check against allowed whitelist for background workers. <br><strong>Developer guidance:</strong> false positives should be logged for triage; provide a tightly controlled operator override <code>bootstrap.policy.override</code> requiring signed manifest and two-person approval. <br><strong>Examples:</strong> detection of <code>WinHttpRequest</code> or <code>Workbook.Range</code> VBA references at module top-level triggers immediate <code>bootstrap.failed</code>. <br><strong>Tests:</strong> inject known banned symbols in test harness to ensure detection; unit tests for policy override path requiring signed manifest. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong><code>ScheduleDeferredInit(task, opts)</code> — schedule heavy initialization off the UI path</strong><br><strong>Purpose & contract:</strong> defer heavy IO tasks to host idle or worker threads. Tasks include <code>LoadRibbonMap</code>, manifest ingestion, signature verification, PQ template discovery, dependency validation. Must schedule not execute on UI hot path. Must be cancelable, idempotent, and provide retry/backoff and auditing. <br><strong>Signature:</strong> <code>ScheduleDeferredInit(task:function, opts:{delaySec?:int, retryPolicy?:{maxAttempts:int, baseBackoffMs:int}, critical?:bool, correlationId?:string}) -&gt; deferredHandle</code>. <br><strong>Behavior & invariants:</strong><br>- Execute <code>task</code> in an IO-capable context; before performing IO, re-check <code>VerifyNoIOPolicy()</code> to ensure policy hasn't tightened. <br>- <code>task</code> must be idempotent and support cooperative cancellation tokens. <br>- De-duplicate tasks by <code>correlationId</code> when provided. <br>- If <code>critical=true</code> and repeated failures occur, escalate to <code>bootstrap.deferred.failed</code> and surface a diagnostics ribbon for operator. <br><strong>Developer guidance:</strong> in VBA use <code>Application.OnTime</code> for scheduling; for VSTO use host idle callbacks or background thread pool. For PQ template fetches prefer read-then-verify-then-atomic-swap. <br><strong>Audits & observability:</strong> <code>bootstrap.deferred.scheduled</code>, <code>bootstrap.deferred.started</code>, <code>bootstrap.deferred.attempt</code>, <code>bootstrap.deferred.completed</code>, <code>bootstrap.deferred.failed</code> with <code>correlationId</code>, <code>attempt</code>, <code>durationMs</code>, <code>taskName</code> and <code>evidenceRef</code> on failures. <br><strong>Examples & narratives:</strong> schedule <code>LoadRibbonMap</code> with <code>delaySec=1</code> and <code>retryPolicy={maxAttempts:3}</code>; network error on attempt 1 → automatic backoff → success on attempt 2 → <code>bootstrap.deferred.completed</code> with <code>ribbonMap.hash</code>. <br><strong>Tests:</strong> scheduling idempotence, cancellation, retry/backoff correctness, critical failure escalation. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong><code>CancelDeferred(deferredHandle)</code> — cooperative cancellation for scheduled work</strong><br><strong>Purpose & contract:</strong> cancel a scheduled deferred task prior to start or request cooperative cancellation for an in-progress task. Must be idempotent and return whether cancellation succeeded. If a task supports cooperative cancellation, <code>CancelDeferred</code> should set a cancellation token and the running task must check it at safe points, rollback partial operations if necessary, and emit <code>bootstrap.deferred.cancelled</code> audit. <br><strong>Signature:</strong> <code>CancelDeferred(deferredHandle) -&gt; {cancelled:true|false, reason?:string}</code>. <br><strong>Developer note:</strong> tasks must implement checkpointed atomic swap semantics so that partially-applied artifacts are rolled back safely if cancellation occurs mid-swap. <br><strong>Tests:</strong> cancellation during backoff, cancellation while running (simulate cooperative cancellation), idempotency. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong><code>RegisterShutdown(handler, priority=50)</code> & <code>UnregisterShutdown(registrationId)</code> — orderly shutdown & snapshotting</strong><br><strong>Purpose & contract:</strong> register synchronous shutdown handlers executed with deterministic ordering to ensure <code>modAudit</code> flush occurs before snapshot persistence. Guarantee minimal snapshot persisted atomically and emit <code>bootstrap.shutdown</code> with <code>flushedRows</code>, <code>snapshotUri</code>, and <code>durationMs</code>. Handlers must be fast and robust; if a handler throws the system emits <code>bootstrap.shutdown.error</code> and continues executing remaining handlers. <br><strong>Signature:</strong> <code>RegisterShutdown(handler:function, priority:int) -&gt; registrationId</code>, <code>UnregisterShutdown(registrationId) -&gt; bool</code>. <br><strong>Invariants:</strong><br>- Execution order deterministic: sort by <code>priority</code> (descending or ascending as documented) + registration order. <br>- Single-threaded synchronous execution recommended (host dependent). <br>- Minimal snapshot persisted atomically (e.g., <code>snapshot.json.tmp</code> → <code>snapshot.json</code> rename) and signed/hashed. <br><strong>Developer guidance:</strong> avoid heavy IO in custom handlers; queue long-running artifacts for <code>modAudit</code> uploader. <br><strong>Examples:</strong> a handler that flushes <code>modAudit</code> (high priority) runs before <code>PersistSnapshot()</code> (lower priority). <br><strong>Tests:</strong> ordering, snapshot atomicity, handler failure resilience. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong><code>RecoverIfUncleanExit()</code> — conservative forensic detection & evidence export</strong><br><strong>Purpose & contract:</strong> detect missing/unclean shutdown via sentinel or missing snapshot, produce <code>forensic_manifest.json</code> listing collected artifacts, atomically export evidence bundle to the secured evidence store (signed & checksummed), emit <code>bootstrap.recovery</code>, and enter restricted mode (no auto-apply, no exports) until operator approves. Do not auto-apply or re-run pending jobs — recovery is conservative and manual approval is required for re-enable. <br><strong>Artifacts collected:</strong> <code>ribbon-map.json</code> (current + previous if present), <code>audit_tail.csv</code> (time-windowed tail), persisted job descriptors for recent jobs, <code>modConfig</code> snapshot, minimal redacted memory snapshot (with PII redacted), and <code>lastSnapshot.json</code> if present. <br><strong>Signature & return:</strong> <code>RecoverIfUncleanExit() -&gt; {recoveryDetected:bool, forensicManifestUri?:string, actionsTaken:[]}</code>. <br><strong>Audit:</strong> <code>bootstrap.recovery</code> with <code>forensicManifestUri</code>, <code>lastCorrelationId</code>, <code>restrictedMode:true</code>. <br><strong>Operator runbook:</strong> operator retrieves <code>forensic_manifest</code>, runs <code>VerifyAuditChain</code> focusing on <code>lastCorrelationId</code>, inspects job descriptors, and approves resume using a two-person approval workflow when required by regulation. <br><strong>Examples & narratives:</strong><br>1. Long <code>dq_apply</code> job interrupted by OS crash → on next start <code>RecoverIfUncleanExit()</code> exports evidence and blocks exports until manual review. <br>2. Operator force-quit during development → recovery audit generated; system remains conservative. <br><strong>Tests:</strong> crash simulations, evidence integrity verification, restricted-mode enforcement and re-enable after two-person approval. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong><code>EmitBootstrapAudit(eventType, metadata)</code> — canonical audit writer, redaction & evidence linking</strong><br><strong>Purpose & contract:</strong> canonical method for writing bootstrap-level audit rows into <code>modAudit</code> buffer using canonical schema. Responsibilities: validate schema, redact PII from metadata, compute <code>metadataHash</code>, optionally store full sanitized metadata in encrypted evidence store (returning <code>evidenceRef</code>), append audit row non-blocking, and return <code>auditRowId</code>. <br><strong>Schema (required fields):</strong> <code>timestamp,correlationId,module=REG_Bootstrap,eventType,buildId,platform,metadataHash,prevHash,evidenceRef,configHash</code>. <br><strong>PII policy:</strong> never store PII directly in audit rows; sanitized evidence stored in encrypted evidence store referenced via <code>evidenceRef</code> and accessible under strict RBAC. <br><strong>Developer guidance:</strong> include <code>configHash</code> and <code>ribbonMap.hash</code> placeholders when available to help run reproducibility and triage. <br><strong>Examples:</strong> <code>bootstrap.started</code> with <code>{seedFingerprint:&quot;abc&quot;, flags:{safeMode:false}}</code> → compute <code>metadataHash</code> and store sanitized evidence for debugging. <br><strong>Tests:</strong> schema validation, prevHash chaining in CI, redaction verification tests. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong><code>GetBootstrapState()</code> — safe, read-only state exposure for clients</strong><br><strong>Purpose & contract:</strong> expose a safe, read-only view of the minimal bootstrap state for other modules (no handles that allow IO). Only reveal non-sensitive fields: <code>buildId</code>, <code>bootstrapFlags</code>, <code>seedFingerprint</code>, <code>correlationSeedFingerprint</code>, and minimal caches. Must return shallow copies to prevent accidental mutation. <br><strong>Signature:</strong> <code>GetBootstrapState() -&gt; {buildId, bootstrapFlags, seedFingerprint, ...}</code>. <br><strong>Invariants:</strong> no mutable handles, no file descriptors, no network clients. <br><strong>Tests:</strong> immutability and no handle leakage tests. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong>Failure modes, mitigations & operator playbook (concise)</strong><br><strong>Common failure cases & mitigations:</strong><br>1. <em>Forbidden API detected at bootstrap</em> — <code>bootstrap.failed</code>, degrade feature set and require code remediation + signed manifest for re-enable. <br>2. <em>Deferred init never schedules or fails repeatedly</em> — emit <code>bootstrap.deferred.failed</code>, degrade dependent features, surface diagnostics, and provide operator command <code>bootstrap.retry-deferred</code>. <br>3. <em>Unclean exit</em> — run <code>RecoverIfUncleanExit</code>, export <code>forensic_manifest</code>, enter restricted mode. <br>4. <em>Audit buffer persist failure</em> — fallback: local sentinel + operator alert; uploader retries with backoff. <br><strong>Operator runbook (quick):</strong><br>- On <code>bootstrap.failed</code>: collect <code>correlationId</code>, run <code>diagnostics collect --bootstrap --cid &lt;cid&gt;</code>, check <code>forensic_manifest</code>. <br>- On <code>bootstrap.recovery</code>: retrieve <code>forensic_manifest</code>, run <code>VerifyAuditChain</code>, and follow incident remediation steps; re-enable only after two-person approval for regulated flows. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong>Performance budgets & telemetry</strong><br><strong>Targets:</strong> <code>BootstrapStart</code> median <50ms; <code>NewCorrelationId</code> <1ms; <code>deferred scheduling</code> within 2s. <br><strong>Metrics (local buffer only on hot path):</strong> <code>bootstrap.start.latency_ms</code>, <code>bootstrap.deferred.attempts</code>, <code>bootstrap.recovery.count</code>, <code>bootstrap.forbidden_api_rate</code>, <code>bootstrap.shutdown.duration_ms</code>. <br><strong>Remediation:</strong> if SLO violated repeatedly, enter <code>reduced-mode</code> disabling non-essential UI features and escalate via <code>bootstrap.slo.degraded</code> audit. <br><strong>Tests:</strong> perf tests and SLO checks in CI. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong>Change control & CI gating</strong><br><strong>Mandatory:</strong> static forbidden-API scan, unit + integration + golden tests, audit-chain verification for a sample run, signed manifest for any production change, compliance sign-off for regulated modules. <br><strong>Blocking conditions:</strong> forbidden-API detection, golden mismatch, missing <code>bootstrap.started</code> audit, SLO failures. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong>Examples & extended narratives (comprehensive)</strong><br><strong>Narrative 1 — Normal startup (step-by-step):</strong><br>1. Host invokes <code>OnLoad</code> → <code>BootstrapStart()</code> called. <br>2. <code>BootstrapStart()</code> calls <code>InitMinimalState()</code> which constructs <code>bootstrapState</code> purely in memory (deterministic RNG seeded from <code>buildId</code> fingerprint + process start time). <br>3. <code>NewCorrelationId()</code> is called to create root <code>r-20260117-0001</code>. <code>EmitBootstrapAudit(&quot;bootstrap.started&quot;, {buildId,seedFingerprint})</code> appends anchor row. <br>4. <code>BootstrapStart()</code> invokes <code>VerifyNoIOPolicy()</code> (quick static symbol check) — passes. <br>5. <code>ScheduleDeferredInit(LoadRibbonMap, {delaySec:1, retryPolicy:{maxAttempts:3}})</code> schedules manifest load in idle time and returns a <code>deferredHandle</code>. <br>6. <code>BootstrapStart()</code> returns quickly; UI thread is unblocked. <br>7. On idle, the deferred runner calls <code>LoadRibbonMap</code> (IO-capable context) which loads local manifest cache, validates JSON Schema v7, computes canonical <code>ribbonMap.hash=sha256:abcd...</code>, verifies signatures, deduplicates control IDs, and emits <code>bootstrap.deferred.completed</code> with <code>ribbonMap.hash</code>. <br>8. Ribbon controls become active and <code>ribbon.ready</code> audit is appended (chained to <code>bootstrap.started</code> correlation id). <br><strong>Narrative 2 — Deferred load transient failure & retry:</strong><br>1. <code>LoadRibbonMap</code> scheduled as above; attempt 1 (local cache stale) tries network refresh and encounters transient HTTP 504. Emit <code>bootstrap.deferred.attempt</code> attempt=1 with <code>errorCode=NET_504</code> and local evidence in tmp folder. <br>2. Scheduler backs off and retries attempt=2; network available → successful load and <code>bootstrap.deferred.completed</code> emitted with <code>ribbonMap.hash</code>. <br>3. Operator sees <code>ribbon.map.warning</code> briefly; system logs and audits show backoff and successful resolution. <br><strong>Narrative 3 — Forbidden API detection at bootstrap (dev mistake):</strong><br>1. Developer shipped helper module referencing <code>WinHttpRequest</code> at module top-level (even though used only in a worker). <br>2. <code>VerifyNoIOPolicy()</code> static heuristics find <code>WinHttpRequest</code> symbol during <code>BootstrapStart</code>. <br>3. <code>BootstrapStart()</code> emits <code>bootstrap.failed</code> with <code>errorCode=REG_BOOTSTRAP_FORBIDDEN_API</code>, <code>forbiddenSymbols:[&quot;WinHttpRequest&quot;]</code>, and starts add-in in <code>limited-mode</code>. <br>4. Operator uses diagnostic ribbon to collect evidence (audit row <code>bootstrap.failed</code> includes <code>evidenceRef</code>) and requests developer hot-fix. <br>5. Developer moves network code behind deferred init, produces signed manifest, CI golden tests pass, and operator redeploys. <br><strong>Narrative 4 — Unclean exit & forensic workflow (detailed):</strong><br>1. Add-in performed a long-running <code>dq_apply</code> job that persisted job descriptor <code>job-901</code> and wrote partial artifacts when the host crashed. <br>2. On startup <code>BootstrapStart</code> detects missing/unclean sentinel and runs <code>RecoverIfUncleanExit</code>. <br>3. <code>RecoverIfUncleanExit()</code> collects evidence: last two <code>audit_tail.csv</code> rotations, <code>ribbon-map.json</code> (previous + current), <code>job-901.json</code>, <code>modConfig</code> snapshot, and redacted memory snapshot. It computes sha256 checksums and writes <code>forensic_manifest.json</code>. <br>4. <code>RecoverIfUncleanExit()</code> uploads evidence atomically to secure evidence store and emits <code>bootstrap.recovery</code> with <code>forensicManifestUri</code>. Add-in enters restricted mode. <br>5. Operator retrieves evidence, runs <code>VerifyAuditChain</code> for the <code>lastCorrelationId</code>, inspects job descriptor <code>job-901</code>, and either requeues the job manually after verification or opens an incident for deeper SRE investigation. <br>6. Re-enable AUTO_APPLY or exports only after two-person approval recorded by <code>bootstrap.recover-approve</code> audit. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong>Concrete operational examples (short)</strong><br><strong>Example: BootstrapStart returns:</strong> <code>{ok:true, correlationId:&quot;r-20260117-0001&quot;, warnings:[]}</code>. Associated audit <code>bootstrap.started</code> appended. <br><strong>Example: Deferred failure audit:</strong> <code>{event: &quot;bootstrap.deferred.failed&quot;, correlationId: &quot;r-20260117-0001&quot;, task:&quot;LoadRibbonMap&quot;, attempt:3, errorCode:&quot;REG_DEFER_NET_504&quot;, evidenceRef:&quot;evi-az-001&quot;}</code>. <br><strong>Example: Forbidden API audit:</strong> <code>{event:&quot;bootstrap.forbiddenApi.detected&quot;, correlationId:&quot;r-20260117-0002&quot;, forbiddenSymbols:[&quot;WinHttpRequest&quot;], errorCode:&quot;REG_BOOTSTRAP_FORBIDDEN_API&quot;, evidenceRef:&quot;evi-xx-42&quot;}</code>. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong>Testing matrix (concrete)</strong><br><strong>Unit:</strong> <code>NewCorrelationId</code> uniqueness & deterministic checks; <code>InitMinimalState</code> deterministic outputs; <code>EmitBootstrapAudit</code> schema tests; <code>VerifyNoIOPolicy</code> static signature checks via injected modules. <br><strong>Integration:</strong> simulated host: <code>BootstrapStart</code> + deferred runner + <code>LoadRibbonMap</code> with mocked IO; verify chain <code>bootstrap.started</code> → <code>bootstrap.deferred.completed</code> → <code>ribbon.ready</code>. <br><strong>Golden:</strong> <code>testMode=true</code> run with fixed <code>seedOverride</code> and golden <code>ribbonMap.hash</code> parity check; <code>correlationId</code> deterministic match. <br><strong>Property & stress:</strong> parallel generation of correlation ids under load (10k calls) for uniqueness; forbidden-API injection tests to ensure detection. <br><strong>CI gates:</strong> static analysis for forbidden APIs, golden parity, <code>bootstrap.started</code> audit presence, SLO verification. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong>Appendices — schemas, error-codes, evidence manifests (references)</strong><br><strong>Bootstrap audit schema fields (canonical):</strong> <code>timestamp,correlationId,module=REG_Bootstrap,eventType,buildId,platform,metadataHash,prevHash,evidenceRef,configHash,notes</code>.<br><strong>Suggested error codes (examples):</strong> <code>REG_BOOTSTRAP_FORBIDDEN_API</code>, <code>REG_BOOTSTRAP_DEFER_SCHED_FAILED</code>, <code>REG_BOOTSTRAP_RECOVERY_EVIDENCE_FAIL</code>, <code>REG_BOOTSTRAP_SHUTDOWN_ERROR</code>, <code>REG_BOOTSTRAP_SYNC_IO_DETECTED</code>. Each maps to operator triage steps. <br><strong>Forensic manifest sample structure (fields):</strong> <code>forensicManifest:{id,createdTs,correlationId,lastSnapshotUri,auditTailUris,jobDescriptorUris,checksums,signatures,notes}</code>. Evidence store returns a signed URI for chain-of-custody. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong>Conceptual Power Query (PQ) guidance (integration with bootstrap & deferred init)</strong><br><strong>Purpose:</strong> PQ guidance here is conceptual: how the bootstrap + deferred init pattern integrates with embedding, validating, parameterizing, and injecting Power Query (M) templates in regulated environments. The guidance assumes manifests, <code>mChecksum</code> provenance, and strict audit linking (each PQ user action anchored to a <code>correlationId</code>).<br><strong>High-level invariants for PQ templates:</strong><br>1. <strong>mChecksum provenance:</strong> every M template has <code>mChecksum</code> (SHA256 of canonicalized M) stored in manifest and evidence. <code>LoadRibbonMap</code> and PQ template loader must verify <code>mChecksum</code> before injection. <br>2. <strong>Deferred template validation:</strong> heavy template validation (connecting to remote data sources or performing full refresh) must run in deferred worker; only a light-weight schema validation and checksum verification is allowed in deferred-init quick checks. <br>3. <strong>Hidden-sheet fallback:</strong> when external repo unreachable, prefer embedded templates in a hidden sheet for offline reliability; ensure embedded templates have <code>mChecksum</code> and owner attribution. <br>4. <strong>Injection safety:</strong> <code>PQ_Injector</code> should expose safe APIs like <code>Add_Query_From_M(name, formula, opts)</code> that perform parameter sanitization, redaction of credentials, and atomic add to <code>Workbook.Queries</code>. Always compute <code>paramsHash</code> and store sanitized evidence when injecting. <br><strong>Template lifecycle & audits:</strong> each template load/inject/preview should produce audit rows: <code>pq.template.loaded</code>, <code>pq.preview</code>, <code>pq.inject</code>, <code>pq.export</code>, each with <code>correlationId</code> and <code>mChecksum</code>. <br><strong>Parameterization & safe default rules:</strong> maintain canonical parameter mapping; sanitize user-supplied parameters (redact credentials), compute <code>paramsHash</code>, and store sanitized parameters in evidence store. Ensure <code>Add_Query_From_M</code> rejects or flags templates that embed raw credentials or connection strings; instead require parameterization with placeholders and secure connection creation APIs. <br><strong>Query folding & determinism:</strong> prefer query designs that fold to source for performance; however, folding depends on provider — do not rely on folding for determinism; always compute canonical transformed artifact <code>mChecksum</code> after template parameterization for auditability. <br><strong>Diagnostics & metrics for PQ:</strong> capture <code>pq.template.load.latency_ms</code>, <code>pq.refresh.duration_ms</code>, <code>pq.diagnostics/lastError</code>, and save detailed diagnostics to evidence store with <code>evidenceRef</code>. <br><strong>Safe injection examples (conceptual):</strong><br>- <em>Preview:</em> operator requests preview → <code>PQ_Injector</code> runs <code>SanitizeTemplate</code> → compute <code>previewM</code> and run limited preview in sandboxed runner (no credentials) → produce <code>pq.preview</code> audit with <code>mChecksum</code> and <code>previewChecksum</code>.<br>- <em>Inject:</em> operator confirms injection → <code>PQ_Injector</code> executes <code>Add_Query_From_M</code> within workbook context (deferred if heavy), adds connection optionally, emits <code>pq.inject</code> audit with <code>mChecksum</code> and <code>queryName</code>. <br><strong>Governance & operator flow:</strong> template modifications to regulated templates require PR + owners sign-off; production injection requires signed manifest and <code>pq.template.inject</code> audit. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong>Conceptual DAX guidance (design, determinism, performance, and audits)</strong><br><strong>Purpose:</strong> DAX guidance here is conceptual and practical patterns consistent with regulated, audited flows: deterministic measure behavior, testability, safe performance, and audit linkage. DAX is executed in workbook/Power BI engine context; bootstrap must not call DAX during startup but should ensure configuration, model versioning, and auditing are in place before exposing UI actions that result in DAX evaluation of regulated outputs. <br><strong>Design & determinism principles:</strong><br>1. <strong>Deterministic inputs:</strong> DAX measures should depend only on model data and explicit parameters; avoid reliance on volatile functions (e.g., <code>NOW()</code>, <code>RAND()</code>) for regulated outputs; if used, document and audit the use and provide deterministic test-mode overrides. <br>2. <strong>Versioned measures & migration:</strong> changes to measure logic must be tracked via <code>measureVersion</code> in manifest and produce migration manifests describing semantic change and test vectors (golden outputs). <br>3. <strong>Testing & golden parity:</strong> provide unit test harness that evaluates measures over canonical sample datasets and asserts measure outputs match golden checksums. <br>4. <strong>Context hygiene:</strong> explicit management of row context vs filter context; prefer <code>VAR</code> usage for readability and stable evaluation order; avoid older constructs like <code>EARLIER</code> when a clearer pattern exists. <br><strong>Performance & safety:</strong><br>- Avoid unbounded row iterators; use <code>SUMX</code> only over necessary, well-bounded tables; prefer <code>CALCULATE</code> + <code>FILTER</code> with indexed columns for better engine optimization. <br>- Avoid expensive nested <code>FILTER</code> over large tables when a pre-aggregated table or calculated column can solve the need. <br>- Use <code>KEEPFILTERS</code>/<code>REMOVEFILTERS</code> intentionally and document expected semantics. <br><strong>Security & PII:</strong> DAX measures that produce PII must be restricted to contexts with approval; DAX debug outputs must be redacted in evidence stores. <br><strong>Audit integration:</strong> model operations that run DAX measures producing artifacts must include audit rows: <code>dax.evaluate</code> (with <code>correlationId</code>, <code>measureName</code>, <code>modelVersion</code>, <code>paramsHash</code>, <code>resultHash</code>) and evidenceRefs for full sanitized result sets. For scheduled or automated DAX runs used in regulated flows, persist job descriptors and follow job scheduling and job persistence rules identical to other heavy jobs. <br><strong>Practical DAX patterns (conceptual snippets in words):</strong><br>- <strong>Measure with VAR and deterministic fallback:</strong> declare <code>VAR</code> inputs, compute intermediate aggregates once, return final expression; provide deterministic <code>testSeed</code> param for any necessary sampling. <br>- <strong>Time intelligence:</strong> prefer <code>DATESBETWEEN</code>/<code>TOTALYTD</code> with explicit <code>yearEndDate</code> parameters for deterministic behavior across locales and fiscal calendars; store <code>fiscalCalendar</code> configuration in <code>modConfig</code> and include <code>configHash</code> in audits. <br>- <strong>Iterator containment:</strong> limit <code>SUMX</code> iterators to <code>TOPN</code>/filtered set to reduce engine pressure and ensure bounded evaluation. <br><strong>Examples & narratives (conceptual):</strong><br>1. <em>Measure change & migration:</em> team changes <code>RevenueNet</code> measure semantics (excluded discount logic). Submit PR with <code>migration_manifest.json</code> describing sample inputs and golden outputs. CI runs golden tests and <code>dax.evaluate</code> audits for sample dataset; after sign-off, change is released. <br>2. <em>Regulated report generation:</em> operator triggers "Regulatory Summary" which runs a set of DAX measures over certified model version. <code>NewCorrelationId()</code> created, job persisted (if heavy) and <code>dax.evaluate</code> audits produced with <code>resultHash</code> referencing sanitized result stored in evidence store. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong>Cross-cutting examples showing PQ + DAX + Bootstrap integration</strong><br><strong>End-to-end narrative (conceptual, regulated flow):</strong><br>1. Operator chooses a canonical PQ template "LoadCustomerBalances" from the PQ library; UI action <code>dq_profile.load_template</code> anchored to correlation <code>r-20260117-0100</code>. <code>pq.preview</code> audit produced with <code>mChecksum</code>. <br>2. Operator injects the query into workbook; <code>pq.inject</code> audit produced with <code>mChecksum</code> and <code>queryName</code>. The injected query produces a table <code>tblCustomerBalances</code>. <br>3. The operator triggers "Compute Regulatory Summary" which runs a set of DAX measures on <code>tblCustomerBalances</code>. Because the measures are heavy, the add-in persists a job via job scheduler: <code>job.persisted:job-033</code> with <code>correlationId=r-20260117-0100.child-1</code>. <br>4. Worker loads dataset snapshot (redacted), uses deterministic RNG seeds from bootstrap <code>bootstrapState.seedFingerprint</code>, evaluates DAX measures in isolated host (or a headless engine), computes <code>resultHash</code>, emits <code>dax.evaluate</code> audits and <code>dq_export</code> of result artifacts with atomic write and checksum. <br>5. Audit chain: <code>bootstrap.started</code> → <code>pq.inject</code> → <code>UserAction</code> → <code>job.persisted</code> → <code>dax.evaluate</code> → <code>dq_export</code>. Evidence references and <code>forensic_manifest</code> allow reproducible trace. <br><strong>Governance & acceptance:</strong> regulated outputs require two-person approval if measures changed since last certified run; golden parity tests ensure reproducibility of exported artifacts. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong>Operator UX, messaging & triage hints (concise)</strong><br>- Always present <code>correlationId</code> in operator messages (copyable). <br>- Short user-facing messages without PII, e.g.: "Startup limited — ref r-20260117-0002. Open Diagnostics." <br>- For deferred failures: "Feature degraded — manifest load failed (ref r-20260117-0001). Retry or contact ops." <br>- Triage quick steps: tail <code>audit_tail.csv</code> for <code>correlationId</code>, fetch <code>forensic_manifest</code> referenced by audit, validate <code>ribbonMap.hash</code> and job descriptors, run <code>VerifyAuditChain</code>. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong>Acceptance criteria (dev/CI) — final checklist</strong><br>1. Unit + integration + golden tests pass. <br>2. No forbidden API references in static analysis. <br>3. <code>bootstrap.started</code> audit emitted on sample run and <code>bootstrap.deferred.completed</code> follows in deferred runner. <br>4. Deterministic <code>NewCorrelationId</code> behavior in <code>testMode</code>. <br>5. Evidence export path for recovery validated and signed. <br>6. Shutdown handlers flush audit buffers and snapshot persisted atomically. <br><strong>If any fails:</strong> block release; produce <code>forensic_manifest</code> artifact for CI triage and fix until gates pass. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong>Appendix: short checklist for implementers</strong><br>1. Keep bootstrap minimal — no IO. <br>2. Defer everything heavy and make it idempotent & cancellable. <br>3. Emit audit rows for every critical transition with <code>correlationId</code>. <br>4. Use deterministic seeding in testMode for CI/golden runs. <br>5. Policy overriders require signed manifests & two-person approval. <br>6. Persist evidence atomically and sign <code>forensic_manifest</code> for chain-of-custody. </td></tr><tr><td data-label="REG_Bootstrap — Per-function Expert Technical Breakdown"> <strong>Closing (operational note)</strong><br>This module specification is intentionally exhaustive: it balances safety (non-IO UI path), observability (canonical audits and evidence), determinism (testMode seeds, golden tests), and governance (signed manifests and two-person approvals). Follow the invariants strictly: minimal hot-path, deferred IO, audit anchoring, deterministic seeds for CI, and conservative recovery. </td></tr></tbody></table></div><div class="row-count">Rows: 26</div></div><script src="assets/xlsx.full.min.js?v=1758605028" defer></script>
<script src="assets/script.js?v=1759748863" defer></script>
<script src="assets/worker.js?v=1758331710" defer></script>
<script>
(function(){
  const template = "{table}_{date}";
  const userVal = "";
  const hrefPrefix = "assets";
  function formatName(tableName) {
    const date = (new Date()).toISOString().slice(0,10);
    return template.replace('{table}', tableName).replace('{date}', date).replace('{user}', userVal);
  }
  const btn = document.getElementById('exportBtn');
  if(btn) {
    btn.addEventListener('click', async function() {
      try {
        const html = document.documentElement.outerHTML;
        if(html.length > 2000000) { alert('Export refused: html too large'); return; }
        if(window.Worker) {
          const workerUrl = (function(){ try{ return hrefPrefix + '/worker.js'; }catch(e){ return null; } })();
          if(workerUrl) {
            try {
              const worker = new Worker(workerUrl);
              worker.postMessage({html: html, format: 'pdf'});
              worker.onmessage = function(e) { console.log('worker:', e.data); alert('Worker replied: '+(e.data.msg||e.data.status)); };
            } catch(err) { console.warn('Worker create failed', err); alert('Worker not available'); }
          } else { alert('Worker not available'); }
        } else { alert('Export worker not supported in this environment.'); }
      } catch(err) { console.warn('Export failed', err); alert('Export worker not available. See console for details.'); }
    });
  }
})();
window.addEventListener('load', function(){ try{ document.querySelectorAll('.table-wrapper').forEach(function(e){ e.style.opacity='1'; }); }catch(e){} });
</script>
</div>
</body>
</html>