<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='utf-8'><meta name='viewport' content='width=device-width,initial-scale=1'>
<link rel="stylesheet" href="assets/style.css?v=1758336167">
<link rel="stylesheet" href="assets/overrides.css?v=1758336167">
</head><body>
<div id="tables-viewer" role="application" aria-label="Tables viewer">
<div id="stickyMainHeader">
<div id="tv-header"><div><h1>Tables Viewer v2.1</h1></div><div style="display:flex;gap:8px;align-items:center;"><input id="searchBox" type="search" placeholder="Search" aria-label="Search tables" style="min-width:420px; width:44ch;"/><button id="modeBtn" onclick="toggleMode()" aria-label="Toggle theme">Theme</button><button id="toggleAllBtn" aria-label="Toggle all" onclick="toggleAllTables()">Collapse All Tables</button><button onclick="copyAllTablesPlain()">Copy All Tables (Plain Text)</button><button onclick="copyAllTablesMarkdown()">Copy All Tables (Markdown)</button><button onclick="resetAllTables()">Reset All Tables</button></div></div>
<noscript><div style='color:#b91c1c'>JavaScript is disabled. Tables will be shown statically. For large tables enable JS for virtualization.</div></noscript>
<div id="tocBar" role="navigation" aria-label="Table of contents"><ul><li class="toc-item"><a class="toc-link" href="#Table1-title">Table 1</a></li>
<li class="toc-item"><a class="toc-link" href="#Table2-title">Table 2</a></li>
<li class="toc-item"><a class="toc-link" href="#Table3-title">Table 3</a></li>
<li class="toc-item"><a class="toc-link" href="#Table4-title">Table 4</a></li>
<li class="toc-item"><a class="toc-link" href="#Table5-title">Table 5</a></li>
<li class="toc-item"><a class="toc-link" href="#Table6-title">Table 6</a></li>
<li class="toc-item"><a class="toc-link" href="#Table7-title">Table 7</a></li></ul></div></div>
<div class="table-wrapper" id="Table1"><h3 id="Table1-title">Table 1</h3><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button onclick="copyTablePlain(this)">Copy Plain Table</button><button onclick="copyTableMarkdown(this)">Copy Markdown Table</button></div><div style="display:flex; align-items:center;"><button class="toggle-table-btn" onclick="toggleTable(this)">Collapse Table</button></div></div><table><thead><tr>
        <th style="width:28.57%;" onclick="sortTableByColumn(0,0)" role="button" aria-label="Sort by **Summary / Core Concepts**">
          <div class="th-with-sort">
            <div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>Summary / Core Concepts</strong></div>
            <button class="sort-btn sort-state-0" onclick="event.stopPropagation(); headerSortButtonClicked(0,0, this)" title="Toggle sort" aria-label="Toggle sort">
              <span class="sort-icon" aria-hidden="true"></span>
            </button>
          </div>
        </th>
        <th style="width:71.43%;" onclick="sortTableByColumn(0,1)" role="button" aria-label="Sort by **Supporting Details / Examples**">
          <div class="th-with-sort">
            <div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>Supporting Details / Examples</strong></div>
            <button class="sort-btn sort-state-0" onclick="event.stopPropagation(); headerSortButtonClicked(0,1, this)" title="Toggle sort" aria-label="Toggle sort">
              <span class="sort-icon" aria-hidden="true"></span>
            </button>
          </div>
        </th></tr></thead><tbody><tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Power and Human Fallibility in Chess</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Garry Kasparov’s 1997 matches with Deep Blue highlighted how human psychology affects interaction with machines. Kasparov tried to intimidate Deep Blue by tapping his watch to signal confidence and boredom, illustrating psychological gamesmanship. Deep Blue’s designers embedded subtle randomness in moves, making it appear human-like and unpredictable. Kasparov overestimated his ability to manipulate a machine psychologically, resulting in a 3½–2½ defeat. This demonstrated that algorithms could exploit human behavioral biases and that human misjudgment, not just computational strength, can determine outcomes.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Algorithm Definition and Everyday Relevance</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Algorithms are formal step-by-step procedures for solving problems or achieving goals. They are used in daily life: recipes, assembly instructions like IKEA manuals, YouTube tutorial sequences, and financial calculations. In computing, algorithms translate abstract problem-solving logic into precise operations a machine can execute. Beyond computing, algorithms structure decisions in medicine (diagnosis), logistics (delivery routing), and law (risk assessment), showing their pervasive influence.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Four Main Algorithm Tasks</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><ol><li><strong>Prioritization:</strong> Algorithms rank items or make decisions based on importance. Examples include Google Search ranking, Netflix recommendation systems, TomTom GPS routing, and Deep Blue selecting chess moves. <p></p>2. <strong>Classification:</strong> Sorting data into categories. Used in social media moderation (Facebook detecting harmful posts), YouTube content classification, OCR (handwriting recognition), and spam filtering. <p></p>3. <strong>Association:</strong> Identifying relationships between items. Examples include Amazon’s product recommendations, OKCupid’s dating suggestions, and LinkedIn’s professional networking recommendations. <p></p>4. <strong>Filtering:</strong> Selecting relevant information from large datasets. Examples include Siri/Alexa interpreting spoken commands, social media feeds personalized via filtering, and UberPool matching riders based on routes and timing. UberPool is an example of a system that simultaneously uses prioritization, association, and filtering for operational efficiency.</li></ol></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Types of Algorithms</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p><strong>Rule-Based Algorithms:</strong> Operate on explicit instructions, easy to understand, predictable, but limited by human knowledge and rules. Examples: tax calculators, basic route planners, chess engines using fixed heuristics. <br> <strong>Machine Learning Algorithms:</strong> Learn patterns from data, adjust outputs dynamically, capable of handling complex problems. Examples: Google Translate, autonomous vehicle navigation, image recognition systems. Caveat: ML models are opaque, often making decisions that humans cannot easily interpret (“black box problem”). Illustrative issue: altering one pixel in an image can cause misclassification in ML models.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Over-trusting Algorithms (Blind Faith)</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Humans often overestimate algorithmic reliability. Example: Robert Jones followed GPS directions blindly and nearly drove off a cliff. Search engine bias experiments (Kadoodle) showed that algorithmic ranking can subtly influence public opinion and voting behavior. People assume algorithms are objective and superior to human judgment, ignoring that algorithms encode human biases and can produce unexpected errors. In professional settings, over-trust can lead to poor medical decisions, financial misjudgments, or operational failures.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Poor Algorithmic Design and Consequences</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Algorithms are only as good as their design, data, and monitoring. Example: Idaho Medicaid used a faulty Excel spreadsheet to allocate benefits. The buggy algorithm resulted in arbitrary benefit denials and wrongful financial decisions. Humans trusted the system blindly, demonstrating a lack of oversight. Legal consequences followed (ACLU lawsuit), revealing how automated decision-making without human checks can violate fairness and constitutional rights. This highlights the importance of auditing algorithms, validating data, and embedding human oversight mechanisms.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>When Humans Must Overrule Algorithms</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Humans should act as final arbiters in critical decisions. <br> <strong>Positive Example:</strong> Stanislav Petrov ignored a false Soviet nuclear alert in 1983, preventing a possible nuclear escalation. <br> <strong>Negative Example:</strong> Alton Towers rollercoaster disaster occurred when engineers manually overrode a safety algorithm, causing injuries. These cases underscore that human intervention must be judicious, informed, and cautious: over-reliance or reckless overrides can both be catastrophic.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Algorithm vs Human Reliability</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Research demonstrates that algorithms often outperform humans in prediction tasks. Paul Meehl <br>(1954) showed simple statistical algorithms predict outcomes (medical, psychological, and financial) more accurately than experts relying on intuition. Yet humans exhibit <strong>algorithm aversion</strong>, disproportionately reacting to rare algorithm errors while ignoring frequent human mistakes. Practical examples include Citymapper and Waze for navigation, where algorithmic suggestions are consistently more accurate than human route selection. The lesson: humans must calibrate trust and remain aware of cognitive biases when evaluating algorithm outputs.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Psychological Interactions and Human Bias</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Algorithms exploit cognitive biases, such as anchoring, confirmation bias, and overconfidence. For instance, chess engines exploit human predictability by choosing suboptimal but psychologically confusing moves. Social media algorithms use engagement heuristics that reinforce confirmation bias, leading to echo chambers. Algorithms can subtly manipulate choices, from online shopping to political opinions, emphasizing the ethical need for transparency, user awareness, and consent.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Balanced Approach to Algorithms</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Optimal use requires neither blind trust nor excessive skepticism. Awareness of algorithmic limitations, human cognitive biases, and ethical considerations is critical. Critical evaluation includes checking for errors, validating datasets, understanding algorithmic objectives, and combining human judgement with automated assistance. This hybrid approach is especially important in healthcare, finance, legal decisions, autonomous vehicles, and social media content moderation.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Case Studies and Lessons</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><ol><li><strong>Deep Blue vs Kasparov:</strong> Machines exploiting human psychological weaknesses, not just computational power. <p></p>2. <strong>Robert Jones GPS near-miss:</strong> Over-trust in navigation systems. <p></p>3. <strong>Idaho Medicaid algorithm:</strong> Poor design, human blind trust, legal consequences. <p></p>4. <strong>Stanislav Petrov nuclear alert:</strong> Critical human intervention preventing disaster. <p></p>5. <strong>Alton Towers rollercoaster crash:</strong> Reckless override of safety algorithms causing harm. <p></p>6. <strong>Machine learning misclassification (image pixel example):</strong> Shows black box vulnerability and fragility of ML systems. <p></p>7. <strong>Kadoodle search bias experiment:</strong> Algorithmic influence on voter behavior and societal outcomes.</li></ol></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Ethical and Societal Implications</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>The rise of algorithms raises ethical questions: fairness, transparency, accountability, and privacy. Automated systems affect life-and-death decisions (medical diagnosis, autonomous cars), resource distribution (social services, finance), and societal perception (news feeds, search engine results). Over-trusting or misusing algorithms can reinforce inequality and bias. Regulatory frameworks, algorithmic audits, and informed human oversight are essential to mitigate these risks.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Recommendations for Human-Algorithm Interaction</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><ol><li>Verify algorithm outputs with human review in critical areas. <p></p>2. Avoid blind reliance; understand algorithmic limitations. <p></p>3. Encourage transparency: explainability of decisions. <p></p>4. Use algorithms for augmentation, not replacement of human judgement. <p></p>5. Embed auditing, feedback loops, and fail-safe mechanisms. <p></p>6. Educate users and professionals about algorithmic biases and reliability issues. <p></p>7. In high-stakes scenarios (nuclear alerts, medical interventions), prioritize layered human-algorithm checks.</li></ol></td></tr>
</tbody></table></div>
  <div class='row-count'></div>
</div>

<div class="table-wrapper" id="Table2"><h3 id="Table2-title">Table 2</h3><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button onclick="copyTablePlain(this)">Copy Plain Table</button><button onclick="copyTableMarkdown(this)">Copy Markdown Table</button></div><div style="display:flex; align-items:center;"><button class="toggle-table-btn" onclick="toggleTable(this)">Collapse Table</button></div></div><table><thead><tr>
        <th style="width:28.57%;" onclick="sortTableByColumn(1,0)" role="button" aria-label="Sort by **Summary / Core Concepts**">
          <div class="th-with-sort">
            <div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>Summary / Core Concepts</strong></div>
            <button class="sort-btn sort-state-0" onclick="event.stopPropagation(); headerSortButtonClicked(1,0, this)" title="Toggle sort" aria-label="Toggle sort">
              <span class="sort-icon" aria-hidden="true"></span>
            </button>
          </div>
        </th>
        <th style="width:71.43%;" onclick="sortTableByColumn(1,1)" role="button" aria-label="Sort by **Supporting Details / Examples**">
          <div class="th-with-sort">
            <div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>Supporting Details / Examples</strong></div>
            <button class="sort-btn sort-state-0" onclick="event.stopPropagation(); headerSortButtonClicked(1,1, this)" title="Toggle sort" aria-label="Toggle sort">
              <span class="sort-icon" aria-hidden="true"></span>
            </button>
          </div>
        </th></tr></thead><tbody><tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Zuckerberg and early Facebook data collection</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>In 2004, Mark Zuckerberg boasted in a private chat about having 4,000 emails, pictures, and addresses from Harvard students, implying people “trust” him and submitted their data freely.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Data trade and user awareness</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Users exchanged personal data for access to Facebook’s network and features. The author notes users often underestimate the long-term value and implications of their shared data.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Tesco Clubcard: early example of data-driven marketing</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Launched in 1993 by Edwina Dunn and Clive Humby; tracked transactions to gather name, address, and purchase info. Insights included: high-value loyal customers, travel distances, competitor influence, shopping patterns. Coupon campaigns increased spending by 4% among Clubcard users.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Sensitive use of personal data</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Example of online shopping: a woman’s “My Favourites” list revealed condoms, leading Tesco to apologize and remove items to avoid personal embarrassment. Eric Schmidt’s “creepy line” principle referenced.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Target’s predictive analytics</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>In 2002, Target identified pregnancy signals from purchasing patterns (e.g., body lotion, vitamins). Algorithm predicted pregnancy timing, allowing targeted coupons. Caused controversy when a father realized ads alerted him to his daughter’s pregnancy. Target mitigated by mixing generic ads with pregnancy-related offers.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Home cooks example in insurance</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Insurance company inferred responsible home cooks from grocery purchases (fresh fennel), linking behavior to lower claim rates, showing offline data profiling.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Rise of data brokers</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Companies like Palantir, Acxiom, Corelogic, Datalogix collect, aggregate, and sell personal data from multiple sources: online forms, shopping, social media, subscriptions. Data is cross-referenced to build detailed digital profiles including health, finance, habits, and behavior.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Targeted online advertising</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Using brokered data, advertisers can match user preferences for micro-targeted ads (e.g., luxury travel company Fry’s). Tracking cookies follow users across websites to deliver personalized adverts.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Ethical and exploitative concerns</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Ads can exploit vulnerabilities: pregnancy loss targeting, payday loans, biased executive job adverts. Algorithms may discriminate based on perceived traits, including ethnicity.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Cambridge Analytica and psychometrics</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Used Facebook Likes to infer personality traits (openness, conscientiousness, extraversion, agreeableness, neuroticism). Personality-targeted ads achieved higher click-throughs and purchases, though absolute effects were small. Applied for political persuasion (Trump campaign) via micro-targeted messaging.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Micro-manipulation effects</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Facebook emotional contagion experiment <br>(2013) showed small changes in newsfeed content influenced user emotions. Targeted ads increase engagement marginally (e.g., 11 → 16 clicks per 1,000), but even tiny effects could influence tight elections.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Legal and regulatory gaps</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Data brokers are largely unregulated in the US. Attempts to curb practices (FCC rules) were reversed in 2017, leaving users largely exposed.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>De-anonymization of browser data</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>German researchers bought 3M anonymized browser histories and used public traces to re-identify users, including politicians, police officers, and judges. Demonstrates how “anonymous” data can reveal sensitive personal information.</p></td></tr>
</tbody></table></div>
  <div class='row-count'></div>
</div>

<div class="table-wrapper" id="Table3"><h3 id="Table3-title">Table 3</h3><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button onclick="copyTablePlain(this)">Copy Plain Table</button><button onclick="copyTableMarkdown(this)">Copy Markdown Table</button></div><div style="display:flex; align-items:center;"><button class="toggle-table-btn" onclick="toggleTable(this)">Collapse Table</button></div></div><table><thead><tr>
        <th style="width:28.57%;" onclick="sortTableByColumn(2,0)" role="button" aria-label="Sort by **Summary / Core Concepts**">
          <div class="th-with-sort">
            <div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>Summary / Core Concepts</strong></div>
            <button class="sort-btn sort-state-0" onclick="event.stopPropagation(); headerSortButtonClicked(2,0, this)" title="Toggle sort" aria-label="Toggle sort">
              <span class="sort-icon" aria-hidden="true"></span>
            </button>
          </div>
        </th>
        <th style="width:71.43%;" onclick="sortTableByColumn(2,1)" role="button" aria-label="Sort by **Supporting Details / Examples**">
          <div class="th-with-sort">
            <div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>Supporting Details / Examples</strong></div>
            <button class="sort-btn sort-state-0" onclick="event.stopPropagation(); headerSortButtonClicked(2,1, this)" title="Toggle sort" aria-label="Toggle sort">
              <span class="sort-icon" aria-hidden="true"></span>
            </button>
          </div>
        </th></tr></thead><tbody><tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Zilly’s Case: Algorithmic Override</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Zilly’s sentence doubled from 1 year in county jail to 2 years in state prison after judges relied on COMPAS risk score rather than the plea bargain. Raises concerns about over-reliance on predictive algorithms with only \~70% accuracy.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Brooks Case: Age Factor in Sentencing</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Christopher Drew Brooks <br>(19) convicted of statutory rape of 14-year-old. Initial sentencing range 7–16 months. After inclusion of risk score (non-COMPAS), upper limit raised to 24 months; sentenced to 18 months. Younger age counted against him, older offenders would have been considered lower risk.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Judicial Discretion vs Algorithm Reliance</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Wisconsin Supreme Court: judges expected to exercise discretion. Richard Berk: judges may rely on algorithm to avoid mistakes and reduce personal accountability. Algorithm acts as a “shortcut” for complex decision-making.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Verification Problem</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Impossible to verify algorithm predictions. High-risk designation may be self-fulfilling (e.g., prison placement affecting behavior). Judges cannot know if algorithm predictions are accurate in practice.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Machine Bias in COMPAS</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>ProPublica reverse-engineered COMPAS predictions for 7,000+ Florida offenders (2013–14). Findings: Algorithm produced similar overall accuracy for Black vs White defendants, but errors were racially skewed. False positives disproportionately Black; false negatives disproportionately White.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Fairness Challenges</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Unbiased prediction vs equal error rates: mathematically incompatible when groups have different base rates. Example: Murder prediction algorithm; 96% of murderers are male, algorithm with 75% accuracy flags more men, creating unavoidable bias in false positives.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Historical Societal Bias in Data</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>African Americans overrepresented in arrest records due to systemic inequalities. Example: marijuana usage equal among races, but Black arrest rates up to 8x higher. Algorithm amplifies disparities in historical data, judging on consequences of inequality rather than race.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Potential Algorithmic Adjustment</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Algorithms can be tweaked to correct for historical bias. Analogy: Google Images search for “maths professor” shows mostly white men; could adjust results to reflect societal aspirations rather than historical data. Justice algorithms can similarly be adjusted for fairness incrementally.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>System 1 vs System 2 Thinking</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Kahneman’s model: System 1 = fast, intuitive, error-prone; System 2 = analytic, deliberate, slow but lazy. Judges often rely on System 1, rationalized by System <br>2. Example: Bat and ball puzzle—most judges intuitively say 10p instead of correct 5p.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Anchoring Effect in Judicial Decisions</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Judges influenced by prosecutor recommendations, prior case outcomes, external cues. Example: Journalist suggesting “3 years” as sentence influences judge. Dice rolls can also influence decisions. Anchoring alters perception of appropriate punishment.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Weber’s Law and Sentencing</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Judges perceive increments in long sentences logarithmically; small increases feel negligible. Study (UK & Australia, 100,000+ sentences) found 99% aligned with Weber’s Law: 3 months added to a 20-year sentence may result in jumping to 25 years.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Other Cognitive Biases</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Judges with daughters favor women; local sports outcomes affect bail decisions; time-of-day effects (pre-lunch = less likely to grant bail); sequence effects (prior successful cases reduce chance of bail); drink temperature may influence perceived warmth of strangers.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Algorithm as Corrective Tool</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Structured, well-designed algorithms can reduce both systematic bias and random error inherent in human judges. Offenders often prefer human intuition, but research suggests reasoned, analytic decisions are superior.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Regulation and Oversight</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Call for an algorithmic “FDA” to approve accuracy, consistency, and fairness. Investigative journalism (e.g., ProPublica) holds private companies accountable. Transparency and regulation crucial to ethical algorithm use.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Decision-Making in Justice</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Algorithms require explicit goals: What should justice achieve? Decisions guided by data rather than flawed intuition. Forces society to define what constitutes fairness and appropriate risk mitigation.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Human vs Algorithm: Comparison</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Not perfect human vs perfect algorithm, but flawed human vs imperfect algorithm. Algorithms reduce systematic bias, random error, and inconsistencies that humans cannot articulate or control. Properly designed algorithms act as a support system for judicial decision-making.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Tensions and Ethical Considerations</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Algorithms raise moral questions: Should risk assessment influence freedom? How to balance fairness with predictive accuracy? They also force debate about human cognitive limitations and societal expectations for justice.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Applications Beyond Criminal Justice</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>In lower-stakes domains, algorithms can provide clearer benefits, where objectives and outcomes are less morally fraught. Algorithmic decision-making in less critical areas allows society to refine methods and oversight.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Conclusion: Purposeful Design</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Effective justice algorithms must be designed with clear, explicit goals, awareness of human limitations, and proper oversight. They require public debate to define objectives, rather than blind reliance on private company models. Well-regulated, transparent, and thoughtfully designed algorithms can improve fairness and reduce error in justice.</p></td></tr>
</tbody></table></div>
  <div class='row-count'></div>
</div>

<div class="table-wrapper" id="Table4"><h3 id="Table4-title">Table 4</h3><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button onclick="copyTablePlain(this)">Copy Plain Table</button><button onclick="copyTableMarkdown(this)">Copy Markdown Table</button></div><div style="display:flex; align-items:center;"><button class="toggle-table-btn" onclick="toggleTable(this)">Collapse Table</button></div></div><table><thead><tr>
        <th style="width:28.57%;" onclick="sortTableByColumn(3,0)" role="button" aria-label="Sort by **Summary / Core Concepts**">
          <div class="th-with-sort">
            <div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>Summary / Core Concepts</strong></div>
            <button class="sort-btn sort-state-0" onclick="event.stopPropagation(); headerSortButtonClicked(3,0, this)" title="Toggle sort" aria-label="Toggle sort">
              <span class="sort-icon" aria-hidden="true"></span>
            </button>
          </div>
        </th>
        <th style="width:71.43%;" onclick="sortTableByColumn(3,1)" role="button" aria-label="Sort by **Supporting Details / Examples**">
          <div class="th-with-sort">
            <div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>Supporting Details / Examples</strong></div>
            <button class="sort-btn sort-state-0" onclick="event.stopPropagation(); headerSortButtonClicked(3,1, this)" title="Toggle sort" aria-label="Toggle sort">
              <span class="sort-icon" aria-hidden="true"></span>
            </button>
          </div>
        </th></tr></thead><tbody><tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Commercial genetic testing treats individuals as products.</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>23andMe and similar companies offer genetic testing ostensibly for personal insight, but the primary business model is <strong>monetizing the collected DNA data</strong>. Customers provide highly personal genetic information; if consent is given (80% of customers do), 23andMe sells anonymized versions to research partners, including academics, pharmaceutical companies, and non-profits, for profit^[56](#endnotes.xhtml#ch04<em>56). As a 23andMe board member told <em>Fast Company</em>: “The long game here is not to make money selling kits, although the kits are essential to get the base level data.” This illustrates the central economic reality: the <strong>user is the product</strong>^[57](#endnotes.xhtml#ch04</em>57).</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Anonymity of genetic data is limited.</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Claims of anonymity in genetic data are <strong>highly questionable</strong>. In 2005, a man conceived via an anonymous sperm donor successfully identified his birth father through DNA analysis^[58](#endnotes.xhtml#ch04\<em>58; #59). A 2013 academic study showed that <strong>millions of individuals could be potentially identified</strong> with publicly available genomic data and simple computer techniques^[60](#endnotes.xhtml#ch04</em>60). These examples challenge the assumption that genetic data can be safely anonymized.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Genetic testing has insurance and legal implications.</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Legal protections against genetic discrimination exist but are <strong>not universal</strong>. In the US, life insurers can inquire about DNA tests that predict disease risk (e.g., Parkinson’s, Alzheimer’s, breast cancer) and may deny coverage based on the results. In the UK, insurers can consider results for Huntington’s disease if coverage exceeds £500,000^[61](#endnotes.xhtml#ch04&#95;61). Attempting to lie about testing can <strong>invalidate insurance policies</strong>, making abstention the only fully safe option. This introduces a <strong>trade-off between curiosity and personal risk management</strong>.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>High societal value of large genomic datasets.</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Sequenced genomes from millions of people represent an <strong>unprecedented resource for medical research</strong>. Large datasets enable studies on hereditary causes of diseases, identification of new drug targets, and better treatment strategies for conditions such as Parkinson’s. Participation is voluntary but widespread: 23andMe has over 2 million genotyped customers^[62](#endnotes.xhtml#ch04&#95;62), while MyHeritage, Ancestry.com, and the National Geographic Genographic Project contribute additional millions. These datasets are used with and without advanced algorithms to detect patterns impacting public health.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Ethical tension: individual vs population interests in medical AI.</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>AI diagnostic machines can have <strong>different objectives depending on design</strong>. A machine serving an individual might over-prescribe interventions (X-rays, blood tests, antibiotics) to maximize comfort, even if benefits are minor. A machine serving the population would consider <strong>antibiotic resistance, resource scarcity, and long-term outcomes</strong>, potentially recommending minimal interventions and conservative treatment. These trade-offs illustrate <strong>tensions between individual welfare and collective benefit</strong>.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Healthcare AI incentives vary by stakeholder.</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Different actors in healthcare impose subtle <strong>incentive biases</strong>. NHS or insurance-focused AIs may prioritize <strong>cost minimization</strong>; pharmaceutical-focused AIs may promote specific medications over others. Even though the <strong>ultimate goal—patient health—aligns broadly</strong>, priorities diverge in practical decision-making, affecting tests, treatments, and resource allocation.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Privacy vs public good trade-offs are complex.</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Algorithms and AI systems create <strong>multi-dimensional trade-offs</strong>: privacy vs benefit, individual vs collective, short-term comfort vs long-term outcomes. Risks are compounded when incentives are <strong>hidden, overstated, or misrepresented</strong>, making transparency and informed consent critical. Public perception may not align with actual ethical and societal implications.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>People’s priorities influence participation in genetic testing.</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Individuals rarely weigh societal benefits heavily when deciding on genetic tests. Many participate for <strong>personal curiosity</strong>, e.g., discovering ancestral composition (“25% Viking”), rather than contributing to research. The <strong>market response</strong>, with millions voluntarily sharing DNA, illustrates that personal satisfaction often outweighs privacy concerns.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>AI and algorithms require careful ethical design.</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Across domains—healthcare, criminal justice, insurance—<strong>algorithmic decision-making carries inherent trade-offs</strong>. Designers must balance conflicting objectives, considering: privacy, fairness, long-term impacts, population vs individual needs, and resource allocation. Ethical design requires clarity about who benefits, who may be harmed, and what incentives are at play.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Genomic datasets are valuable but ethically complex.</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>While there is no dataset more valuable for understanding human health than millions of sequenced genomes, participation <strong>is voluntary and involves privacy trade-offs</strong>. Millions contribute for altruistic or curiosity-driven reasons, yet each contribution carries potential risks for identity exposure, insurance discrimination, or commercial exploitation. This underscores the <strong>ethical complexity</strong> of genomic research at scale.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Healthcare decisions highlight systemic incentives.</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Even in ostensibly aligned systems like healthcare, stakeholders have subtly different priorities. Decisions about tests, prescriptions, and treatment plans may differ when AI serves: individual patients, insurers, the population, or pharmaceutical companies. Understanding these incentive structures is essential for evaluating <strong>algorithmic fairness and societal benefit</strong>.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Trade-offs extend beyond healthcare.</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>The challenges discussed (privacy, incentives, collective vs individual benefit) are applicable in broader contexts: criminal justice algorithms, insurance risk assessments, and commercial data collection. Ethical, transparent design is crucial to prevent <strong>hidden exploitation and skewed decision-making</strong>.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Individual choice vs collective outcomes.</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Personal decisions—whether to undergo genetic testing or rely on AI-guided healthcare—illustrate the broader tension between <strong>individual preferences</strong> and <strong>societal goals</strong>. Machines and algorithms may prioritize different objectives than humans, raising questions about consent, transparency, and long-term societal impact.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Market participation reflects social attitudes.</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>The widespread adoption of commercial DNA testing (23andMe, MyHeritage, Ancestry.com, National Geographic) indicates a societal willingness to trade privacy for information and entertainment. This raises questions about <strong>whether societal benefits from shared data justify individual privacy risks</strong>.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Summary insight: ethical vigilance is needed.</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Ultimately, the text emphasizes that <strong>ethical, technical, and policy vigilance</strong> is required when deploying AI, collecting genetic data, or designing systems with trade-offs between individuals and populations. Understanding incentives, risks, and benefits is essential for responsible innovation in healthcare and beyond.</p></td></tr>
</tbody></table></div>
  <div class='row-count'></div>
</div>

<div class="table-wrapper" id="Table5"><h3 id="Table5-title">Table 5</h3><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button onclick="copyTablePlain(this)">Copy Plain Table</button><button onclick="copyTableMarkdown(this)">Copy Markdown Table</button></div><div style="display:flex; align-items:center;"><button class="toggle-table-btn" onclick="toggleTable(this)">Collapse Table</button></div></div><table><thead><tr>
        <th style="width:28.57%;" onclick="sortTableByColumn(4,0)" role="button" aria-label="Sort by Summary / Core Concepts">
          <div class="th-with-sort">
            <div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Summary / Core Concepts</div>
            <button class="sort-btn sort-state-0" onclick="event.stopPropagation(); headerSortButtonClicked(4,0, this)" title="Toggle sort" aria-label="Toggle sort">
              <span class="sort-icon" aria-hidden="true"></span>
            </button>
          </div>
        </th>
        <th style="width:71.43%;" onclick="sortTableByColumn(4,1)" role="button" aria-label="Sort by Supporting Details / Examples">
          <div class="th-with-sort">
            <div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Supporting Details / Examples</div>
            <button class="sort-btn sort-state-0" onclick="event.stopPropagation(); headerSortButtonClicked(4,1, this)" title="Toggle sort" aria-label="Toggle sort">
              <span class="sort-icon" aria-hidden="true"></span>
            </button>
          </div>
        </th></tr></thead><tbody><tr><td data-label="Summary / Core Concepts"><p><strong>Driverless cars face critical decision-making challenges.</strong> Vehicles must avoid collisions without overreacting or panicking in situations where a collision appears imminent but is avoidable.</p></td><td data-label="Supporting Details / Examples"><p>Paul Newman explains that driverless cars must “guess right” every time in near-collision scenarios. For example, if two cars approach head-on but can safely move a couple of meters aside, the algorithm must avoid both panic (driving off-road) and complacency (failing to avoid a real collision).</p></td></tr>
<tr><td data-label="Summary / Core Concepts"><p><strong>Autonomous systems require models of human behavior.</strong> Predicting the actions of other sane drivers allows cars to make safer choices, but unpredictable environmental cues pose further difficulty.</p></td><td data-label="Supporting Details / Examples"><p>Newman cites examples such as an ice-cream van’s music signaling children nearby or unusual animal movement (e.g., kangaroos in Australia). Volvo admitted struggling with such rare, context-specific scenarios.</p></td></tr>
<tr><td data-label="Summary / Core Concepts"><p><strong>Ethical and rule-breaking decisions are difficult for autonomous systems.</strong> Cars may need to break rules to avoid danger or respond to emergencies.</p></td><td data-label="Supporting Details / Examples"><p>Examples include edging forward when someone frantically signals at a red light, moving aside for an ambulance on a narrow street, or avoiding an oil tanker blocking a rural road. These scenarios are absent from the Highway Code but are crucial for full autonomy.</p></td></tr>
<tr><td data-label="Summary / Core Concepts"><p><strong>Social dynamics complicate automation.</strong> Human behavior can exploit the cautious nature of autonomous vehicles.</p></td><td data-label="Supporting Details / Examples"><p>Sociologist Jack Stilgoe points out that people are “active agents” on roads. In a 2016 LSE focus group, participants noted that driverless cars could be “bullied” at junctions or by cyclists, slowing them down or forcing submissive behavior.</p></td></tr>
<tr><td data-label="Summary / Core Concepts"><p><strong>Full autonomy is geographically and operationally constrained.</strong> Early autonomous cars are restricted to specific areas and conditions.</p></td><td data-label="Supporting Details / Examples"><p>Waymo vehicles in Phoenix, Arizona, are geo-fenced to defined areas. Daimler and Ford’s ride-hailing autonomous cars are also confined to pre-decided zones. Newman describes this as operating in areas “very well known” to the system, potentially as a transport service, not full autonomy. Stilgoe calls this “constrained autonomy,” where the world is adapted to make machines appear autonomous.</p></td></tr>
<tr><td data-label="Summary / Core Concepts"><p><strong>Driverless technology is stratified into levels.</strong> Levels 0–5 indicate increasing autonomy, from no automation to full human-free driving.</p></td><td data-label="Supporting Details / Examples"><p>Level 2: hands off; Level 3: eyes off; Level 4: brain off; Level 5: fully autonomous. Tesla’s autopilot exemplifies Level 2: it can steer, brake, and accelerate on motorways, but still expects the human to monitor the road. Audi’s traffic-jam pilot approaches Level 3 by taking over in slow-moving traffic.</p></td></tr>
<tr><td data-label="Summary / Core Concepts"><p><strong>Partial automation introduces human skill degradation.</strong> Overreliance on autopilot can reduce drivers’ ability to respond in emergencies.</p></td><td data-label="Supporting Details / Examples"><p>Air France flight 447 illustrates this: pilot Pierre-Cédric Bonin, inexperienced with manual flying, failed to respond correctly when the autopilot disengaged due to iced air-speed sensors. The plane entered a steep climb, lost lift, and crashed. This aligns with Lisanne Bainbridge’s 1983 warning: automation that improves human performance can ironically erode it.</p></td></tr>
<tr><td data-label="Summary / Core Concepts"><p><strong>Hands-off systems create delayed response hazards.</strong> When drivers are inattentive, regaining control during emergencies takes significant time.</p></td><td data-label="Supporting Details / Examples"><p>Simulations show it may take up to 40 seconds for a driver to regain control after an alarm. Tesla driver Joshua Brown’s fatal 2016 crash highlights real-world consequences: he had been in Autopilot mode for 37½ minutes and was not paying attention when a truck crossed his lane. Uber’s driverless cars require human intervention every 13 miles, yet attention lapses remain frequent.</p></td></tr>
<tr><td data-label="Summary / Core Concepts"><p><strong>Automation paradox: more reliable systems increase risk.</strong> Smooth, rarely-failing systems reduce driver engagement, making emergencies more dangerous.</p></td><td data-label="Supporting Details / Examples"><p>Gill Pratt of Toyota warns that if a car rarely requires intervention (once every 200,000 miles), drivers may never be prepared for critical handovers. This “almost always reliable” paradox mirrors Air France 447, where Bonin was unpracticed in manual control.</p></td></tr>
<tr><td data-label="Summary / Core Concepts"><p><strong>Self-driving technology still offers substantial benefits.</strong> Even partial automation reduces accidents, fuel consumption, and congestion.</p></td><td data-label="Supporting Details / Examples"><p>Forward Collision Warning systems, as in Tesla, detect imminent risks and trigger alerts. Volvo’s Autonomous Emergency Braking has prevented fatalities in over 50,000 XC90 vehicles in the UK since 2002.</p></td></tr>
<tr><td data-label="Summary / Core Concepts"><p><strong>Expectation management is crucial.</strong> Marketing often exaggerates capabilities, leading to dangerous misinterpretation by drivers.</p></td><td data-label="Supporting Details / Examples"><p>Tesla advertised “full self-driving hardware,” but drivers must maintain control and responsibility. Some users attempt to circumvent warnings (e.g., taping a can of Red Bull or using devices to trick attention sensors). Misleading language can encourage unsafe behavior.</p></td></tr>
<tr><td data-label="Summary / Core Concepts"><p><strong>Human–machine complementary systems may be more effective.</strong> Instead of replacing drivers entirely, automation can augment human skills.</p></td><td data-label="Supporting Details / Examples"><p>Toyota’s “guardian” mode monitors the road while a human drives, acting as a safety net. Audi and Volvo also emphasize systems that intervene selectively, supporting drivers rather than substituting for them entirely. This mirrors medical AI, which complements human pattern recognition rather than fully replacing it.</p></td></tr>
<tr><td data-label="Summary / Core Concepts"><p><strong>Policy and societal considerations shape autonomous deployment.</strong> Roads may need regulation or modification to accommodate cautious algorithms.</p></td><td data-label="Supporting Details / Examples"><p>Historical analogies: motor cars initially excluded bicycles, horses, and pedestrians from certain roads. Fully autonomous vehicles may require restrictions on aggressive cyclists, pedestrians, emergency vehicles, and unusual hazards, highlighting that technology alone cannot solve all challenges.</p></td></tr>
<tr><td data-label="Summary / Core Concepts"><p><strong>The path to full autonomy will be incremental.</strong> Levels of automation allow for gradual adaptation while managing risks and human oversight.</p></td><td data-label="Supporting Details / Examples"><p>Level 2–3 technologies (hands-off, eyes-off) act as transitional phases. Systems like Tesla’s Autopilot or Audi’s traffic-jam pilot help drivers while keeping them engaged. Full Level 5 autonomy, capable of any driving scenario without human intervention, remains a distant goal due to technical, social, and ethical challenges.</p></td></tr>
<tr><td data-label="Summary / Core Concepts"><p><strong>Conclusion: embracing uncertainty is essential.</strong> Algorithms will inevitably fail at times, and society must define thresholds for acceptable risk.</p></td><td data-label="Supporting Details / Examples"><p>Even as automation reduces fatalities, it cannot guarantee perfection. Users and regulators must decide “how good is good enough” before deploying autonomous systems widely. Lessons extend beyond motoring: trust in algorithms, human skill maintenance, and expectation management are universal concerns in technology adoption.</p></td></tr>
</tbody></table></div>
  <div class='row-count'></div>
</div>

<div class="table-wrapper" id="Table6"><h3 id="Table6-title">Table 6</h3><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button onclick="copyTablePlain(this)">Copy Plain Table</button><button onclick="copyTableMarkdown(this)">Copy Markdown Table</button></div><div style="display:flex; align-items:center;"><button class="toggle-table-btn" onclick="toggleTable(this)">Collapse Table</button></div></div><table><thead><tr>
        <th style="width:28.57%;" onclick="sortTableByColumn(5,0)" role="button" aria-label="Sort by **Summary / Core Concepts**">
          <div class="th-with-sort">
            <div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>Summary / Core Concepts</strong></div>
            <button class="sort-btn sort-state-0" onclick="event.stopPropagation(); headerSortButtonClicked(5,0, this)" title="Toggle sort" aria-label="Toggle sort">
              <span class="sort-icon" aria-hidden="true"></span>
            </button>
          </div>
        </th>
        <th style="width:71.43%;" onclick="sortTableByColumn(5,1)" role="button" aria-label="Sort by **Supporting Details / Examples**">
          <div class="th-with-sort">
            <div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>Supporting Details / Examples</strong></div>
            <button class="sort-btn sort-state-0" onclick="event.stopPropagation(); headerSortButtonClicked(5,1, this)" title="Toggle sort" aria-label="Toggle sort">
              <span class="sort-icon" aria-hidden="true"></span>
            </button>
          </div>
        </th></tr></thead><tbody><tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Facial recognition in retail: purpose and implementation</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Retailers are using facial recognition technology (FRT) to identify known shoplifters. Cameras at store entrances scan faces against a database, and alerts are sent to security staff via smartphones or monitoring stations. This allows proactive intervention before theft occurs. FaceFirst, a major supplier, claims it doesn’t store images of ordinary customers, though there are concerns about covert tracking of shopping habits for marketing.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Scale and cost of retail crime</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>In the UK, 3.6 million retail offenses occur yearly, costing \~£660 million. In 2016, 91 shoplifting suspects died during incidents in U.S. stores. These figures justify using advanced technology to reduce losses and prevent violent encounters.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Primary benefit: proactive crime prevention</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>FRT can prevent persistent offenders from entering stores, reducing theft, protecting employees and customers, and lowering insurance and security costs. Early identification minimizes risk before escalation.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Ethical and privacy concerns</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Issues include: <br>- <strong>Privacy:</strong> potential tracking of customer habits. <br>- <strong>Digital blacklists:</strong> risk of wrongful inclusion. <br>- <strong>Due process:</strong> assumption of guilt without evidence. <br>- <strong>Error rates:</strong> misidentifications can harm innocent people. <br>- <strong>Redress:</strong> unclear procedures to remove wrongly flagged individuals.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Retail adoption: mixed results</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Some retailers adopt FRT, while others, such as Walmart, terminated trials due to insufficient ROI. This reflects ongoing debate on cost-effectiveness and ethical considerations.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Accuracy compared to traditional forensics</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>FRT is imperfect, like other forensic methods. Fingerprints, bite marks, blood spatter, and ballistics all have error rates; only DNA is highly accurate. Despite imperfections, these tools remain valuable when used cautiously, similar to how FRT can aid investigations without being the sole determinant.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Trade-offs: risk vs reward</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>FRT introduces ethical dilemmas: minor error rates can have severe consequences, including wrongful arrests or restrictions. Society must decide how much risk is acceptable to reduce crime.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Case study: David Baril</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>In 2015, Baril attacked multiple pedestrians in Manhattan with a claw hammer. FRT matched him to a prior Instagram post depicting a bloody hammer. He was convicted and sentenced to 22 years, showing FRT’s potential for rapid identification in violent crimes.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Case study: Neil Stammer</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Stammer, a fugitive for 15 years, was found when FRT matched a U.S. 'Wanted' poster photo with a passport in Nepal under a false identity. FRT solved a long-term cold case and apprehended a high-risk criminal.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Case study: Youssef Zaghba (London Bridge <br>2017)</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Zaghba, involved in the London Bridge terrorist attack, was on an international watchlist. FRT could have identified him upon entering the UK, potentially preventing the attack. This highlights FRT’s role in national security and terrorism prevention.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Statistical insight: NYPD example</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>In 2015, the NYPD used FRT to identify 1,700 suspects, resulting in 900 arrests and 5 mismatches. While misidentifications are concerning, the ratio demonstrates FRT’s effectiveness in crime prevention. Policymakers must weigh the societal cost of errors against overall crime reduction.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Other algorithmic crime tools</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Tools like PredPol, HunchLab, Strategic Subject Lists, and Kim Rossmo’s geoprofiling also promise predictive policing. They enhance law enforcement but raise bias, privacy, and accountability concerns, reflecting the broader trade-offs inherent in crime-related algorithms.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Need for algorithmic regulation</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Questions arise: <br>- Should only transparent, understandable algorithms be used, even if less effective? <br>- How much bias/error is acceptable? <br>- How should harms to misidentified individuals be weighed against benefits to victims of preventable crimes? Regulation must ensure transparency, oversight, and safeguards.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Societal trade-offs</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Decisions involve ethical prioritization: <br>- <strong>Crime reduction vs individual freedom:</strong> How much privacy or liberty can be sacrificed to reduce crime? <br>- <strong>Prevention vs due process:</strong> Should restrictions be applied based on algorithmic predictions rather than evidence? <br>- <strong>Success metrics:</strong> Is success minimizing preventable harm or avoiding misidentification?</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Philosophical reflection</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>MIT sociologist Gary Marx notes that authoritarian systems, like the Soviet Union, maintained low crime but at immense societal cost. Similarly, algorithmic surveillance may reduce crime while eroding civil liberties if unchecked.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Conclusion: limits and scope</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Algorithms can enhance safety but must be constrained by ethics, fairness, and privacy. Societies should define clear boundaries for algorithmic intervention, ensuring human values guide deployment rather than leaving decisions solely to machines.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Overall insights</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>FRT in crime prevention exemplifies the broader tension between efficiency, accuracy, and human rights. While it offers significant benefits, adoption requires oversight, ethical deliberation, and societal consensus on acceptable trade-offs. Practical implementation must include audits, transparency, redress mechanisms, and ongoing public dialogue.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>FRT in broader algorithmic governance</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Facial Recognition Technology (FRT) is part of a wider debate on algorithmic governance and predictive policing, involving how automated systems influence societal decision-making.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>High stakes of crime prevention</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Misidentifications by FRT can harm innocent people, yet failing to act may result in serious injury or death. Crime prevention highlights the balance of risk vs. consequence.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Imperfection of algorithms</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>All algorithms, including FRT, are inherently imperfect. Oversight frameworks should include transparency, accountability, and procedures to remedy errors.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Context matters</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Different scenarios demand different approaches. Small retail stores face distinct cost/benefit trade-offs compared to national security or law enforcement applications.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Necessity of societal debate</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Adoption of FRT requires ethical and civic discussion, not merely technical evaluation, to ensure alignment with societal norms and values.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Regulatory safeguards</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Regulation should establish procedural safeguards, define scope limits, and implement auditing standards to prevent misuse of FRT systems.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Key takeaway</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Facial recognition and predictive policing offer powerful tools, but they cannot override human ethics, privacy, or fairness. The challenge is balancing crime reduction with individual rights, defining acceptable risk, and ensuring societal values guide deployment.</p></td></tr>
</tbody></table></div>
  <div class='row-count'></div>
</div>

<div class="table-wrapper" id="Table7"><h3 id="Table7-title">Table 7</h3><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button onclick="copyTablePlain(this)">Copy Plain Table</button><button onclick="copyTableMarkdown(this)">Copy Markdown Table</button></div><div style="display:flex; align-items:center;"><button class="toggle-table-btn" onclick="toggleTable(this)">Collapse Table</button></div></div><table><thead><tr>
        <th style="width:28.57%;" onclick="sortTableByColumn(6,0)" role="button" aria-label="Sort by **Summary / Core Concepts**">
          <div class="th-with-sort">
            <div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>Summary / Core Concepts</strong></div>
            <button class="sort-btn sort-state-0" onclick="event.stopPropagation(); headerSortButtonClicked(6,0, this)" title="Toggle sort" aria-label="Toggle sort">
              <span class="sort-icon" aria-hidden="true"></span>
            </button>
          </div>
        </th>
        <th style="width:71.43%;" onclick="sortTableByColumn(6,1)" role="button" aria-label="Sort by **Supporting Details / Examples**">
          <div class="th-with-sort">
            <div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>Supporting Details / Examples</strong></div>
            <button class="sort-btn sort-state-0" onclick="event.stopPropagation(); headerSortButtonClicked(6,1, this)" title="Toggle sort" aria-label="Toggle sort">
              <span class="sort-icon" aria-hidden="true"></span>
            </button>
          </div>
        </th></tr></thead><tbody><tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>EMI and Algorithmic Music</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>David Cope’s <em>Experiments in Musical Intelligence</em> (EMI) challenged ideas of machine creativity. Hofstadter admitted he was “baffled and troubled” by EMI’s output, as it mimicked composers but produced convincing results indistinguishable from human compositions.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p>EMI encodes music for machine interpretation.</p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Each note in Bach’s music is represented with five attributes: onset time, duration, pitch, loudness, and instrument. Cope manually entered tens of thousands of notes for 371 Bach chorales, requiring months of obsessive data entry.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p>EMI predicts subsequent notes using a dictionary approach.</p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Each chord in Bach’s chorales is logged with possible subsequent notes; the algorithm selects the next chord randomly from recorded possibilities. This creates an original composition resembling Bach’s style.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p>EMI imitates but does not originate musical style.</p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Cope likens the process to grating Parmesan cheese: the end product is recombined from existing material, not a truly new creation.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p>EMI as a precursor to predictive algorithms.</p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>EMI operates like smartphone predictive text: it predicts next elements based on historical patterns, demonstrating early AI’s strength in recombination rather than invention.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Genetic Algorithms and Evolutionary Music</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Beyond pure recombination, genetic algorithms attempt to “breed” aesthetically pleasing compositions using principles inspired by natural selection.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p>Notes treated as DNA; songs evolve across generations.</p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Algorithms begin with random sequences of notes (“initial population”) and select for features deemed beautiful, producing increasingly refined compositions.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p>Beauty is subjective; algorithmic “success” is based on similarity.</p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Algorithms still rely on historical or predefined standards of what is considered aesthetically pleasing; no objective measure of beauty exists.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p>Applications in content creation.</p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Useful for background music, royalty-free content, or website/YouTube tracks. British startups Jukebox and AI Music produce algorithmic music that is original enough to avoid copyright issues, sometimes producing surprisingly pleasing results.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Comparison to Human Music Creation</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Human creativity often recombines existing ideas.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p>Pop music may be formulaic and predictable.</p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Armand Leroi critiques Pharrell Williams’ “Happy” for its repetitive lyrics and simple structure; Adele’s lyrics could be generated by a “sad song generator.”</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p>Mark Twain: all ideas are recombinations.</p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Twain: “There is no such thing as a new idea… we simply take a lot of old ideas and put them into a sort of mental kaleidoscope… making new and curious combinations.”</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p>Cope’s definition of creativity aligns with algorithmic function.</p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Creativity: “finding an association between two things which ordinarily would not seem related.” EMI demonstrates this, but in a limited, derivative sense.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Creativity vs. True Art</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Algorithmic compositions may be creative, but lack the depth and human connection required for true art.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p>Tolstoy’s view: art transmits human emotion.</p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>“Art is not a handicraft, it is the transmission of feeling the artist has experienced.” Machines cannot experience or transmit emotion, limiting the depth of algorithmic art.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p>Hofstadter emphasizes lived experience as essential.</p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>A program creating music would need to understand joy, grief, desire, mortality, and the complexities of life; only through lived experience can music have profound meaning.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p>Algorithmic art is culturally shallow.</p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>EMI produces aesthetically pleasing but emotionally one-dimensional music—“cultural comfort food.” Human audiences might assign meaning, but the machine itself cannot experience it.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Human + Machine Collaboration</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Automation has delivered enormous benefits but also significant risks when systems are treated as authoritative.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p>Automation errors have real-world consequences.</p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Rahinah Ibrahim was added to the no-fly list due to human error in a machine-controlled system, resulting in nearly a decade-long ban from the U.S. Despite holding a PhD, being an American citizen, and having family ties, she faced years of travel restrictions.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p>Algorithms can encode bias, error, and opacity.</p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Examples include: recidivism algorithms that over-score Black defendants, kidney injury detection systems demanding invasive data access, supermarket algorithms that misrepresent customer events, and Strategic Subject Lists misused by police.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p>Even accurate systems have limitations.</p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Autopilot in planes can disadvantage humans trained under automation; medical AI may misdiagnose certain ethnic groups; perfect fairness is nearly impossible, with or without algorithms.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p>Accepting imperfection reduces undue algorithmic authority.</p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Algorithms will make mistakes; humans should focus on redress, contestability, and transparency rather than assuming perfection.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Design Principles for Responsible Algorithms</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Build systems to assist, not replace humans; ensure transparency, contestability, and oversight.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p>Show uncertainty and alternative possibilities.</p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>IBM Watson presented multiple possible answers with confidence scores; facial recognition systems should provide multiple potential matches; neural networks highlight suspicious areas for pathologists rather than dictate diagnoses.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p>Collaborative human-machine approaches enhance outcomes.</p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Centaur Chess: humans focus on strategic planning while computers calculate all move consequences. Outcome: superior chess performance with meaningful strategy and flawless tactics.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p>Algorithms should embrace flaws openly.</p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Transparency about uncertainty and limitations encourages human questioning and accountability, reducing blind trust in outputs.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Ethical and Philosophical Takeaways</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>True creativity, art, fairness, and judgment remain inherently human domains.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p>Humans remain central in guiding, judging, and interpreting algorithms.</p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Responsibility for ethical outcomes, oversight of bias, error correction, and ensuring accountability lies with humans.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p>Collaboration allows augmentation of human capabilities.</p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Algorithms extend human reach, enabling efficiency, enhanced decision-making, and elevated creative work, but humans remain essential decision-makers.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p>The future is hybrid: human plus machine.</p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Systems designed with contestability, transparency, and human oversight create positive societal impact; humans maintain moral and cultural authority in an automated world.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p>Arrogant, dictatorial algorithms are to be avoided.</p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Blind acceptance of algorithmic authority risks exploitation, harm, and erosion of human judgment; humans must question, analyze, and engage critically.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p><strong>Conclusion: Algorithms as Tools</strong></p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Algorithms are powerful but imperfect; the key lies in thoughtful integration and human-centered design.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p>Positive examples: IBM Watson, Centaur Chess, medical AI as decision support.</p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Systems that complement human judgment, highlight uncertainty, and provide actionable insights improve outcomes and enhance human creativity, judgment, and emotional engagement.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p>Negative examples: No-fly list errors, biased predictive policing, flawed financial or medical algorithms.</p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Human errors embedded in automated systems can have long-lasting and unfair consequences; oversight and accountability are essential.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p>Human responsibility is paramount.</p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>Algorithms should never replace human oversight; they should assist, augment, and provide insights while humans remain morally, legally, and socially accountable.</p></td></tr>
<tr><td data-label="<strong>Summary / Core Concepts</strong>"><p>True art, creativity, and fairness require human experience.</p></td><td data-label="<strong>Supporting Details / Examples</strong>"><p>No machine can fully replicate emotion, lived experience, or the subtleties of cultural meaning; human judgment remains central to meaningful creation and decision-making.</p></td></tr>
</tbody></table></div>
  <div class='row-count'></div>
</div>

<script src="assets/script.js?v=1758336167" defer></script>

<script>
(function(){
  const template = "{table}_{date}";
  function formatName(tableName) {
    const date = (new Date()).toISOString().slice(0,10);
    return template.replace('{table}', tableName).replace('{date}', date).replace('{user}', '');
  }
  const btn = document.getElementById('exportBtn');
  if(btn) {
    btn.addEventListener('click', async function() {
      try {
        const html = document.documentElement.outerHTML;
        if(html.length > 2000000) {
          alert('Export refused: html too large');
          return;
        }
        if(window.Worker) {
          const worker = new Worker('assets/worker.js');
          worker.postMessage({html:html, format:'pdf'});
          worker.onmessage = function(e){ console.log('worker:', e.data); alert('Worker replied: '+(e.data.msg||e.data.status)); };
        } else {
          alert('Export worker not supported in this environment.');
        }
      } catch(err) {
        console.warn('Export failed', err);
        alert('Export worker not available. See console for details.');
      }
    });
  }
})();
</script>

</div></body></html>
