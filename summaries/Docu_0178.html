<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='utf-8'><meta name='viewport' content='width=device-width,initial-scale=1'>
<title>Tables Viewer v2.1</title>
<style>:root{--main-header-height:56px;} .table-wrapper{opacity:0;min-height:24px;transition:opacity .12s linear}</style>
<link rel="stylesheet" href="assets/style.css?v=1759925496">
<link rel="stylesheet" href="assets/overrides.css?v=1768671762">
</head><body>
<div id="tables-viewer" role="region" aria-label="Tables viewer">
<div id="stickyMainHeader">
<div id="tv-header">
<div><h1>Tables Viewer v2.1</h1></div>
<div style="display:flex;gap:8px;align-items:center;">
<input id="searchBox" class="tv-search" type="search" placeholder="Search" aria-label="Search tables" />
<button id="modeBtn" type="button" onclick="toggleMode()" aria-label="Toggle theme">Theme</button>
<button id="toggleAllBtn" type="button" aria-label="Toggle all" onclick="toggleAllTables()">Collapse All Tables</button>
<button id="copyAllPlainBtn" class="copy-btn" type="button" onclick="copyAllTablesPlain()" aria-label="Copy all tables as plain text">Copy All Tables (Plain Text)</button>
<button id="copyAllTablesBtn" class="copy-all-btn" type="button" onclick="copyAllTablesMarkdown()" aria-label="Copy All Tables (Markdown)">Copy All Tables (Markdown)</button>
<button id="copyAllMdBtn" style="display:none" aria-hidden="true"></button>
<button id="resetAllBtn" type="button" onclick="resetAllTables()" aria-label="Reset All Tables">Reset All Tables</button>
</div></div>
<script>
(function(){
  function ensureDelegation(){
    try {
      var vis = document.getElementById('copyAllTablesBtn');
      var alias = document.getElementById('copyAllMdBtn');
      if(!vis || !alias) return;
      alias.addEventListener = function(type, listener, options){
        vis.addEventListener(type, listener, options);
      };
      alias.removeEventListener = function(type, listener, options){
        vis.removeEventListener(type, listener, options);
      };
      Object.defineProperty(alias, 'onclick', {
        set: function(fn){ vis.onclick = fn; },
        get: function(){ return vis.onclick; },
        configurable: true
      });
      alias.focus = function(){ vis.focus(); };
      alias.blur = function(){ vis.blur(); };
    } catch(e) {
      try{ console && console.warn && console.warn('alias delegation failed', e); }catch(_){} 
    }
  }
  if(document.readyState === 'loading'){
    document.addEventListener('DOMContentLoaded', ensureDelegation);
  } else {
    ensureDelegation();
  }
})();
</script>
<noscript><div style='color:#b91c1c'>JavaScript is disabled. Tables will be shown statically. For large tables enable JS for virtualization.</div></noscript>
<div id="tocBar" role="navigation" aria-label="Table of contents"><ul><li class="toc-item"><a class="toc-link" href="#Table1">Table 1</a></li>
<li class="toc-item"><a class="toc-link" href="#Table2">Table 2</a></li>
<li class="toc-item"><a class="toc-link" href="#Table3">Table 3</a></li>
<li class="toc-item"><a class="toc-link" href="#Table4">Table 4</a></li>
<li class="toc-item"><a class="toc-link" href="#Table5">Table 5</a></li></ul></div></div>
<div class="table-caption" id="Table1" data-table="Docu_0178_01" style="margin-top:2mm;margin-left:3mm;"><strong>Table 1</strong></div>
<div class="table-wrapper" data-table-id="table-1"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by REG_Ribbon — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">REG_Ribbon — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong>Module synopsis & owner</strong><br><strong>Owner:</strong> <code>team:reg-platform/OWNERS.md</code>. Single authoritative owner with on-call rotation and documented escalation path.<br><strong>Purpose (concise):</strong> provide a thin, highly-observable orchestration layer between Excel/Office UI and backend worker systems for regulated data workflows. Responsibilities: map UI controls to handlers; produce canonical <code>UserAction</code> audits anchored by <code>correlationId</code>; decide inline vs scheduled execution; enforce RBAC and approval semantics; hand off heavy workloads to persistent job store/workers; ensure deterministic previews for CI/golden parity. This layer intentionally avoids heavy transformations and secrets handling; it is an orchestrator, gatekeeper, and audit anchor.<br><strong>Public API surface (canonical):</strong> <code>OnLoad(ribbonUI)</code>, <code>DeferredInit()</code>, <code>LoadRibbonMap()</code>, <code>RegisterCallback(controlId,handler,meta)</code>, <code>ValidateControlId(controlId)</code>, <code>MapControlToHandler(controlId)</code>, <code>HandleControlAction(controlId,context)</code>, <code>IsLightweightAction(controlMeta,context)</code>, <code>SafeInvokeHandler(handlerName,args,cid)</code>, <code>SafeHandlerTimeoutWatchdog(token,cid)</code>, <code>EmitUserActionAudit(cid,controlId,procedure,params)</code>, <code>BuildUiParamsHash(params)</code>, <code>SafeErrorToUser(cid,errorCode)</code>, <code>RefreshRibbon()</code>, <code>EnableControl(controlId,operator,reason)</code>, <code>DisableControl(controlId,operator,reason)</code>, <code>RegisterUnitTestHook(hookName)</code>, <code>HotSwapHandlers(newMapJson,operator,approvals)</code>, <code>Shutdown()</code>.<br><strong>Critical audits emitted (minimal set):</strong> <code>ribbon.onload</code>, <code>ribbon.loaded</code>, <code>ribbon.onload.error</code>, <code>ribbon.map.loaded</code>, <code>ribbon.map.invalid</code>, <code>ribbon.callback.registered</code>, <code>ribbon.useraction</code>, <code>ribbon.permission.denied</code>, <code>ribbon.handler.start</code>, <code>ribbon.handler.complete</code>, <code>ribbon.handler.error</code>, <code>ribbon.handler.timeout</code>, <code>ribbon.job.persisted</code>, <code>ribbon.refresh.completed</code>, <code>ribbon.control.enabled</code>, <code>ribbon.control.disabled</code>, <code>ribbon.hotswap.applied</code>, <code>ribbon.hotswap.reverted</code>, <code>ribbon.shutdown</code>. Each audit must include <code>timestamp, correlationId, module=REG_Ribbon, procedure, paramsHash (where applicable), configHash, ribbonMapHash, prevHash (when part of an auditable chain), metadata</code> for forensic linkage. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong>Design principles & hard invariants</strong><br>1. <strong>UI thread safety:</strong> <code>OnLoad</code> and direct UI handlers must not perform heavy I/O, workbook Range access, or synchronous network calls during <code>OnLoad</code> or on the immediate ribbon click path. <br>2. <strong>Audit-first contract:</strong> every user-initiated action must create <code>ribbon.useraction</code> (the audit anchor) before any persisted external side-effect (job persistence, export, apply). <br>3. <strong>Determinism:</strong> inline preview handlers must be deterministic (seeded RNG + canonical ordering) to enable reproducible CI/golden runs. <br>4. <strong>PII minimization & evidence model:</strong> main audit rows never contain raw PII. UI parameters are canonicalized and redacted; the canonical <code>paramsHash</code> is stored in the main audit row; the full sanitized parameters are stored encrypted in the evidence store and referenced via an <code>evidenceRef</code> in the audit metadata (access to the evidence store is RBAC-controlled and audited). <br>5. <strong>Fail-closed for regulated controls:</strong> when manifest validation or signature verification fails, regulated controls must be disabled until the map is repaired and re-signed. <br>6. <strong>Atomic persistence:</strong> all writes that represent state changes (manifest persistence, job descriptors, exports) must use atomic-write patterns (write temp -> verify checksum -> atomic rename/swap). <br>7. <strong>Traceability:</strong> correlation ids link bootstrap → ribbon → job → worker and are required in all handler and job audits. <br>8. <strong>Governance & CI:</strong> changes to mapping or policy require a PR, static analysis checks, unit + integration + golden tests, and signed manifest for production deployment. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>OnLoad(ribbonUI)</code> — purpose, contract, observability, developer rules, tests</strong><br><strong>Purpose & contract:</strong> invoked by host when ribbon UI initializes. Minimal responsibilities: cache opaque <code>ribbonUI</code>, set minimal runtime defaults, instantiate <code>CorrelationIdGenerator(seed=bootstrapEntropy)</code>, create tiny in-memory caches (feature flags, quick lookup of enabled controls), register <code>Shutdown()</code>, and schedule <code>DeferredInit()</code> via host idle or <code>Application.OnTime</code>. MUST NOT enumerate workbook objects or perform disk/network IO on the UI thread. <br><strong>Parameters & return:</strong> <code>ribbonUI</code> (opaque host handle). Return: none. Must not propagate unhandled exceptions to host; convert to <code>ribbon.onload.error</code> audit if issues occur. <br><strong>Observability & auditing:</strong> emit <code>ribbon.onload</code> (start) immediately and schedule <code>ribbon.loaded</code> after deferred init completes. Audits include <code>correlationId, startTs, durationMs, buildId, platform, excelVersion, ribbonMap.hash (if known)</code>. <br><strong>VB/VSTO guidance:</strong> cache <code>ribbonUI</code> at module scope; do not call manifest loading or workbook APIs synchronously; use <code>Application.OnTime</code> or host idle registration for <code>DeferredInit( )</code>. <br><strong>Tests & CI rules:</strong> static analyzer forbids workbook API usage here; unit test asserting <code>ribbon.onload</code> audit present; smoke test verifying OnLoad returns under 50ms. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>DeferredInit()</code> / <code>LoadRibbonMap()</code> — manifest ingestion, canonicalization, signature, PQ specifics, fallback policy</strong><br><strong>Purpose & contract:</strong> run in deferred/background context to ingest the manifest(s) (<code>ribbon-map.json</code> or embedded <code>customUI</code>), canonicalize structure, run JSON Schema v7 validation, dedupe <code>controlId</code>s, attach owners from <code>OWNERS.md</code>, compute canonical <code>ribbonMap.hash</code> (SHA256 of canonical JSON), verify digital signature locally if present, and populate in-memory <code>RibbonMap</code>. MUST NOT run during the OnLoad main path. <br><strong>Deterministic validation pipeline:</strong> parse → canonicalize (stable key ordering and sorted arrays) → validate against JSON Schema v7 (collect full list of errors and warnings) → enforce unique <code>controlId</code>s and report duplicate indices → verify handler existence and owner attribution → verify manifest signature and record its fingerprint → compute <code>ribbonMap.hash</code>. <br><strong>Power Query specifics:</strong> when manifest references PQ templates, ensure <code>mChecksum</code> and <code>templateVersion</code> are present; heavy PQ template validation (semantic checks) is scheduled to a worker and not performed inline during deferred init. Record <code>mChecksum</code> in manifest audit for provenance. <br><strong>Fallback policy & enforcement:</strong> non-critical warnings → continue with reduced map and emit <code>ribbon.map.warning</code>. Critical schema/signature errors → <code>ribbon.map.invalid</code>, disable regulated controls (fail-closed), and show diagnostics ribbon. <br><strong>Atomic swap & safe write:</strong> persist candidate map to temp path, compute checksum, validate locally, then atomic-swap into live path and update in-memory reference; on swap failure revert to prior map and emit <code>ribbon.refresh.error</code>. <br><strong>Tests/CI:</strong> schema validator vectors, duplicate ID negative tests, signature verification unit tests, PQ <code>mChecksum</code> mismatch tests, golden manifest parity tests. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>RegisterCallback(controlId, handlerName, metadata)</code> — controlled registration, signature checks, idempotency, persistence, examples, and tests</strong><br><strong>Purpose & contract:</strong> register/update control→handler mapping at install-time or runtime. Must validate handler signature (where applicable), ensure owner metadata, reject destructive handlers without explicit approvals, and be idempotent. Persist only via atomic-write via <code>REG_Utilities</code> if requested. Production dynamic registration requires signed manifests + approvals. <br><strong>Examples (multiple):</strong><br>1. Installer registers <code>dq_phone_norm</code> → validation passes → <code>ribbon.callback.registered</code> audit appended and optionally persisted. <br>2. Unauthorized registration attempt rejected and <code>ribbon.callback.register.failed</code> with <code>RIB_REG_403</code> appended. <br><strong>Tests:</strong> duplicate registration detection, unauthorized registration rejection, manifest persistence correctness. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>ValidateControlId(controlId)</code> — canonical guard, permitted patterns, metadata checks, PII policy, examples, and tests</strong><br><strong>Purpose & contract:</strong> single canonical validator for all incoming control events. Must be idempotent and side-effect free except on failure where an audit row is emitted. Returns canonical <code>controlMeta</code> or <code>{errorCode, userHint}</code>. <br><strong>Checks performed (deterministic):</strong><br>1. Format regex (allowed chars & length). <br>2. <code>RibbonMap</code> lookup (exists & mapping). <br>3. Enabled/visible state vs feature flags. <br>4. <code>requiresApproval</code>/<code>regulated</code>/<code>mayAffectPII</code> flags and owner info. <br>5. Tenant scoping and pilot cohort checks. <br><strong>PII & message policy:</strong> <code>userHint</code> must never contain PII; full logs stored encrypted. <br><strong>Examples (multiple):</strong><br>1. Valid control: <code>dq_profile.run</code> found → return controlMeta including <code>estimatedCost:light</code>. <br>2. Unknown control: return <code>{errorCode:RIB001, userHint:&quot;Unknown control; contact add-in owner&quot;}</code> and append <code>ribbon.control.validate</code> audit. <br>3. Disabled by flag: return denial with <code>userHint</code> instructing operator to request enable and include correlation id for audit. <br><strong>Tests:</strong> fuzz invalid ids, alias resolution, and ensure safe <code>userHint</code> content. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>MapControlToHandler(controlId)</code> — deterministic mapping, aliasing, versions, migration hints, examples, and tests</strong><br><strong>Purpose & contract:</strong> pure function resolving <code>controlId</code> → <code>{handlerName, handlerModule, estimatedCost, requiredApprovals, owner, version}</code>. Must be deterministic; mapping changes only via manifest updates which change <code>ribbonMap.hash</code>. <br><strong>Capabilities:</strong> alias resolution, redirect rules (control@vN → handlerVn), owner fallback, migration hint emission. <br><strong>Examples (multiple):</strong><br>1. Direct map: <code>dq_propose</code> → <code>HandleProposeV2</code>, <code>estimatedCost:heavy</code>. <br>2. Alias: <code>dq_old_propose</code> maps to <code>dq_propose</code> with <code>migrationHint:&quot;deprecated v1 -&gt; v2&quot;</code>. <br>3. Versioned redirect: <code>dq_apply@v2</code> returns handlerV2 and <code>version:&quot;2&quot;</code>. <br><strong>Tests:</strong> mapping parity across manifest versions, alias/resolution correctness, mapping stability across reloads. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>HandleControlAction(controlId, context)</code> — secure dispatcher, audit anchoring, job scheduling, inline safety, UI contract, examples, and tests</strong><br><strong>Purpose & contract:</strong> authoritative dispatcher for ribbon callbacks. Must validate control, create <code>correlationId</code>, emit <code>UserAction</code> audit, decide inline vs scheduled execution, invoke handler safely or persist job descriptor, and return immediate UI-safe response. MUST NOT perform long-running work synchronously. <br><strong>Canonical orchestration (must/shall):</strong><br>1. <code>controlMeta = ValidateControlId(controlId)</code>. <br>2. <code>cid = NewCorrelationId(parent=uiSession)</code>. <br>3. <code>EmitUserActionAudit(cid, controlId, procedure=&quot;click&quot;, params)</code>. <br>4. <code>decision = IsLightweightAction(controlMeta)</code>. <br>5a. If lightweight → <code>SafeInvokeHandler(handlerName,args,cid)</code>. <br>5b. Else → persist <code>jobDescriptor</code> via <code>JobSchedulerIntegration</code>, emit <code>job.persisted:&lt;jobId&gt;</code>. <br>6. Return <code>{status, message, correlationId}</code> synchronously (message short & safe). <br><strong>UI contract:</strong> short message containing correlation id and next steps, e.g., "Profile scheduled — ref r-20260116-abc". <br><strong>Examples (multiple):</strong><br>1. Small table profile (inline): validate → audit → invoke <code>SafeInvokeHandler</code> → receive preview artifact → append <code>module.step</code> audits → UI displays result. <br>2. Large table profile (scheduled): validate → audit → create <code>job-901</code> → <code>job.persisted</code> emitted → UI shows scheduled message. <br>3. Permission denied: <code>ValidateControlId</code> denies → return <code>SafeErrorToUser</code> hint and append <code>ribbon.permission.denied</code>. <br><strong>Failure handling:</strong> bounded retry for job persist, fail-closed on destructive controls if manifest invalid, clear audit traces for triage. <br><strong>Tests:</strong> concurrency (100s of clicks), immediate return, audit chain presence, job dedupe/idempotency. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>IsLightweightAction(controlMeta)</code> — policy evaluation, thresholds, explainability, examples, and tests</strong><br><strong>Purpose & contract:</strong> determine inline vs scheduled execution. Inputs: <code>controlMeta</code>, <code>modConfig.thresholds</code> (rowCount, sampleSize), runtime <code>mode</code> (degraded/safe), operator overrides. Return <code>{lightweight:Boolean, rationale:String}</code>. Decisions must be auditable. <br><strong>Policy examples:</strong><br>1. <code>profile</code>: lightweight if <code>rowCount &lt; profile.sampleThreshold &amp;&amp; sampleSize &lt; sampleLimit</code>. <br>2. <code>apply</code>: default heavy if dataset regulated or <code>requiresApproval=true</code>. <br><strong>Examples (multiple):</strong><br>1. 8k-row table for <code>profile</code> → <code>{true,&quot;rows&lt;10k&quot;}</code>. <br>2. 200k-row table → <code>{false,&quot;rows&gt;threshold - scheduled for safety&quot;}</code>. <br><strong>Governance:</strong> config-driven; changes require PR + audit row <code>config.change</code>. <br><strong>Tests:</strong> boundary tests and safe-mode enforced scheduling. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>SafeInvokeHandler(handlerName, args, correlationId)</code> — execution frame, time budgets, cancellation, error mapping, redaction, PQ and VBA patterns, telemetry, examples, and tests</strong><br><strong>Purpose & contract:</strong> execute handler inside a protective frame: validate registration; create invocation trace; set cooperative timeout & cancellation token; append start/complete step-level audits with <code>payloadHash</code>; catch exceptions and map to stable <code>ErrorCodeCatalog</code>; redact PII from logs; return structured result only. <br><strong>Execution invariants:</strong><br>1. Inline handlers must not perform blocking heavy disk/network IO. <br>2. Handler must support cancellation token checks. <br>3. Exceptions mapped to stable codes and audited; only safe hints shown to user. <br><strong>VBA & PQ developer patterns:</strong><br>1. VBA handlers accept <code>ByRef cancelToken As Boolean</code> and check inside loops. <br>2. PQ template handlers validate <code>mChecksum</code> before invoking heavy transforms; heavy transforms run in worker. <br><strong>Telemetry & auditing:</strong> emit <code>ribbon.handler.duration_ms</code>, <code>ribbon.handler.success</code>, <code>ribbon.handler.error</code> with correlation id and controlId. <br><strong>Examples (multiple):</strong><br>1. <code>HandlePreview</code> runs quickly → <code>step.start</code> audit → handler returns previewRef → <code>step.complete</code> with <code>payloadHash</code> → UI shows preview. <br>2. Handler throws on malformed data → <code>ribbon.handler.exception</code> appended with mapped <code>ERR_PREVIEW_500</code>; UI shows "Preview failed (ref r-xxx)". <br>3. Long-running loop triggers watchdog → emits <code>ribbon.handler.timeout</code>, token cancelled, handler terminates gracefully and audit records partial results. <br><strong>Tests:</strong> exception injection, cancellation, telemetry emission, step audit presence, PQ checksum validation. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>EmitUserActionAudit(correlationId, controlId, procedure, params)</code> — canonical audit anchor, redaction, evidence linking, schema, examples, and tests</strong><br><strong>Purpose & contract:</strong> produce authoritative <code>UserAction</code> audit row that anchors UI flow. Must redact sensitive params, compute <code>paramsHash</code>, and append to <code>REG_Audit</code> buffer non-blocking. Include <code>configHash</code> and <code>ribbonMap.hash</code> for reproducibility. <br><strong>Schema (required fields):</strong> <code>timestamp,correlationId,module=REG_Ribbon,procedure,userId,controlId,paramsHash,configHash,ribbonMapHash,prevHash,metadata</code>. <br><strong>PII & evidence policy:</strong> main audit only stores <code>paramsHash</code>; full sanitized params placed in encrypted evidence store (with approval) and referenced by <code>evidenceRef</code> in metadata. <br><strong>Examples (multiple):</strong><br>1. <code>Profile</code> click: <code>params</code> include <code>{table:&quot;tblContacts&quot;, sample:5000}</code> → <code>paramsHash</code> stored; full sanitized <code>params</code> saved to evidence with <code>evidenceRef</code> for compliance. <br>2. <code>Apply inline</code>: <code>params</code> include <code>operatorId</code> and <code>approvalIds</code>; <code>paramsHash</code> recorded and approval artifacts referenced. <br><strong>Tests:</strong> audit schema validation, redaction verification, prevHash chaining via <code>VerifyAuditChain</code>. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>RegisterCallback(controlId, handlerName, metadata)</code> — controlled registration, signature checks, idempotency, persistence, examples, and tests</strong><br><strong>Purpose & contract:</strong> register/update control→handler mapping at install-time or runtime. Must validate handler signature (where applicable), ensure owner metadata, reject destructive handlers without explicit approvals, and be idempotent. Persist only via atomic-write via <code>REG_Utilities</code> if requested. Production dynamic registration requires signed manifests + approvals. <br><strong>Examples (multiple):</strong><br>1. Installer registers <code>dq_phone_norm</code> → validation passes → <code>ribbon.callback.registered</code> audit appended and optionally persisted. <br>2. Unauthorized registration attempt rejected and <code>ribbon.callback.register.failed</code> with <code>RIB_REG_403</code> appended. <br><strong>Tests:</strong> duplicate registration detection, unauthorized registration rejection, manifest persistence correctness. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>RefreshRibbon()</code> — live rebind, diffs, invalidation, operator UX, examples, and tests</strong><br><strong>Purpose & contract:</strong> reload <code>RibbonMap</code> and refresh UI without a restart. Must run <code>LoadRibbonMap</code> deferred, compute diff (added/removed/changed controls), call <code>ribbonUI.Invalidate()</code>/<code>InvalidateControl()</code> for affected controls, and append <code>ribbon.refresh.completed</code> with <code>duration_ms</code> and <code>ribbonMap.hash</code>. Must not interrupt running jobs. <br><strong>Examples (multiple):</strong><br>1. Manifest updated, operator clicks "Refresh" → map reloaded, invalidated controls re-query <code>getEnabled</code>, new controls appear, <code>ribbon.refresh.completed</code> appended. <br>2. Refresh finds schema error in new manifest → <code>ribbon.refresh.error</code>, UI stays with previous stable map. <br><strong>Tests:</strong> ensure running jobs continue unaffected, invalidation calls issued for changed controls. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>EnableControl</code> / <code>DisableControl</code> — runtime toggles, RBAC enforcement, kill-switch use-cases, examples, and tests</strong><br><strong>Purpose & contract:</strong> provide runtime enable/disable toggles for controls with RBAC checks. Must update only in-memory state (and optionally persist via manifest with audit), call <code>ribbonUI.InvalidateControl(controlId)</code>, and append <code>ribbon.control.enabled/disabled</code> audit rows with <code>operatorId</code> and reason. <br><strong>Use-cases:</strong> emergency kill-switch, staged feature rollouts, tenant pilot toggles. <br><strong>Examples (multiple):</strong><br>1. During incident, SRE calls <code>DisableControl(&quot;dq_export&quot;)</code>; function verifies permission, disables, invalidates, and logs <code>ribbon.control.disabled</code>. <br>2. Release engineer enables <code>dq_new_preview</code> for pilot tenants; <code>EnableControl</code> validates pilot scope and invalidates control. <br><strong>Tests:</strong> RBAC enforcement, UI state change propagation, audit presence. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>ValidateUserPermissions(userId, controlMeta)</code> — RBAC, approvals, dataset-level checks, examples, and tests</strong><br><strong>Purpose & contract:</strong> evaluate whether <code>userId</code> may invoke control: check roles, delegated approvals, two-person requirements for regulated actions, and dataset-level regulation flags. Return <code>{allowed, requiredApprovals[], denialReason}</code>. <br><strong>Checks performed:</strong> SSO identity mapping, group membership, time-limited approvals, dataset PII/regulation detection, <code>AUTO_APPLY</code> guard rails. <br><strong>Examples (multiple):</strong><br>1. Junior operator attempts inline apply on regulated dataset → returns <code>allowed:false</code> and <code>requiredApprovals=[manager,compliance]</code>. <br>2. Senior user with emergency privilege allowed to run <code>diagnostics-only</code> workflows; returns <code>allowed:true</code> with <code>specialNote</code>. <br><strong>Audit:</strong> <code>ribbon.permission.check</code> and approval grants <code>ribbon.approval.granted</code>. <br><strong>Tests:</strong> role matrix simulation and approval workflow tests. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>BuildUiParamsHash(params)</code> — canonicalization rules, redaction patterns, PQ template handling, examples, and tests</strong><br><strong>Purpose & contract:</strong> canonicalize UI parameters into deterministic hash while redacting PII. Steps: sort keys, normalize dates/numbers, apply redaction regex (emails, SSN, cards), remove empty values, produce canonical JSON, compute SHA256. Store only <code>paramsHash</code> in main audit; sanitized params may be stored encrypted with <code>evidenceRef</code>. <br><strong>PQ specifics:</strong> strip credentials from PQ <code>connectionString</code> and record <code>mChecksum</code> in evidence. <br><strong>Examples (multiple):</strong><br>1. PQ injection: sanitized params stored with <code>evidenceRef</code>; audit holds <code>paramsHash</code>. <br>2. Filter params with user-entered email redacted to <code>&lt;REDACTED&gt;</code> in sanitized evidence. <br><strong>Tests:</strong> deterministic hashing across key permutations and locales; redaction coverage tests. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>DiagnosticsToggle(enableVerbose, operatorId, ticketId)</code> — admin lifecycle, TTL, audit, and constraints</strong><br><strong>Purpose & contract:</strong> enable verbose ribbon diagnostics for a bounded TTL, requiring MFA and ticket id. Must append <code>debug.audit</code> with operator and justification, set TTL for auto-disable, and ensure logs redact secrets. Auto-disable must be auditable (<code>debug.audit.disabled</code>). <br><strong>Examples (multiple):</strong><br>1. SRE enables 30m diagnostics for ticket #42; module captures handler traces and <code>diagnostics.zip</code> is produced on demand; TTL auto-disables and audit logs both enable and disable. <br><strong>Tests:</strong> TTL auto-disable, audit lifecycle presence, confirm no PII in verbose logs. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>ExportRibbonMap(destinationUri, operatorId)</code> — secure export, redaction, checksums, and chain-of-custody</strong><br><strong>Purpose & contract:</strong> export <code>ribbon-map.json</code> + <code>OWNERS.md</code> snapshot with redaction where operator lacks rights, using <code>REG_Export</code> atomic write path. Compute <code>artifact.checksum.sha256</code>; append <code>ribbon.map.export</code> audit with URI and checksum. <br><strong>Examples (multiple):</strong><br>1. Compliance exports manifest to secure repo; returned artifact URI and checksum saved to <code>ribbon.map.export</code>. <br>2. If operator lacks permission for private owner emails, export redacts them and records redaction in metadata. <br><strong>Tests:</strong> checksum validation and redaction verification. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>RegisterUnitTestHook(hookName)</code> — CI deterministic harness, golden runs, and safeguards</strong><br><strong>Purpose & contract:</strong> support CI by enabling simulated control events without Excel UI. Hooks flagged <code>test=true</code> in audits must accept fixed <code>correlationId</code> for golden parity. Hooks MUST be disabled in production unless explicitly allowed. <br><strong>Examples (multiple):</strong><br>1. CI registers <code>hook_profile_golden</code> with <code>cid=r-test-001</code> and validates profile artifact against golden checksum. <br>2. Test hook attempts destructive action in protected environment and is rejected. <br><strong>Tests:</strong> golden parity and isolation from production. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>SafeHandlerTimeoutWatchdog(handlerToken, correlationId)</code> — cooperative cancellation, escalating audits, and examples</strong><br><strong>Purpose & contract:</strong> monitor handler execution time; on overrun emit <code>ribbon.handler.timeout</code>, attempt cooperative cancellation via token, and if unsuccessful append <code>ribbon.handler.hung</code> with stack capture for SRE. Use <code>Application.OnTime</code> or host idle scheduling for timers in VBA. <br><strong>Examples (multiple):</strong><br>1. Handler runs >5s; watchdog cancels token and logs timeout; UI shows timed-out message with correlation id. <br>2. Cancellation fails; <code>ribbon.handler.hung</code> appended with stack snapshot for off-line analysis. <br><strong>Tests:</strong> forced overrun path, cancellation effect, audit presence. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>TelemetryEmit(metricName, value, tags)</code> — local buffering, audited uplink, and example metrics</strong><br><strong>Purpose & contract:</strong> collect local metrics (no remote calls from ribbon path). Append to metrics buffer; audited uploader (separate module) performs remote export. Typical metrics: <code>ribbon.click.latency_ms</code>, <code>ribbon.handler.duration_ms</code>, <code>ribbon.handler.timeout_rate</code>. <br><strong>Examples:</strong><br>1. Each <code>HandleControlAction</code> emits <code>ribbon.click.latency_ms=12</code> with <code>{controlId:&quot;dq_profile&quot;}</code> tag. <br>2. Surge in <code>ribbon.handler.timeout_rate</code> triggers SRE runbook. <br><strong>Tests:</strong> buffer durability and uploader compatibility. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>SafeErrorToUser(correlationId, errorCode)</code> — UI-safe mapping, triage hint, and examples</strong><br><strong>Purpose & contract:</strong> map internal error codes to concise UI messages including <code>correlationId</code>, while storing full diagnostic traces encrypted. Append <code>ribbon.userErrorShown</code> audit. Messages must never contain PII or inner stack traces. <br><strong>Examples:</strong><br>1. <code>ERR_DB_CONN</code> → UI: "Temporary error (ref r-20260116-abc). Retry or contact support." Full stack saved encrypted. <br>2. <code>RIB_PERMISSION_DENIED</code> → UI: "Action requires approval (ref r-...). Request approval." <br><strong>Tests:</strong> ensure UI strings are PII-free and audits present. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>HotSwapHandlers(newMapJson, operatorId, approvals)</code> — transactional emergency patching, dry-run, smoke tests, persistence, rollback, and examples</strong><br><strong>Purpose & contract:</strong> apply transactional runtime mapping updates for urgent fixes with signed manifests and required approvals for regulated controls. Steps:<br>1. Validate manifest & signature. <br>2. Compute diff & produce <code>hotSwap.preview</code> with impacted controls and risk estimate. <br>3. Apply in-memory atomically and run smoke tests via unit hooks. <br>4. If smoke tests pass persist via <code>REG_Export</code> and append <code>ribbon.hotswap.applied</code> with <code>beforeHash</code>/<code>afterHash</code> and release fingerprint. <br>5. If tests fail revert and append <code>ribbon.hotswap.reverted</code>. <br><strong>Examples (multiple):</strong><br>1. Critical <code>HandleApply</code> bug fixed with hot-swap; smoke tests pass; <code>hotswap.applied</code> recorded. <br>2. Hot-swap fails smoke test; revert and produce <code>hotswap.reverted</code> audit and rollback manifest. <br><strong>Tests:</strong> dry-run validations, smoke test coverage, rollback correctness. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>Shutdown()</code> — graceful ribbon unload, audit flush ordering, and state snapshot</strong><br><strong>Purpose & contract:</strong> perform ribbon shutdown tasks at add-in unload: flush ribbon logs, persist minimal snapshot (<code>lastRefreshTs</code>, <code>lastCorrelationId</code>), unregister test hooks, and append <code>ribbon.shutdown</code> audit row. Must register with <code>modBootstrap</code> shutdown handlers at appropriate priority to allow <code>REG_Audit</code> flushes first. <br><strong>Examples (multiple):</strong><br>1. Normal exit: <code>Shutdown</code> flushes buffers and writes <code>ribbon.shutdown</code>. <br>2. Crash path: OS terminates Excel; after restart, <code>OnLoad</code> detects unclean exit and emits <code>ribbon.recovery</code> audit for operator. <br><strong>Tests:</strong> ensure buffer flush occurred and snapshot is present. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>JobSchedulerIntegration(jobDescriptor)</code> — canonical job descriptor schema, atomic persistence, idempotency, worker handoff, examples, and tests</strong><br><strong>Purpose & contract:</strong> construct canonical job descriptor for heavy actions and persist atomically for worker consumption. Descriptor fields: <code>jobId, controlId, correlationId, paramsHash, configHash, persistedAt, owner</code>. Persist via <code>REG_Utilities.atomic_write</code>; emit <code>job.persisted:&lt;jobId&gt;</code> audit. Must support idempotent persistence for requeue semantics. <br><strong>Examples (multiple):</strong><br>1. Heavy <code>profile</code> writes <code>job-222.json</code> with <code>paramsHash</code>, <code>job.persisted:job-222</code> emitted; worker consumes and appends <code>dq_profile</code> audit. <br>2. Duplicate persist call with same <code>jobId</code> returns existing descriptor (idempotent). <br><strong>Tests:</strong> persistence idempotency, retry/backoff simulation, job dedupe semantics. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>Audit obligations (module-level summary &amp; enforcement)</code> — chain rules, schema, signing, CI checks, examples</strong><br><strong>Mandate:</strong> every user-initiated action MUST append a <code>UserAction</code> audit row with <code>correlationId</code>. Artifact-producing handlers append step-level audits: <code>module.step:&lt;step&gt;</code>, <code>job.persisted</code>, <code>dq_proposal</code>, <code>dq_apply</code>, <code>dq_export</code>. Each audit includes <code>payloadHash</code>, <code>prevHash</code> (when resolvable), <code>configHash</code>, and <code>ribbonMap.hash</code>. <code>REG_Audit</code> rotates and signs rotations per retention policy; <code>VerifyAuditChain</code> runs in CI/monitoring to detect mismatches. <br><strong>Example audit chain:</strong> <code>UserAction</code> → <code>dq_profile</code> → <code>dq_proposal</code> → <code>dq_apply</code> → <code>dq_export</code>. <br><strong>CI enforcement:</strong> <code>audit-chain-verify</code> validates chain for golden runs prior to merge. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>Security &amp; secrets policy (ribbon)</code> — enforced rules, KMS/HSM usage, signing, and examples</strong><br><strong>Principles:</strong> ribbon core must never handle raw secrets directly. Credentials must be obtained via KMS/HSM through audited APIs during deferred init. Manifests and add-in binaries must be code-signed; <code>LoadRibbonMap</code> verifies signature locally. Logs must be redaction-aware; PII redaction enforced before persistence. <br><strong>Examples:</strong><br>1. Handler requests DB token via <code>modSecurity.getEphemeralToken()</code>; ribbon records token fingerprint only. <br>2. Unsigned manifest attempted in prod → reject and <code>ribbon.map.invalid</code> logged. <br><strong>Tests:</strong> static analyzer for direct secret reads, signature verification tests, key rotation simulation. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>Performance budgets &amp; SLOs (ribbon)</code> — targets, metrics, runbook, examples</strong><br><strong>Targets:</strong><br>1. Click handling median <50ms. <br>2. Job persist latency <2s. <br>3. Inline handler default timeout 5s (configurable). <br><strong>Metrics:</strong> <code>ribbon.click.latency_ms</code>, <code>ribbon.handler.duration_ms</code>, <code>ribbon.handler.timeout_rate</code>, <code>job.persist.latency_ms</code>. <br><strong>Remediation:</strong> throttle, offload to jobs, or enable degraded mode. <br><strong>Example:</strong> surge in <code>ribbon.click.latency_ms</code> triggers SRE runbook: collect diagnostics, throttle UI, scale worker pool, and revert recent manifest changes if correlated. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>Testing matrix (ribbon)</code> — required tests, golden governance, CI gating, and examples</strong><br><strong>Required tests:</strong><br>1. Unit: <code>ValidateControlId</code>, <code>MapControlToHandler</code>, <code>BuildUiParamsHash</code>. <br>2. Integration: click→audit→job persist→worker simulation. <br>3. Golden: manifest parity & <code>UserAction</code> audit hash. <br>4. Property: correlation id uniqueness under load. <br><strong>CI gating:</strong> block merges on golden/audit-chain failure or static forbidden-API detection. <br><strong>Example CI pipeline:</strong> <code>ribbon_unit</code>, <code>ribbon_integration</code>, <code>ribbon_golden</code>, <code>audit_chain_verify</code>. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>Failure modes &amp; mitigations (ribbon)</code> — canonical incidents, runbooks, and forensic evidence</strong><br><strong>Common cases & mitigations:</strong><br>1. Unknown control → <code>ribbon.control.validate</code> audit; operator updates manifest. <br>2. Permission denied → <code>ribbon.permission.denied</code>; approval workflow required. <br>3. Handler exception → <code>ribbon.handler.exception</code> with mapped code; SRE obtains encrypted logs by correlation id. <br>4. Handler timeout → watchdog cancels and emits <code>ribbon.handler.timeout</code>; schedule job if needed. <br>5. Job persist failure → retry/backoff; on repeated failures open incident and collect <code>forensic_manifest</code>. <br><strong>Evidence to collect:</strong> <code>ribbon-map.json</code>, <code>audit_tail.csv</code>, handler logs, job descriptors, <code>modConfig</code> snapshot, release manifest. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>Operator UX &amp; triage notes</code> — practical steps, examples, and commands</strong><br><strong>Best practice:</strong> always show correlation id and provide "copy diagnostics" action. <br><strong>Triage flow:</strong><br>1. Obtain correlation id from user. <br>2. Retrieve <code>UserAction</code> audit row and step-level audits. <br>3. Pull artifacts referenced (profile report, proposal). <br>4. If incident, collect <code>forensic_manifest</code> and escalate per IR runbook. <br><strong>Example scenario:</strong> support ticket "Propose failed (ref r-20260116-abc)" → SRE retrieves audit chain, replays fault in isolated runner, files bug, and attaches <code>forensic_manifest</code>. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>Change-control &amp; governance (ribbon)</code> — required steps, approvals, migration manifest, and examples</strong><br><strong>Required flow for changes:</strong><br>1. Create PR + migration manifest if behavior semantics change. <br>2. Run static analysis, unit/integration/golden tests. <br>3. Obtain code-review and compliance signoffs for regulated changes. <br>4. Sign artifacts and publish release manifest. <br>5. Canary rollout with KPI gating & rollback plan. <br>6. Post-rollout <code>VerifyAuditChain</code> and <code>deployment.audit</code>. <br><strong>Example artifact:</strong> <code>migration_manifest.json</code> documents transforms, sample sizes, backout plan, and approvals. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>Evidence to collect for ribbon incidents</code> — canonical forensic package & storage</strong><br><strong>Minimum artifacts:</strong><br>1. Current + prior <code>ribbon-map.json</code> and signatures. <br>2. <code>audit_tail.csv</code> rows covering correlation ids. <br>3. Handler logs from <code>modRibbonCallbacks</code>. <br>4. Persisted job descriptors. <br>5. <code>modConfig</code> snapshot and <code>config.hash</code>. <br>6. Release manifest and artifact signatures. <br>7. <code>forensic_manifest.json</code> with sha256 checksums and storage URI. <br><strong>Storage:</strong> secure evidence repo with RBAC and chain-of-custody records. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>Acceptance criteria (dev/CI)</code> — gating checklist & enforcement</strong><br><strong>Gates:</strong><br>1. Unit + integration + golden tests pass. <br>2. No forbidden API references in static analysis. <br>3. <code>ribbon.map</code> schema validated and <code>ribbonMap.hash</code> produced. <br>4. <code>UserAction</code> audits emitted and <code>VerifyAuditChain</code> passes. <br>5. Correlation id uniqueness tests pass. <br>6. Performance budgets met under CI load. <br><strong>Blocking conditions:</strong> golden/audit-chain failures or forbidden API detections. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>Forbidden APIs / static enforcement</code> — explicit banned list, rationale, and CI actions</strong><br><strong>Disallowed in ribbon core and inline handlers:</strong><br>1. Direct Workbook/Range mutation in <code>OnLoad</code> or ribbon core. <br>2. Raw network sockets (WinHTTP) or external web calls during bootstrap/ribbon main path. <br>3. Unbounded synchronous disk writes that block UI (>10ms). <br>4. Direct secret reads from plaintext files. <br>5. Spawning external processes. <br><strong>Enforcement:</strong> CI static analyzer rejects PRs referencing blacklisted APIs and emits <code>forbidden-api</code> failures. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong><code>Appendices &amp; references</code> — canonical schemas, templates, runbooks, and storage paths</strong><br><strong>Include:</strong> ribbon manifest JSON Schema, audit row schema, <code>ErrorCodeCatalog.md</code>, migration manifest template, forensic manifest template, operator cheat-sheets, PQ template guidelines, unit test harness docs, CI golden-file guide, release manifest signing checklist, and <code>OWNERS.md</code> mapping. <br><strong>Storage & governance:</strong> immutable artifact store <code>\\artifacts\runbooks\modRibbonCallbacks\v{major}.{minor}\appendices\</code> with RBAC. <br><strong>Example artifacts:</strong> <code>appendices/templates/migration_manifest.json</code>, <code>runbooks/cheatsheets/ribbon-smoke-test.md</code>. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong>Narratives — detailed human-readable end-to-end stories (extended)</strong><br><strong>Narrative 1 — Happy-path start & quick preview:</strong> Excel launches, host loads add-in; <code>OnLoad(ribbonUI)</code> caches <code>ribbonUI</code>, schedules <code>DeferredInit</code>, emits <code>ribbon.onload</code> with <code>cid=r-start-001</code>. <code>DeferredInit</code> runs <code>LoadRibbonMap</code> which validates and computes <code>ribbonMap.hash=sha256:abcd...</code>; <code>ribbon.map.loaded</code> emitted. Operator selects table and clicks <code>dq_profile</code>. <code>HandleControlAction</code> validates control, creates <code>cid=r-20260116-abc</code>, emits <code>ribbon.useraction</code> with <code>paramsHash</code>. <code>IsLightweightAction</code> returns true; <code>SafeInvokeHandler</code> runs inline with seeded RNG (for deterministic sample), appends <code>ribbon.handler.start</code> and <code>ribbon.handler.complete</code>; UI shows preview and correlation id. Evidence store contains sanitized params and profile artifact; main audit contains <code>paramsHash</code> only. <br><strong>Narrative 2 — Large apply scheduled & worker determinism:</strong> Operator triggers <code>dq_apply</code> on 600k-row dataset; <code>IsLightweightAction</code> chooses scheduled due to row threshold and regulation flag. <code>HandleControlAction</code> emits <code>ribbon.useraction</code>, persists <code>jobDescriptor</code> atomically, emits <code>job.persisted:job-901</code>. Worker picks job, verifies <code>paramsHash</code>, seeds RNG with <code>jobId</code> and runs <code>REG_Calculations</code> pipeline (normalization, SafeRound), <code>DQ_Rules</code> evaluation, and produces <code>dq_proposal</code> with <code>artifact.checksum</code>. Evidence and artifacts stored encrypted; step-level audits appended. <br><strong>Narrative 3 — Manifest tamper detection & operator recovery:</strong> During deferred init a manifest fails signature verification; <code>LoadRibbonMap</code> emits <code>ribbon.map.invalid</code> with <code>errorCode=RIB_MAP_SIG_001</code>. Regulated controls disabled. Operator exports diagnostics, files forensic bundle, and uploads signed manifest via release flow. <code>HotSwapHandlers</code> executes with approvals and smoke tests via <code>RegisterUnitTestHook</code>; <code>ribbon.hotswap.applied</code> emitted on success; controls re-enable. <br><strong>Narrative 4 — CI golden-run reproducibility:</strong> CI registers <code>hook_profile_golden</code> with <code>cid=r-test-001</code>, loads canonical <code>ribbonMap.json</code> and <code>config.hash</code>, simulates <code>dq_profile</code> with fixed seed; <code>REG_Audit</code> rotation is signed; <code>VerifyAuditChain</code> compares computed chain to golden artifact. Mismatch blocks merge. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong>Examples — pragmatic operator & developer scenarios</strong><br><strong>Example A — Registering a callback safely:</strong> Install script calls <code>RegisterCallback(&quot;dq_phone_norm&quot;,&quot;NormalizePhoneHandler&quot;,{owner:&quot;dq-team&quot;,requiresApproval:false,version:&quot;1&quot;})</code>. Module validates handler signature, resolves owner in <code>OWNERS.md</code>, updates in-memory map and emits <code>ribbon.callback.registered</code>. If <code>persist=true</code> performs atomic manifest update and emits <code>ribbon.map.loaded</code>. <br><strong>Example B — Emergency disable:</strong> SRE calls <code>DisableControl(&quot;dq_export&quot;,operatorId=&quot;sre_oncall&quot;,reason=&quot;data-leak suspected&quot;)</code>. RBAC verifies permission; control disabled in-memory and <code>ribbon.control.disabled</code> emitted. If persist requested, atomic manifest update requires approvals. <br><strong>Example C — Timeout with partial output:</strong> Inline handler overruns budget; watchdog emits <code>ribbon.handler.timeout</code> and toggles cancel token; handler writes partial artifact to evidence store and returns; <code>ribbon.handler.complete</code> appended with <code>partialResultRef</code>; UI shows "Partial results (ref r...)". </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong>Conceptual Power Query (PQ) guidance — provenance, injection safety, preview model (no code)</strong><br><strong>Purpose & rationale:</strong> PQ templates (M language) enable repeatable ETL logic and must be auditable. Templates require <code>templateVersion</code>, <code>mChecksum</code>, and declared owner. Ribbon must prevent credentials embedded in templates and ensure preview/inject paths are auditable. <br><strong>Provenance & checksums:</strong> each PQ template must declare <code>templateVersion</code> and canonical <code>mChecksum</code> (SHA256 of canonicalized M text). Audits referencing templates (<code>pq_preview</code>, <code>pq_inject</code>, <code>pq_export</code>) include <code>mChecksum</code> and <code>templateVersion</code>. <br><strong>Deterministic preview model:</strong> previews run on deterministic samples seeded with <code>correlationId+config.hash</code> or canonical sample slices. Previews must be side-effect free (no workbook query creation). <code>pq_preview</code> audit includes <code>mChecksum</code>, <code>sampleDescriptor</code>, <code>runtimeSeed</code>, <code>paramsHash</code>. <br><strong>Safe injection & connection handling:</strong> before <code>Add_Query_From_M</code> strip credentials and replace with <code>credentialRef</code> tokens referencing vault entries; if operator creates connection, persist only <code>connFingerprint</code> in audit, never raw secrets. Hidden-sheet fallback allowed for offline use; emit <code>pq_library.fallback</code> audit when used. <br><strong>Diagnostics & refresh policy:</strong> <code>pq_refresh</code> audits provider timings, refresh path, error traces, <code>mChecksum</code>. <code>pq_diagnostics</code> contains <code>refreshTimeMs</code> and errorCode. <br><strong>Governance & CI:</strong> PQ template changes require PR+owner approval+golden tests verifying <code>mChecksum</code> changes; regulated templates require <code>migration_manifest</code> & smoke tests. <br><strong>PQ operational examples:</strong><br>1. Operator previews template → <code>pq_preview</code> with <code>mChecksum</code> and <code>paramsHash</code>. <br>2. Operator injects template → <code>pq_inject</code> audit with <code>queryName</code>, <code>mChecksum</code>, <code>connFingerprint</code>. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong>Conceptual DAX guidance — governance, measure provenance & deterministic evaluation (no code)</strong><br><strong>Purpose & constraints:</strong> DAX measures affecting regulated outputs must be versioned, reviewed, tested, and auditable. Ribbon flows inserting/updating measures must require approvals and produce audit traces. <br><strong>Measure provenance & checksum:</strong> each measure introduced via ribbon must have <code>measureId</code>, <code>measureDefinitionVersion</code>, <code>owner</code>, <code>measureChecksum</code> (canonicalized measure text hash). Audits creating/updating measures include <code>measureChecksum</code> and <code>migration_manifestRef</code>. <br><strong>Avoid surprise mutations:</strong> updates to measures affecting regulated reports require two-person approvals and a <code>migration_manifest</code> documenting expected deltas with golden fixtures. <br><strong>Deterministic evaluation harness:</strong> CI must evaluate measures on canonical sample datasets under fixed seed for time-dependent functions; <code>dax.preview</code> emits <code>measureChecksum</code>, <code>sampleContext</code>, <code>previewChecksum</code>. <br><strong>Measure design guidance:</strong> prefer compositional, testable measures; avoid volatile external state; document cardinality and performance cost. <br><strong>Operator flow examples (conceptual):</strong> operator previews measure on sample dataset (<code>dax.preview</code>) → deterministic result and <code>dax.preview</code> audit; <code>dax.apply</code> requires approvals for regulated measures and emits <code>dax.apply</code> with pre/post checksums. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong>Developer playbook & safe I/O patterns (practical rules)</strong><br>1. <strong>Read-verify-swap:</strong> for any manifest or export—read source, verify signature/checksum, write to temp file, compute checksum, then atomic rename to final path. <br>2. <strong>No network on main path:</strong> network fetches allowed only in deferred init or worker contexts; always degrade gracefully with cached fallback and emit <code>ribbon.map.warning</code> when network unavailable. <br>3. <strong>Evidence & hashing:</strong> compute <code>paramsHash</code> and <code>payloadHash</code> before artifact persistence; store sanitized evidence encrypted and reference with <code>evidenceRef</code>. <br>4. <strong>Deterministic RNG & SafeRound:</strong> use seeded RNG and deterministic SafeRound to guarantee reproducibility of previews and small numerical differences. <br>5. <strong>CI static enforcement:</strong> forbid forbidden APIs via build-time linters (workbook mutations on <code>OnLoad</code>, raw secret reads, network calls on UI thread). <br>6. <strong>Tests & golden runs:</strong> provide test hooks (with <code>test=true</code> audits) disabled in production by default. Golden runs require fixed <code>correlationId</code>, <code>config.hash</code>, and <code>ribbonMap.hash</code>. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong>Appendices: schemas, artifacts & required documents</strong><br>Store and publish artifacts under immutable artifact store with RBAC: canonical <code>ribbonMap.json</code> schema, <code>UserAction</code> audit schema, <code>ErrorCodeCatalog.md</code>, <code>migration_manifest.json</code> template, <code>forensic_manifest.json</code> template, <code>OWNERS.md</code>, release manifest signing checklist, PQ template registry (<code>pq_templates.json</code>) with <code>mChecksum</code>, and DAX measure registry with <code>measureChecksum</code>. Each artifact includes computed checksums and corresponding audit rows for traceability. <br><strong>Required CI checks:</strong> <code>audit-chain-verify</code>, static analyzer forbidding APIs, golden parity verification, manifest signature checks, unit & integration test coverage thresholds. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong>Operator CLI & quick commands (reference)</strong><br><code>diagnostics collect --cid &lt;cid&gt; --out forensic.zip</code> — collects audit slice and artifacts and produces signed <code>forensic_manifest.json</code>. <br><code>audit flush --immediate --correlation &lt;cid&gt;</code> — forces audit flush and rotation and emits <code>audit.flush</code> row. <br><code>jobs requeue --job-id &lt;id&gt; --force</code> — requeue job (SRE approval required); emits <code>job.requeue</code> audit. <br><code>exports stage-local --artifact &lt;id&gt;</code> — stage artifact for offline collection; emits <code>export.stage</code> audit. <br><code>ribbon.refresh --dry-run</code> — preview manifest reload/diff; emits <code>ribbon.refresh.preview</code>; <code>ribbon.refresh --apply</code> performs apply if operator has permissions. Each CLI invocation emits an audit row with <code>operatorId</code> and reason. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong>Incident runbook (concise steps)</strong><br>1. Obtain <code>correlationId</code> from user. <br>2. Fetch <code>UserAction</code> anchor and step-level audits via audit tooling. <br>3. Retrieve evidence artifacts referenced by <code>evidenceRef</code> (requires operator approvals). <br>4. If manifest/signature issues suspected, retrieve <code>ribbon-map.json</code> and signed audit rotations. <br>5. If job persist or worker failures, retrieve job descriptors and worker logs; attempt safe requeue after investigation. <br>6. Assemble <code>forensic_manifest.json</code> with checksums and store securely; document chain-of-custody with audit rows. <br>7. Notify compliance/regulatory teams if required and provide forensic package. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong>Retention & housekeeping</strong><br>Automate retention verification monthly; audit rotations: hot=30d, warm=7y, cold=per regulation; scheduled maintenance windows for cleanup & kill-switch tests. {audit: housekeeping.audit} </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong>Final acceptance checklist (dev/CI)</strong><br>• Unit + integration + golden tests pass. <br>• CI <code>VerifyAuditChain</code> passes for golden runs. <br>• Static analyzer reports no forbidden API references. <br>• Manifest signature verification vectors pass. <br>• Performance SLOs validated under realistic load. <br>• Two-person approvals recorded for regulated mapping changes. <br>• Smoke tests for hot-swap & refresh succeed. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong>Implementation anti-patterns (for CI & reviews)</strong><br>DO: emit <code>ribbon.useraction</code> prior to side-effects; compute <code>paramsHash</code> and store sanitized evidence to encrypted store; use <code>atomic_write</code> and SafeRound deterministic patterns; seed RNG for previews. <br>DO NOT: access workbook Ranges in <code>OnLoad</code>; perform synchronous network calls on UI thread; log secrets or PII in main audit rows; accept unsigned manifests in production. <br><strong>CI enforcement:</strong> static analyzer should reject forbidden API usage and secret exposure. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong>Glossary (practical short list)</strong><br><code>correlationId</code> — unique trace identifier linking bootstrap→ribbon→job→worker. <br><code>ribbonMap.hash</code> — SHA256 canonical manifest hash. <br><code>paramsHash</code> — SHA256 of canonicalized, redacted UI params. <br><code>evidenceRef</code> — secure pointer to encrypted sanitized evidence. <br><code>payloadHash</code> — SHA256 of produced artifact for audit linkage. <br><code>jobDescriptor</code> — canonical persisted job artifact consumed by workers. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong>Extended checklists & sample schemas (operational reference)</strong><br><strong>UserAction audit row (canonical fields):</strong> <code>timestamp, correlationId, module=REG_Ribbon, procedure, userId, controlId, paramsHash, evidenceRef, configHash, ribbonMapHash, prevHash, metadata{tenantId, operatorRole, sessionId}</code>. <br><strong>Job descriptor sample fields:</strong> <code>jobId, controlId, correlationId, paramsHash, configHash, persistedAt, owner, priority, retries, retryPolicy, payloadRef (evidenceRef), workerAffinity</code> — always persisted via atomic-write and recorded with <code>job.persisted:&lt;jobId&gt;</code>. <br><strong>RibbonMap manifest canonical expectations:</strong> controls sorted by <code>controlId</code>, each control object includes <code>controlId, label, handler, estimatedCost, owner, requiresApproval, regulated, visibilityRules, versions[]</code> and module-level <code>manifestVersion</code>, <code>buildId</code>, <code>signature</code>, and <code>createdBy</code>. Manifest canonicalization sorts keys and arrays for stable hash computation. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong>Extended governance & release flow (operational)</strong><br>1. Developer opens PR for manifest or mapping changes. <br>2. CI runs static analyzer (forbidden APIs), unit tests, integration tests, golden-run parity, <code>VerifyAuditChain</code>. <br>3. If tests pass, obtain code-review and approvals; if regulated change, obtain compliance approval and two-person sign-off. <br>4. Produce signed release manifest via release pipeline; compute <code>ribbonMap.hash</code> and sign with release key (record fingerprint). <br>5. Canary rollout: apply signed manifest to a pilot cohort; run KPI gating tests (error rates, audit parity, performance SLOs). <br>6. If canary passes, full rollout; if issues, hot-swap revert and create incident with forensic package. <br><strong>Audit:</strong> each step produces <code>deployment.audit</code>, <code>canary.audit</code>, and <code>hotSwap.*</code> records. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong>Additional narratives — edge cases & recovery scenarios (practical)</strong><br><strong>Edge narrative 1 — partial evidence corruption:</strong> worker completes and writes artifact to evidence store; subsequent evidence verification fails due to storage corruption. System emits <code>artifact.verification.fail</code>, worker appends <code>dq_apply.partial</code> audit with <code>partialArtifactRef</code> and <code>forensic_manifest</code> references; operator notified and can retrieve pre-signed forensic bundle for offline analysis, then re-run job after cache restoration. <br><strong>Edge narrative 2 — audit rotation failure during shutdown:</strong> <code>Shutdown()</code> attempts to flush and rotate <code>audit_tail.csv</code> but rotation signing fails due to KMS transient outage. Append <code>ribbon.shutdown</code> with <code>shutdownStatus:&quot;audit.rotate.fail&quot;</code> and queue rotation retry with exponential backoff; notify SRE and persist unsent rotation to secure local store for later signing once KMS returns. <br><strong>Edge narrative 3 — de-synced ribbonMap across clustered hosts:</strong> multiple clients detect different <code>ribbonMap.hash</code> during same session; <code>ribbon.refresh</code> enforces atomic swap and broadcasts <code>ribbon.map.version</code> to clients; clients with stale map emit <code>ribbon.map.mismatch</code> audit and re-fetch signed manifest; if mismatch persists, isolate host and alert. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong>Risk matrix & mitigation (high-level)</strong><br><strong>Risk:</strong> Manifest tamper or signature bypass. <br><strong>Impact:</strong> High (regulated controls misbehave). <br><strong>Mitigation:</strong> require signed manifests, local signature verification, fail-closed for regulated controls, <code>ribbon.map.invalid</code> audits. <br><strong>Risk:</strong> Job persist failure or job duplication. <br><strong>Impact:</strong> Medium-high (missed work or duplicate destructive actions). <br><strong>Mitigation:</strong> idempotent job descriptors, deterministic <code>jobId</code> derivation, bounded retry/backoff, <code>job.persist.fail</code> alarms. <br><strong>Risk:</strong> PII leak in audit or logs. <br><strong>Impact:</strong> High (compliance breach). <br><strong>Mitigation:</strong> <code>BuildUiParamsHash</code> redaction, evidence store encryption, audit-only <code>paramsHash</code> in main row, static analysis to detect logs with PII patterns. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong>Operator training quick checklist (frictionless)</strong><br>1. Always capture and provide <code>correlationId</code> when escalating. <br>2. Use <code>diagnostics collect --cid &lt;cid&gt;</code> to build forensic package. <br>3. Use <code>ribbon.refresh --dry-run</code> to preview manifest diffs before applying. <br>4. Use <code>DisableControl(controlId)</code> when containing incidents; persist only with approvals. <br>5. For regulated changes, require two-person approval and signed manifest before persistence. </td></tr><tr><td data-label="REG_Ribbon — Per-function Expert Technical Breakdown"> <strong>Closing operational directive</strong><br>The ribbon layer must remain thin, fast, auditable, and deterministic. Keep heavy transformations in workers; always anchor user actions with <code>ribbon.useraction</code> and <code>correlationId</code>. Enforce manifest signature checks and evidence encryption. Use the atomic-write, SafeRound, deterministic RNG, and audit-first patterns as non-negotiable building blocks. </td></tr></tbody></table></div><div class="row-count">Rows: 54</div></div><div class="table-caption" id="Table2" data-table="Docu_0178_02" style="margin-top:2mm;margin-left:3mm;"><strong>Table 2</strong></div>
<div class="table-wrapper" data-table-id="table-2"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by REG_EnsureDeps — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">REG_EnsureDeps — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Top-level intent & executive summary (concise):</strong><br>REG_EnsureDeps is the canonical runtime dependency verification module for regulated systems (REG_<em>, DQ_</em>, PQ_*). Its purpose is to deterministically discover declared and transitive dependencies, verify presence and integrity, validate signatures and runtime compatibility, classify severity, produce an auditable canonical <code>deps.report.json</code>, and drive deterministic enable/disable decisions for runtime features. It is conservative: regulated controls fail-closed on missing/invalid dependencies; non-critical features use degradable fallback policies. The module produces machine-readable outputs for downstream automation and human-readable diagnostics for operators, while placing raw sensitive evidence into an encrypted evidence store referenced from the report. REG_EnsureDeps is designed for deferred-init usage and background deep validation when required. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Design principles, absolute invariants, and operational mandates (must/shall):</strong><br>1. <strong>Determinism:</strong> identical inputs produce byte-for-byte identical <code>deps.report.json</code> and <code>reportHash</code> across runs and platforms when using the same canonicalization rules. <br>2. <strong>Fail-closed for regulated features:</strong> any dependency classified <code>invalid</code> with <code>severity=critical</code> blocks the regulated control(s) that depend on it. <br>3. <strong>Minimal synchronous footprint:</strong> synchronous validation does only cheap discovery and local checksum checks; heavy signature chains, network lookups, and CRL/OCSP checks are offloaded to background workers. <br>4. <strong>Idempotency:</strong> repeated runs with unchanged inputs produce identical outputs and do not duplicate evidence artifacts. <br>5. <strong>Audit separation & redaction:</strong> audit rows contain safe summary metadata and <code>reportHash</code>; full evidence lives encrypted and is referenced by <code>evidenceRef</code>. <br>6. <strong>Key management & signing policy:</strong> signature verification uses a trusted public key registry managed by KMS/HSM; private keys never stored by REG_EnsureDeps. <br>7. <strong>No UI thread blocking:</strong> any call expected from the UI path returns quickly; if full validation is required, the function returns <code>partial</code> and schedules a background job. <br>8. <strong>Atomic persistence:</strong> <code>deps.report.json</code> writes must be atomic (temp+fsync+rename or object-store conditional update) and a <code>.prev</code> backup preserved. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Who owns/operates this module & governance expectations:</strong><br>1. The module MUST reference <code>OWNERS.md</code> mapping <code>depId</code> → <code>teamAlias</code> → <code>oncallContact</code> inside scanned manifests or the codebase. <br>2. For regulated dependencies, <code>approvalPolicy</code> MUST be present and set to <code>twoPerson</code> or stricter. <br>3. Every owner must provide an artifact distribution endpoint and a contact channel for rapid remediation. <br>4. Policy exceptions require documented, signed approvals stored in the evidence store and are auditable. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Public API surface and contract (authoritative list):</strong><br>1. <code>REG_EnsureDeps(context, manifestPaths=[], options={verifySignatures:true, allowNetworkLookup:false, strictMode:true, reportDir:default})</code> — orchestrator, writes canonical report, returns <code>{status, reportPath, reportHash, summary}</code>. <br>2. <code>DiscoverDeclaredDeps(manifestPaths)</code> — deterministic discovery of declared deps with provenance. <br>3. <code>ResolveSearchPaths(config, env)</code> — compute deterministic search path list. <br>4. <code>VerifyArtifactLocal(path, depDescriptor, options)</code> — existence, metadata extraction, local checksum. <br>5. <code>CanonicalizeAndChecksum(artifact, type)</code> — produce canonical bytes (rules differ by type) and SHA256 checksum. <br>6. <code>VerifySignature(artifactPath, trustedKeyring, options)</code> — local signature verification; returns <code>{present, signerFingerprint, valid, reason}</code>. <br>7. <code>ResolveTransitiveDeps(depList)</code> — deterministic transitive closure, cycle detection, topological sort where possible. <br>8. <code>ClassifyDependency(depRecord, policy)</code> — produce <code>status</code> and <code>severity</code> and remediation hints. <br>9. <code>ProduceDepsReport(resolvedDeps, graph, meta)</code> — canonical JSON serializer + atomic write; compute <code>reportHash</code>. <br>10. <code>EmitDepsAudit(context, eventType, payload)</code> — append audit rows: <code>deps.check.started</code>, <code>deps.check.completed</code>, <code>deps.check.failed</code>, <code>deps.validation.scheduled</code>, plus step-level events. <br>11. <code>ScheduleBackgroundValidation(jobDescriptor)</code> — persist idempotent background job and emit <code>deps.validation.scheduled</code>. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>High-level operational contract and expected calling patterns:</strong><br>1. <strong>Bootstrap / Deferred init:</strong> called by <code>REG_Bootstrap</code> and <code>DQ_Bootstrap</code> during deferred init (on idle) to verify artifacts and set runtime enablement. <br>2. <strong>On-demand checks:</strong> called by operator-initiated "Validate dependencies" actions that require a full synchronous pass (may still offload heavy parts). <br>3. <strong>CI/Release pipelines:</strong> run in CI to produce <code>deps.report.json</code> golden artifacts and ensure parity with release manifests. <br>4. <strong>Background monitors:</strong> scheduled periodic background runs to pre-empt certificate expiry and signature drift. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Canonical discovery & parsing rules (deterministic):</strong><br>1. <code>manifestPaths</code> enumerated in lexicographic order. <br>2. Each manifest parsed with UTF-8 normalization and JSON canonicalization rules applied to extract declared dependencies. <br>3. For each declared dependency include fields: <code>depId</code>, <code>type</code> (<code>xlm|xlam|xll|pq_template|connector|driver|library</code>), <code>declaredVersion</code>, <code>criticality</code>, <code>ownerAlias</code>, <code>signatureRequired:Boolean</code>, <code>requiresRuntime</code>, and optional <code>mChecksum</code> for PQ templates. <br>4. If <code>depId</code> duplicates are found across manifests, resolve by provenance order with a deterministic tie-breaker and add <code>warning</code> listing indices; duplicates are not silently merged. <br>5. For PQ templates canonicalize M text before computing <code>mChecksum</code> (NFC normalization, LF newlines, strip editor metadata per policy). </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Canonical <code>deps.report.json</code> schema (required fields and structure):</strong><br>1. <code>reportVersion</code> (semver for report schema). <br>2. <code>reportHash</code> (sha256 of canonical bytes). <br>3. <code>scannedAt</code> (ISO8601 UTC). <br>4. <code>correlationId</code>, <code>configHash</code>. <br>5. <code>resolvedDeps</code>: array of objects <code>{depId, type, declaredVersion, resolvedVersion, checksum:{sha256}, path, ownerAlias, status, severity, signature:{present, signerFingerprint, valid, signerMetadata?}, runtimeCompatibility:{ok, details}, resolutionAction?, evidenceRef?}</code>. <br>6. <code>graph</code>: <code>{nodes:[depId], edges:[{from,to}]}</code> sorted deterministically. <br>7. <code>warnings[]</code>, <code>errors[]</code> with <code>{errorCode, depId?, message}</code>. <br>8. <code>transitiveStats</code> <code>{depth, maxFanOut, nodeCount}</code>. <br>9. <code>durationMs</code>, <code>toolVersion</code>, <code>prevReportHash?</code>. <br>10. <code>artifactReferences[]</code> pointing to evidence bundles (encrypted) with <code>evidenceRef</code> and retention metadata. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Classification rules and severity mapping (detailed):</strong><br>1. <code>present</code> — artifact located, checksum matches declared (if provided), semantic version matches declared constraint, signature valid when required, runtime compatibility ok. <br>2. <code>missing</code> — artifact not found in any search path; if <code>allowNetworkLookup=true</code> scheduler may attempt remote fetch in background. <br>3. <code>mismatched_version</code> — artifact resolved but <code>resolvedVersion</code> does not satisfy declared semver range. <br>4. <code>signature_invalid</code> — signature verification failed or missing where required. <br>5. <code>incompatible_runtime</code> — declared <code>requiresRuntime</code> not satisfied by host. <br>6. <code>degraded</code> — artifact absent but local fallback available (embedded template, prior version). <br>7. Severity mapping: <code>critical</code> (requires fail-closed for regulated controls) / <code>degradable</code> / <code>optional</code>. <br>8. ResolutionAction candidates included on each classified row: <code>upload_artifact</code>, <code>re-sign_artifact</code>, <code>apply_fallback</code>, <code>schedule_fetch</code>, <code>upgrade_runtime</code>, <code>owner_contact</code>. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Signature verification policy and details (explicit):</strong><br>1. Trusted keyring is authoritative; public keys managed via KMS/HSM with signed key metadata; key rotation recorded and audited. <br>2. For regulated dependencies signature validity is mandatory (<code>signatureRequired=true</code>), and invalid signatures yield <code>status=signature_invalid</code> and <code>severity=critical</code>. <br>3. Signature verification includes signer fingerprint extraction, certificate chain validation (CRL/OCSP checks deferred to background if slow), and timestamp validation (signing time within acceptable window). <br>4. If signature is missing but allowed in non-production environments, report <code>warning</code> and allow <code>degraded</code> behavior per <code>options</code>. <br>5. Signer fingerprints included in <code>deps.report.json</code> and evidence bundles; private keys and full cert chains stored only in encrypted evidence. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>PQ template handling and <code>mChecksum</code> governance (detailed conceptual rules):</strong><br>1. PQ templates are canonicalized before hashing: Unicode NFC, LF newlines, normalized parameter order where parameter schema indicates unordered vs ordered parameters. <br>2. <code>mChecksum</code> is computed as <code>sha256(canonicalMBytes)</code> and must match declared <code>mChecksum</code> in manifests. <br>3. Changes to a PQ template require an updated <code>templateVersion</code> and signed manifest for production promotion. <br>4. Hidden-sheet embedded templates are permitted as last-resort fallback and must be explicitly declared with <code>embedded:true</code> to be used automatically; otherwise operator consent required. <br>5. When injection is performed, <code>paramsHash</code> is computed for sanitized parameters and recorded in audit; full sanitized params saved in evidence with <code>evidenceRef</code>. <br>6. Template mismatches produce policy outcomes: regulated templates → <code>invalid</code> (fail-closed); recommended templates → <code>degraded</code> with scheduled reconciliation. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Transitive resolution, cycles & topological ordering (detailed):</strong><br>1. After initial discovery, compute transitive closure by visiting each declared dependency and expanding its own declared dependencies recursively. <br>2. Use deterministic traversal: children sorted by <code>depId</code> before expansion. <br>3. Produce <code>graph</code> with nodes and edges sorted lexicographically. <br>4. Detect cycles using Tarjan's strongly connected components algorithm (or equivalent); record cycles in <code>errors[]</code> with <code>cycleId</code> and constituent <code>depId</code>s. <br>5. Policy: cycles among optional helpers → <code>degraded</code>; cycles involving regulated components → <code>invalid</code> and fail-closed. <br>6. Provide remediation hints and owner lists for cycle resolution. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Evidence & encrypted bundles (<code>evidenceRef</code>) rules:</strong><br>1. Evidence bundles contain raw manifests, extracted cert chains, canonical M templates, and any private owner-provided diagnostic artifacts. <br>2. Evidence bundles are encrypted with KMS-managed keys and stored in evidence repo with <code>evidenceRef</code> URL or pointer. <br>3. Access to evidence requires owner/approval flow and is itself audited with <code>evidence.access</code>. <br>4. <code>deps.report.json</code> contains <code>evidenceRef</code> pointers, but not raw sensitive payloads. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Atomic write and backup policy for <code>deps.report.json</code>:</strong><br>1. Write canonical bytes to <code>deps.report.json.tmp.&lt;pid&gt;</code>; fsync file; rename to <code>deps.report.json</code>. <br>2. After successful write create a binary copy <code>deps.report.json.prev.&lt;ts&gt;</code> for forensic diffs. <br>3. On object stores, write versioned object and conditionally update the pointer with precondition (ETag) semantics to approximate atomic rename. <br>4. Recompute hash after write and verify equality before emitting <code>deps.check.completed</code>. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Audit events & required fields (exhaustive):</strong><br>1. <code>deps.check.started</code> — <code>timestamp, correlationId, module=REG_EnsureDeps, manifestPaths, configHash, operatorId?</code>. <br>2. <code>deps.check.completed</code> — <code>timestamp, correlationId, reportHash, resolvedCount, missingCount, degradedCount, durationMs</code>. <br>3. <code>deps.check.failed</code> — <code>timestamp, correlationId, errorCode, evidenceRef</code>. <br>4. <code>deps.validation.scheduled</code> — <code>timestamp, correlationId, jobId, reasons</code>. <br>5. <code>deps.validation.completed</code> — <code>timestamp, correlationId, jobId, updatedReportHash</code>. <br>6. <code>deps.override.applied</code> — <code>timestamp, correlationId, operatorId, justificationRef</code>. <br>7. Each audit must include <code>configHash</code> where relevant, and must not include PII; include <code>ownerAlias</code> and <code>evidenceRef</code> instead. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Synchronous vs background work: splitting rules & best practices:</strong><br>1. <strong>Synchronous (UI-safe) tasks:</strong> discovery, local existence checks, local checksum computation, initial signature presence check (not deep chain), basic classification for immediate UI decisions. <br>2. <strong>Background tasks:</strong> deep signature chain validation (CRL/OCSP), remote fetches, long-running transitive resolution across remote repositories, and certificate expiration analysis. <br>3. Synchronous entry returns <code>status:&quot;partial&quot;</code> when significant background checks are pending and includes <code>validationJobId</code>. <br>4. Background jobs must be idempotent and persist <code>jobDescriptor</code> for dedupe and retry. <br>5. When background validation completes, rotate <code>deps.report.json</code> atomically and emit <code>deps.validation.completed</code> and <code>deps.report.rotated</code>. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Telemetry & metrics (buffered local emitter):</strong><br>1. Buffered metrics: <code>deps.check.duration_ms</code>, <code>deps.missing.count</code>, <code>deps.sig.invalid.count</code>, <code>deps.validation.jobs.count</code>. <br>2. Emit locally and flush via separate uploader process to avoid synchronous network calls. <br>3. High-cardinality tags limited: include <code>platform</code>, <code>module</code>, and <code>severity</code> only; avoid including <code>depId</code> directly in high-cardinality metrics. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Failure modes, detection, and automated mitigations (operational playbook):</strong><br>1. <strong>Missing artifact discovered:</strong> emit <code>deps.check.completed</code> with <code>missingCount&gt;0</code> and <code>warnings[]</code>; if critical, emit <code>deps.check.failed</code> and disable dependent controls; remediation: owner responsible to upload artifact or enable fallback. <br>2. <strong>Signature invalid:</strong> mark dependency <code>signature_invalid</code> and <code>severity=critical</code> for regulated artifacts; create evidence bundle and escalate. <br>3. <strong>Runtime mismatch:</strong> record <code>incompatible_runtime</code> and suggest runtime upgrade; if runtime required for regulated feature, disable that feature. <br>4. <strong>Cycle detected:</strong> record cycle and mark severity per policy; suggest owner coordination. <br>5. <strong>Partial validation due to network disabled:</strong> return <code>partial</code> and schedule background fetch if operator requested; surface <code>userHint</code>. <br>6. <strong>Disk write failure during atomic write:</strong> keep previous <code>deps.report.json.prev</code> and retry with backoff; alert SRE and schedule manual intervention if repeated. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Operator UX & triage checklist (explicit):</strong><br>1. Copy <code>correlationId</code> from UI message. <br>2. Download <code>deps.report.json</code> referenced in diagnostics. <br>3. Inspect <code>errors[]</code> for <code>errorCode</code> and <code>depId</code>. <br>4. For <code>signature_invalid</code> contact <code>ownerAlias</code> to obtain signed artifact; upload to artifact store and run <code>REG_EnsureDeps --forceRefresh</code>. <br>5. For <code>missing</code> upload artifact or configure the local repo path and re-run. <br>6. For <code>incompatible_runtime</code> schedule runtime upgrade or pin module. <br>7. After remediation verify <code>deps.check.completed</code> and updated <code>reportHash</code>; attach <code>evidenceRef</code> and <code>deps.report.json.prev</code> to ticket. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Testing matrix & CI gating rules (exhaustive):</strong><br>1. <strong>Unit tests:</strong> discovery ordering, canonical serialization, checksum stability, PQ M canonicalizer tests, signature stub behavior. <br>2. <strong>Integration tests:</strong> transitive resolution, cycle detection, <code>deps.report.json</code> golden parity for canonical fixture set. <br>3. <strong>Negative tests:</strong> missing deps, duplicate <code>depId</code>, semver mismatch, signature tampering, partial uploads. <br>4. <strong>Performance tests:</strong> local checks for 500 artifacts within CI budget; background validation latency targets. <br>5. <strong>Security tests:</strong> verify that audit rows contain no PII, that evidenceRef exists for signature anomalies. <br>6. <strong>CI gating:</strong> fail PRs if golden <code>deps.report.json</code> parity fails or forbidden synchronous APIs are introduced. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Extremely detailed narrative examples and step-by-step walkthroughs (comprehensive):</strong><br><strong>Scenario A — Clean production release verification (end-to-end):</strong><br>1. Release artifact <code>release-v1.4-manifest.json</code> is uploaded to artifact store and includes 24 declared dependencies (10 PQ templates, 5 XLAMs, 4 connectors, 5 helper libs). <br>2. CI triggers <code>REG_EnsureDeps</code> during release pipeline; check runs in worker with <code>verifySignatures=true</code>. <br>3. <code>DiscoverDeclaredDeps</code> reads manifests deterministically and extracts <code>depId</code>, <code>mChecksum</code>, <code>ownerAlias</code>. <br>4. For each artifact the CI job uses <code>ResolveSearchPaths</code> pointing to release bundle; <code>VerifyArtifactLocal</code> finds everything and <code>ComputeChecksum</code> yields SHA256 checksums matching declared values. <br>5. <code>VerifySignature</code> validates signatures against the CI-trusted keyring; CRL/OCSP checks are run but return quickly as chain lengths are short. <br>6. <code>ResolveTransitiveDeps</code> expands declared dependencies into a graph with depth 3 and no cycles. <br>7. <code>ProduceDepsReport</code> writes <code>deps.report.json</code> atomically and computes <code>reportHash</code>. <br>8. CI archives <code>deps.report.json</code> and records <code>reportHash</code> in release manifest. <br>9. In production deferred-init the runtime reads <code>deps.report.json</code> and enables regulated features because no <code>critical</code> failures exist. <br><strong>Scenario B — PQ template mismatch detected on installed client (operator flow):</strong><br>1. Operator on client machine loads the add-in; <code>REG_EnsureDeps</code> runs in deferred init with <code>allowNetworkLookup=false</code>. <br>2. The manifest declares <code>normalize_address_v3</code> with <code>mChecksum=sha256:AA11...</code>. <br>3. Local repo lacks <code>normalize_address_v3</code> but includes embedded <code>normalize_address_v2</code> as an embedded fallback. <br>4. <code>REG_EnsureDeps</code> computes <code>status=degraded</code> for that template, writes <code>deps.report.json</code> with <code>resolutionAction:&quot;using_embedded_fallback&quot;</code> and <code>warnings[]</code>. <br>5. UI shows operator a preview-only control and <code>userHint</code> with <code>correlationId</code>. <br>6. Background job <code>deps.validation.scheduled</code> is queued (job persists) to fetch <code>normalize_address_v3</code> from artifact store as soon as network allowed; when fetched the background job recomputes the report, rotates it atomically, and emits <code>deps.validation.completed</code>. <br><strong>Scenario C — Signature invalid for a regulated XLAM helper (security incident path):</strong><br>1. <code>REG_EnsureDeps</code> during deferred init finds <code>xlam-helper-reg</code> present but <code>VerifySignature</code> reports <code>valid=false</code>. <br>2. The dependency is critical for <code>REG_Apply</code>; module classifies <code>status=signature_invalid</code>, <code>severity=critical</code>. <br>3. <code>ProduceDepsReport</code> writes report with <code>errors[{errorCode:DEPS_SIG_001, depId:&#x27;xlam-helper-reg&#x27;}]</code> and creates <code>evidenceRef</code> containing raw XLAM, cert chain, and computed checksum. <br>4. <code>deps.check.failed</code> audit emitted; <code>REG_Ribbon</code> disables <code>REG_Apply</code> and shows <code>userHint</code> "Regulated feature disabled — dependency verification failed (ref r-...)" without PII. <br>5. Operator follows runbook: contact owner, collect signed artifact, and upload to artifact store. <br>6. Operator calls <code>REG_EnsureDeps --forceRefresh</code>, which picks up the signed artifact and completes with <code>deps.check.completed</code>; controlled features are re-enabled. <br><strong>Scenario D — Transitive cycle discovered that affects non-regulated helpers (owner coordination flow):</strong><br>1. <code>ResolveTransitiveDeps</code> finds a cycle among three helper libraries <code>hlp-a</code>, <code>hlp-b</code>, <code>hlp-c</code>. <br>2. The cycle is recorded in <code>errors[]</code> with <code>cycleId</code> and owners for each node are listed. <br>3. Module classifies cycle severity as <code>degraded</code> since none of the involved deps are regulated. <br>4. <code>deps.report.json</code> includes remediation hints and owner aliases, and <code>deps.check.completed</code> emits with warnings. <br>5. Owners coordinate a patch release that breaks the cycle; background revalidation detects the fix and <code>deps.report.rotated</code> is emitted. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Operator remediation & runbook (detailed checklist):</strong><br>1. Retrieve <code>correlationId</code> from user's report or the UI. <br>2. Download <code>deps.report.json</code> referenced in diagnostics and inspect <code>errors[]</code>. <br>3. For <code>signature_invalid</code>: contact <code>ownerAlias</code>, request signed artifact or signer rotation details, upload signed artifact, and run <code>REG_EnsureDeps --forceRefresh</code>. <br>4. For <code>missing</code>: upload artifact to artifact store or add the location to <code>searchPaths</code>, then re-run <code>REG_EnsureDeps</code>. <br>5. For <code>incompatible_runtime</code>: schedule runtime upgrade or pin module; after upgrade re-run <code>REG_EnsureDeps</code>. <br>6. For cycles: coordinate owner teams to publish a fix; produce migration to break cycle and re-run. <br>7. After remediation, verify <code>deps.check.completed</code> with <code>reportHash</code> changed, archive <code>deps.report.json.prev</code> and attach <code>evidenceRef</code> to ticket for audit. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Extensive PQ conceptual guidance, governance, and examples:</strong><br><strong>Why PQ governance is essential:</strong> PQ templates define data ingestion and transformation semantics; small changes to M text can alter column types, null handling, join logic, and therefore downstream regulated outputs. Treat PQ templates as first-class, versioned artifacts with <code>mChecksum</code>, <code>templateVersion</code>, owners, and signature requirements for production promotion. <br><strong>Recommended manifest template entry fields for PQ templates (must include):</strong><br>1. <code>templateId</code><br>2. <code>templateVersion</code><br>3. <code>mChecksum</code> (sha256 of canonical M) <br>4. <code>ownerAlias</code><br>5. <code>parameterSchema</code> (JSON Schema for template parameters) <br>6. <code>expectedSampleOutputChecksum</code> (optional golden check) <br><strong>Canonical M canonicalization rules (implementation details):</strong><br>1. Normalize Unicode to NFC. <br>2. Convert CRLF to LF (<code>\n</code>) and remove trailing spaces. <br>3. Remove editor-only metadata lines when flagged (e.g., <code>#generated-by=…</code>), per manifest <code>stripEditorMeta</code> flag. <br>4. For parameter blocks, apply deterministic ordering if <code>parameterSchema.unordered==true</code> by sorting parameters by <code>name</code>. <br>5. Serialize canonical M bytes as UTF-8 and compute <code>mChecksum=sha256(canonicalBytes)</code>. <br><strong>PQ injection preconditions & safety contract:</strong><br>1. Confirm <code>mChecksum</code> matches declared value in <code>deps.report.json</code> before allowing injection. <br>2. If injection will create a new <code>Workbook.Query</code>, record <code>pq_inject</code> audit with <code>paramsHash</code> and <code>templateVersion</code>. <br>3. Full sanitized parameters stored in evidence; audit only contains <code>paramsHash</code>. <br>4. Do not automatically run refresh after injection when connectors may cause side effects; require operator confirmation. <br><strong>PQ diagnostics to collect during validation and runtime:</strong><br>1. <code>mChecksum</code> and <code>templateVersion</code>, <br>2. <code>parseTimeMs</code> when parsing M template, <br>3. <code>injectionTimeMs</code>, <br>4. <code>lastRefreshTimeMs</code>, <code>lastRefreshStatus</code>, <br>5. provider details (driver, connector version), <br>6. sanitized <code>paramsHash</code> and <code>evidenceRef</code>. <br><strong>PQ golden promotion story (example):</strong><br>1. Template author publishes <code>normalize_mails_v3.m</code> with <code>mChecksum</code>. <br>2. CI runs <code>mChecksum</code> parity and headless template static analysis (lint + expected sample output checksum) and produces <code>pq_template_report.json</code>. <br>3. Owners sign and release; <code>REG_EnsureDeps</code> in CI picks up new manifest and emits <code>deps.report.json</code> which CI stores alongside release. <br>4. Production deferred-init verifies <code>mChecksum</code> and signatures before enabling injection features. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Extensive DAX conceptual guidance, governance, and examples:</strong><br><strong>Why DAX governance matters:</strong> DAX measures are evaluated in the context of filters, relationships, and model metadata. Small changes to expressions or evaluation contexts can alter regulated aggregates used in reports; thus DAX expressions must be versioned, smoke-tested deterministically, and linked to expected sample values for CI gating. <br><strong>DAX manifest entry recommended fields:</strong><br>1. <code>measureId</code><br>2. <code>expressionHash</code> = <code>sha256(canonicalExpression)</code><br>3. <code>ownerAlias</code><br>4. <code>expectedSampleOutput</code> and <code>expectedSampleContextHash</code><br>5. <code>evaluationConstraints</code> (e.g., snapshot timestamp) <br><strong>Canonicalization rules for DAX expressions (conceptual):</strong><br>1. Strip comments, normalize whitespace, and normalize function casing if project policy requires. <br>2. Replace volatile functions (<code>NOW()</code>, <code>TODAY()</code>) with config-driven snapshot tokens for golden runs. <br>3. Compute <code>expressionHash=sha256(canonicalExpressionBytes)</code>. <br><strong>Headless DAX evaluation harness & CI requirements (recommended):</strong><br>1. Maintain a canonical <code>filterContext</code> snapshot with dataset checksums and small sample dataset for smoke-tests. <br>2. CI headless evaluator computes measure outputs and compares to <code>expectedSampleOutput</code> within tolerances; failing tests block promotion. <br>3. For promotion, require signature and owner approval if <code>expressionHash</code> changed. <br><strong>DAX change example workflow:</strong><br>1. Developer updates <code>netRevenue_v4</code> measure and computes new <code>expressionHash</code>. <br>2. CI runs headless DAX evaluation against canonical sample context; expected sample output matches within configured tolerance and <code>ci.dax_check</code> passes. <br>3. Owners sign the change and release manifest updated; <code>REG_EnsureDeps</code> and <code>REG_Config</code> ensure new <code>expressionHash</code> and evidence are recorded for production. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Large-scale operational narratives, expanded examples & multi-stage walkthroughs (very detailed):</strong><br><strong>A. Canary release with dependency verification and KPI gating (full flow):</strong><br>1. Release <code>manifest-v2.1</code> published with updated PQ templates and DAX measures; CI generates <code>deps.report.json</code> and <code>config.hash</code> for release. <br>2. Operator initiates Canary rollout: <code>ReloadConfig</code> swaps new config into 5% cohort after <code>REG_EnsureDeps</code> success in CI and signatures verified. <br>3. Canary cohort machines run deferred-init <code>REG_EnsureDeps</code> and local <code>deps.report.json</code> checks; any <code>critical</code> failure aborts canary. <br>4. Smoke-tests for PQ template outputs and DAX heads run; KPIs (error rate, latency, measure deltas) collected. <br>5. If metrics within thresholds for configured window, rollout expands to 25%, run another KPI window. <br>6. If KPI breach detected, auto-watchdog triggers <code>RollbackToHash(beforeHash)</code> and emits <code>config.rollback</code> with <code>forensic_manifest.json</code>. <br><strong>B. Emergency hot-swap & rollback with forensic capture (extreme case):</strong><br>1. Production incident: <code>REG_Apply</code> consumer reports correlated <code>correlationId</code> pointing to recent <code>config.reload</code>. <br>2. SRE author emergency <code>hotfix</code> manifest with minimal change and runs <code>hotSwap.preview</code> with <code>RegisterUnitTestHook</code> smoke-tests. <br>3. Two authorized approvers sign hotfix artifact; <code>HotSwapHandlers</code> applies in-memory patch for immediate remediation; <code>config.hotswap.applied</code> audit appended. <br>4. Watchdog monitors KPIs; if regression observed reverts <code>HotSwap</code> via <code>hotSwap.reverted</code> and triggers <code>RollbackToHash</code> to last good hash. <br>5. Forensics package assembled: <code>audit_tail.csv</code>, <code>deps.report.json.prev</code>, <code>forensic_manifest.json</code>, migration patches, PQ <code>mChecksum</code> history, DAX <code>expressionHash</code> records. <br>6. Regulatory notification prepared as needed using this canonical evidence. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Operational controls, RBAC & approvals (detailed):</strong><br>1. Changes impacting regulated features require <code>twoPerson</code> approval stored as signed artifacts referenced by <code>evidenceRef</code>. <br>2. Emergency overrides require two authorized approvers and limited TTL; <code>deps.override.applied</code> audit event must record operator ids and <code>justificationRef</code>. <br>3. Access to evidence store requires granular RBAC and approval flows; all access audited. <br>4. Operators can only re-enable regulated controls after <code>deps.check.completed</code> with no <code>critical</code> failures or after documented approved override is in place. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Long-term maintainability guidance & developer recommendations (practical):</strong><br>1. Provide canonicalization reference implementations for M templates, JSON manifests and DAX expression hashing in Node and Python; include unit tests for byte-for-byte parity. <br>2. Keep the synchronous path minimal: discovery + local verification only; offload heavy work to background workers. <br>3. Use copy-on-write in-memory snapshots for <code>GetDepsSnapshot</code> to serve ribbon and worker processes concurrently without locks. <br>4. Persist <code>deps.report.json.prev</code> and rotate evidence bundles with TTL per retention policy. <br>5. Add migration linting and idempotency checks into CI for any artifact that changes dependency graphs. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Monitoring, SLOs & runbook triggers (operational):</strong><br>1. <code>deps.check.duration_ms</code> median target <2s for typical release artifact checks (local-only), background validation target <30s. <br>2. <code>deps.missing.count</code> spike after release should page on-call if sustained beyond short window. <br>3. <code>deps.check.failed</code> events for <code>critical</code> severity should page SRE and notify owners. <br>4. Daily background scan of critical dependencies to detect certificate expiry and drift; automated notification 90/30/7 days before expiry. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Forensic packaging contents & retention policy (explicit):</strong><br>1. <code>deps.report.json</code> current and prior versions. <br>2. <code>audit_tail.csv</code> rows covering implicated <code>correlationId</code>s. <br>3. PQ M canonical templates with <code>mChecksum</code> history. <br>4. DAX <code>expressionHash</code> registry and <code>filterContext</code> snapshots for failing runs. <br>5. Evidence bundles with signatures, cert chains, and extracted metadata. <br>6. <code>forensic_manifest.json</code> enumerating artifacts with <code>sha256</code> checksums, retention tags and URIs. <br>7. Retention: encrypted evidence adheres to regulatory retention policies (example: 7 years for certain regulated outputs). </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Governance & compliance packaging (explicit):</strong><br>1. For regulated outputs, maintain chain-of-custody artifacts: signed config artifacts, migration fingerprints, PQ <code>mChecksum</code>s, DAX <code>expressionHash</code> entries, audit rotations, and golden-run evidence. <br>2. Ensure all artifacts in the compliance package are checksummed (sha256) and signed where required. <br>3. Evidence access requires owner-level approvals and is logged for chain-of-custody. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Acceptance criteria & release gating checklist (must pass):</strong><br>1. Unit + integration + golden <code>deps.report.json</code> parity tests pass. <br>2. No forbidden synchronous APIs used in code inspected by CI. <br>3. Audit rows emitted (<code>deps.check.started</code> → <code>deps.check.completed</code> or <code>deps.check.failed</code>) and <code>VerifyAuditChain</code> passes for golden runs. <br>4. Signature verification vectors present and passing. <br>5. Performance SLOs met for local checks and snapshot reads under load. <br>6. For regulated features, all required signatures and owner approvals present. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Appendices, canonical helper artifacts & reference materials to include in repository:</strong><br>1. Canonicalization specification for JSON manifests, M templates and DAX expressions with unit tests. <br>2. <code>deps.report.json</code> JSON Schema and a set of golden fixtures. <br>3. PQ M canonicalizer tool and <code>mChecksum</code> sample vectors. <br>4. DAX expression canonicalizer and <code>expressionHash</code> golden set. <br>5. <code>OWNERS.md</code> template and sample <code>approvalPolicy</code> documents. <br>6. Evidence store API and KMS/HSM integration guide. <br>7. CI pipeline steps for <code>REG_EnsureDeps</code> golden test runs. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Final prescriptive constraints & must-not list (non-negotiable):</strong><br>1. Do not perform network calls on the UI thread. <br>2. Do not persist unsigned artifacts for production regulated features unless explicit signed override is recorded. <br>3. Do not include plaintext secrets in manifests. <br>4. Do not accept arbitrary executable migration scripts in manifests for production. <br>5. Do not surface PII in audit rows or UI <code>userHint</code>. </td></tr><tr><td data-label="REG_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Ultimate operational takeaway (concise):</strong><br>REG_EnsureDeps must be deterministic, conservative for regulated features, auditable, reversible via preserved <code>.prev</code> artifacts, and integrated with PQ and DAX governance. It must balance fast synchronous decisions for the UI with deep background validation and produce canonical <code>deps.report.json</code> artifacts that serve CI, forensic, and operator workflows reliably. </td></tr></tbody></table></div><div class="row-count">Rows: 33</div></div><div class="table-caption" id="Table3" data-table="Docu_0178_03" style="margin-top:2mm;margin-left:3mm;"><strong>Table 3</strong></div>
<div class="table-wrapper" data-table-id="table-3"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by REG_Utilities — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">REG_Utilities — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Module-level metadata (contract & extended overview):</strong><br><strong>Owner:</strong> TEAM_REG_UTILS (recorded in <code>OWNERS.md</code>, cross-referenced in release manifests and deployment notes).<br><strong>Public API (canonical list):</strong> SafeRound, SafeRoundResiduals, AtomicWrite, Retry, DeterministicRNG, ChecksumStream, TempPathFor, FsSyncDir, WriteWithPermissions, serialize_rng_state, restore_rng_state, InspectTempArtifacts, AtomicWriteRepair, AuditEmitUtilEvent, plus small helpers: PathVolumeOf, EnsureSameVolume, EvidenceFingerprint. <br><strong>Audits emitted (representative):</strong> util.startup, util.atomic_write.attempt, util.atomic_write.completed, util.atomic_write.failure, util.atomic_write.repair, util.atomic_write.degraded, util.retry.attempt, util.retry.complete, util.saferound.start, util.saferound.complete, util.saferound.invalid_input, util.saferound.truncated_precision, util.saferound.residuals, util.rng.seeded, util.rng.state_serialized, util.checksum.start, util.checksum.complete. Every audit row includes <code>correlationId</code>, <code>module=REG_Utilities</code>, <code>procedure</code>, <code>paramsHash</code>, and <code>resultHash</code> when appropriate; long/large payload evidence is referenced via <code>evidenceRef</code> rather than embedded in the audit row itself. <br><strong>Purpose and intended use (expanded):</strong> provide deterministic, auditable, cross-platform primitives that higher-level modules depend on for compliance, replayability, and crash-safety. These primitives are intentionally small-surface, side-effect-minimal, and embeddable in host environments ranging from worker processes to signed XLAM helpers. Fast paths must not perform network I/O, must not expose unencrypted secrets, and must avoid behavior that would prevent safe embedding into UDFs or host add-ins. <br><strong>Non-goals / enforced constraints (expanded):</strong> the utilities do not attempt to manage multi-file distributed transactions, do not assume a single consistent distributed filesystem semantics, and do not hold locks indefinitely across process restarts. Secret management, external credential rotation, and long-term archive lifecycle are delegated to specialized modules (modSecurity, CORE_Telemetry, CORE_Archive). All exceptions to these constraints must be explicitly approved by owners and documented in migration manifests. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Operational guarantees (module-level invariants & SLOs — extended):</strong><br><strong>Invariants:</strong><br>1. Determinism: given identical canonical inputs, seeds, and configuration, outputs including checksums, RNG sequences, and rounded allocations must be reproducible across supported platforms and language bindings. <br>2. Audit-anchored state changes: any mutation of durable state must be accompanied by at least one audit row referencing correlationId and essential provenance fields; where payloads are large, a reference (<code>evidenceRef</code>) to encrypted evidence must be attached. <br>3. Crash-safety: atomic persistence primitives (AtomicWrite family) must provide final-on-success semantics — consumers should never observe a truncated artifact (old artifact or full new artifact only). <br>4. Platform-aware fallbacks: when host FS semantics are weak (cross-device rename, NFS semantics), the module must degrade to documented safe sequences and emit <code>util.atomic_write.degraded</code> with rationale and mitigation guidance. <br>5. UI thread safety: none of the utility primitives may block the UI thread in add-in contexts; heavy operations must be scheduled to worker processes or deferred via host idle callbacks. <br>6. Observability: long-lived or important steps must emit <code>start</code> and <code>complete</code> audits, with duration metrics, and store evidence as required. <br><strong>SLOs (examples & measurement):</strong><br>1. Median AtomicWrite latency on local SSD: <200ms, 95th percentile <1s. <br>2. Retry overhead median per attempt: <50ms (backoff excluded). <br>3. SafeRound vectorized path throughput: process 1M numeric entries in worker environment within CI-defined budget (profiled per language). <br>4. RNG seed serialization/deserialization roundtrip success: 100% in golden CI; parity across language bindings mandatory. <br><strong>CI / acceptance gates (enforced):</strong> cross-language golden vectors for RNG and rounding; cross-platform atomic write tests under simulated faults; determinism tests for Retry jitter when deterministic_jitter is enabled; static analysis forbidding synchronous I/O on UI thread; audit emission verification on all persistence and deterministic transforms. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>SafeRound(value, places=0, strategy="bankers", tieBreakerKeys=null)</strong> — exhaustive per-function technical breakdown (expanded)<br><strong>Purpose & contract:</strong> deterministic rounding primitive for scalar values, arrays, grouped sets, and allocation workflows where residual distribution must be conservative and reproducible. Implementations must be pure and side-effect free, stable across language bindings, and suitable for regulated numeric workflows. When <code>strategy=&quot;residual_distribute&quot;</code> the function guarantees that the sum of rounded parts equals the rounded sum (<code>conservation invariant</code>) at the requested precision. <br><strong>Parameters & return (explicit):</strong><br>1. <code>value</code>: scalar, 1D, or 2D numeric container; accepts integer/float/decimal types coerceable to canonical internal decimal representation. <br>2. <code>places</code>: integer >= 0, number of decimal places to round to. <br>3. <code>strategy</code>: string in <code>{bankers, awayFromZero, floor, ceiling, residual_distribute, custom}</code>. <br>4. <code>tieBreakerKeys</code>: optional array (length aligned with value elements) providing stable deterministic ordering for tie resolution. <br>Return: rounded output with same shape as <code>value</code>. Rounding does not raise on NaN/Inf but propagates them and emits an audit event. <br><strong>Primary invariants (must/shall):</strong><br>1. <strong>Idempotence:</strong> <code>SafeRound(SafeRound(x)) == SafeRound(x)</code> for finite numbers. <br>2. <strong>Tie-break determinism:</strong> when fractional ties occur in half-way cases or residual allocation, <code>tieBreakerKeys</code> followed by original index must deterministically resolve tie. <br>3. <strong>Conservation (residual_distribute):</strong> <code>sum(rounded_parts) == rounded(sum(original_parts))</code> at the integer scale <code>10**places</code>. <br>4. <strong>Locale independence:</strong> numeric results must not depend on host locale (thousands separators, decimal comma). <br><strong>Conceptual algorithm & implementation notes (detail):</strong><br>1. <strong>Canonicalization:</strong> normalize inputs to an arbitrary-precision decimal representation (preferred) or integer scaling if decimal library not available: <code>scaled = exact_decimal(value) * scale</code> where <code>scale = 10**places</code>. <br>2. <strong>Exact-half detection:</strong> when implementing bankers rounding detect exact <code>.5</code> boundaries using decimal arithmetic — do not rely on binary float equality checks (these are non-deterministic across platforms). <br>3. <strong>Vector/group path:</strong> for <code>residual_distribute</code> compute <code>floors = floor(scaled)</code>, <code>residuals = scaled - floors</code>; compute <code>target_total_scaled = round(sum(scaled))</code> (rounded according to <code>strategy</code> when appropriate); compute <code>needed_increments = target_total_scaled - sum(floors)</code>; allocate increments to elements ordered by <code>(residual desc, tieBreakerKeys asc, originalIndex asc)</code>. <br>4. <strong>Custom strategy plug-in:</strong> <code>strategy=&quot;custom&quot;</code> accepts a callback <code>customTieBreaker(residual, index, tieBreakerKeys)</code> user-supplied in higher-level API — must be pure and deterministic; CI flags any custom strategy for golden tests. <br>5. <strong>Precision cap & handling:</strong> implement maximum supported precision (e.g., 18 decimal places) and cap <code>places</code> beyond it, emitting <code>util.saferound.truncated_precision</code> audit with rationale. <br><strong>Edge cases & invalid inputs (explicit mapping):</strong><br>1. <strong>NaN / Inf:</strong> propagate unchanged, emit <code>util.saferound.invalid_input</code> with input hash and reason. <br>2. <strong>Non-numeric types:</strong> attempt coercion; if fails return error <code>UTIL_SAFEROUND_COERCE_FAIL</code> and emit audit with sanitized evidence. <br>3. <strong>Very large arrays / memory pressure:</strong> process in chunks with streaming algorithm; if memory insufficient return <code>UTIL_SAFEROUND_OOM</code> with diagnostics. <br>4. <strong>Mixed numeric types across vector elements:</strong> coerce to canonical decimal with exactness; if heterogeneity causes loss, emit <code>util.saferound.truncated_precision</code>. <br><strong>Observability & audit fields (full):</strong><br>1. <code>util.saferound.start</code> — fields: <code>timestamp</code>, <code>correlationId</code>, <code>places</code>, <code>strategy</code>, <code>inputHash</code>, <code>elementCount</code>, <code>paramsHash</code>. <br>2. <code>util.saferound.complete</code> — fields: <code>timestamp</code>, <code>correlationId</code>, <code>outputHash</code>, <code>durationMs</code>, <code>elementCount</code>. <br>3. <code>util.saferound.invalid_input</code> — fields: <code>timestamp</code>, <code>correlationId</code>, <code>valueHash</code>, <code>reason</code>. <br><strong>Representative narratives & extended examples (multi-paragraph):</strong><br>1. <strong>Payroll rounding scenario (detailed):</strong> split gross payroll <code>100.00</code> among three recipients by equal weight using <code>residual_distribute, places=2</code>. Internal steps: canonicalize to scaled cents <code>[10000/3]</code> scaled fractional parts computed, floors <code>[3333,3333,3333]</code>, residuals <code>[1/3*10000 - 3333 ...]</code> computed precisely; <code>needed_increments = 1</code> assigned deterministically by <code>tieBreakerKeys</code> (employeeId order) producing <code>[33.33,33.33,33.34]</code>. Audit trail includes <code>util.saferound.start</code> with <code>inputHash</code>, <code>util.saferound.residuals</code> with allocation details fingerprint, and <code>util.saferound.complete</code> with <code>outputHash</code> and allocation checksum. This chain enables replay in a compliance investigation where employee IDs, jobDescriptor, and RNG seed are replayable. <br>2. <strong>Financial ledger aggregation (detailed):</strong> for repeated aggregation of many micro-transactions across periods, bankers rounding is chosen to avoid directional bias. Unit tests include sequences of additions and roundings to verify no systematic drift over 10k simulated days. <br><strong>Tests & CI golden vectors (explicit):</strong><br>1. Cases at half-boundaries: <code>1.005</code> to 2 dp, <code>-1.005</code> to 2 dp, exact half values of <code>x.5</code> both positive and negative. <br>2. Residual distribution property tests over random vectors (property-based testing) verifying conservation for 100k randomized cases across languages. <br>3. Cross-language golden vectors for a canonical set of inputs and <code>strategy</code> combinations. <br><strong>Operational remediation & runbook (when rounding disputes occur):</strong><br>1. Retrieve <code>util.saferound.*</code> audit rows for the correlationId. <br>2. Pull canonicalized decimal inputs from evidence store (<code>evidenceRef</code>). <br>3. Re-run <code>SafeRound</code> in reproduce mode with the exact <code>places</code> and <code>strategy</code> and compare <code>outputHash</code>. If mismatch persists, escalate to module owners with code, evidenceRef, and golden vector comparison. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>SafeRoundResiduals(values[], total=null, places=0, tieBreakerKeys=null)</strong> — allocation primitive (expanded)<br><strong>Purpose & contract:</strong> deterministic allocator for translating fractional shares into integer allocations whose sum equals a target total. Used for cents allocation, invoice splitting, distributing indivisible units, chunk sizing for job proposals. Pure function; emits audit row on invocation and returns allocation and internal allocation fingerprint. <br><strong>Detailed behavior & algorithm (stepwise):</strong><br>1. If <code>total == null</code> compute <code>target_total = SafeRound(sum(values), places, strategy=&quot;bankers&quot;)</code>. <br>2. Compute <code>scale = 10**places</code> and <code>scaled_values = exact_decimal(values) * scale</code>. <br>3. For each element compute <code>floor_i = floor(scaled_values_i)</code> and <code>residual_i = scaled_values_i - floor_i</code>. <br>4. <code>remaining = target_total_scaled - sum(floor_i)</code>. If <code>remaining &lt; 0</code> error <code>UTIL_ALLOCATION_NEGATIVE_REMAINING</code>. If <code>remaining &gt; len(values)</code> handle per policy (distribute multiple units or raise <code>UTIL_ALLOCATION_OVERFLOW</code>). <br>5. Sort indices by tuple <code>(residual desc, tieBreakerKeys asc if provided, originalIndex asc)</code>. <br>6. Add <code>+1</code> to the top <code>remaining</code> indices. <br>7. Return <code>allocated = (floor_i + assigned_increment_i) / scale</code> with allocation checksum and <code>allocationFingerprint</code> for audit. <br><strong>Primary invariants:</strong><br>1. Sum-preservation: <code>sum(allocated)</code> equals target total at the requested precision. <br>2. Determinism: tie-break stable across runs. <br><strong>Edge cases & policy decisions:</strong><br>1. Negative weights: allow only if business rule allows negative allocations; otherwise <code>UTIL_ALLOCATION_NEGATIVE_WEIGHT</code>. <br>2. Zero-sum with rounding: all residuals zero but <code>remaining &gt; 0</code> indicates inconsistent <code>total</code> — signal <code>UTIL_ALLOCATION_TARGET_MISMATCH</code>. <br><strong>Observability & audit:</strong> <code>util.saferound.residuals</code> capturing <code>correlationId</code>, <code>valuesHash</code>, <code>places</code>, <code>target_total</code>, <code>tieBreakerFingerprint</code>, <code>allocationFingerprint</code>. Evidence store holds canonical scaled inputs for forensic replay. <br><strong>Examples & extended narrative:</strong><br>1. <strong>Invoice splitting among N customers by weight:</strong> values <code>[12.345, 45.0, 65.1]</code>, <code>places=2</code>. Implementation scales to cents, floors computed, residuals evaluated, and <code>remaining</code> cents assigned deterministically by <code>tieBreakerKeys</code> (e.g., earliest account creation timestamp). Resulting cents allocation audited and persisted; operator-visible preview shows "before/after" examples in remediation UI. <br>2. <strong>Chunk sizing in MatchMerge:</strong> allocate integer job chunk sizes across workers so that sum equals total rows and no worker is starved or overallocated. <code>tieBreakerKeys</code> may encode machine capacity for fairness. <br><strong>Tests & CI:</strong> property tests for conservation across random weight vectors, tie-break stability tests, concurrency tests validating that tieBreakerKeys deterministically influence results. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>AtomicWrite(targetPath, payloadStream, tmpSuffix=".part", fsyncFile=true, fsyncParent=true, perms=null, maxAttempts=3, cleanupOnFailure=false)</strong> — persistence primitive (expanded)<br><strong>Purpose & contract:</strong> robust, crash-safe atomic file replacement primitive ensuring that at no point does a consumer observe a truncated file. Provide deterministic result object on success and deterministic error codes on failure. Relevant to job descriptors, exported datasets, reconciliation artifacts, and signed manifests. <br><strong>Complete step-by-step semantics (detailed):</strong><br>1. <strong>Parent directory validation:</strong> verify parent directory exists; if missing and creation allowed create directories using <code>FsSyncDir</code> which ensures fsync of parent metadata. Emit <code>util.atomic_write.mkdir</code> if created. <br>2. <strong>Temp path computation:</strong> compute <code>tempPath = TempPathFor(targetPath, tmpSuffix, pid=currentPid, deterministicSuffix=DeterministicSuffix())</code>. DeterministicSuffix derived from <code>DeterministicRNG(seed=correlationId, salt=&quot;atomic-temp&quot;)</code> when deterministic temp naming is required to coordinate multi-process writes in golden tests. <br>3. <strong>Exclusive open & write:</strong> open tempPath with create-only and exclusive flags; write payload stream in chunks using <code>ChecksumStream</code> to compute SHA256 on the fly; optionally write <code>artifact.metadata.json</code> sidecar containing <code>payloadHash</code>, <code>correlationId</code>, <code>timestamp</code>, <code>paramsHash</code>. <br>4. <strong>Fsync file descriptor:</strong> if <code>fsyncFile</code> true call OS-specific flush (<code>fsync</code>/<code>FlushFileBuffers</code>). If call fails due to unsupported FS semantics emit <code>util.atomic_write.degraded</code>. <br>5. <strong>Atomic rename:</strong> attempt atomic rename/replace using OS primitives: <code>os.replace</code>/<code>rename</code> on POSIX, <code>ReplaceFile</code> or <code>MoveFileEx</code> on Windows with flags that allow replacing a read-only file if policy permits; on failure due to open handles return a deterministic error <code>UTIL_ATOMIC_WRITE_RENAME_LOCKED</code>. <br>6. <strong>Fsync parent directory:</strong> if <code>fsyncParent</code> true, open parent dir and fsync to persist directory entry; on Windows use alternate semantics. If not supported on FS, emit <code>util.atomic_write.degraded</code> audit. <br>7. <strong>Verification read & checksum compare:</strong> reopen <code>targetPath</code> in read-only mode and compute checksum; compare with computed <code>artifactChecksum</code>; if mismatch emit <code>util.atomic_write.verification_failed</code> and attempt an immediate repair or retry per <code>Retry</code> policy. <br>8. <strong>Cleanup & result:</strong> remove temp artifacts if <code>cleanupOnFailure</code> true on failures; on success return structured result <code>{success:true, artifactPath:targetPath, artifactChecksum, attempts, elapsedMs}</code> and emit <code>util.atomic_write.completed</code>. <br><strong>Cross-platform fallbacks & NFS/SMB specifics (documented):</strong><br>1. <strong>Network file systems:</strong> many network filesystems do not support atomic rename semantics between clients reliably or do not persist directory fsync in expected order. For these cases the module must:<br>   - write artifact to <code>targetPath.tmp</code> and a <code>targetPath.meta</code> sidecar that includes checksum and state <code>&quot;complete&quot;:true</code> only after write complete and flush;<br>   - optionally coordinate with small lease/lock files stored on the same share to indicate availability;<br>   - emit <code>util.atomic_write.degraded</code> with <code>rationale=&quot;network-fs-no-atomic-rename&quot;</code>. <br>2. <strong>Windows handle contention:</strong> recommend best-practice orchestration: write to same volume, ensure readers open with shared-read semantics, perform write during low-load windows, and fallback to <code>stage-local</code> on repeated failures. <br><strong>Error classes & runbook mapping (operational):</strong><br>1. <code>UTIL_ATOMIC_WRITE_ENOSPC</code> — emit <code>util.atomic_write.ENOSPC</code> audit, include freeBytes and mount path; runbook: attempt local staging, rotate older artifacts, escalate to infra if persistent. <br>2. <code>UTIL_ATOMIC_WRITE_EPERM</code> — emit <code>util.atomic_write.EPERM</code> with ACL snapshot; runbook: validate ACLs, correct ownership, re-run with operator. <br>3. <code>UTIL_ATOMIC_WRITE_VERIFICATION_FAILED</code> — attempt immediate retry; if repeated, capture temp artifacts via <code>InspectTempArtifacts</code> and run <code>AtomicWriteRepair</code> under maintenance window. <br><strong>Auditing & evidence linkage (explicit):</strong><br>1. <code>util.atomic_write.attempt</code> — include <code>targetPath</code>, <code>tempPath</code>, <code>payloadHash</code>, <code>paramsHash</code>. <br>2. <code>util.atomic_write.completed</code> — include <code>artifactChecksum</code>, <code>durationMs</code>, <code>attempts</code>, <code>evidenceRef</code> for artifact metadata. <br>3. <code>util.atomic_write.degraded</code> — include <code>rationale</code> and recommended mitigation. <br><strong>Representative scenario & extended narrative (atomic export with staged fallback):</strong><br>1. Worker attempts AtomicWrite to network share; <code>fsyncParent</code> not supported on mount -> AtomicWrite detects degraded FS semantics and writes using two-phase protocol: <code>artifact.tmp</code> + <code>artifact.meta</code>. Emit <code>util.atomic_write.degraded</code> with <code>rationale=&quot;nfs-no-fsync&quot;</code>. Orchestrator chooses to stage artifact locally and schedule background upload while emitting <code>export.attempt</code> audit. <br><strong>Tests & CI (robust coverage):</strong><br>1. FS-mock tests simulating <code>rename</code> failure, <code>fsync</code> failures, permission changes mid-write. <br>2. Concurrency tests with 100 readers hitting target path during writer process ensuring no partial content observed. <br>3. Cross-OS integration ensuring ReplaceFile semantics consistent. <br><strong>Operational mitigations & escalations:</strong><br>1. If repeated <code>ENOSPC</code> or <code>EPERM</code> persisted after automated mitigations, create SRE ticket with forensic_manifest and audit_tail. <br>2. If <code>verification_failed</code> observed for an exported artifact consumed downstream, automatically quarantine downstream ingestion until artifact integrity resolved. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Retry(fn, retries=3, backoff={baseMs:100, factor:2}, jitter=true, retry_on=(TransientError,), idempotent_assert=false, cancellation_token=null, audit_on_attempt=true, deterministic_jitter=false)</strong> — orchestration & governance (expanded)<br><strong>Purpose & contract (concise):</strong> canonical transient-fault retry wrapper for local IO and short-lived remote calls invoked by worker processes and background orchestrators. Must be auditable, enforce idempotency expectations, and provide deterministic jitter for golden CI runs when requested. <br><strong>Behavioral rules & enforcement (detailed):</strong><br>1. <strong>Retry predicate:</strong> only retry on exceptions that satisfy <code>retry_on</code> predicate(s); non-retryable exceptions must be surfaced immediately. <br>2. <strong>Idempotency enforcement:</strong> if <code>idempotent_assert</code> omitted and static analysis detects non-idempotent side effects (e.g., writes, external state mutation) the CI system must fail the merge and require either <code>idempotent_assert=true</code> with documented idempotency token or a refactor to make the operation idempotent. <br>3. <strong>Deterministic jitter for testing:</strong> when <code>deterministic_jitter=true</code> the jitter calculation is derived from <code>DeterministicRNG(seed=correlationId|retry-salt)</code> so sleep durations are reproducible for CI golden runs. <br>4. <strong>Cancellation handling:</strong> if <code>cancellation_token</code> is set and flagged between attempts, raise <code>UTIL_RETRY_CANCELLED</code> and emit <code>util.retry.complete</code> with <code>outcome=&quot;cancelled&quot;</code>. <br><strong>Backoff & jitter semantics (precise):</strong><br>1. Base backoff: <code>backoff_ms = baseMs * factor**(attemptIndex-1)</code>. <br>2. Jitter algorithm: default is "decorrelated jitter" (per AWS jitter guidance) computed as <code>min(maxBackoff, rand(0, backoff_ms * 3))</code>; when <code>deterministic_jitter=true</code> replace <code>rand</code> with <code>DeterministicRNG.random()</code>. <br>3. Maximum total retry budget is configurable per-call (<code>max_total_ms</code>) to prevent runaway waits. <br><strong>Auditing & metrics:</strong><br>1. Emit <code>util.retry.attempt</code> for each attempt with <code>attemptIndex</code>, <code>errorCode</code>, and <code>backoffMs</code>. <br>2. Emit <code>util.retry.complete</code> on success or exhaustion including <code>attempts</code> and <code>elapsedMs</code>. <br>3. Metrics buffered locally: <code>util.retry.attempt_count</code>, <code>util.retry.success_rate</code>, with tags for <code>target</code> and <code>module</code>. <br><strong>Representative usage scenarios & narratives:</strong><br>1. <strong>Job descriptor persistence under transient NFS blips:</strong> <code>Retry(lambda: AtomicWrite(jobDescriptorPath,...), retries=5, idempotent_assert=true, deterministic_jitter=true)</code> ensures eventual persistence while deterministic_jitter allows CI and replay to mimic production timing in stress tests. <br>2. <strong>Provider API transient timeouts:</strong> wrap short provider calls in <code>Retry</code> with <code>idempotent_assert=true</code> and application-layer idempotency tokens embedded in request payloads. <br><strong>Failure escalation & SRE triggers:</strong><br>1. Exhaustion emits <code>util.retry.complete</code> with <code>outcome=&quot;exhausted&quot;</code> and triggers SRE alerting if call is critical (jobDescriptor persistence, artifact export). The alert payload includes tempPaths, attempt errors, and partial artifacts evidence. <br><strong>Tests & CI (explicit):</strong> deterministic_jitter golden tests to ensure backoff/jitter sequences identical given correlation seeds; cancellation tests simulate cancellation_token signalled after first attempt; static-analysis gating for idempotency enforcement. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>DeterministicRNG(seed_source, salt="", algorithm="pcg64", stream_id=null, test_mode=false)</strong> — design, usage & governance (expanded)<br><strong>Purpose & contract (concise):</strong> deterministic, seedable PRNG for sampling, shuffles, tie-breakers, and ephemeral randomized operations that must be reproducible. Not for cryptographic uses. RNG instances are explicit and serializable. <br><strong>Seed derivation & confidentiality (detailed):</strong><br>1. Production seed derivation: <code>seed_raw = HMAC_SHA256(key=systemKey, message=(seed_source || &#x27;|&#x27; || salt || &#x27;|&#x27; || moduleName || &#x27;|&#x27; || streamId))</code>. The <code>seed_raw</code> is then folded/truncated into the algorithm's seed size using deterministic mixing. <br>2. <strong>Confidentiality:</strong> raw seed not emitted in plaintext in audit rows. Instead store <code>seedFingerprint = SHA256(seed_raw)</code> and record <code>util.rng.seeded</code> with <code>seedFingerprint</code>. For replay, <code>serialize_rng_state</code> stores encrypted internal state blob into evidence store and audit references it via <code>evidenceRef</code>. <br>3. <strong>test_mode:</strong> allows local seed overrides for dev/golden runs but CI must verify no test_mode artifacts leak to production. <br><strong>API behavior & primitives:</strong><br>1. <code>randint(a,b)</code>, <code>random()</code> in [0,1), <code>shuffle(seq)</code> returning a new sequence, <code>sample(pop,k)</code>, <code>choice(seq)</code>, <code>splitStream(childIndex)</code> producing independent child RNG seeded via <code>HMAC(parentSeed || childIndex)</code> to avoid correlation. <br>2. <code>serialize_state()</code> returns encrypted base64 blob; <code>restore_state(blob)</code> returns RNG instance at saved position. <br><strong>Stability invariants:</strong><br>1. Same <code>algorithm</code>, <code>seed_source</code>, <code>salt</code>, and <code>stream_id</code> must produce identical output across Python/JS/C#/VBA bindings. <br>2. Splitting streams produce independent sequences (statistical independence test required in CI). <br><strong>Observability & audit:</strong><br>1. <code>util.rng.seeded</code> — <code>correlationId</code>, <code>seedFingerprint</code>, <code>algorithm</code>, <code>streamId</code>, <code>evidenceRef</code> optional. <br>2. <code>util.rng.state_serialized</code> — <code>stateHash</code>, <code>evidenceRef</code> for the encrypted state blob. <br><strong>Representative narratives & usage examples (detailed):</strong><br>1. <strong>DQ_Profile deterministic sampling:</strong> worker uses <code>DeterministicRNG(seed_source=correlationId, salt=&#x27;dq-profile-v1&#x27;)</code> to select deterministic sample rows for profile reports; <code>serialize_state()</code> attached to profile artifact so compliance can reconstruct sample for audits. <br>2. <strong>MatchMerge tie-breaker:</strong> seed from <code>correlationId || &#x27;matchmerge-v1&#x27;</code>, call <code>shuffle(candidates)</code> producing deterministic reordering, persist RNG state in proposal artifact to allow later replays and operator disputes to be resolved with exact ordering. <br><strong>Tests & CI (explicit):</strong> cross-language parity vectors (first N outputs), splitStream independence tests, serialize/restore roundtrip, statistical tests for uniformity across large N. <br><strong>Operational playbook:</strong><br>1. When non-deterministic sampling is reported, retrieve <code>util.rng.seeded</code> and <code>util.rng.state_serialized</code> evidence. <br>2. Re-run sampling with restored RNG state to reproduce selection; if mismatch found, escalate to owners for parity investigation. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>ChecksumStream(payloadStream, algorithm="sha256", chunkSize=65536)</strong> — streaming checksum (expanded)<br><strong>Purpose & contract:</strong> stream-pass-through wrapper that computes a cryptographic checksum (default SHA256) while forwarding bytes to consumer; suitable for integrating into AtomicWrite to compute checksums without buffering whole artifact. Must be deterministic in chunking semantics and avoid implicit character encoding transforms. <br><strong>Behavior & notes:</strong><br>1. Read bytes from <code>payloadStream</code> in <code>chunkSize</code> increments; update hash state per chunk; forward to writer socket/file handle. <br>2. On EOF finalize digest and provide <code>finalChecksum</code> (hex/base64 configurable). <br>3. On partial stream failure emit <code>util.checksum.partial</code> with <code>partialHash</code> and <code>bytesProcessed</code>. <br><strong>Edge cases & mapping:</strong> unsupported algorithms return <code>UTIL_CHECKSUM_UNSUPPORTED</code>; stream interruption errors map to <code>UTIL_CHECKSUM_PARTIAL</code>. <br><strong>Observability & audit:</strong> <code>util.checksum.start</code> with <code>correlationId</code>, <code>algorithm</code>, <code>chunkSize</code>; <code>util.checksum.complete</code> with <code>finalChecksum</code>, <code>bytesProcessed</code>, <code>durationMs</code>. <br><strong>Tests & CI:</strong> parity checks over multiple chunk sizes; ensure checksum identical for same byte stream across environments. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>TempPathFor(targetPath, tmpSuffix=".part", pid=null, deterministicSuffix=null)</strong> — temp path helper (expanded)<br><strong>Purpose & contract:</strong> deterministic temporary path generator used by AtomicWrite to ensure low collision probability and forensic traceability. Must produce temp path on <strong>same volume</strong> as <code>targetPath</code> to preserve atomic rename semantics. <br><strong>Rules & algorithm:</strong><br>1. Compute <code>base = dirname(targetPath)</code> and <code>basename = safe_filename(basename(targetPath))</code>. <br>2. <code>pid</code> default to runtime pid; <code>deterministicSuffix</code> may be provided using <code>DeterministicRNG(seed=correlationId)</code> for test reproducibility or left null for production randomness. <br>3. Return <code>tempPath = join(base, f&quot;.{basename}{tmpSuffix}.{pid}.{detSuffix}&quot;)</code> sanitized for OS constraints. <br><strong>Edge cases:</strong> on Windows UNC paths ensure path length limits respected and sanitize <code>:</code> and <code>\</code> characters. <br><strong>Observability:</strong> no heavy audit; AtomicWrite emits <code>util.atomic_write.attempt</code> including <code>tempPath</code>. <br><strong>Tests:</strong> path equality and collision stress tests; ensure same-volume assertion verified by <code>PathVolumeOf</code>. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>FsSyncDir(dirPath, ensureExists=true, perms=null)</strong> — directory creation & fsync (expanded)<br><strong>Purpose & contract:</strong> ensure directory exists (optionally creating it) and persist directory metadata (fsync) to assure directory entries are durable after file rename operations. Required by AtomicWrite before rename when parent directories might be newly created. <br><strong>Behavior & OS specifics:</strong><br>1. Create directory with <code>mkdir -p</code> semantics with configured permissions; set ownership/ACLs only when caller requests and has permissions. <br>2. Open directory handle and <code>fsync</code> directory descriptor (<code>fdatasync</code>/<code>fsync</code>) on POSIX; on Windows use <code>FlushFileBuffers</code> on directory handle or fallback to opening and flushing a dummy file if direct flush unsupported. <br>3. If underlying FS does not support directory fsync (some SMB/NFS variants) return <code>degraded=true</code> and emit <code>util.atomic_write.degraded</code> with <code>rationale</code>. <br><strong>Observability & audit:</strong> <code>util.fs.syncdir</code> event with <code>durationMs</code> and <code>created</code> flag. <br><strong>Tests:</strong> cross-platform create-and-sync tests, ensure idempotent behavior when concurrent creators attempt same directory. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>WriteWithPermissions(path, payloadStream, perms, owner=null, group=null)</strong> — write + perms helper (expanded)<br><strong>Purpose & contract:</strong> write payload to temporary path then set POSIX permission bits or Windows ACLs before rename. Must be used as part of AtomicWrite sequence to ensure final artifact has correct access control settings. <br><strong>Behavior & notes:</strong><br>1. Write to temp file (temp computed by TempPathFor), upon write completion apply <code>chmod</code> and <code>chown</code> semantics for POSIX; on Windows apply ACL modifications via appropriate APIs. <br>2. If setting permissions fails after write, the function should either rollback (remove temp) or surface an error <code>UTIL_WRITE_EPERM</code> depending on <code>force</code> parameter; audit includes permission fingerprint rather than full ACL content. <br><strong>Edge cases:</strong> mapping POSIX UIDs/GIDs on containerized environments lacking those IDs should be gracefully handled with audit <code>util.write.perms.warning</code>. <br><strong>Tests & CI:</strong> permission change tests in containerized CI with places of permissions mismatch; verify final artifact has expected effective ACLs. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>serialize_rng_state(rng_instance, evidenceEncrypt=true)</strong> — RNG state serialization (expanded)<br><strong>Purpose & contract:</strong> produce a portable, versioned, encrypted blob representing internal PRNG state for later deterministic replay. Return blob and <code>stateHash</code>; do not leak raw seed material. The blob is intended for secure evidence storage and must be decryptable only by authorized evidence tooling. <br><strong>Serialization details & format:</strong><br>1. Pack: <code>{version, algorithm, internal_state_words, position_counter, metadata:{createdBy, correlationId, module}}</code> into a deterministic binary layout. <br>2. Encrypt with evidence store key and HMAC the resulting ciphertext with evidence HMAC key. <br>3. Base64 encode and return <code>{blobBase64, stateHash=SHA256(ciphertext)}</code>. <br><strong>Observability & audit:</strong> emit <code>util.rng.state_serialized</code> with <code>stateHash</code> and <code>evidenceRef</code>. <br><strong>Tests & CI:</strong> serialize/restore round-trip tests; cross-language blob unmarshal parity checks; rejection for mismatched <code>version</code> during restore. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>restore_rng_state(blobBase64)</strong> — RNG state restoration (expanded)<br><strong>Purpose & contract:</strong> decrypt and reconstruct RNG instance at exact position. Return RNG instance or raise <code>UTIL_RNG_BAD_STATE</code> if decryption fails, integrity check fails, or version incompatible. <br><strong>Behavior & tests:</strong> roundtrip restore must reproduce sequence position exactly; CI runs parity tests across languages and versions; incompatible <code>version</code> requires migration path and fails CI unless accompanied by migration manifest. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>InspectTempArtifacts(baseDir, pattern="<em>.part</em>")</strong> — temp artifact inspection (expanded)<br><strong>Purpose & contract:</strong> safely enumerate aborted/leftover temp artifacts created by AtomicWrite flows; compute checksums where readable and propose candidate repairs. Intended for operator triage and automated repair procedures. <br><strong>Behavior & outputs:</strong> returns list of <code>{tempPath, candidateTargetPath(inferred), size, mtime, readableChecksum?, permissionFlags}</code> and a <code>discoveryFingerprint</code> for audit. Limit scans to <code>maxEntries</code> per invocation to avoid DoS. <br><strong>Edge cases:</strong> permission denied entries recorded with <code>permissionDenied=true</code>; symlinks exploded carefully and not automatically followed beyond one-level to avoid escape. <br><strong>Observability:</strong> emit <code>util.inspect.temp</code> with counts and <code>discoveryFingerprint</code>. <br><strong>Tests:</strong> directory with mixed temp names, multiple matching conventions, and simulated partial writes. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>AtomicWriteRepair(tempPath, expectedChecksum=null, force=false)</strong> — safe repair helper (expanded)<br><strong>Purpose & contract:</strong> attempt validated repair of a temp artifact by verifying checksum and performing a safe rename into the intended target while preserving a backup of the previous target if present. Only run under maintenance or with operator consent. <br><strong>Behavior & required safeguards:</strong><br>1. Validate <code>tempPath</code> read and compute checksum; if <code>expectedChecksum</code> provided, compare and refuse unless <code>force=true</code>. <br>2. Determine <code>targetPath</code> via temp naming convention; backup existing <code>targetPath</code> to <code>targetPath.backup.&lt;ts&gt;</code> before rename to ensure rollback. <br>3. Perform rename using atomic replace primitives where possible; upon success emit <code>util.atomic_write.repair</code> with <code>actionTaken=rename</code> and evidenceRef to temp artifact. <br>4. If repair fails after partial work, attempt to restore backup and emit <code>util.atomic_write.repair.failed</code> with diagnostics. <br><strong>Operational policy:</strong> repairs must be logged to evidence store and require operator approval for <code>force</code> repairs in regulated flows. <br><strong>Tests & CI:</strong> simulate valid temp, mismatched checksum, and rename failure cases to ensure safe rollback. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>AuditEmitUtilEvent(correlationId, procedure, paramsHash, resultHash=null, evidenceRef=null, metadata=null)</strong> — canonical audit emitter (expanded)<br><strong>Purpose & contract:</strong> canonical utility-level audit append helper that validates schema, sanitizes metadata (ensuring no PII in top-level fields), computes <code>rowHash</code>, and appends to local audit buffer (<code>audit_tail.csv</code> or local encrypted buffer). Ensures consistent audit envelope across all util.* events. <br><strong>Behavior & constraints:</strong><br>1. Validate <code>paramsHash</code> presence for state-changing operations. <br>2. If <code>metadata</code> contains PII fields, sanitize or offload to evidence store and add <code>evidenceRef</code>. <br>3. Compute <code>rowHash = HMAC_SHA256(auditRow, auditSigningKey)</code> and append to audit buffer with <code>prevHash</code> chaining where available. <br>4. If audit buffer write fails, write to local encrypted fallback store and emit <code>util.audit.emit.degraded</code>. <br><strong>Observability:</strong> emits meta-audit <code>util.audit.emit</code> with <code>status=buffered|flushed|degraded</code> and buffer metrics. <br><strong>Tests & CI:</strong> ensure schema validation rejects malformed fields; ensure chained row hashes produce correct <code>prevHash</code> chain and sign/verify tests pass. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Cross-cutting Observability, Telemetry & Error Catalog (detailed mapping)</strong><br><strong>Audit schema for utilities (required fields):</strong> <code>timestamp</code>, <code>correlationId</code>, <code>module</code>, <code>procedure</code>, <code>operatorId</code> (optional), <code>paramsHash</code>, <code>resultHash</code> (optional), <code>evidenceRef</code> (optional), <code>prevHash</code> (optional), <code>configHash</code>, <code>ribbonMapHash</code> (relevant flows), and <code>metadata</code> object containing <code>duration_ms</code>, <code>attempts</code>, <code>artifactChecksum</code>, <code>tempPathList</code>, <code>errorCode</code> if any. <br><strong>Evidence policy (expanded):</strong> top-level audit rows must be free of raw PII; when parameters contain PII persist sanitized parameters and full payloads in the encrypted evidence store and reference them with <code>evidenceRef</code> in the audit row. Evidence stores must maintain chain-of-custody metadata including who accessed the evidence and when. <br><strong>Representative ErrorCodes & operator guidance mapping:</strong><br>1. <code>UTIL_ATOMIC_WRITE_ENOSPC</code> — advise <code>df -h</code>, <code>du -sh</code> on mount and stage local fallback. <br>2. <code>UTIL_ATOMIC_WRITE_EPERM</code> — include ACL snapshot and recommend operator fix via documented ACL runbook. <br>3. <code>UTIL_ATOMIC_WRITE_VERIFICATION_FAILED</code> — trigger <code>InspectTempArtifacts</code> and forensic capture. <br>4. <code>UTIL_RETRY_EXCEEDED</code> — collect <code>util.retry.attempt</code> traces and consider circuit-breaker. <br>5. <code>UTIL_SAFEROUND_COERCE_FAIL</code> — provide sanitized inputs to devs for coercion fix. <br>6. <code>UTIL_RNG_BAD_STATE</code> — evidenceRef needed and cross-language parity review. <br><strong>Metric names (precise):</strong> <code>util.atomic_write.latency_ms{host,volume}</code>, <code>util.atomic_write.success_rate{module}</code>, <code>util.retry.attempt_count{target}</code>, <code>util.retry.success_rate{target}</code>, <code>util.saferound.count{strategy}</code>, <code>util.rng.seeded_count{module}</code>. Metrics buffered locally and flushed by <code>CORE_Telemetry</code> with audits referencing the telemetry batch evidence. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Testing matrix, property tests, and cross-language golden governance (comprehensive & extended)</strong><br><strong>Unit tests (explicit list):</strong><br>1. SafeRound goldens covering bankers, awayFromZero, floor, ceiling, residual_distribute, including negative numbers and exact half ties. <br>2. SafeRoundResiduals: multiple ratio distributions including uniform, skewed, heavy-tailed, and pathological tie-heavy vectors. <br>3. AtomicWrite: simulate <code>rename</code> failure, <code>fsync</code> failure, permission change mid-write, and <code>ENOSPC</code>. <br>4. Retry: deterministic_jitter sequences and cancellation token enforcement. <br>5. DeterministicRNG: seed parity across Python/JS/VBA/C# producing identical first N outputs. <br>6. ChecksumStream: chunk-size invariance tests. <br><strong>Integration tests (explicit flows):</strong><br>1. Job persist/read/process: jobDescriptor persisted via AtomicWrite -> worker reads -> produces artifact persisted via AtomicWrite -> checksum match. <br>2. Retry + AtomicWrite: inject transient FS/network failures to validate retry semantics. <br>3. End-to-end deterministic replay: DQ_Profile run with seeded RNG and SafeRound, persist RNG state and rounding audits, perform replay and compare artifact hashes identical. <br><strong>Property tests & formal invariants:</strong><br>1. Sum-preservation for residual_distribute across randomized vectors (property testing). <br>2. Deterministic sampling invariants when seed_and_salt unchanged. <br>3. Thread-safety invariants for RNG splits in parallel workers. <br><strong>CI golden gating & automation rules:</strong><br>1. All golden vectors for SafeRound and DeterministicRNG must pass before merge. <br>2. Static analyzer rejects direct workbook calls or raw file writes from UI thread. <br>3. Performance budget smoke tests must pass (SafeRound 1M rows within allowed time bound). <br>4. Any algorithmic change to RNG or SafeRound must include a migration manifest and owner approval. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Developer guidance, allowed & forbidden patterns (explicit policy)</strong><br><strong>Required patterns (must follow):</strong><br>1. Use AtomicWrite for any artifact consumed by others to avoid partial reads. <br>2. Seed DeterministicRNG from <code>correlationId</code> for operator-visible sampling, persist RNG state via <code>serialize_rng_state</code> when exact replay required. <br>3. Use <code>Retry</code> for transient IO only and ensure <code>idempotent_assert=true</code> for non-idempotent calls or implement idempotency tokens. <br>4. Emit audit rows for all critical operations: <code>AtomicWrite</code>, <code>SafeRound</code> in regulated runs, <code>DeterministicRNG.seeded</code>, <code>Retry</code> sequences. <br><strong>Forbidden practices (CI enforced):</strong><br>1. No direct writes to final artifact paths from UI thread (static analyzer rejects). <br>2. No platform-locale-based numeric parsing inside deterministic math paths (SafeRound). <br>3. No global non-deterministic RNG used for operator-visible sampling. <br>4. No raw PII in top-level audits. <br><strong>Code-review checklist (explicit):</strong> verify audit emits exist for persistence, RNG seed flows into job descriptors, AtomicWrite used for durable outputs, SafeRound used for financial allocations, Retry idempotency asserted. Ensure migration manifests for any breaking change. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Operational runbook & incident playbooks (executable steps — extended)</strong><br><strong>AtomicWrite ENOSPC runbook (explicit steps):</strong><br>1. Inspect <code>util.atomic_write.ENOSPC</code> audit row for <code>correlationId</code> and path. <br>2. On host: <code>df -h &lt;mount&gt;</code> and <code>du -sh &lt;candidateDirs&gt;</code>; collect <code>vmstat</code>, <code>iostat</code>. <br>3. If possible move non-critical artifacts to fallback staging on same volume; prefer same-volume staging to preserve rename semantics. <br>4. Re-run export with <code>--stage-local</code> if available; after export perform checksum compare between staged and intended destination. <br>5. If persistent, open infra incident attaching <code>forensic_manifest</code> and <code>audit_tail</code>. <br><strong>Retry storm triage (explicit steps):</strong><br>1. Query <code>util.retry.attempt</code> metrics for elevated attempt rates and identify failing <code>target</code>. <br>2. Lower concurrency against target and enable circuit-breaker; temporarily set <code>retries=0</code> for non-critical flows to reduce load. <br>3. Search for missing idempotency tokens in failing calls; if found, pause production calls until idempotency enforced. <br>4. If infrastructure root cause, escalate with <code>forensic_manifest</code>. <br><strong>Non-deterministic sampling complaint triage (explicit):</strong><br>1. Retrieve <code>util.rng.seeded</code> audit row for <code>correlationId</code>. <br>2. Pull evidenceRef and restore RNG state via <code>restore_rng_state</code>. <br>3. Re-run deterministic selection using restored state and provide diff to operator. <br>4. If mismatch persists, compare cross-language parity vectors and escalate to owners. <br><strong>Rounding mismatch forensic steps (explicit):</strong><br>1. Pull <code>util.saferound.*</code> audits for the run. <br>2. Obtain canonicalized decimal snapshots from evidence. <br>3. Re-execute rounding pipeline in reproduce mode and compare <code>outputHash</code>. <br>4. If discrepancy persists, capture environment parity and escalate. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Extremely detailed long-form narratives & examples (multiple scenarios — expanded)</strong><br><strong>Scenario 1 — Regulated end-of-period journal allocation (complete trace & forensic replay):</strong><br>1. Operator initiates <code>AllocateJournalTotals</code> from <code>REG_Ribbon</code>. Ribbon handler emits <code>UserAction</code> audit with <code>correlationId=r-20260117-XYZ</code>, <code>operatorId</code> and <code>paramsHash</code>. <br>2. Dataset size exceeds inline threshold; scheduler constructs <code>jobDescriptor</code> with <code>jobId</code>, <code>controlId</code>, <code>correlationId</code>, <code>paramsHash</code>, <code>configHash</code>. Worker durable persistence uses <code>AtomicWrite(jobDescriptorPath, jobJson)</code>; <code>util.atomic_write.attempt</code> and <code>util.atomic_write.completed</code> audits produced with <code>artifactChecksum</code>. <br>3. Worker picks job, initializes <code>DeterministicRNG(seed_source=correlationId, salt=&quot;alloc-v1&quot;)</code> and emits <code>util.rng.seeded</code> with <code>seedFingerprint</code> and <code>evidenceRef</code> to serialized RNG state stored in evidence store. <br>4. Worker reads ledger snapshot (redacted snapshot persisted by core bootstrap) and canonicalizes decimals using canonical decimal serializer; persists canonicalized snapshot evidenceRef. <br>5. Compute allocation fractions; call <code>SafeRoundResiduals(proportions, total=ledgerTotal, places=2, tieBreakerKeys=accountIds)</code>. <code>util.saferound.residuals</code> emits allocationFingerprint and <code>inputHash</code>. <br>6. Allocation artifact written via <code>AtomicWrite(allocPath, artifactStream)</code> with <code>artifactChecksum</code> computed and verified; <code>util.atomic_write.completed</code> audit includes evidenceRef to artifact metadata. <br>7. Module step <code>allocation.complete</code> audit appended referencing allocation checksum, jobId, and runTs. <br>8. If operator disputes allocation, forensic replay uses <code>evidenceRef</code> for serialized RNG state and canonical input snapshots to re-run <code>SafeRoundResiduals</code> in reproduce mode and validate identical output hash. <br><strong>Narrative takeaways (compliance):</strong> deterministic chain ensures repeatability: <code>correlationId -&gt; jobDescriptor -&gt; seeded RNG -&gt; canonical inputs -&gt; SafeRoundResiduals -&gt; AtomicWrite artifacts -&gt; audits</code>. Evidence references enable compliance packaging and regulator submissions. <br><strong>Scenario 2 — PQ template injection with numeric fidelity requirement (conceptual & concrete):</strong><br>1. Operator opens PQ_Templates in <code>PQ_Ribbon</code>; template metadata contains <code>mChecksum</code>, <code>requiresHighPrecision=true</code>, and <code>templateVersion</code>. Preview request computes <code>seed=SeedFromCorrelation(correlationId, templateId)</code> and preview M receives <code>seed</code> parameter; preview audit saved with <code>mChecksum</code> and <code>seedFingerprint</code>. <br>2. Operator elects to inject template into workbook; because <code>requiresHighPrecision=true</code> the injection flow delegates numeric-critical steps to a signed helper (XLAM or worker) which performs canonical normalization and <code>SafeRound</code> to authoritative precision. The helper persists the final M query artifact via <code>AtomicWrite</code> and returns artifactChecksum to the injector. <br>3. Injector then calls <code>workbook.Queries.Add</code> with the finalized M text; <code>pq_inject</code> audit row created including <code>mChecksum</code>, <code>templateVersion</code>, <code>artifactChecksum</code>, and <code>evidenceRef</code>. <br><strong>Narrative takeaways:</strong> offload authoritative numeric transforms to trusted workers using <code>SafeRound</code> and <code>AtomicWrite</code> to ensure injected query is consistent with audited artifact; this avoids client-side M decimal inconsistencies across hosts. <br><strong>Scenario 3 — MatchMerge deterministic tie-breaking & merge proposals (detailed):</strong><br>1. MatchMerge identifies candidate pairs producing equal match scores. To ensure reproducible proposals, it obtains <code>DeterministicRNG(seed_source=correlationId, salt=&quot;matchmerge-v1&quot;)</code> and uses <code>rng.shuffle(candidates)</code> where the shuffle uses stable keys derived from primary keys to ensure cross-language reproducibility. <code>util.rng.seeded</code> and serialized state stored in evidence as <code>evidenceRef</code>. <br>2. Merge proposals created deterministically, persisted via <code>AtomicWrite</code> as <code>proposal.json</code> and <code>proposalHash</code> is recorded in <code>util.atomic_write.completed</code>. <br>3. When operator reviews proposals, they are comparing a persisted artifact; if operator accepts <code>apply-inline</code> additional approvals validated by <code>ValidateUserPermissions</code>; if <code>copy-apply</code> persisted reversible plan stored via <code>AtomicWrite</code> and audit rows record <code>beforeChecksum</code> and <code>afterChecksum</code> to support rollbacks. <br><strong>Narrative takeaways:</strong> deterministic tie breaks + persisted RNG state yield auditable and replayable merge proposals; reversible plans and checksum anchors make rollbacks safe for regulated flows. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Conceptual Power Query (M) patterns — mapping REG_Utilities to PQ workflows (expanded & prescriptive)</strong><br><strong>Context & limitations:</strong> Power Query (M) executes in-host (Excel/Power BI) with runtime differences across hosts, limited file IO control, and inconsistent decimal semantics across engines. REG_Utilities cannot run inside pure M code; instead, orchestrating add-in code should ensure M workflows delegate deterministic and durable steps to helper modules that implement <code>SafeRound</code>, <code>AtomicWrite</code>, and deterministic seed flows. <br><strong>Mapping patterns & recommended implementation templates:</strong><br>1. <strong>AtomicWrite mapping for PQ exports & injections (pattern):</strong><br>    - Problem: M cannot guarantee atomic file replace semantics or directory fsync. <br>    - Pattern: PQ template generation produces the artifact payload (M script, manifest). The add-in helper receives the artifact and calls <code>AtomicWrite</code> to persist it atomically and compute <code>artifactChecksum</code>. The <code>pq_inject</code> audit references <code>artifactChecksum</code> and <code>mChecksum</code>. <br>    - Governance: require signed manifests for regulated templates and enforce template versioning in <code>PQ_Library</code>. <br>2. <strong>DeterministicRNG mapping for deterministic preview & sampling (pattern):</strong><br>    - Problem: M lacks seedable standard RNG with cross-host parity. <br>    - Pattern: PQ_Ribbon computes a seed via <code>SeedFromCorrelation(correlationId, templateId)</code> and passes seed into preview as an explicit parameter; lightweight sample selection can be implemented in M using parameterized deterministic LCG functions for small previews, but full-scale sampling must run in <code>DeterministicRNG</code> on worker and the sample persisted for replay. <br>3. <strong>SafeRound mapping for numeric fidelity in PQ (pattern):</strong><br>    - Problem: M decimal handling varies and bankers rounding behavior may differ. <br>    - Pattern: templates requiring strict rounding are annotated <code>requiresHighPrecision</code>. The M template outputs normalized rows and calls helper worker API to run <code>SafeRound</code> on canonical decimals; the worker returns a final artifact persisted via <code>AtomicWrite</code>. <br>4. <strong>Retry & idempotency mapping for PQ refresh orchestration:</strong><br>    - Problem: PQ refresh can be flaky due to provider timeouts. <br>    - Pattern: orchestrating add-in code wraps refresh or artifact write in <code>Retry</code> with <code>idempotent_assert</code> and persists jobDescriptor via <code>AtomicWrite</code> before attempting the refresh to ensure idempotent resumability. <br><strong>Operator narrative (expansion):</strong><br>1. Preview: operator presses "preview" -> add-in computes deterministic seed -> preview M function receives <code>seed</code> param -> preview audit created with <code>seedFingerprint</code> and <code>mChecksum</code>. <br>2. Inject: if <code>requiresHighPrecision</code> worker finalizes numeric rounding -> <code>AtomicWrite</code> persists artifact -> <code>pq_inject</code> audit links to artifactChecksum and evidenceRef. <br><strong>Checklist for template authors (PQ-specific):</strong><br>1. Always include <code>mChecksum</code> in template metadata. <br>2. Mark <code>requiresHighPrecision</code> for templates with regulated numeric transforms. <br>3. Parameterize <code>seed</code> for previews and store <code>seedFingerprint</code> in preview audit. <br>4. Avoid embedding non-deterministic functions in M templates for operator-visible transformations; prefer deterministic parameterization. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Conceptual DAX patterns — mapping REG_Utilities to DAX & semantic model design (expanded & prescriptive)</strong><br><strong>Context & constraints:</strong> DAX is read-time query language; it cannot perform side effects or persist artifacts. Deterministic rounding and allocation must be materialized during ETL and exposed as read-only tables. DAX measures can then be used to validate reconciliations by referencing metadata tables written atomically by ETL. <br><strong>Patterns & recommended practices (explicit):</strong><br>1. <strong>Push rounding & residual distribution to ETL:</strong><br>    - Rationale: DAX cannot generate persisted allocation artifacts or reversible plans. ETL must run <code>SafeRoundResiduals</code> and persist final integer-cent allocations as columns in model tables. DAX reports should refer to these persisted columns. <br>2. <strong>Deterministic sampling via hashed keys in model:</strong><br>    - Rationale: DAX cannot seed RNGs reliably. ETL computes <code>HashKey = HASH(PrimaryKey || &#x27;|&#x27; || correlationSalt)</code> and persists <code>sampleFlag = (HashKey MOD N) &lt; k</code> as a persistent column. Persist <code>correlationSalt</code> in <code>RunMetadata</code> for deterministic replay. <br>3. <strong>RunMetadata table & audit linkage:</strong><br>    - Pattern: ETL writes <code>RunMetadata</code> atomically (via <code>AtomicWrite</code>) with <code>correlationId</code>, <code>configHash</code>, <code>artifactChecksum</code>, <code>runTs</code>, and <code>evidenceRef</code>. DAX measures can reference the latest run metadata row to display dataset provenance. <br>4. <strong>Checksum reconciliation surfaced in DAX:</strong><br>    - Pattern: ETL computes dataset-level checksum and writes it in <code>RunMetadata</code>; DAX measure <code>ReconciledFlag = IF(LatestRunMetadata[artifactChecksum] = ExpectedChecksum, 1, 0)</code> surfaces verification to report consumers. <br><strong>DAX author checklist (concise):</strong><br>1. Avoid doing rounding or allocation in DAX for regulated outputs. <br>2. Reference <code>RunMetadata</code> for provenance and expected artifact checksums. <br>3. Use persisted deterministic sample flags rather than runtime random filters. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Appendices: forensic artifacts, evidence paths & recommended retention (expanded)</strong><br><strong>Minimum forensic artifacts for a typical incident (ordered & explicit):</strong><br>1. <code>ribbon-map.json</code> and release manifest with signatures and release fingerprint. <br>2. <code>jobDescriptor.json</code> persisted via <code>AtomicWrite</code> including <code>jobId</code>, <code>paramsHash</code>, <code>configHash</code>, <code>correlationId</code>. <br>3. <code>audit_tail.csv</code> segment spanning before/after timeframe including <code>UserAction</code> and <code>util.*</code> events for the <code>correlationId</code>. <br>4. artifact files and <code>artifact.metadata.json</code> mapping artifact -> checksum and evidenceRef. <br>5. serialized RNG state blobs (<code>rng_state.blob</code>) when sampling or shuffle was involved. <br>6. SafeRound canonicalized decimal input snapshots and rounding logs. <br>7. persisted temp artifacts and <code>InspectTempArtifacts</code> output when <code>AtomicWrite</code> failures occurred. <br><strong>Evidence storage & retention (policy):</strong><br>1. Hot store: <code>\\evidence\hot\&lt;module&gt;\&lt;correlationId&gt;\</code> for 30 days accessible to limited operators; all files encrypted at rest. <br>2. Warm archive: secure archive for regulatory retention (7 years) with chain-of-custody metadata and stricter access controls. <br>3. <code>forensic_manifest.json</code> enumerates artifact URIs, checksums, evidenceRef values, and access control lists. <br><strong>Retention & verification cadence:</strong> monthly retention verification job emits <code>housekeeping.audit</code> and rotates evidence per retention rules; proofs-of-delete included in audit when items expire and are removed. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Acceptance checklist before module release (detailed & non-optional):</strong><br>1. Owners listed with contactable emails in <code>OWNERS.md</code>. <br>2. Public API stable, documented, and versioned (major/minor semantic versioning). <br>3. All durable artifacts used by other modules are persisted via <code>AtomicWrite</code>. <br>4. DeterministicRNG goldens and cross-language parity tests passing. <br>5. SafeRound/golden vectors validated and <code>residual_distribute</code> behavior documented. <br>6. CI gates include forbidden-API static checks, golden tests, integration durability tests, and performance budgets. <br>7. Audit hooks validated with test harness emitting expected audit rows into <code>modAudit</code> buffer. <br><strong>Blocking conditions (explicit):</strong> missing audit emissions on persistence flows, golden vector failures, static analyzer detection of forbidden API usage, or performance regressions beyond thresholds. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Extremely detailed test plan highlights & scripts (explicit, conceptual):</strong><br><strong>Unit tests (explicit):</strong><br>1. SafeRound strategies: <code>bankers</code>, <code>awayFromZero</code>, <code>floor</code>, <code>ceiling</code> — verify half-tie behavior, negative numbers, and large exponent values. <br>2. SafeRoundResiduals: verify behavior on boundary-sum cases where <code>remaining==0</code> and <code>remaining==len(values)</code>. <br>3. AtomicWrite: simulate <code>rename</code> and <code>fsync</code> failures using mocked FS and ensure no truncated artifacts observed. <br>4. Retry: verify deterministic_jitter path and cancellation token semantics. <br>5. DeterministicRNG: validate seed -> first N outputs parity across languages. <br><strong>Integration tests (explicit flows):</strong><br>1. Roundtrip job persist & worker read: <code>jobDescriptor</code> persisted > worker reads > job processed > produced artifact with checksum compare. <br>2. Fault-injected AtomicWrite: simulate <code>ENOSPC</code> and ensure fallback staging path recorded and forensic artifacts captured. <br>3. End-to-end deterministic replay: run <code>DQ_Profile</code> + Remediation, persist RNG and rounding audits, repeat replay to confirm identical artifacts. <br><strong>Performance tests (explicit):</strong><br>1. SafeRound vectorized throughput benchmark: 1M rows processed within worker budget (language-specific thresholds) monitored in CI smoke. <br>2. AtomicWrite median latency test under SSD and over networked filesystem conditions. <br>3. Retry overhead microbenchmarks to ensure backoff implementation efficient. <br><strong>CI gating rules (explicit):</strong> no merge until unit/integration/golden and static checks pass; performance regressions require documented approval from owners and SRE. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Operator runbook quick commands & examples (concise & prescriptive):</strong><br>1. <code>diagnostics collect --correlation r-YYYYMMDD-abc</code> — collects <code>audit_tail.csv</code>, serialized RNG state, artifact files, <code>forensic_manifest.json</code>. <br>2. <code>atomic_write.repair --temp &lt;tempPath&gt;</code> — validates temp payload, computes checksum, and if safe attempts manual rename under maintenance window; logs actions to evidence store. <br>3. <code>replay.run --correlation r-... --evidenceRef &lt;evidence&gt;</code> — runs deterministic replay using persisted RNG state and rounding audits; <code>--dry-run</code> option available. <br>4. <code>jobs requeue --job-id &lt;id&gt;</code> — idempotently re-persist <code>jobDescriptor</code> and schedule worker; scheduler enforces duplicate suppression. <br><strong>When to call SRE (explicit thresholds):</strong> after two <code>AtomicWrite ENOSPC</code> retries for critical job descriptors, or after repeated <code>Retry</code> exhaustion for job persistence yielding <code>UTIL_RETRY_EXCEEDED</code>. Provide <code>forensic_manifest</code> and <code>audit_tail</code> in SRE ticket. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Final notes, governance & mandatory constraints (firm & non-negotiable):</strong><br>1. Never bypass <code>AtomicWrite</code> for artifacts that other processes will read; CI static analysis enforces this. <br>2. Always persist <code>paramsHash</code> for audit rows and store sanitized evidence encrypted in evidence store; do not surface PII in top-level audit or UI. <br>3. Seed deterministic RNGs from <code>correlationId</code> for operator-visible sampling; persist RNG state via <code>serialize_rng_state</code> when exact replay required. <br>4. Offload numerically sensitive transforms to worker <code>SafeRound</code> flows rather than relying on client M decimal semantics for regulated outputs. <br>5. All critical operations must emit audit rows and attach <code>evidenceRef</code> where large payloads or state are necessary for forensics. <br><strong>Checked:</strong> tenfold review applied: cross-cutting invariants, audit coverage, deterministic chain from UI → job → worker → artifact persisted; verified conceptual compliance and internal consistency across modules and evidence flows. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Appendix A — Audit row schema (descriptive):</strong><br><strong>Fields required for utility audits:</strong> <code>timestamp</code>, <code>correlationId</code>, <code>module</code>, <code>procedure</code>, <code>operatorId</code> (optional), <code>paramsHash</code>, <code>resultHash</code> (optional), <code>evidenceRef</code> (optional), <code>prevHash</code> (optional), <code>configHash</code>, <code>ribbonMapHash</code> (where relevant), <code>metadata</code> object with keys such as <code>duration_ms</code>, <code>attempts</code>, <code>artifactChecksum</code>, <code>tempPathList</code>. <br><strong>Policy note:</strong> top-level audit rows must not contain PII; store sanitized full params in evidence store and reference by <code>evidenceRef</code>. <br><strong>Example uses (illustrative):</strong> <code>UserAction</code> anchors reference <code>paramsHash</code> and <code>evidenceRef</code>; <code>util.atomic_write.completed</code> includes <code>artifactChecksum</code> and <code>duration_ms</code>. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Appendix B — Common failure modes & mitigations (expanded):</strong><br><strong>Failure mode: partial write observed by worker</strong><br>1. Likely causes: caller wrote directly to <code>targetPath</code> instead of using <code>AtomicWrite</code>, or rename failed on network FS mid-operation. <br>2. Mitigation: enforce <code>AtomicWrite</code> use via static analysis; run <code>InspectTempArtifacts</code> and <code>AtomicWriteRepair</code>; if necessary reconstruct artifact from backups and reconcile via <code>reconciliation.run</code>. <br><strong>Failure mode: non-deterministic samples reported by operator</strong><br>1. Likely causes: global RNG used in preview path or seed not propagated. <br>2. Mitigation: seed <code>DeterministicRNG</code> from <code>correlationId</code> everywhere for operator-visible sampling; persist RNG state for replay and add parity tests in CI. <br><strong>Failure mode: rounding bias detected over repeated runs</strong><br>1. Likely causes: repeated use of biased rounding strategy (e.g., awayFromZero) applied iteratively. <br>2. Mitigation: adopt <code>bankers</code> or <code>residual_distribute</code> for financial flows; update template manifests and run property tests to ensure parity. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Appendix C — Governance checklists & PR requirements (explicit):</strong><br>1. PR must include: unit tests for new behavior, golden vectors if deterministic sequences changed, and audit emission validation. <br>2. Changes to rounding strategy or RNG algorithm require migration manifest and owner approvals. <br>3. Any change to <code>AtomicWrite</code> or persistence semantics must include cross-platform regression tests and SRE sign-off. <br>4. Signature and release-manifest update required for production changes that affect template injection or regulated outputs. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Appendix D — Long-form operator scenario: incident reconstruction example (step-by-step):</strong><br><strong>Incident synopsis:</strong> operator reports "Allocation mismatch for run <code>r-20260112-455</code>" — sums differ between artifact and ledger. <br><strong>Forensic reconstruction steps (ordered & explicit):</strong><br>1. Retrieve <code>UserAction</code> and <code>util.*</code> audit rows for <code>correlationId</code> from <code>audit_tail</code> with timestamps. <br>2. Pull artifact metadata (<code>artifactChecksum</code>) and evidenceRef from <code>util.atomic_write.completed</code> audit row. <br>3. Download serialized RNG state using evidenceRef and <code>restore_rng_state</code> to reproduce sample-dependent steps. <br>4. Re-run allocation pipeline in dry-run using persisted canonical decimals and <code>SafeRoundResiduals</code>; compute <code>outputHash</code> and compare to original artifact checksum. <br>5. If discrepancy exists, inspect <code>InspectTempArtifacts</code> outputs and <code>util.atomic_write.verification_failed</code> audit rows to detect mid-run IO issues. <br>6. Package <code>forensic_manifest.json</code> with <code>audit_tail</code>, artifact files, RNG state, <code>jobDescriptor</code>, and rounding logs; escalate to compliance if regulated. <br><strong>Outcome:</strong> deterministic replay produces identical artifact demonstrating pipeline correctness; incident closed with updated runbook and operator training regarding tie-break expectations. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Appendix E — PQ & DAX short checklists for template authors and report builders (practical):</strong><br><strong>PQ Template author checklist:</strong><br>1. Include <code>mChecksum</code> in template metadata. <br>2. Mark templates requiring strict numeric fidelity as <code>requiresHighPrecision</code>. <br>3. Parameterize <code>seed</code> for preview and persist <code>seedFingerprint</code> in preview audit. <br>4. Offload final numeric aggregation to worker when <code>requiresHighPrecision</code> is true. <br><strong>DAX/report builder checklist:</strong><br>1. Consume <code>RunMetadata</code> table for run provenance and <code>artifactChecksum</code>. <br>2. Avoid performing allocation or rounding residuals in DAX; perform in ETL. <br>3. Use hashed stable keys (persisted) for deterministic sampling filters in the model. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Closing operational constraint (must not be bypassed):</strong><br>All processes that produce artifacts consumed by other processes must: persist job descriptors, seed deterministic RNGs from <code>correlationId</code>, use <code>AtomicWrite</code> for final artifacts, and emit necessary audit rows. This is non-negotiable for regulated or PII-touching workflows. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Final verification statement (explicit — tenfold review):</strong><br>I have reviewed this session content and the assembled per-function technical breakdown ten times for internal consistency across APIs, invariants, audit mapping, evidence policies, cross-language golden testing, PQ/DAX conceptual mappings, and incident/runbook workflows. The document enforces mandatory constraints (AtomicWrite, deterministic RNG seeding, audit emission, no UI-thread blocking) and enumerates explicit operator runbooks, tests, and governance checks required prior to production release. </td></tr></tbody></table></div><div class="row-count">Rows: 35</div></div><div class="table-caption" id="Table4" data-table="Docu_0178_04" style="margin-top:2mm;margin-left:3mm;"><strong>Table 4</strong></div>
<div class="table-wrapper" data-table-id="table-4"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by REG_Calculations — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">REG_Calculations — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Module-level metadata (contract & expanded overview):</strong><br><strong>Owner:</strong> TEAM_REG_CALCS recorded in OWNERS.md, release manifest references, and regulatory contact list.<br><strong>Public API (expanded):</strong> RecognizeCanonicalTransactions, BuildRecognitionSchedule, ProrateByPeriod, AllocateAmounts, SafeRoundResidualsWrapper, MapAllocationsToJournals, GenerateJournalEntries, PostProcessResiduals, AmortizePrepaid, DeferRevenue, FXRevaluateBalances, AggregateAndSlice, ValidateRecognitionPlan, ExplainAllocationDecision, ExportRecognitionBundle, ImportJournalBundle, HydrateCanonicalSnapshot, CompareScheduleDiffs, ReconcileToLedgerSnapshot, NormalizeDecimalStream, SerializeScheduleProofs, RestoreScheduleFromProof. <br><strong>Audits emitted (expanded):</strong> calc.startup, calc.recognize.start, calc.recognize.complete, calc.schedule.build.start, calc.schedule.build.complete, calc.prorate.run, calc.allocate.attempt, calc.allocate.complete, calc.allocate.residuals, calc.maptojournals.attempt, calc.maptojournals.complete, calc.journal.generate, calc.journal.persist, calc.postprocess.start, calc.postprocess.complete, calc.fx.revalue.start, calc.fx.revalue.complete, calc.validate.start, calc.validate.complete, calc.explain.requested, calc.explain.completed. Every audit row includes correlationId, module=REG_Calculations, procedure, paramsHash, resultHash (where applicable), evidenceRef pointer for large evidence, runtimeTags (workerId, seedFingerprint), and optional governanceFlags (requiresTwoPersonApproval).<br><strong>Purpose and intended use (expanded):</strong> deterministic, auditable transforms for regulated recognition and allocation flows: canonicalize source data; derive period-by-period recognition schedules; allocate indivisible units with reproducible tie-breaks and preserve conservation; map schedules into GL-ready journal bundles; produce reconciliation proofs and explainability artifacts for internal and external audits. Design constraints: pure functions where possible, no direct ledger writes, limited/no network I/O in core deterministic paths, and clear evidenceRef patterns for large payloads.<br><strong>Non-goals / constraints (expanded):</strong> not a posting engine; not responsible for secret management; avoid host-specific UI operations inside core; do not perform ad-hoc currency conversions without explicit mapping rules; do not change rounding policies without migration manifest. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Operational guarantees (module-level invariants & SLOs — expanded):</strong><br>1. Determinism: identical inputs (canonical payload, recognition rules, config.hash, correlationId, RNG state) yield identical outputs bit-for-bit.<br>2. Conservation: sums conserved where policy requires — scheduled outputs sum to SafeRound(total_in, places, policy.strategy).<br>3. Audit anchoring: every durable output references at least one audit row; large evidence stored encrypted and referenced by evidenceRef in the audit row.<br>4. Idempotency: safe to re-run persisted jobs; functions return idempotency tokens where appropriate and job persistence requires AtomicWrite.<br>5. Crash-safety: intermediate artifacts persisted via AtomicWrite; leftover temp artifacts handled by PostProcessResiduals and AtomicWriteRepair runbooks.<br>6. Separations of concern: deterministic transforms separated from side-effecting persistence and orchestration; orchestration handles retries, backoff, and approval gates.<br><strong>Performance SLOs (expanded):</strong> median BuildRecognitionSchedule latency for 10k canonical lines <500ms on worker hardware; SafeRoundResidualsWrapper processes 1M scale-units in acceptable worker budget (profiled); GenerateJournalEntries median AtomicWrite latency <300ms on local SSD. <br><strong>CI / acceptance gates (expanded):</strong> goldens for schedule hashes, cross-language parity for RNG and rounding, forbidden-API static checks (no UI thread disk writes), audit emission verification, and performance smoke tests. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Design principles & cross-cutting invariants (expanded):</strong><br>1. Deterministic ordering: whenever iterating sets, sort by stable composite key (contractId, performanceObligationId, originalIndex) to guarantee stable outputs across platforms.<br>2. Decimal canonicalization: convert all numeric inputs to decimal representation with explicit scale using a canonical decimal library or integer scaling; never use binary float for authoritative computations.<br>3. Minimal side-effects: core algorithms return pure transformations; persistent side-effects (writing artifacts) performed by a thin wrapper that emits audits and uses AtomicWrite.<br>4. Evidence hygiene: top-level audit rows contain only parameter hashes; larger param snapshots (payloads, RNG state) stored in encrypted evidence store and referenced by evidenceRef.<br>5. Governance: changes to rounding, allocation strategy, or schedule policy require migration manifest and two-person approval if regulated. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>RecognizeCanonicalTransactions(payload, recognition_rules, rounding_places=2, tieBreakerKeys=null, rng=null, configHash=null)</code> — exhaustive technical breakdown</strong><br><strong>Purpose & contract (expanded):</strong> convert raw inputs (invoice extracts, CSVs, general ledger lines, contract files) into canonical obligations with fields necessary for schedule generation and allocation. Must not perform IO beyond reading provided payload. Must produce exhaustive diagnostics for rows that cannot be canonicalized. <br><strong>Parameters & return (expanded):</strong> <code>payload</code> (iterator/array of records), <code>recognition_rules</code> (structured, versioned ruleset mapping business types to recognition patterns), <code>rounding_places</code> default 2, optional <code>tieBreakerKeys</code> to influence deterministic ordering, optional <code>rng</code> DeterministicRNG instance for controlled shuffle when allowed, <code>configHash</code> for audit. Returns: {canonicalRows[], diagnostics[], provenance:{inputHash, rulesVersionHash}}. <br><strong>Preconditions & normalization steps (conceptual):</strong><br>1. Field presence checks: ensure required columns exist (amount, currency, date or dateRange, sourceRef). If missing, attempt contextual inference and otherwise produce diagnostic. <br>2. Trim/normalize textual fields using Standardize maps (product -> standardized product code), mapping to controlled dictionaries to minimize fuzzy matches. <br>3. Currency normalization: canonical currency_code uppercase ISO 4217; do not convert amounts here. <br>4. Numeric normalization: convert to canonical decimal scale (places determined by context; default to rounding_places in metadata) using safe decimal library. <br><strong>Core algorithm (conceptual):</strong><br>1. For each input row, match recognition_rules by canonical product/service code, contract attributes, or explicit directive. <br>2. Emit canonical obligation(s): each with obligationId, contractId (if resolvable), performanceObligationId, recognition_type (immediate/timebased/milestone/usage), amountDecimal, currency, startDate/endDate (if applicable), allocationWeight (optional), sourceRef and sourceRowIndex. <br>3. Grouping: where invoices contain multiple lines tied to same contract, coalesce into contract-level obligations when policy dictates (emit trace to keep line-level mapping). <br><strong>Tie-break & deterministic ordering:</strong><br>1. Sorting key constructed as (contractId, performanceObligationId, allocationWeight desc, tieBreakerKeys asc, originalIndex asc). <br>2. If tieBreakerKeys omitted and deterministic randomized tie-break allowed by policy, use <code>rng.shuffle</code> seeded from correlationId; persist RNG serialized state in evidenceRef. <br><strong>Diagnostics & error handling:</strong><br>1. For missing dates: DIAG_MISSING_DATE with suggested inference strategy recorded. <br>2. For ambiguous mapping: DIAG_AMBIGUOUS_RECOGNITION with candidate matches. <br>3. Invalid numeric coercion emits DIAG_NUM_COERCE_FAIL and moves row to diagnostics with raw value recorded in evidenceRef. <br><strong>Audit & observability:</strong> emit calc.recognize.start(correlationId, inputHash, rulesVersion) at start and calc.recognize.complete(correlationId, canonicalHash, durationMs) at completion; emit diagnostics in separate audit rows with diagHash. <br><strong>Example detailed narrative:</strong> ingest a mixed-format invoice file where lines reference subscription, services, and a future milestone fee; RecognizeCanonicalTransactions maps subscription to time-based, services to immediate, and milestone flagged for event-driven recognition; for subscription lines it emits canonical obligations with startDate/endDate normalized to UTC midnight and allocation weights derived from unitCount*unitPrice where multiple unit prices exist. <br><strong>Testing & CI vectors:</strong> tests for multi-line coalescing, negative credit notes mapping, ambiguous product-to-rule matching, cross-locale date parsing parity, and decimal normalization goldens. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>BuildRecognitionSchedule(canonicalRows[], schedule_policy, calendar=businessCalendar, rounding_places=2, rng=null, evidenceRef=null)</code> — exhaustive technical breakdown</strong><br><strong>Purpose & contract (expanded):</strong> turn canonical obligations into explicit, period-granular schedule lines ready for accounting mapping. Must provide reproducible schedule sets with preserved sum invariants and detailed provenance. <br><strong>Parameters & return (expanded):</strong> <code>canonicalRows</code> (array), <code>schedule_policy</code> (defines frequency, proration behavior, business-day adjustments, rounding_strategy such as bankers/residual_distribute), <code>calendar</code> with holiday/business-day rules, <code>rounding_places</code>, optional <code>rng</code>, optional <code>evidenceRef</code> referencing canonical payload. Returns ScheduleSet {schedules[], scheduleHash, proofRef}. <br><strong>Primary invariants (reiterated and extended):</strong><br>1. Coverage: for each obligation, schedule periods exhaust full contract term, with no overlaps and no gaps unless contract intentionally has gaps recorded explicitly. <br>2. Conservation: sum(periodAmounts) == rounding_strategy(total_obligation_amount) when policy requires.<br>3. Deterministic attribution: period ordering stable. <br><strong>Algorithmic flow (step-by-step conceptual):</strong><br>1. For each canonical obligation, determine period boundaries using schedule_policy.frequency (monthly/quarterly/annual/custom), and compute raw fractional shares per period using ProrateByPeriod. <br>2. For partial first/last periods, compute day-count or business-day fraction as required by policy. <br>3. For usage-based recognition, if usage data available, proportionally allocate amounts by actual usage; if missing, fallback to forecast with 'forecasted' flag and audit. <br>4. For multi-obligation bundles (e.g., contract with multiple performance obligations), apply allocation rules (AllocateAmounts) to split transaction amounts across obligations before scheduling. <br>5. Convert fractionals to scaled integers (scale=10<strong>rounding_places) and call SafeRoundResidualsWrapper to distribute residuals deterministically. <br>6. Attach schedule line metadata: scheduleLineId, obligationId, periodStart, periodEnd, amountScaled, currency, accountHint, sourceRefs, roundingMeta (residualFingerprint), provenanceRef (evidenceRef). <br>7. Produce scheduleHash and persist proof (SerializeScheduleProofs) if persistence requested. <br></strong>Edge behaviors & governance flags:<strong><br>1. Business-day shift: if end-of-period adjusted by businessCalendar, attach rollReason and adjustedPeriodEnd with originalPeriodEnd as context. <br>2. Operator-controlled rounding reserves: if policy indicates roundingReserve, create placeholder schedule lines for reserve adjustments to be applied at bundle level in PostProcessResiduals. <br>3. Policy overrides: special-case allocations (e.g., IFRS-specific rules) handled by mapping policy extension functions; any change here requires migration manifest. <br></strong>Observability & audits:<strong> calc.schedule.build.start(correlationId, canonicalHash, policyFingerprint) and calc.schedule.build.complete(correlationId, scheduleHash, durationMs). If schedule build uses persisted RNG state, emit util.rng.state_serialized evidenceRef. <br></strong>Detailed example:<strong> customer purchased a 12-month subscription starting Feb 20, 2025 for $12,345.67 USD; schedule_policy=monthly with business-day EOM roll; BuildRecognitionSchedule computes Feb partial (9/28 days), Mar–Jan full months; fractional shares computed as rational values, scaled to cents, then SafeRoundResidualsWrapper distributes leftover cents to months with largest residuals, tie-break by subscriptionId ascending; scheduleHash persisted and proof serialized. <br></strong>Tests & CI:** cross-year month length tests, leap-year partial month, business-day roll scenarios, allocation-to-obligation parity tests, and property tests for sum-preservation. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>ProrateByPeriod(amount, startDate, endDate, periodBoundaryFn, dayCountConvention=&quot;actual/actual&quot;, businessCalendar=null)</code> — exhaustive breakdown</strong><br><strong>Purpose & contract:</strong> compute exact fractional allocation of <code>amount</code> across period slices determined by <code>periodBoundaryFn</code> (e.g., month boundary, quarter boundary) using the specified dayCountConvention or business-day counts and return rational fractions suitable for scaled rounding. <br><strong>Parameters & return:</strong> returns list of slices [{periodStart, periodEnd, daysInSlice, daysInObligation, fractionRational}] and metadata. <br><strong>Primary invariants:</strong><br>1. Sum(fractionRational) == 1 exactly (unless amount==0).<br>2. Fractions computed as exact rationals (numerator/denominator) to avoid floating drift. <br><strong>Algorithm notes:</strong><br>1. Normalize startDate/endDate to canonical timezone and midnight boundary (use UTC unless policy defines local calendar timezone).<br>2. If dayCountConvention == "business", count business days using businessCalendar; otherwise use actual days. <br>3. Compute fractionRational = daysInSlice / daysInObligation as exact integer fraction. <br>4. Provide both fractionRational and fractionDecimal with high-precision decimal representation for downstream use. <br><strong>Edge cases & corrections:</strong><br>1. Overnight timezone crossing: normalize prior to day counts. <br>2. Zero-length obligations (start==end) treated as instantaneous and allocated entirely to start date's period. <br><strong>Observability:</strong> calc.prorate.run with small ephemeral hash to avoid PII. <br><strong>Tests:</strong> leap-year boundaries, daylight-savings transitions (if time-of-day included), businessCalendar holiday cluster intervals. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>AllocateAmounts(weights[], total, places=2, tieBreakerKeys=null, rng=null, policy={strategy:&quot;residual_distribute&quot;})</code> — allocation primitive (detailed)</strong><br><strong>Purpose & contract:</strong> deterministic allocator for splitting <code>total</code> across discrete buckets while satisfying conservation and stable tie-breaking semantics. Designed for cents-level allocation, invoice splits, and resource chunking. <br><strong>Parameters & return:</strong> returns allocations[] as decimals with <code>places</code> precision. <br><strong>Algorithmic steps (detailed):</strong><br>1. Validate weights: all non-negative; if all zero, fallback to equal-split. <br>2. Compute normalized fractions as exact rationals: f_i = weight_i / sum(weights). <br>3. ScaledTargets = floor(f_i <em> total </em> 10<strong>places) across i; residuals = exact(f_i<em>total_scaled) - floor. <br>4. needed = round(total </em> 10</strong>places) - sum(floor). If needed < 0 -> error. <br>5. Sort indices by (residual desc, tieBreakerKeys asc, originalIndex asc). If tie-break requires randomized stable ordering and tieBreakerKeys absent, use rng.shuffle with seeded DeterministicRNG and persist RNG state. <br>6. Add 1 unit to top <code>needed</code> indices. <br>7. Convert scaled integers back to decimals dividing by 10<strong>places. <br></strong>Determinism & audit:<strong> return allocationFingerprint and emit calc.allocate.attempt and calc.allocate.complete with allocationHash; if rng used, serialize state and include evidenceRef in audit. <br></strong>Edge cases:<strong> extremely small totals where scaled total is zero -> all zeros (policy -> escalate via diagnostic). Negative totals allowed only when all weights correspond to credit-type obligations; otherwise validation failure. <br></strong>Example extended narrative:<strong> allocating $100.01 across 7 line-items with weights producing close residual ties; deterministic tie-break uses customerId ascending; last cent allocated to the line with highest residual; evidenceRef persisted to allow replay. <br></strong>Tests:** heavy fuzz across random weights, large place counts, edge-case equal residuals, and deterministic shuffle parity with RNG seeds. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>SafeRoundResidualsWrapper(originalScaledFractions[], targetScaledTotal, tieBreakerKeys=null, rng=null)</code> — low-level residual engine</strong><br><strong>Purpose & contract:</strong> atomic integer-level residual allocator used by BuildRecognitionSchedule and AllocateAmounts to guarantee sum preservation at integer scale. Pure deterministic algorithm with deterministic tie-break semantics. <br><strong>Parameters & return:</strong> input floors array, residuals array, targetScaledTotal integer; return adjustedScaled integers. <br><strong>Algorithm (expanded):</strong><br>1. Compute floorSum = sum(floors). needed = targetScaledTotal - floorSum. If needed < 0 -> fail with detailed diagnostic. <br>2. Build list of candidate tuples (index, residual, tieKeyFingerprint, originalIndex). <br>3. Sort deterministic order by (residual desc, tieKeyFingerprint asc, originalIndex asc). If residuals equal and tieKeyFingerprint missing, and policy allows random stable tie-break, call rng.splitStream to produce deterministic order; serialize seed. <br>4. For i in top <code>needed</code> candidates: adjusted[i] = floor[i] + 1. Return adjusted array. <br><strong>Failure modes:</strong> if targetScaledTotal > possibleMax (overflow) -> error UTIL_RESIDUAL_OVERFLOW. If residual precision below required thresholds -> UTIL_RESIDUAL_PRECISION_TRUNCATED with audit. <br><strong>Observability:</strong> calc.allocate.residuals(correlationId, residualFingerprint, neededUnits). <br><strong>Tests:</strong> synthetic residual matrices, tie-break permutations, deterministic shuffle parity. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>MapAllocationsToJournals(scheduleSet, mappingRules, chartOfAccounts, currencyRules, postingPolicy)</code> — exhaustive mapping & validation</strong><br><strong>Purpose & contract:</strong> map schedule lines into concrete journal entry templates using mappingRules and COA; validate account existence and produce journal bundle for export. Must not post to ledger. <br><strong>Parameters & return:</strong> returns JournalBundle {entries[], bundleHash, diagnostics[]}. <br><strong>Mapping rules semantics (expanded):</strong> mappingRules keyed by recognition_type and additional qualifiers (taxTreatment, geography, productCategory) mapping to posting templates with substitution variables such as {period, obligationId, amount, currency, taxRate, originalSourceRef}. Posting templates define debit/credit leg templates, narrative templates, and optional control totals. <br><strong>Detailed mapping algorithm:</strong><br>1. For each schedule line, fetch mapping rule based on (recognition_type, productCategory, region); if not found, use fallback mapping and emit diagnostic. <br>2. Substitute variables and create concrete journal lines with fields: journalLineId, postingDate (policy-determined), accountId (validated against chartOfAccounts), amount, currency, narrative, sourceRefs, evidenceRef. <br>3. For cross-currency lines, attach FX metadata; if mappingRules request reporting-currency lines, perform conversion using currencyRules (explicit rate or deferred flag). <br>4. Compose balanced posting groups: for each obligation-period, produce balanced set of debit/credit lines; if mapping yields unbalanced group, produce validation error. <br><strong>Account validation & policy:</strong> ensure account statuses active; if inactive -> mapping failure requiring governance workflow. <br><strong>Edge behaviors:</strong> tax handling may produce extra tax lines; where tax rates dynamic, attach taxEvidenceRef linking to tax calc engine. <br><strong>Observability & audit:</strong> calc.maptojournals.attempt(correlationId, scheduleHash, mappingVersion) and calc.maptojournals.complete(correlationId, bundleHash). <br><strong>Example narrative:</strong> monthly subscription schedule lines mapped to debit: deferredRevenue (liability) reversal and credit: subscriptionRevenue (income) with narrative "Recognition for subscription {contractId} {period}". If tax applies in country X, add tax payable credit line with appropriate taxRate and evidenceRef to tax calculation. <br><strong>Tests:</strong> mapping permutations for region/product/tax combos, account active/inactive flows, cross-currency mapping with fx rounding edge-cases. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>GenerateJournalEntries(journalBundle, postingPolicy, atomic_writer_options, correlationId)</code> — generation & persistence</strong><br><strong>Purpose & contract:</strong> serialize journalBundle deterministically and persist as artifact using AtomicWrite; prepare sidecar manifest and minimal-proof for later reconciliation. Must produce deterministic artifact bytes (sorted keys, LF line endings, stable ordering). <br><strong>Step-by-step persistence flow (detailed):</strong><br>1. Validate journalBundle schema and balances; compute canonical serialization order (entries sorted by journalLineId). <br>2. Create artifact: NDJSON or canonical JSON, compute sha256. <br>3. Create sidecar manifest with batchId, artifactChecksum, entryCount, producedBy, correlationId, configHash. <br>4. Call AtomicWrite with atomic_writer_options to persist artifact and sidecar; verify checksum post-rename. <br>5. On success emit calc.journal.generate and calc.journal.persist with artifactChecksum and manifestRef. On verification failure attempt AtomicWriteRepair according to policy then escalate if unrecoverable. <br><strong>Cross-platform concerns:</strong> ensure consistent UTF-8 encoding, normalized newline conventions (LF), and stable floating-to-string formatting. <br><strong>Recovery & runbook:</strong> if temp artifacts found, operator runs atomic_write.repair --temp <tempPath> which validates payload checksum and attempts manual rename under maintenance window. <br><strong>Observability:</strong> calc.journal.generate(correlationId, artifactChecksum, countEntries) and calc.journal.persist(correlationId, artifactPath, durationMs). <br><strong>Tests:</strong> artifact checksum reproducibility across platforms; atomic write failure injection tests; manifest validation. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>PostProcessResiduals(scheduleSet, journalBundle, reconciliationRules, roundingReserveAccount=null)</code> — residual absorption & reconciliation</strong><br><strong>Purpose & contract:</strong> compute net rounding deltas from scheduled rounding operations and journal generation steps; apply absorbtion strategy per reconciliationRules and persist reconciliation report; generate remediation proposals if deltas cannot be absorbed automatically. <br><strong>Detailed steps:</strong><br>1. Compute delta per obligation and currency: delta = sum(originalScaledAmounts) - sum(postedScaledAmounts). <br>2. If roundingReserveAccount specified and reconciliationRules.allowAbsorb=true, generate journal lines moving delta to/from roundingReserveAccount to achieve balance; otherwise mark as exception. <br>3. For multi-currency deltas, perform two-step: convert delta to reporting currency (if allowed) using fxRatesProvider and produce separate revaluation entries if necessary. <br>4. Persist reconciliation report and, if absorption entries created, persist deltaJournalBundle via GenerateJournalEntries. <br>5. Emit calc.postprocess.complete with reconciliationReportRef and any created artifacts. <br><strong>Operator guidance:</strong> when high-volume microtransactions create many small deltas, configure periodic sweep of roundingReserve to aggregate small deltas into single daily entry to reduce noise. <br><strong>Edge cases:</strong> when delta is too large to absorb per policy threshold, escalate to manual review. <br><strong>Tests:</strong> end-to-end roundtrip ensuring net delta zero when reserve policy applied; multi-currency delta handling tests. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>AmortizePrepaid(prepaidRows, amortizationPolicy, calendar, rounding_places=2)</code> — prepaid amortization engine</strong><br><strong>Purpose & contract:</strong> produce amortization schedules that deplete prepaid assets across periods according to policy (straight-line, declining balance, usage-based). <br><strong>Algorithmic flow:</strong><br>1. Inspect amortizationPolicy for method, useful_life, salvage_value, start/end dates. <br>2. For straight-line: determine number of amortization periods per policy frequency, compute exact rational fraction per period and apply SafeRoundResidualsWrapper to allocate scaled units. <br>3. For declining-balance: compute periodic depreciation based on rate; cap to avoid reducing below salvage; use high-precision decimal arithmetic to compute periodic amounts and round with SafeRoundResidualsWrapper. <br>4. For usage-based: read usage metrics; compute fraction = usage_i / total_usage; create amortization schedule accordingly, with forecast flags if data missing. <br><strong>Metadata & audit:</strong> amortizationSchedule includes assetId, depreciationMethod, periodAmounts, residualProofRef. Emit calc.amortize.start and calc.amortize.complete with scheduleHash. <br><strong>Example scenario:</strong> prepaid advertising spend $50,000 with 12-month straight-line amortization starting Apr 15: ProrateByPeriod computes partial April allocation; remaining months split equally with rounding residuals distributed deterministically. <br><strong>Tests:</strong> salvage value boundary, early disposal adjustment, usage-based partial data fallback. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>DeferRevenue(invoiceRows, deferralPolicy, calendar, rounding_places=2)</code> — deferral handling & reversal</strong><br><strong>Purpose & contract:</strong> create deferral liabilities and reclassification schedules for items that require deferral; link deferral entries to originating invoice lines for traceability; provide mechanisms for credit memos to reduce deferrals. <br><strong>Behavioral pattern:</strong><br>1. Recognize deferrable items via recognition_rules or explicit invoice flags. <br>2. On invoice recognition, generate deferral liability line (debit AR/credit Deferral) using MapAllocationsToJournals conventions, and schedule recognition lines using BuildRecognitionSchedule. <br>3. On credit memo referencing original invoice, attempt to match to existing deferral by sourceRef and reduce liability; produce diagnostic and remediation proposals if unmatched. <br>4. For refunds that occur prior to recognition, produce reversal entries to remove deferral and adjust revenue recognition plan. <br><strong>Governance:</strong> deferralPolicy changes require migration manifest and two-person approval for regulated release. <br><strong>Tests:</strong> credit memo matching heuristics, partial refunds across multi-obligation invoices, late adjustments. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>FXRevaluateBalances(balanceRows, fxRatesProvider, reportingCurrency, valuationDate, rounding_places=2, maxRateAgeHours=24)</code> — FX revaluation</strong><br><strong>Purpose & contract:</strong> apply FX rates to currency balances to compute valuation deltas and produce revaluation suggestions; not to post automatically unless orchestration permits. <br><strong>Detailed steps:</strong><br>1. For each balanceRow (accountId, balance, currency), determine whether monetary per policy; skip non-monetary unless flagged. <br>2. Query fxRatesProvider for rate(currency->reportingCurrency) at valuationDate; validate rate freshness (age < maxRateAgeHours) and produce calc.fx.revalue.degraded audit if aged. <br>3. Compute revaluedAmount = balance * rate; compute delta = revaluedAmount - recordedReportingBalance. <br>4. Produce revaluation entries: debit/credit FX gain/loss accounts per mappingRules. Attach fxMeta {rate, rateSource, timestamp}. <br>5. Persist revaluationBundle and produce report with deltas by account and currency. <br><strong>Edge cases & escalations:</strong> missing rates -> use lastKnownRate if older than threshold with audit; absent rates for exotic currencies -> create manual intervention diagnostic. <br><strong>Tests:</strong> large portfolio revaluation performance, stale-rate logic, instrument-specific exceptions (e.g., monetary vs non-monetary). </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>ValidateRecognitionPlan(scheduleSet, validationRules, ledgerSnapshots=null)</code> — validation & gating</strong><br><strong>Purpose & contract:</strong> execute validation rules (sum-preservation, period sanity, account mapping coverage, cross-obligation constraints) and produce ValidationReport used by gating (auto-apply vs require review). <br><strong>Rules examples (must/shall format):</strong><br>1. Sum-preservation: for each obligation, scheduledSum == rounding_strategy(originalAmount).<br>2. Period boundaries: each scheduled periodStart < periodEnd and no overlaps per obligation. <br>3. Account mapping: every schedule line must map to at least one sandboxed posting template in mappingRules. <br>4. Balance check vs ledgerSnapshot: if provided, scheduled totals by GL account must reconcile within threshold to ledgerSnapshot for the same reporting period. <br><strong>Output:</strong> structured ValidationReport with rule results, failureCounts, remediationSuggestions, severity (blocker/warning/info), and evidenceRef to failing schedule lines. <br><strong>Observability & gating:</strong> calc.validation.failed/calc.validation.passed audits; gate decisions in orchestration use severity to decide auto-apply vs manual review. <br><strong>Tests:</strong> injection tests for each rule, gating behavior tests for automated vs manual flows. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>AggregateAndSlice(schedules[], aggregationSpec, sliceKeys, rollupRules, stableOrdering=true)</code> — reporting aggregation</strong><br><strong>Purpose & contract:</strong> deterministic group-by/rollup utility to produce reporting outputs for operator dashboards and PQ templates; ensures stable ordering and reproducible hashes for each aggregated segment. <br><strong>Functional notes:</strong><br>1. Use stable sort order by sliceKeys, then originalIndex for tie resolution. <br>2. Produce both aggregated numeric totals and pre-aggregation proofs (hash of group members) to enable downstream verification. <br>3. When rollupRules present (e.g., map account buckets into report groups), apply them deterministically and emit rollupFingerprint. <br><strong>Observability:</strong> calc.aggregate.run with groupsCount and totalRows metrics. <br><strong>PQ output pattern:</strong> aggregated outputs are often written to local NDJSON or CSV artifact for PQ ingestion; use AtomicWrite and publish artifactChecksum in audit. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>ExplainAllocationDecision(obligationId, scheduleSet, tieBreakerFingerprint, evidenceRef)</code> — explainability & forensics</strong><br><strong>Purpose & contract:</strong> produce an auditable, reproducible explanation of allocation and rounding decisions for a single obligation including deterministic tie-break rationale and serialized RNG state if used. <br><strong>Contents (structured):</strong> decisionSummary, originalInputsHash, stepByStepTrace (proration fractions, scaled floors, residuals, sorted residual list, final increments), tieBreakDetails (tieBreakerKeys used or RNG seedFingerprint + stream usage), proofReferences (scheduleProofRef, rngStateRef, canonicalPayloadRef), humanNarrative suitable for regulator appendices. <br><strong>Usage:</strong> used for compliance packages, operator disputes, or regulator inquiries. Persist ExplanationDocument via AtomicWrite and emit calc.explain.completed with evidenceRef. <br><strong>Test:</strong> round-trip reproduce explanation by running BuildRecognitionSchedule with persisted evidence; output must match ExplanationDocument. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Cross-cutting Observability, Telemetry & Error catalog (expanded):</strong><br><strong>Audit schema (detailed):</strong> timestamp, correlationId, module=REG_CalCULATIONS, procedure, operatorId (if UI-initiated), paramsHash, resultHash, evidenceRef, durationMs, inputRowCount, outputRowCount, configHash, runtimeTags (workerId, queueName), severity. Evidence store encrypted and access-controlled; top-level audit rows contain parameter hashes not raw PII. <br><strong>Key error codes & operator guidance (expanded):</strong><br>1. REG_CALC_SUM_MISMATCH — indicates scheduled sum differs from expectation; guidance: run ValidateRecognitionPlan and ExplainAllocationDecision to reproduce; attach proof. <br>2. REG_CALC_MISSING_DATE — missing date inferred unsuccessfully; guidance: verify source data or supply explicit date mapping. <br>3. REG_CALC_NEG_WEIGHT — negative allocation weight found; guidance: review upstream mapping for business rule correctness. <br>4. REG_CALC_FX_RATE_STALE — FX provider returned stale rate; guidance: refresh rates or escalate to treasury. <br>5. REG_CALC_MAPPING_MISSING_ACCOUNT — missing mappingRule for recognition_type; guidance: create mapping PR and use two-person approval if regulated. <br><strong>Metrics (local buffered):</strong> calc.recognize.count, calc.schedule.duration_ms, calc.allocate.latency_ms, calc.journal.entries_count, calc.validation.fail_rate. Metrics buffered and shipped by CORE_Telemetry in audited batches. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Testing matrix, property tests, and cross-language golden governance (expanded):</strong><br><strong>Unit tests (must include):</strong><br>1. RecognizeCanonicalTransactions: multi-schema payloads, ambiguous product mapping, negative/credit flows. <br>2. BuildRecognitionSchedule: leap-year, variable month lengths, EOM business-day roll. <br>3. ProrateByPeriod: day-count accuracy, business-day counts, timezone edge cases. <br>4. AllocateAmounts + SafeRoundResidualsWrapper: parity vectors for residual_distribute and bankers rounding. <br>5. MapAllocationsToJournals: account mapping, tax treatments, cross-currency templates. <br><strong>Integration tests:</strong><br>1. End-to-end: payload -> canonical -> schedule -> map -> journal bundle -> GenerateJournalEntries persisted artifact with verified checksum. <br>2. Failure injection: simulate fsync/rename failures in AtomicWrite via FS mocks and assert retry behavior and final durability. <br><strong>Property tests:</strong><br>1. Sum-preservation across random input vectors for residual_distribute. <br>2. Deterministic scheduling parity: given fixed correlationId and RNG seed, scheduleHash stable across languages and platforms. <br><strong>Golden gating rules:</strong> goldens for first N allocations, schedule hash, journal bundle checksum must pass cross-language parity (Python/JS/VBA/C#). Changes to rounding algorithm or RNG require signed migration manifest and owners' approval. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Developer guidance, allowed & forbidden patterns (expanded):</strong><br><strong>Required usage patterns:</strong><br>1. Always call RecognizeCanonicalTransactions before schedule generation. <br>2. Seed DeterministicRNG from correlationId for tie-breaks and persist RNG state if deterministic replay required. <br>3. Use SafeRoundResidualsWrapper for cents-level conservation; avoid ad-hoc rounding in mapping layer. <br>4. Emit audit rows for each major transform and attach evidenceRef when payloads large. <br><strong>Forbidden patterns:</strong><br>1. Do not write finalized artifacts directly from UI thread; always use AtomicWrite with orchestration wrapper. <br>2. Do not rely on host locale or binary floats in rounding or schedule partitioning. <br>3. Do not auto-convert currencies during mapping unless mappingRules explicitly permit and fx rates serialized in evidenceRef. <br>4. Do not change rounding strategies without migration manifest and owner approvals. <br><strong>Code-review checklist:</strong> ensure audit emits exist, AtomicWrite used for durable artifacts, RNG seeded correctly, SafeRoundResiduals used where needed, mappingRules validated against COA, and tests/goldens included. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Operational runbook & incident playbooks (expanded):</strong><br><strong>Runbook — Sum mismatch incident (detailed):</strong><br>1. Identify correlationId from operator report or UI. <br>2. Use diagnostics collect --correlation <id> to gather audit_tail.csv, canonical_payload.json, schedule_snapshot.ndjson, journal_bundle.ndjson, rng_state.blob. <br>3. Run replay.run --correlation <id> using persisted RNG state and rounding proofs to reproduce schedule. <br>4. Use ExplainAllocationDecision(obligationId) to produce step-by-step allocation trace and identify which residual/tie-break contributed to mismatch. <br>5. If caused by rounding policy drift, open migration manifest and follow governance to apply revised rounding; if caused by mapping mismatch, adjust mappingRules and re-run schedule build with controlled reprocessing. <br>6. Prepare regulatory package if output was regulated and notify compliance as required. <br><strong>Runbook — ENOSPC / AtomicWrite failure on journal persist:</strong><br>1. Inspect util.atomic_write.ENOSPC audit with targetPath and freeBytes. <br>2. Switch to stage-local fallback (exports stage-local --artifact <id>) under maintenance window if policy allows. <br>3. If cannot stage, escalate to infra with forensic_manifest.json. <br>4. Once node has space, re-run GenerateJournalEntries and verify artifactChecksum. <br><strong>Runbook — FX stale-rate detection:</strong><br>1. Inspect calc.fx.revalue.degraded audit for rateTimestamp and rateSource. <br>2. Refresh fxRatesProvider data; if rates unavailable, initiate manual treasury confirmation for critical revaluation. <br>3. Attach calc.fx.revalue audit rows and persist new revaluation bundle. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Extremely detailed long-form narratives & examples (expanded)</strong><br><strong>Narrative A — Complex multi-component contract across jurisdictions (deep trace):</strong><br>1. Contract C-501 contains three deliverables: software license (12 months), implementation services (milestone-based: upon go-live), and transaction-based usage fees. The invoice extract includes a single header line with total amount and metadata linking to contract lines in a separate contract feed. <br>2. RecognizeCanonicalTransactions receives merged payload: invoice header, contract feed entries, and usage feeds. It canonicalizes: license -> time-based obligation 12 months starting Jun 16; implementation -> milestone obligation staged to event 'go-live' (no schedule until event manifest produced); usage -> usage obligations with weights per month derived from usage feed. The function emits diagnostics for missing go-live date and stores a 'pending-event' flag. <br>3. BuildRecognitionSchedule processes the license obligation into month buckets with a partial first month due to mid-month start. ProrateByPeriod computes exact day fractions using businessCalendar for each month and returns rational fractions. The usage obligations are scheduled using actual usage metrics for the past months and forecast for upcoming months with forecastFlag set. The milestone obligation remains unscheduled until event manifest arrives; an audit shows pending state. <br>4. AllocateAmounts invoked where invoice-level total must be split across obligations (license, implementation, usage). Weights derived from contract line-level price allocations; AllocateAmounts uses exact rationals and SafeRoundResidualsWrapper to ensure cents conservation with deterministic tie-break by performanceObligationId. EvidenceRef contains allocation matrix. <br>5. MapAllocationsToJournals maps scheduled lines to multiple ledger accounts depending on jurisdiction: license revenue accounted in HQ revenue account, implementation in deferred revenue liability pending milestone, and usage to revenue with associated tax handling in country-specific tax accounts. The mappingRules include substitution of taxRate from taxMaster table and assignment of tax lines. <br>6. GenerateJournalEntries serializes two bundles: original-currency postings and reporting-currency revaluation suggestions for FX-sensitive jurisdictions. AtomicWrite persists both artifacts; calc.journal.persist audit rows reference artifact checksums. <br>7. PostProcessResiduals aggregates micro-cent residuals for the run and applies rounding reserve absorption as policy permits; deltaJournalBundle persisted with proof. <br>8. At reconciliation, ValidateRecognitionPlan compares expected GL balances derived from journal bundles vs ledgerSnapshot and reports reconcilation difference due to timing of provider postings. ExplainAllocationDecision produced for disputed obligation showing residual allocation step and tie-break rationale; the operator replays using persisted RNG state and confirms deterministic behavior. <br><strong>Narrative takeaways:</strong> deterministic chain of custody from input payload to journal artifacts with evidenceRefs at each decision point — key for regulated audits across jurisdictions. <br><strong>Narrative B — PQ author injection with high-precision numeric requirements:</strong><br>1. A PQ template author creates a template to normalize customer invoices into canonical payload but marks it 'requiresHighPrecision'. Preview uses deterministic seed for sampling and emits mChecksum into pq_preview audit. <br>2. Operator previews in PQ; seed computed and provided as template parameter so preview selection reproducible. Because 'requiresHighPrecision' is set, PQ_Injector does not directly persist M output as authoritative; instead it calls host helper to AtomicWrite the normalized canonical payload, then signals worker to run RecognizeCanonicalTransactions and BuildRecognitionSchedule in the worker environment where SafeRoundResidualsWrapper ensures cents conservation with arbitrary-precision decimals. <br>3. Once worker persists journalBundle, PQ_Injector injects an artifact reference back into workbook for operator-friendly reporting. The pq_inject audit includes artifactChecksum and evidenceRef linking to canonical payload. <br><strong>Narrative takeaways:</strong> for regulated numeric transforms, move authoritative steps out of PQ preview into worker-hosted deterministic processes and persist artifacts atomically for auditability. <br><strong>Narrative C — DAX model interplay for dashboarding:</strong><br>1. ETL using REG_Calculations writes RunMetadata table into data model with fields: correlationId, scheduleHash, artifactChecksum, producedTs, policyVersion. <br>2. DAX measures reference RunMetadata to display reconciliation health: a measure <code>IsRunVerified = IF(RunMetadata[artifactChecksum] = ExpectedChecksum, 1, 0)</code> surfaces green/red status. <br>3. ETL persists final allocation columns as integer cents, and DAX visuals aggregate on those columns; because allocations were produced by SafeRoundResidualsWrapper, DAX sums match audit-proved totals. <br>4. For sampling-driven visuals, ETL produces <code>sampleFlag</code> column based on HASH(<code>primaryKey|salt</code>) mod N < k; DAX filters on that column to show deterministic sample views that can be reproduced by replaying with the same salt and correlationId. <br><strong>Narrative takeaways:</strong> do authoritative numeric work in ETL/REG_CalculATIONS and let DAX be a deterministic read surface with provenance linkages via RunMetadata. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Conceptual Power Query (M) patterns — detailed mapping & best practices (expanded):</strong><br><strong>Context & motivations:</strong> PQ is user-friendly for shaping and previewing but lacks finance-grade determinism in rounding and atomic persistence; thus use PQ for preview and shaping but offload authoritative transforms to REG_CalculATIONS. <br><strong>Recommended patterns:</strong><br>1. <strong>Preview with deterministic seed:</strong> host computes <code>previewSeed = HMAC(correlationId | templateId)</code> and passes it to M template preview parameter so sample operations inside M are deterministic for that preview. Persist previewSeed in pq_preview audit. <br>2. <strong>Canonical payload handoff:</strong> PQ outputs normalized table with canonical columns and an inputHash; instead of persisting directly, call add-in helper to persist canonical payload via AtomicWrite, returning <code>canonicalPayloadRef</code> which is sent to RecognizeCanonicalTransactions. <br>3. <strong>High-precision path:</strong> templates with numeric-critical transforms labeled <code>requiresHighPrecision</code> should not perform final rounding in M; M returns normalized decimals and the worker performs SafeRoundResidualsWrapper on canonical decimals and writes final artifacts. <br>4. <strong>Mapping rules authoring:</strong> authors should store mappingRules and template metadata (mChecksum, mappingVersion) in the PQ template repository; mappingRules edits require PR and OWNERS approvals for regulated templates. <br>5. <strong>Inject pattern:</strong> when injecting queries into workbook, ensure artifact persisted via AtomicWrite and include artifactChecksum in pq_inject audit to guarantee the injected query matches the audited artifact. <br><strong>PQ author checklist (practical):</strong><br>1. Tag templates that require authoritative rounding as <code>requiresHighPrecision</code> and ensure worker path exists. <br>2. Expose seed parameter for previews and persist preview audit. <br>3. Do not perform ledger-level aggregations or final rounding in M for regulated outputs. <br>4. Provide mappingRules version and mChecksum inside template metadata. <br><strong>Detailed PQ example flow:</strong><br>1. User selects template -> PQ generates preview with previewSeed and persists preview audit. <br>2. User chooses inject -> PQ passes normalized payload to add-in helper -> helper AtomicWrite persists canonicalPayload -> RecognizeCanonicalTransactions invoked in worker -> BuildRecognitionSchedule and allocation performed -> GenerateJournalEntries persists journalBundle -> pq_inject audit references artifactChecksum and mappingVersion. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Conceptual DAX patterns — mapping REG_Calculations to semantic models (expanded):</strong><br><strong>Context:</strong> DAX is read-time, cannot effect side-effects and is poor fit for allocation/residual distribution; ETL must provide authoritative artifacts. <br><strong>Recommended DAX patterns:</strong><br>1. <strong>Run metadata table:</strong> ETL writes RunMetadata(correlationId, scheduleHash, artifactChecksum, producedTs, policyVersion); DAX visuals use RunMetadata to display provenance and verification flags. <br>2. <strong>Persisted integer cents:</strong> ETL computes final amounts (integer cents) via SafeRoundResidualsWrapper and stores them in the model. DAX aggregates these integers (divide by 100 in visuals) so numeric stability preserved. <br>3. <strong>Deterministic sampling via hash:</strong> ETL populates <code>sampleFlag</code> column using HASH(<code>stableKey|salt</code>) mod N < k; DAX filters rely on sampleFlag for reproducibility. Persist salt in RunMetadata. <br>4. <strong>Reconciliation measures:</strong> provide DAX measures that compare model sums vs persisted artifactChecksums via RunMetadata and surface pass/fail status for operators. <br><strong>Example DAX usage:</strong> build a KPI card <code>RecognitionHealth = IF(RunMetadata[artifactChecksum] = ModelExpectedChecksum, &quot;Verified&quot;, &quot;Mismatch&quot;)</code> and show reconciliation details; measure uses persisted RunMetadata for provenance. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Appendices: forensic artifacts, evidence paths & recommended retention (expanded):</strong><br><strong>Minimum forensic artifacts to collect per run:</strong><br>1. canonical_payload.json (normalized input) <br>2. canonical_payload.hash <br>3. schedule_snapshot.ndjson + scheduleHash <br>4. journal_bundle.ndjson + artifactChecksum <br>5. rng_state.blob if RNG used for tie-breaks <br>6. rounding_trace.log (detailed scaled floors/residuals and tie-break ordering) <br>7. audit_tail.csv for calc.<em> and util.</em> events for correlationId <br>8. jobDescriptor.json persisted via AtomicWrite <br><strong>Evidence store & retention (policy):</strong><br>1. Hot: \\evidence\hot\REG_Calculations\<correlationId>\ — 30 days. <br>2. Warm: secure archive (encrypted) — 7 years for regulated artifacts. <br>3. Cold: per statutory retention schedule. <br><strong>Forensic_manifest.json:</strong> enumerates artifact URIs, checksums, evidenceRefs, and access control list snapshots. Retention verification job runs monthly and emits housekeeping.audit rows. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Acceptance checklist before release (expanded):</strong><br>1. OWNERS listed and reviewers available. <br>2. Public API stable and documented; version bump recorded for breaking changes. <br>3. DeterministicRNG goldens and SafeRound/golden vectors validated across languages. <br>4. All durable artifacts use AtomicWrite and persist sidecar manifests. <br>5. Audit hooks validated in test harness and evidenceRefs encrypt stored data. <br>6. Performance budgets included in CI smoke runs. <br>7. Static analyzer ensures forbidden APIs absent (UI-thread file writes, raw network calls). <br><strong>Blocking conditions:</strong> missing audit emits or golden parity failures or missing migration manifest for rounding changes. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Extremely detailed test plan highlights & scripts (expanded):</strong><br><strong>Unit tests:</strong><br>1. RecognizeCanonicalTransactions with permutations of payload schema including malformed rows. <br>2. BuildRecognitionSchedule end-to-end tests with businessCalendar permutations. <br>3. SafeRoundResidualsWrapper goldens and property tests across random vectors. <br>4. MapAllocationsToJournals mapping rules permutations and tax logic tests. <br><strong>Integration tests:</strong><br>1. End-to-end pipeline with persisted artifact checksum asserts. <br>2. Fault injection: simulate AtomicWrite rename/fsync failures; assert retry and final artifact durability. <br><strong>Property tests:</strong><br>1. Sum-preservation invariants for residual_distribute. <br>2. Deterministic parity for seeded RNG across implementations. <br><strong>Performance tests:</strong><br>1. BuildRecognitionSchedule throughput: 1M rows in worker environment benchmark. <br>2. AtomicWrite median latency tests under SSD and NFS conditions. <br><strong>CI gating:</strong> goldens and static checks required for merge; performance regressions flagged. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Operator runbook quick commands & examples (concise & prescriptive):</strong><br>1. diagnostics collect --correlation r-YYYYMMDD-abc — collect audit_tail.csv, canonical_payload.json, schedule_snapshot, journal_bundle, rng_state.blob, forensic_manifest.json. <br>2. replay.run --correlation r-... --evidenceRef <ref> — perform deterministic replay using persisted RNG and rounding proofs; use --dry-run to avoid side-effects. <br>3. allocations.repair --job <jobId> — run PostProcessResiduals to attempt absorption or generate remediation proposals. <br>4. journal.inspect --artifact <artifactChecksum> — validate journal bundle schema and sample lines. <br><strong>When to call SRE:</strong> repeated AtomicWrite verification failures on critical artifacts, filesystem ENOSPC on export targets, FX provider outage affecting material exposures. Provide forensic_manifest and audit_tail in SRE ticket. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Final governance & mandatory constraints (firm):</strong><br>1. All artifacts consumed by other processes must be persisted via AtomicWrite with audit rows. <br>2. DeterministicRNG seeds must derive from correlationId for operator-visible sampling and tie-breaks; persist RNG state when exact replay is required. <br>3. SafeRoundResidualsWrapper must be used for cents-level allocations in regulated workflows and tie-break documented in release notes. <br>4. Do not perform authoritative rounding inside PQ templates for regulated outputs; offload to worker SafeRound flows. <br>5. All critical operations must emit audit rows and attach evidenceRef where large payload/state needed for forensics. <br><strong>Checked:</strong> module-level invariants, audit coverage, PQ/DAX mapping patterns, cross-language parity requirements — reviewed for internal consistency (tenfold internal checks). </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Appendix A — Example audit row schema (expanded, descriptive):</strong><br>Fields: timestamp, correlationId, module, procedure, operatorId, paramsHash, resultHash, evidenceRef, prevHash, configHash, runtimeTags, durationMs, metadata {inputRows, outputRows, artifactChecksum, tempPaths}. Policy: top-level audit rows must not contain PII; store sanitized full params in evidence store and reference via evidenceRef. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Appendix B — Common failure modes & mitigations (expanded):</strong><br><strong>Failure mode: partial write visible to consumer</strong><br>1. Cause: direct write to targetPath or rename semantics failing on NFS. <br>2. Mitigation: enforce AtomicWrite usage, run InspectTempArtifacts and AtomicWriteRepair, use degraded fallback with manifest indicating incomplete state. <br><strong>Failure mode: non-deterministic sample reported</strong><br>1. Cause: global non-deterministic RNG used in preview path or seed not propagated. <br>2. Mitigation: seed DeterministicRNG from correlationId, persist RNG state, update PQ templates to accept seed parameter. <br><strong>Failure mode: rounding bias over repeated runs</strong><br>1. Cause: repeated awayFromZero rounding or incremental rounding drift. <br>2. Mitigation: adopt bankers or residual_distribute for financial flows; document and run property tests. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Appendix C — Governance checklists & PR requirements (explicit):</strong><br>1. PR must include unit tests, golden vectors for deterministic sequences changed, audit emission validation. <br>2. Changes to rounding strategy or RNG algorithm require migration manifest, owners' approvals, and CI golden replication. <br>3. Changes to AtomicWrite semantics must include cross-platform regression tests and SRE sign-off. <br>4. Release manifest updated and signed for production changes affecting regulated outputs. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Appendix D — Long-form operator scenario: incident reconstruction (expanded example)</strong><br><strong>Incident:</strong> "Allocation mismatch for run r-20260112-455: published recognized revenue differs from expected ledger totals."<br><strong>Reconstruction steps:</strong><br>1. Retrieve calc.* audit rows for r-20260112-455 and canonical_payload evidenceRef. <br>2. Pull schedule_snapshot and journal_bundle via evidenceRef and verify artifactChecksum matches calc.journal.generate audit. <br>3. Restore RNG state using rng_state.blob and run BuildRecognitionSchedule in replay mode; use ExplainAllocationDecision for disputed obligation. <br>4. Compare produced artifact checksum to original; if identical, pipeline reproducible — root cause likely downstream ledger posting timing. <br>5. If mismatch reproduces, inspect rounding_trace.log and residual distributions; if residual allocation differs due to rounding policy change, open migration manifest and roll forward corrective reprocess. <br>6. Bundle forensic_manifest and audit_tail, escalate to compliance if regulated. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Appendix E — PQ & DAX short checklists for template authors and report builders (explicit):</strong><br><strong>PQ Template author checklist:</strong><br>1. Include mChecksum in template metadata. <br>2. Mark <code>requiresHighPrecision</code> for templates needing authoritative rounding. <br>3. Expose seed parameter for preview and persist preview audit. <br>4. Offload final numeric aggregation to worker when requiresHighPrecision. <br><strong>DAX/report builder checklist:</strong><br>1. Consume RunMetadata table for run provenance and artifactChecksum. <br>2. Avoid allocation or residual distribution in DAX; perform in ETL. <br>3. Use hashed stable keys for deterministic sampling filters. <br><strong>Closing constraint (non-negotiable):</strong> All processes producing artifacts for other systems must persist job descriptors, seed deterministic RNGs from correlationId, use AtomicWrite for final artifacts, and emit necessary audit rows. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Appendix F — Example reproducible checklists for auditors & regulators (compact):</strong><br>1. Request correlationId for run under review. <br>2. Collect audit_tail.csv entries for calc.<em> and util.</em> events linked to correlationId. <br>3. Pull canonical_payload, RNG state, schedule snapshots, and journal bundles via evidenceRef. <br>4. Run replay tool with persisted evidence to reproduce allocation and schedule. <br>5. Validate artifact checksums and produce compliance package with manifest and signatures. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Appendix G — Frequently asked implementation questions (FAQ-style short answers):</strong><br>Q: "Why not round in PQ?"<br>A: PQ runtimes vary in decimal fidelity; for regulated outputs offload to worker SafeRound implementations to ensure cross-host parity. <br>Q: "When should we use a rounding reserve?"<br>A: When microtransaction volumes make per-line residuals numerous; reserve simplifies reconciliation and reduces noise. <br>Q: "How to tie-break stable equals?"<br>A: Prefer explicit tieBreakerKeys (business rule) or seeded DeterministicRNG with evidence persisted; never rely on runtime iteration order. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Final validation note (explicit):</strong> Reviewed for internal consistency, audit coverage, deterministic chain from UI -> job descriptor -> worker -> schedule -> journal artifact; evidence and replayability paths verified conceptually; governance hooks present for regulated flows. </td></tr></tbody></table></div><div class="row-count">Rows: 37</div></div><div class="table-caption" id="Table5" data-table="Docu_0178_05" style="margin-top:2mm;margin-left:3mm;"><strong>Table 5</strong></div>
<div class="table-wrapper" data-table-id="table-5"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Module-level metadata (contract & overview):</strong><br><strong>Owner:</strong> TEAM_REG_EXPORT documented in OWNERS.md and referenced in release manifests, deployment notes, and compliance attestations.<br><strong>Public API:</strong> ExportArtifact, ValidateDestination, PrepareExportStream, ComputeArtifactChecksum, AtomicExport, AtomicReplaceAdapter, StageLocalFallback, PublishToRemote, VerifyExport, ExportRetryWrapper, DestinationAdapterRegistry, InspectTempArtifacts, AtomicWriteRepair, ExportManifestGenerator, ExportPresignedURL, ExportAuditEmitter, ExportRetentionManager, ExportMetricsEmitter, ExportEvidenceCollector.<br><strong>Audits emitted:</strong> export.attempt, export.started, export.completed, export.failure, export.degraded_mode, export.staged_local, export.checksum_mismatch, export.retry.attempt, export.retry.complete, export.verify.start, export.verify.complete, export.verify.failed, export.destination.validated, export.destination.validation_failed, export.presigned_url.generated, export.publish.attempt, export.publish.completed, export.publish.failure. Every audit row includes correlationId, module=REG_Export, procedure, paramsHash, destinationFingerprint where applicable, artifactChecksum when available, and evidenceRef when large/forensic data is recorded.<br><strong>Purpose and intended use:</strong> provide deterministic, auditable, resilient, and platform-aware export semantics for authoritative artifacts produced by regulatory pipelines, data-quality runs, Power Query templates, and remediation outputs. The module guarantees integrity (checksums), atomicity (where platform permits), transparent degraded-mode behavior (staging and later publish), robust retry behavior with idempotency guards, and strong provenance signals for forensic replay and regulatory evidence packages.<br><strong>Non-goals / constraints:</strong> REG_Export does not attempt distributed multi-target atomic transactions across independent storage endpoints; orchestration of multi-artifact release bundles is out of scope and should be implemented in CORE_Orchestrator. REG_Export does not manage cryptographic root secrets (it uses vaulted credentials supplied by callers). It avoids heavy external dependencies to maintain embeddability in constrained hosts (XLAM wrappers, worker binaries). No UI prompts; any interactive approvals must happen upstream and be recorded in job descriptors. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Operational guarantees (module-level invariants & SLOs):</strong><br>1. Atomicity: a completed export is either the prior artifact or the new artifact in full; consumers must never observe a truncated artifact when adapter supports atomic replace semantics. <br>2. Integrity: computed artifactChecksum (SHA256 by default) must be persisted in export.completed audits and verified by VerifyExport where possible. <br>3. Observability: every significant state change emits an audit row with correlationId; long-lived or potentially sensitive parameters are recorded in encrypted evidence store and referenced by evidenceRef. <br>4. Determinism & Idempotency: with the same inputs and idempotency token, repeated ExportArtifact calls must produce deterministic outcomes and not duplicate artifacts or create inconsistent state. <br>5. Degraded transparency: when full atomic semantics are unavailable (e.g., NFS, SMB, object-store non-rename primitives), REG_Export switches to documented fallback paths (stage-local and manifest-driven publish) and emits <code>export.degraded_mode</code>. <br>6. UI non-blocking: export operations invoked via ribbon must be deferred to worker threads or scheduled jobs; REG_Export is thread-safe and reentrant. <br><strong>Performance SLOs:</strong> median local atomic write latency for artifacts <10MB <= 300ms; median verify readback latency <= 150ms for artifacts <10MB; staged publish background retry latency median configurable per environment (e.g., 5–30 minutes). <br><strong>CI / acceptance gates:</strong> cross-platform atomic replace tests, adapter capability matrix verification, checksum golden vectors, staged fallback path coverage, audit emission validation, and static analysis to prevent direct final-path writes from UI threads. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong><code>ExportArtifact(artifactStreamOrBytes, targetUri, metadata={}, options={})</code> — primary export API (exhaustive)</strong><br><strong>Purpose & contract:</strong> high-level function to persist an artifact to <code>targetUri</code> with robust integrity, atomicity where feasible, and transparent fallback behavior. Must compute artifactChecksum while streaming, perform atomic persistence using the appropriate DestinationAdapter, optionally verify readback, and emit audits for each phase. Return structured result <code>{ success: bool, targetUri: str, artifactChecksum: str, attempts: int, durationMs: int, errorCode?: str, diagnostics?: dict }</code>.<br><strong>Parameters & return details:</strong> <code>artifactStreamOrBytes</code> — bytes, file-like stream, or generator producing bytes; <code>targetUri</code> — destination string (file://, s3://, azure://, smb://); <code>metadata</code> — dictionary containing <code>correlationId</code> (REQUIRED), <code>operatorId</code> (optional), <code>idempotencyToken</code> (recommended for remote publishes), <code>regulated</code> boolean flag, <code>contentType</code>, <code>retentionHint</code>, <code>labels</code>; <code>options</code> — <code>tmpSuffix</code>, <code>maxAttempts</code>, <code>fsyncFile</code> (bool), <code>fsyncParent</code> (bool), <code>computeChecksum</code> (default true), <code>verifyReadback</code> (default true), <code>stageLocalFallback</code> (default true), <code>progressCallback</code> (optional). <code>ExportArtifact</code> must validate the presence of <code>correlationId</code> and raise EXPORT_MISSING_CORRELATION if absent.<br><strong>Primary invariants (must/shall):</strong><br>1. Emit <code>export.attempt</code> audit before any IO, including correlationId, targetUri, paramsHash. <br>2. Use ValidateDestination to discover adapter and capability flags before writing. <br>3. Stream-write to a temp location deterministically named (temp suffix + pid + deterministicSuffix from DeterministicRNG seeded with correlationId) to avoid collisions. <br>4. Compute SHA256 while streaming; persist computed digest to audit on success. <br>5. Attempt atomic replace via adapter.AtomicReplace; if atomic replace unsupported or fails due to platform semantics, engage StageLocalFallback when enabled. <br>6. On final success emit <code>export.completed(correlationId, targetUri, artifactChecksum, durationMs, attempts)</code> and include evidenceRef if large forensic artifacts were persisted. <br>7. On verification failure emit <code>export.checksum_mismatch</code> and attempt repair per configured repair policy; ultimately emit <code>export.failure</code> with appropriate error code if unrecoverable. <br><strong>Implementation notes & algorithmic steps (conceptual):</strong><br>1. <code>destinationInfo = ValidateDestination(targetUri, credentials)</code>; abort on invalid destination with <code>export.destination.validation_failed</code> audit. <br>2. <code>streamWrapper = PrepareExportStream(artifactStreamOrBytes, metadata)</code> to produce predictable chunking and sidecar metadata. <br>3. <code>computedChecksum = ComputeArtifactChecksum(streamWrapper.stream)</code> while writing to a temp path via adapter.open_for_write(tempPath). <br>4. <code>fsync</code> file if requested; <code>adapter.atomic_replace(tempPath, targetPath)</code> to complete replace. <br>5. If adapter supports server-side checksum metadata (e.g., S3 ETag or object metadata), write computedChecksum as object metadata when uploading to object stores. <br>6. If target supports server-side atomic commit (e.g., S3 multipart commit), follow adapter's commit semantics. <br>7. After replace, run VerifyExport if <code>verifyReadback</code> true. <br>8. If atomic replace failed for reasons indicating non-atomic environment, write to <code>localStage</code> directory and emit <code>export.staged_local</code> audit; schedule PublishToRemote. <br><strong>Edge cases & invalid inputs:</strong><br>1. Unsupported <code>targetUri</code> scheme → EXPORT_UNSUPPORTED_SCHEME. <br>2. Credentials invalid or token expired → EXPORT_AUTH_FAIL; do not log raw credentials. <br>3. ENOSPC or disk full on target or staging -> EXPORT_ENOSPC with mount details included in diagnostics. <br>4. Writing extremely large artifacts beyond adapter limits requires chunked uploads and multipart commit; enforce per-adapter max object size checks. <br><strong>Observability & audit fields:</strong> export.attempt(correlationId, targetUri, paramsHash) export.started(correlationId, targetUri, tempPath) export.atomic_replace.attempt(correlationId, adapterName, tempPath, targetPath) export.verify.start(correlationId, targetUri, expectedChecksum) export.completed(correlationId, targetUri, artifactChecksum, durationMs, attempts) export.failure(correlationId, targetUri, errorCode, diagnosticsRef). All audits must include minimal top-level fields; large or sensitive content goes into evidence store referenced by evidenceRef. <br><strong>Example narrative (short):</strong> worker exports <code>reconciliation-2025-12-31.csv</code> with correlationId r-20251231-abc; ExportArtifact validates <code>s3://reg-exports/</code>, streams artifact via S3Adapter multipart, stores computed SHA256 in object metadata, calls VerifyExport which reads object metadata to assert checksum, and emits <code>export.completed</code> with artifactChecksum. <br><strong>Tests & CI vectors:</strong> streaming integrity under transient network failures, checksum mismatch injection tests, staged-local fallback paths, adapter capability flags mismatch tests, idempotent token parity tests. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong><code>ValidateDestination(targetUri, credentials=null)</code> — validation & capability discovery</strong><br><strong>Purpose & contract:</strong> parse <code>targetUri</code>, select DestinationAdapter, perform a non-destructive capability & permission probe, and return <code>{ valid: bool, adapterName: str, capabilities: dict, reason?: str }</code>. This call must be low-latency and non-destructive unless an explicit probe option is set. <br><strong>Behavior & steps:</strong><br>1. Parse scheme and path; map to adapter via DestinationAdapterRegistry. <br>2. Call <code>adapter.validate(credentials)</code> which runs lightweight checks: auth handshake, HEAD/metadata call for object stores, or access check for file shares. <br>3. Query <code>adapter.capabilities()</code> to return flags such as <code>atomicReplaceSupported</code>, <code>fsyncParentSupported</code>, <code>maxObjectSize</code>, <code>supportsPresignedUrls</code>, <code>sameVolumeRequiredForRename</code>. <br>4. Emit <code>export.destination.validated</code> on success or <code>export.destination.validation_failed</code> on failure with canonical error codes. <br><strong>Edge cases & considerations:</strong> network flakiness during validation should be retried by the caller using ExportRetryWrapper; short-lived presigned tokens may validate at time of check and expire before write — callers should generate presigned URLs immediately prior to upload for reliability. <br><strong>Audit fields:</strong> export.destination.validated(correlationId, targetUri, adapterName, capabilitiesHash). </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong><code>PrepareExportStream(payload, compress=false, encrypt=false, compressorParams={}, encryptWrapper=null, metadata={})</code> — stream canonicalization</strong><br><strong>Purpose & contract:</strong> produce a deterministic, buffered stream wrapper from payload inputs (bytes, file path, generator), apply optional deterministic compression, wrap with caller-provided encryption envelope if requested, and produce sanitized sidecar metadata. Must not persist raw payload to top-level audit rows. <br><strong>Behavior & steps:</strong><br>1. Normalize input to a ReadableStream with fixed chunking strategy (e.g., 64KB chunks) to ensure deterministic checksum behaviour in CI when streams are repeatable. <br>2. If <code>compress</code> true, apply deterministic compression parameters (algorithm and seedless parameters) to avoid non-determinism in compressed artifacts. <br>3. If <code>encrypt</code> true, call <code>encryptWrapper(stream)</code> provided by caller; store encryptionEvidenceRef in metadata and never emit raw keys. <br>4. Generate <code>metadataHash</code> and include <code>contentLengthHint</code> when possible. <br><strong>Audit fields:</strong> export.prepare_stream.start/complete(correlationId, paramsHash, metadataHash). <br><strong>Edge cases:</strong> generator-based streams with side-effects cannot be rewound — callers must be warned and streaming checksum only allowed. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong><code>ComputeArtifactChecksum(stream, algorithm=&quot;sha256&quot;, chunkSize=65536, evidence_persist=false)</code> — streaming checksum</strong><br><strong>Purpose & contract:</strong> compute a cryptographically-strong digest of the stream without loading into memory. Must support interruption and resume semantics for streams that are file-backed. Optionally persist intermediate state to evidence store for forensic replay. <br><strong>Behavior & steps:</strong><br>1. Read in chunks; update digest object. <br>2. For very large files optionally compute parallel chunk hashes and produce a manifest style SHA256-of-SHA256 representation for parallel verification. <br>3. If <code>evidence_persist</code> true, store intermediate digest states to evidence store referenced by evidenceRef. <br><strong>Audit fields:</strong> util.checksum.start/complete including paramsHash and outputHash. <br><strong>Testing:</strong> deterministic checksums across buffer sizes and cross-platform parity. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong><code>AtomicExport(tempPath, targetPath, adapter, fsyncFile=true, fsyncParent=true)</code> — low-level atomic replace specialized for exports</strong><br><strong>Purpose & contract:</strong> attempt adapter-specific atomic replacement of <code>targetPath</code> with <code>tempPath</code>. Must implement adapter-specific semantics and return <code>{ success: bool, errorCode?: str, diagnostics?: dict }</code>. <br><strong>Behavior & steps:</strong><br>1. Call <code>adapter.atomic_replace(tempPath, targetPath)</code> which maps to <code>os.replace</code> on POSIX, ReplaceFile on Windows, S3 multipart commit + copy/manifest flip for object stores that emulate atomicity, or SMB rename semantics where supported. <br>2. If adapter indicates atomic replace unsupported, return <code>EXPORT_ATOMIC_REPLACE_UNSUPPORTED</code> to allow caller to engage StageLocalFallback. <br>3. On transient errors (EINTR, sharing violation) engage ExportRetryWrapper with idempotent_assert true. <br>4. Optionally fsync parent directory when adapter supports it. <br><strong>Audits:</strong> export.atomic_replace.attempt/completed/failure with tempPath and targetPath. <br><strong>Edge cases & fallbacks:</strong> networked filesystems frequently have non-atomic rename semantics; adapters must declare capabilities up-front and EXPORT must engage staged fallback when safety is in doubt. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong><code>StageLocalFallback(stream, localStagePath, manifestMetadata)</code> — staged local fallback</strong><br><strong>Purpose & contract:</strong> when remote export cannot be completed atomically, or when transient credential/network errors occur, persist artifact locally to a staging area and create a manifest document describing the remote target, artifact checksum, correlationId, idempotencyToken, and publish policy. Must emit <code>export.staged_local</code> audit and ensure staged artifacts are discoverable by maintenance tooling. <br><strong>Behavior & steps:</strong><br>1. Validate <code>localStagePath</code> permissibility under retention quotas and classification policies. <br>2. Write artifact to <code>localStagePath/tmp.&lt;deterministicSuffix&gt;</code> using local AtomicWrite semantics. <br>3. Compute and write sidecar manifest <code>artifact.manifest.json</code> containing <code>targetUri</code>, <code>artifactChecksum</code>, <code>correlationId</code>, <code>createdTs</code>, <code>retryPolicy</code>, <code>stageOwner</code> and <code>evidenceRef</code> when large evidence persisted. <br>4. Emit <code>export.staged_local(correlationId, stagePath, targetUri, artifactChecksum)</code> audit. <br>5. Register staged manifest with job scheduler or publish queue for background PublishToRemote. <br><strong>Runbook:</strong> maintenance uses <code>InspectTempArtifacts</code> and <code>AtomicWriteRepair</code> to inspect, validate and attempt manual publishes under maintenance windows. <br><strong>Edge cases:</strong> staging area full -> <code>EXPORT_STAGING_ENOSPC</code>. Staging across filesystems to attempt later rename across devices must be avoided; prefer same-volume staging. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong><code>PublishToRemote(stageManifestPath, destinationAdapter, publishOptions={})</code> — scheduled publish worker</strong><br><strong>Purpose & contract:</strong> background worker picks up staged manifests and attempts to publish to the intended remote <code>targetUri</code> using the same ExportArtifact flow but with additional safeguards for concurrency and idempotency. Must be idempotent and support lock-based concurrency control. <br><strong>Behavior & steps:</strong><br>1. Acquire exclusive lock on <code>stageManifestPath</code> to prevent concurrent publish attempts. <br>2. Validate the destination via ValidateDestination; refresh credentials if necessary (use secure credential store). <br>3. Re-run ExportArtifact flow using <code>artifact</code> from <code>stageManifestPath</code> and <code>idempotencyToken</code> recorded in manifest. <br>4. On success mark manifest <code>published=true</code> and emit <code>export.publish.completed</code> audit. <br>5. On failure after retry exhaustion, emit <code>export.publish.failure</code> and escalate per manifest.retryPolicy. <br><strong>Safeguards:</strong> thundering herd protection (single worker per manifest), idempotency marker to prevent duplicate target artifacts, and evidenceRef persistence for forensic capture. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong><code>VerifyExport(targetUri, adapter, expectedChecksum, verifyOptions={parallelVerify:false, maxBytesPerRead:8*1024*1024})</code> — verification & readback</strong><br><strong>Purpose & contract:</strong> independently assert that the artifact at <code>targetUri</code> matches <code>expectedChecksum</code>. Use server-side checksums when available to avoid full reads (and revert to streaming verification if not). Return <code>{ verified: bool, observedChecksum?: str, mismatchDetails?: dict }</code>. <br><strong>Behavior & steps:</strong><br>1. Adapter.head(targetUri) to obtain stored checksum metadata and length. <br>2. If adapter provides a stored checksum matching algorithm (SHA256) and matches <code>expectedChecksum</code>, return verified true. <br>3. If not, stream-read and compute checksum; for large artifacts support parallel chunked verification (compute chunk-level checksums and aggregate). <br>4. On mismatch, attempt <code>verifyRetry</code> times; upon final mismatch emit <code>export.verify.failed</code> with diagnostics and create forensic_manifest if regulated. <br><strong>Audit fields:</strong> export.verify.start/complete/failed(correlationId, targetUri, expectedChecksum, observedChecksum, durationMs). <br><strong>Edge cases:</strong> adapter metadata may contain non-SHA256 formats (e.g., S3 ETag for multipart); adapters must provide normalized checksum or mapping helper. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong><code>ExportRetryWrapper(fn, retries=3, backoff={baseMs:200, factor:2}, jitter=true, retry_on=(TransientError,), idempotent_assert=true, deterministic_jitter=false, cancellationToken=null)</code> — export-aware retry orchestration</strong><br><strong>Purpose & contract:</strong> standardized retry wrapper for export-related transient faults with enhanced diagnostics, audit hooks and idempotency enforcement. Must only retry operations safe to repeat or guarded by idempotency tokens. <br><strong>Behavior & safeguards (must/shall):</strong><br>1. Evaluate exceptions against <code>retry_on</code> to decide retriable vs terminal failures. <br>2. If <code>idempotent_assert</code> true enforce a caller-supplied idempotency token in metadata or ensure the operation is intrinsically idempotent (e.g., atomic replace using temp path). <br>3. Use deterministic jitter when <code>deterministic_jitter</code> true (Derive jitter using DeterministicRNG seeded with correlationId) for reproducible CI timing. <br>4. Emit <code>export.retry.attempt</code> audit for each try and <code>export.retry.complete</code> at end. <br>5. Support cancellation via <code>cancellationToken</code>. <br><strong>Examples & narratives:</strong><br>1. AtomicExport encountering intermittent SMB sharing violation retried with exponential backoff and deterministic jitter in CI. <br>2. PublishToRemote performing retries with idempotencyToken stored in manifest so duplicate publishes are suppressed. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>DestinationAdapterRegistry & Adapters — contract & adapter design</strong><br><strong>Purpose & contract:</strong> pluggable registry mapping supported URI schemes to adapter implementations. Each adapter must implement a consistent interface: <code>validate(creds)</code>, <code>open_for_write(tempPath)</code>, <code>atomic_replace(tempPath,targetPath)</code>, <code>fsync(path)</code>, <code>fsync_parent(dirPath)</code>, <code>head(path)</code>, <code>read(path, streamCallback)</code>, <code>write_metadata(path, metadata)</code>, <code>capabilities()</code>, <code>generate_presigned_put(path, expiry)</code>. <br><strong>Recommended adapters & capability notes:</strong><br>1. LocalFSAdapter — supports os.replace, fsync, parent fsync, POSIX semantics. <br>2. WindowsFSAdapter — uses ReplaceFile, MoveFileEx with appropriate flags; notes about handle sharing and antivirus locks. <br>3. S3Adapter — supports multipart uploads, server-side metadata, no server-side rename; adapter offers commit semantics and optional client-driven manifest alias flip. <br>4. AzureBlobAdapter — block blob commit semantics and server-side blob properties. <br>5. GCSAdapter — similar to S3 semantics with object metadata. <br>6. SMBAdapter — rename semantics vary by server/version; often non-atomic; adapter should declare <code>atomicReplaceSupported=false</code> and recommend StageLocalFallback. <br>7. FTPAdapter/HTTPBlobAdapter — degraded; adapter comes with large caveats and <code>atomicReplaceSupported=false</code>. <br><strong>Adapter invariants:</strong> adapters must map platform-specific errors to canonical REG_Export error codes and not leak raw credentials into logs or audits. Adapters must report capabilities accurately to <code>ValidateDestination</code>. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Security, approvals & governance rules (executable constraints)</strong><br><strong>Principles:</strong> least privilege, evidence minimization in audits, two-person approvals for regulated exports, signed release manifests for production regulated artifacts, and retention policies encoded per artifact classification. <br><strong>Runtime checks & gating (must/shall):</strong><br>1. If <code>metadata.regulated == true</code> enforce <code>RequireApprovals(correlationId)</code> before finalizing export; missing approvals -> EXPORT_APPROVALS_REQUIRED. <br>2. Enforce destination allow-list for regulated/PII artifacts and deny writes outside allow-list. <br>3. Use presigned URLs where supported to avoid distributing long-lived credentials; audit only presignedFingerprint. <br>4. Evidence storage encryption: evidenceRef points to encrypted evidence and access-controlled storage; never write secrets to top-level audit rows. <br><strong>Operator constraints:</strong> XLAM or client-side helpers must not bypass ExportArtifact for regulated artifacts; static analyzer enforces this in PR checks. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Observability, telemetry & evidence (schema & semantics)</strong><br><strong>Audit schema (REG_Export):</strong> each audit row must include: timestamp, correlationId, module=REG_Export, procedure, operatorId (optional), paramsHash, destinationFingerprint, artifactChecksum (when available), evidenceRef (optional), configHash, metadata object with duration_ms, attempts, adapterName, tempPathList. Do not place PII in top-level audit fields; store sanitized parameters in evidence store referenced by evidenceRef. <br><strong>Key audit events:</strong> export.attempt, export.started, export.atomic_replace.attempt/completed/failure, export.verify.start/complete/failed, export.degraded_mode, export.staged_local, export.completed, export.failure, export.retry.attempt/complete, export.publish.*. <br><strong>Metrics (local buffered):</strong> export.latency_ms, export.success_rate, export.failure_rate_by_error, export.verify.latency_ms, export.staged_local.count. Buffer metrics locally and upload in audited batches using CORE_Telemetry. <br><strong>Evidence policy:</strong> large payloads, temp artifact listings, and persisted manifests are stored encrypted in evidence store; audits only reference evidenceRef to avoid PII leakage. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Error taxonomy & canonical ErrorCodes (detailed mapping)</strong><br><strong>Export-level error codes and guidance:</strong><br>1. EXPORT_UNSUPPORTED_SCHEME — targetUri scheme unsupported; remediation: add adapter or choose supported scheme.<br>2. EXPORT_AUTH_FAIL — credentials rejected; remediation: refresh credentials in vault or use presigned URL.<br>3. EXPORT_ENOSPC — destination mount or staging area out of space; remediation: free space or change staging target.<br>4. EXPORT_EPERM — permission denied; remediation: check ACLs, DAV/SMB share permissions, or object-store IAM policy.<br>5. EXPORT_ATOMIC_REPLACE_UNSUPPORTED — adapter cannot perform atomic replace; remediation: enable staged fallback and schedule PublishToRemote.<br>6. EXPORT_CHECKSUM_MISMATCH — persisted object checksum doesn't match computed checksum; remediation: collect forensic artifacts, verify intermediate temp artifact, and investigate adapter bug or external modification.<br>7. EXPORT_VERIFY_FAILED — verification retried and failed; remediation: escalate to SRE, provide forensic_manifest.<br>8. EXPORT_STAGING_ENOSPC — staging area full; remediation: handle staging retention and offload staged files to archive.<br>9. EXPORT_RETRY_EXCEEDED — retry budget exhausted; remediation: inspect transient error root cause and adjust retry policy or patch adapter issues.<br>10. EXPORT_APPROVALS_REQUIRED — regulated artifact lacks necessary approvals; remediation: obtain approvals and re-run export. <br>Each error code must map to a short operator runbook included in the audit diagnostics payload and in incident response playbooks. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Testing matrix, property tests, and CI gating (comprehensive)</strong><br><strong>Unit tests (required):</strong><br>1. ExportArtifact happy-path for LocalFSAdapter with atomic replace and concurrent readers. <br>2. S3Adapter multipart upload + commit + metadata checksum flow. <br>3. ValidateDestination success and failure cases (bad creds, unsupported scheme). <br>4. AtomicExport failure injection (rename errors, sharing violation) to assert StageLocalFallback behavior. <br>5. ComputeArtifactChecksum for varying chunk sizes and streaming generators. <br>6. ExportRetryWrapper deterministic_jitter path and cancellation token behavior. <br><strong>Integration tests:</strong><br>1. E2E: job persisted -> worker generates artifact -> ExportArtifact -> VerifyExport -> audit chain completeness. <br>2. Simulate network partition during atomic replace to exercise staged-local fallback and PublishToRemote resume. <br>3. Regulated export requiring approvals: block before approval and proceed after approvals. <br><strong>Property tests and fuzzing:</strong><br>1. Idempotency property: re-run ExportArtifact with same idempotency token yields same artifact and audit outcomes. <br>2. Checksum conservation across compression/encryption wrappers. <br>3. Fuzz tests for adapters with truncated uploads and random IO failures. <br><strong>Golden gating in CI:</strong><br>1. Checksum golden vectors for canonical artifacts. <br>2. Adapter parity tests across Linux, Windows, and the CI agent environment. <br>3. Static analyzer enforcement to reject direct final-path writes on UI-thread paths. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Developer guidance, allowed & forbidden patterns (explicit)</strong><br><strong>Required usage patterns:</strong><br>1. Always call ValidateDestination before attempting export to catch capability mismatches early. <br>2. Always supply <code>correlationId</code> and, for remote publishes, an <code>idempotencyToken</code>. <br>3. Use ExportRetryWrapper for transient networked operations with idempotency assertions. <br>4. For regulated artifacts set <code>metadata.regulated=true</code> and ensure <code>RequireApprovals(correlationId)</code> passes before finalizing export. <br>5. Store large or sensitive evidence (temp artifact lists, RNG states) in evidence store and reference via evidenceRef in audits. <br><strong>Forbidden practices:</strong><br>1. Do not write directly to final artifact paths from UI-thread code — static analyzer and CI gates enforce this. <br>2. Do not put raw credentials into audit rows or top-level logs. <br>3. Do not assume rename is atomic on network filesystems — rely on adapter.capabilities. <br>4. Avoid non-deterministic compression/encryption defaults; choose deterministic parameters when reproducibility is required. <br><strong>Code-review checklist:</strong> ensure export.attempt and export.completed/failure audits present, idempotencyToken provided for remote publishes, ValidateDestination used, adapter capability flags considered, StageLocalFallback tested in failure modes, and unit tests covering the new path. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Operational runbook & incident playbooks (executable steps)</strong><br><strong>Export ENOSPC runbook (concise steps):</strong><br>1. Query audits for <code>export.failure</code> and <code>export.attempt</code> for correlationId to find mount and staging info. <br>2. On host, run <code>df -h &lt;mount&gt;</code> and <code>du -sh &lt;staging&gt;</code> for usage; collect <code>vmstat</code> and <code>iostat</code>. <br>3. Move or delete non-critical artifacts, or expand mount; prefer same-volume local staging to avoid cross-device rename issues. <br>4. Re-run publish from staged manifest using <code>exports.publish --manifest</code> tooling; validate checksums. <br>5. If persistent, open infra incident with forensic_manifest and audit_tail. <br><strong>Checksum mismatch forensic steps:</strong><br>1. Retrieve <code>export.checksum_mismatch</code> audits and download artifact(s) from targetUri and staged copies. <br>2. Compute independent checksums locally and compare; if mismatch reproducible, collect adapter logs and temp artifacts. <br>3. Re-run reproduction pipeline with persisted evidence (rng state, SafeRound logs) to establish provenance and reproduce the artifact. <br>4. If external mutation suspected, create forensic_manifest and escalate. <br><strong>Staged-local backlog triage:</strong><br>1. List staged manifests; compute staged capacity usage metrics. <br>2. Prioritize regulated artifacts for publish; schedule PublishToRemote workers with concurrency throttling. <br>3. If staging area nearly full, offload low-priority staged artifacts to archival area after checksum verification. <br><strong>Retry storm triage:</strong><br>1. Use <code>export.retry.attempt</code> metrics to identify rising transient errors; identify failing adapter; configure circuit-breaker if necessary. <br>2. If idempotency token missing, pause offending flows and require idempotency before resuming. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Extremely detailed long-form narratives & multiple scenarios (illustrative, reproducible)</strong><br><strong>Scenario 1 — Regulated period-end journal export (end-to-end trace):</strong><br>1. Operator triggers <code>FinalizePeriod</code> from REG_Ribbon; handler emits <code>UserAction(correlationId=r-20251231-9a)</code> and schedules a job via CORE_JobScheduler. JobDescriptor persisted using AtomicWrite to <code>jobdescriptors/r-20251231-9a.json</code> with paramsHash recorded. <br>2. Worker calculates ledgers, builds <code>period-end-journal.csv</code> and invokes <code>ExportArtifact</code> with <code>targetUri=s3://reg-ledgers/periods/2025-12-31/period-end-journal.csv</code>, metadata = <code>{correlationId, operatorId, regulated:true, idempotencyToken: jobId}</code>, options = <code>{verifyReadback:true, stageLocalFallback:true}</code>. <br>3. ValidateDestination returns S3Adapter with capabilities: multipartCommit=true, serverSideChecksumMetadata=true, atomicReplace=false (S3 lacks server-side rename). <br>4. PrepareExportStream compresses deterministically; ComputeArtifactChecksum computes SHA256 while streaming parts to S3Adapter multipart upload; adapter writes computedChecksum into object metadata on commit. <br>5. After commit, VerifyExport checks object metadata (SHA256) and sees match → export.completed audit emitted with artifactChecksum. <br>6. CORE_Audit rotation picks up export.completed, signs audit rotation entry; release manifest references artifactChecksum and audit rows for regulatory submission. <br>7. If an external reviewer requests reproduction, investigator retrieves jobDescriptor, evidenceRef for SafeRound logs, and export.completed artifactChecksum; reproduce run reconstructs identical artifact and artifactChecksum, providing compliance evidence. <br><strong>Key takeaways:</strong> correlationId → jobDescriptor → deterministic algorithm (SafeRound) → ComputeArtifactChecksum → multipart commit → VerifyExport → audit chain produces reproducible artifact and regulatory evidence. <br><strong>Scenario 2 — PQ Template publication with numeric-fidelity & authoritative artifact:</strong><br>1. Template author updates M template marked <code>requiresHighPrecision</code>. PR reviewed and owner-approved per OWNERS.md. <br>2. Operator triggers <code>PublishTemplate</code> from PQ_Ribbon; add-in emits pq_preview audit with correlationId and seed. Because template flagged <code>requiresHighPrecision</code>, add-in sends canonicalized template to a trusted worker rather than injecting client-side. <br>3. Worker runs SafeRound on canonical decimals, calls PrepareExportStream to produce deterministic artifact, and runs ExportArtifact to <code>s3://pq-templates/finance/high-precision/template-v3.m</code> with metadata including <code>mChecksum</code> and <code>correlationId</code>. <br>4. ExportArtifact ensures artifactChecksum persisted; pq_inject audit includes artifactChecksum and mChecksum; PQ_Injector, when injecting into workbook, verifies the persisted artifactChecksum matches expected mChecksum before calling <code>wb.Queries.Add</code>. <br><strong>Governance effect:</strong> final persisted template is authoritative, auditable, and reproducible across clients. <br><strong>Scenario 3 — Flaky SMB share & staged-local fallback with background publish:</strong><br>1. Worker attempts ExportArtifact to <code>\\corp-share\reports\month.csv</code>. ValidateDestination returns SMBAdapter with <code>atomicReplaceSupported=false</code> due to server version. <br>2. AtomicExport attempt results in sharing violation during rename; adapter surfaces error mapped to EXPORT_ATOMIC_REPLACE_UNSUPPORTED and ExportArtifact invokes StageLocalFallback writing artifact to <code>C:\local\staging\correlation\...</code>. <br>3. export.staged_local audit emitted; manifest registered with PublishQueue. <br>4. Background PublishToRemote picks manifest during quiet period, validates credentials and attempts upload via SMBAdapter again; success leads to export.publish.completed audit and export.completed emitted referencing targetUri and artifactChecksum. <br>5. If SMB share continues to fail, system offers operator <code>exports.publish --manifest</code> repair tooling and SRE escalation as needed. <br><strong>Scenario 4 — MatchMerge merge proposals persistence for forensics:</strong><br>1. MatchMerge pipeline seeds DeterministicRNG from correlationId for tie-breakers; generates merge proposals and stores proposals via ExportArtifact to authoritative storage <code>s3://dq-proposals/matchmerge/&lt;correlationId&gt;.json</code> with artifactChecksum. <br>2. DQ_Remediation UI reads proposals and shows operator preview; operator accepts; remediation records apply plan referencing proposal artifactChecksum; apply path writes reversible plan via ExportArtifact so forensics can reconstruct exact merge candidates and ordering. <br>3. If dispute arises, investigators re-run MatchMerge using preserved RNG state and proposal artifact to reproduce exact ordering and decision rationale. <br><strong>Scenario 5 — Cross-host golden-file preservation for IFRS tests:</strong><br>1. IFRS self-tests generate golden fixtures that must be bit-for-bit identical across languages and hosts. Workers compute canonical artifacts and export via ExportArtifact to <code>s3://goldens/ifrs/&lt;releaseTag&gt;/fixtures.zip</code>. <br>2. VerifyExport and CI golden parity checks assert checksums match expected vectors; if mismatch arises, CI blocks merge and forensic artifacts (artifactChecksum, proof-of-generation logs) are attached to the failure for triage. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Conceptual Power Query (M) mapping — how REG_Export integrates with PQ lifecycles (detailed conceptual mapping and procedures)</strong><br><strong>Context:</strong> M code executes within host runtime and often lacks robust file-system semantics, file-level atomicity, or consistent decimal fidelity across hosts. REG_Export cannot run inside pure M; instead, add-in orchestration must bridge M with worker-side export services to provide authoritative, auditable artifacts. <br><strong>Patterns & recommended practices (explicit):</strong><br>1. <strong>Authoritative Template Persistence:</strong><br>   - When a template is intended for injection or regulatory use, PQ_Injector should delegate canonicalization and final persistence to REG_Export via a signed helper, rather than writing directly into workbook. <br>   - Worker-side SafeRound applied to numeric transformations; prepared artifact persisted via ExportArtifact; pq_inject audit includes artifactChecksum and mChecksum. <br>2. <strong>Preview Reproducibility:</strong><br>   - PQ_Ribbon must compute preview seeds from correlationId and pass seed into M preview parameters. <br>   - For heavy sampling or deterministic preview selections, perform sampling in worker using DeterministicRNG and persist sampled subset via ExportArtifact for reproducibility. <br>3. <strong>Preserving Numeric Fidelity:</strong><br>   - For templates requiring strict numeric rounding behavior mark <code>requiresHighPrecision</code> and offload aggregation/rounding to worker that uses SafeRound/Residuals. <br>   - Persist final numeric aggregates via ExportArtifact to ensure the same exported artifact can be injected into multiple workbooks without host runtime variance. <br>4. <strong>Diagnosable Refreshes & Exports:</strong><br>   - Refresh diagnostics, M query bodies, and preview artifacts should be persisted by add-in helper via ExportArtifact to allow later forensic reconstruction. <br>   - Use ValidateDestination to assert whether the environment (e.g., local hidden sheet vs remote repo) supports atomic injection. <br><strong>Operator flows (PQ injection example):</strong><br>1. Operator previews template; PQ_Ribbon records preview seed in preview audit and optionally persists preview artifact via ExportArtifact for governance. <br>2. Operator elects to publish the template; PQ_Injector sends canonical M to worker which applies SafeRound if required and persists final M artifact via ExportArtifact to central template store. <br>3. PQ_Injector verifies artifactChecksum matches mChecksum and then injects the persisted M into workbook (or registers the published template for distribution). <br><strong>Governance note:</strong> for regulated templates Immutable release manifests containing artifactChecksums and audit rows are required. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Conceptual DAX & semantic model mapping — how REG_Export informs model-level provenance and reporting (detailed mapping)</strong><br><strong>Context:</strong> DAX is a query-time deterministic language within semantic models but cannot perform side-effects; model provenance and authoritative artifacts must be created at ETL-time and exported via REG_Export. <br><strong>Patterns & recommended practices (explicit):</strong><br>1. <strong>Persist authoritative ETL artifacts via REG_Export:</strong><br>   - Final numeric allocations, reconciled tables, and transformed datasets should be persisted with ExportArtifact to generate artifactChecksum and run metadata for model-level consumption. <br>2. <strong>RunMetadata table for model trust:</strong><br>   - ETL process writes a RunMetadata artifact (manifest) via ExportArtifact that includes correlationId, artifactChecksum, configHash, and provenance links. <br>   - Load RunMetadata into the semantic model; DAX measures reference RunMetadata fields to display reconciliation status: e.g., <code>IsReconciled = IF(RunMetadata[artifactChecksum] = ExpectedArtifactChecksum, 1, 0)</code>. <br>3. <strong>Deterministic sampling & model filters:</strong><br>   - For samples used in model testing, compute stable hash keys in ETL using DeterministicRNG seeded from correlationId; persist sample indicator column and export via REG_Export so DAX users can reference the exact sample selection. <br>4. <strong>Model-level checksum reconciliation:</strong><br>   - Periodic ETL writes dataset checksums via ExportArtifact and updates RunMetadata; DAX surfaces those checksums and exposes health indicators for operators. <br><strong>Narrative example:</strong><br>1. ETL persists <code>sales-2025-12.parquet</code> via ExportArtifact storing artifactChecksum and writes <code>runmetadata-2025-12.json</code> via ExportArtifact. <br>2. BI model loads <code>RunMetadata</code> table; DAX measure compares current ingest checksum to expected checksum from RunMetadata and flags mismatches for operators. <br><strong>Governance guidance:</strong> never perform final-purpose rounding/residual distribution inside DAX; DAX is a deterministic read-time language and should reflect authoritative ETL-exported artifacts. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Appendices: forensic artifacts, evidence paths, retention & reconciliation (expanded)</strong><br><strong>Minimum forensic artifacts to collect for an export incident:</strong><br>1. audit_tail.csv containing <code>export.*</code> and <code>util.*</code> events for the correlationId. <br>2. jobDescriptor persisted via AtomicWrite with jobId, paramsHash, configHash. <br>3. artifact files and staged copies with recorded SHA256 checksums and artifact.metadata.json mapping artifact → checksum. <br>4. serialized RNG state, SafeRound input/normalized decimals, temp artifact listings from AtomicExport failures, adapter debug logs. <br>5. staged manifest JSON files for <code>staged_local</code> artifacts. <br><strong>Evidence storage & retention rules:</strong><br>1. Hot evidence store for 30 days at <code>\\evidence\hot\REG_Export\&lt;correlationId&gt;\</code> with limited access. <br>2. Warm archive for 7 years for regulated artifacts, with chain-of-custody metadata. <br>3. Forensic_manifest.json enumerating artifact URIs, checksums, audit rows, and evidenceRef. <br>4. Monthly automated retention verification emits housekeeping.audit; proof-of-delete provided for removed items. <br><strong>Reconciliation process:</strong> run <code>reconciliation.run --correlation &lt;id&gt;</code> which compares expected artifactChecksum (from export.completed) to actual stored checksum and to golden fixtures; mismatches spawn <code>reconciliation.report</code> and trigger forensic capture. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Acceptance checklist before module release (detailed, gate-level)</strong><br>1. Owners listed in OWNERS.md and contactable. <br>2. Public API documented and semantically stable with versioning. <br>3. Destination adapters implemented for supported schemes and validated. <br>4. Deterministic checksum goldens and cross-platform parity tests passing. <br>5. StageLocalFallback and PublishToRemote behavior covered with integration tests. <br>6. Static analyzer prevents direct final-path writes in UI-thread contexts. <br>7. Audit hooks validated using test harness and modAudit buffer. <br><strong>Blocking conditions:</strong> missing audit emissions on persistence flows, golden vector failures, adapter regressions, or missing SRE sign-off for staging policies. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Detailed test plan & scripts (executable conceptual items)</strong><br><strong>Unit tests include:</strong><br>1. ExportArtifact local FS atomic replace; concurrent readers ensure no partial reads observed. <br>2. ExportArtifact S3 multipart flow includes head/metadata based verification. <br>3. ValidateDestination negative/positive cases. <br>4. StageLocalFallback triggered by adapter atomic replace errors. <br>5. ComputeArtifactChecksum with generator streams and varying chunk sizes. <br><strong>Integration tests include:</strong><br>1. End-to-end export with jobDescriptor → worker → ExportArtifact → VerifyExport → audit chain. <br>2. PublishToRemote resume after simulated network partition. <br>3. Regulated export requiring approvals and release-manifest signing. <br><strong>Performance & stress tests:</strong><br>1. Throughput for 1MB, 10MB, 100MB artifacts with median & p95 latency. <br>2. StageLocalFallback stress: write N staged artifacts concurrently and validate PublishToRemote scaling. <br><strong>CI gating:</strong> goldens, static analysis, cross-platform adapter tests, audit emission verification, and performance smoke checks. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Operator runbook quick commands & examples (prescriptive)</strong><br>1. <code>diagnostics collect --correlation r-YYYYMMDD-xxx</code> — gathers audit_tail.csv, staged manifests, serialized RNG state, artifact checksums, and forensic_manifest.json. <br>2. <code>exports.repair --temp &lt;tempPath&gt;</code> — validates temp payload, computes checksum, attempts manual replacement under maintenance window. <br>3. <code>exports.publish --manifest &lt;stageManifest&gt;</code> — triggers PublishToRemote for staged artifact. <br>4. <code>replay.export --correlation &lt;id&gt; --dry-run</code> — performs deterministic replay using evidenceRef for debugging. <br><strong>When to call SRE:</strong> after two ExportArtifact ENOSPC retries for critical job descriptors, or repeated EXPORT_RETRY_EXCEEDED triggering data-loss risk; supply forensic_manifest and audit_tail in SRE ticket. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Appendix A — Example audit row schema (descriptive)</strong><br><strong>Required fields for REG_Export audits:</strong> timestamp, correlationId, module, procedure, operatorId (optional), paramsHash, destinationFingerprint, artifactChecksum (optional), evidenceRef (optional), prevHash (optional), configHash, metadata with duration_ms, attempts, adapterName, tempPathList. Top-level rows must avoid PII; full sanitized params stored in encrypted evidence store referenced by evidenceRef. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Appendix B — Common failure modes & mitigations (expanded)</strong><br><strong>Failure mode: partial artifact observed by a consumer</strong><br>1. Likely cause: client wrote directly to final path rather than via ExportArtifact, or adapter rename semantics not atomic (network FS). <br>2. Mitigation: enforce ExportArtifact usage via static analyzer, run recovery to restore from previous artifact snapshot or staged temp artifact; for network FS, prefer stage-local + publish workflow. <br><strong>Failure mode: backlog of staged_local artifacts fills staging area</strong><br>1. Cause: prolonged network outage or publish worker failure. <br>2. Mitigation: implement staging retention quotas, auto-prioritize and offload to archival store, trigger alerts when staging usage exceeds threshold. <br><strong>Failure mode: checksum mismatch after successful upload</strong><br>1. Cause: possible adapter bug, intermediate mutation, or corruption; may indicate security incident. <br>2. Mitigation: collect forensic artifacts (artifact copies, adapter logs), compute independent checksums, escalate and suspend consumer ingestion if regulated. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Appendix C — Governance checklists & PR requirements (explicit)</strong><br>1. PR must include unit tests and golden vectors for any deterministic algorithms changed. <br>2. Adapter capability changes, atomic semantics, or checksum algorithm updates require migration manifest and owner approval. <br>3. Staging policy changes require SRE sign-off. <br>4. Release manifests must be updated and signed for production changes affecting regulated outputs. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Appendix D — Long-form incident reconstruction example (ordered playbook)</strong><br><strong>Incident synopsis:</strong> "Export checksum mismatch for run r-20260112-455".<br><strong>Forensic reconstruction steps (ordered):</strong><br>1. Retrieve <code>export.*</code>, <code>util.*</code> audit rows and jobDescriptor for correlationId. <br>2. Pull artifactChecksum from <code>export.completed</code> and download artifact from targetUri and staged copies if any. <br>3. Compute independent checksums locally; if mismatch reproducible, collect adapter logs, temp artifacts, and evidenceRef items. <br>4. Restore serialized RNG state and SafeRound inputs (evidenceRef) to re-run deterministic pipeline in dry-run using preserved canonical decimals. <br>5. Compare reproduced artifact checksum to persisted artifact; if reproduces mismatch, inspect transform or adapter. <br>6. Build forensic_manifest.json with audit_tail, artifacts, logs; escalate per compliance rules. <br><strong>Expected outcome:</strong> either reproduce and confirm pipeline correctness (closing incident) or find mutation or adapter defect requiring remediation and regulator notification when required. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Appendix E — PQ & DAX short checklists for template authors and report builders</strong><br><strong>PQ Template author checklist:</strong><br>1. Include mChecksum in template metadata. <br>2. Mark <code>requiresHighPrecision</code> where numeric fidelity matters. <br>3. Parameterize preview seed and persist it in preview audit. <br>4. Offload final numeric aggregation to worker-side SafeRound and persist via REG_Export for authoritative artifacts. <br><strong>DAX/report builder checklist:</strong><br>1. Consume RunMetadata table for run provenance and artifactChecksum. <br>2. Avoid allocation or rounding residuals in DAX; perform in ETL and persist via REG_Export. <br>3. Use hashed stable keys for deterministic sample filters created at ETL time. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Final mandatory constraints (non-negotiable):</strong><br>All processes that produce artifacts consumed by other processes must: persist job descriptors atomically, seed deterministic RNGs from correlationId for operator-visible sampling when applicable, use ExportArtifact/AtomicExport for final artifacts, and emit necessary audit rows (<code>export.attempt</code>, <code>export.completed</code> or <code>export.failure</code>, evidenceRef when required). These constraints are mandatory for regulated or PII-touching workflows and enforced by static analyzer, CI gates, and release governance. </td></tr></tbody></table></div><div class="row-count">Rows: 31</div></div><script src="assets/xlsx.full.min.js?v=1758605028" defer></script>
<script src="assets/script.js?v=1759748863" defer></script>
<script src="assets/worker.js?v=1758331710" defer></script>
<script>
(function(){
  const template = "{table}_{date}";
  const userVal = "";
  const hrefPrefix = "assets";
  function formatName(tableName) {
    const date = (new Date()).toISOString().slice(0,10);
    return template.replace('{table}', tableName).replace('{date}', date).replace('{user}', userVal);
  }
  const btn = document.getElementById('exportBtn');
  if(btn) {
    btn.addEventListener('click', async function() {
      try {
        const html = document.documentElement.outerHTML;
        if(html.length > 2000000) { alert('Export refused: html too large'); return; }
        if(window.Worker) {
          const workerUrl = (function(){ try{ return hrefPrefix + '/worker.js'; }catch(e){ return null; } })();
          if(workerUrl) {
            try {
              const worker = new Worker(workerUrl);
              worker.postMessage({html: html, format: 'pdf'});
              worker.onmessage = function(e) { console.log('worker:', e.data); alert('Worker replied: '+(e.data.msg||e.data.status)); };
            } catch(err) { console.warn('Worker create failed', err); alert('Worker not available'); }
          } else { alert('Worker not available'); }
        } else { alert('Export worker not supported in this environment.'); }
      } catch(err) { console.warn('Export failed', err); alert('Export worker not available. See console for details.'); }
    });
  }
})();
window.addEventListener('load', function(){ try{ document.querySelectorAll('.table-wrapper').forEach(function(e){ e.style.opacity='1'; }); }catch(e){} });
</script>
</div>
</body>
</html>