<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='utf-8'><meta name='viewport' content='width=device-width,initial-scale=1'>
<title>Tables Viewer v2.1</title>
<style>:root{--main-header-height:56px;} .table-wrapper{opacity:0;min-height:24px;transition:opacity .12s linear}</style>
<link rel="stylesheet" href="assets/style.css?v=1759925496">
<link rel="stylesheet" href="assets/overrides.css?v=1768656562">
</head><body>
<div id="tables-viewer" role="region" aria-label="Tables viewer">
<div id="stickyMainHeader">
<div id="tv-header">
<div><h1>Tables Viewer v2.1</h1></div>
<div style="display:flex;gap:8px;align-items:center;">
<input id="searchBox" class="tv-search" type="search" placeholder="Search" aria-label="Search tables" />
<button id="modeBtn" type="button" onclick="toggleMode()" aria-label="Toggle theme">Theme</button>
<button id="toggleAllBtn" type="button" aria-label="Toggle all" onclick="toggleAllTables()">Collapse All Tables</button>
<button id="copyAllPlainBtn" class="copy-btn" type="button" onclick="copyAllTablesPlain()" aria-label="Copy all tables as plain text">Copy All Tables (Plain Text)</button>
<button id="copyAllTablesBtn" class="copy-all-btn" type="button" onclick="copyAllTablesMarkdown()" aria-label="Copy All Tables (Markdown)">Copy All Tables (Markdown)</button>
<button id="copyAllMdBtn" style="display:none" aria-hidden="true"></button>
<button id="resetAllBtn" type="button" onclick="resetAllTables()" aria-label="Reset All Tables">Reset All Tables</button>
</div></div>
<script>
(function(){
  function ensureDelegation(){
    try {
      var vis = document.getElementById('copyAllTablesBtn');
      var alias = document.getElementById('copyAllMdBtn');
      if(!vis || !alias) return;
      alias.addEventListener = function(type, listener, options){
        vis.addEventListener(type, listener, options);
      };
      alias.removeEventListener = function(type, listener, options){
        vis.removeEventListener(type, listener, options);
      };
      Object.defineProperty(alias, 'onclick', {
        set: function(fn){ vis.onclick = fn; },
        get: function(){ return vis.onclick; },
        configurable: true
      });
      alias.focus = function(){ vis.focus(); };
      alias.blur = function(){ vis.blur(); };
    } catch(e) {
      try{ console && console.warn && console.warn('alias delegation failed', e); }catch(_){} 
    }
  }
  if(document.readyState === 'loading'){
    document.addEventListener('DOMContentLoaded', ensureDelegation);
  } else {
    ensureDelegation();
  }
})();
</script>
<noscript><div style='color:#b91c1c'>JavaScript is disabled. Tables will be shown statically. For large tables enable JS for virtualization.</div></noscript>
<div id="tocBar" role="navigation" aria-label="Table of contents"><ul><li class="toc-item"><a class="toc-link" href="#Table1">Table 1</a></li>
<li class="toc-item"><a class="toc-link" href="#Table2">Table 2</a></li>
<li class="toc-item"><a class="toc-link" href="#Table3">Table 3</a></li></ul></div></div>
<div class="table-caption" id="Table1" data-table="Docu_0178_01" style="margin-top:2mm;margin-left:3mm;"><strong>Table 1</strong></div>
<div class="table-wrapper" data-table-id="table-1"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by REG_Utilities — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">REG_Utilities — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Module-level metadata (contract & extended overview):</strong><br><strong>Owner:</strong> TEAM_REG_UTILS (recorded in <code>OWNERS.md</code>, cross-referenced in release manifests and deployment notes).<br><strong>Public API (canonical list):</strong> SafeRound, SafeRoundResiduals, AtomicWrite, Retry, DeterministicRNG, ChecksumStream, TempPathFor, FsSyncDir, WriteWithPermissions, serialize_rng_state, restore_rng_state, InspectTempArtifacts, AtomicWriteRepair, AuditEmitUtilEvent, plus small helpers: PathVolumeOf, EnsureSameVolume, EvidenceFingerprint. <br><strong>Audits emitted (representative):</strong> util.startup, util.atomic_write.attempt, util.atomic_write.completed, util.atomic_write.failure, util.atomic_write.repair, util.atomic_write.degraded, util.retry.attempt, util.retry.complete, util.saferound.start, util.saferound.complete, util.saferound.invalid_input, util.saferound.truncated_precision, util.saferound.residuals, util.rng.seeded, util.rng.state_serialized, util.checksum.start, util.checksum.complete. Every audit row includes <code>correlationId</code>, <code>module=REG_Utilities</code>, <code>procedure</code>, <code>paramsHash</code>, and <code>resultHash</code> when appropriate; long/large payload evidence is referenced via <code>evidenceRef</code> rather than embedded in the audit row itself. <br><strong>Purpose and intended use (expanded):</strong> provide deterministic, auditable, cross-platform primitives that higher-level modules depend on for compliance, replayability, and crash-safety. These primitives are intentionally small-surface, side-effect-minimal, and embeddable in host environments ranging from worker processes to signed XLAM helpers. Fast paths must not perform network I/O, must not expose unencrypted secrets, and must avoid behavior that would prevent safe embedding into UDFs or host add-ins. <br><strong>Non-goals / enforced constraints (expanded):</strong> the utilities do not attempt to manage multi-file distributed transactions, do not assume a single consistent distributed filesystem semantics, and do not hold locks indefinitely across process restarts. Secret management, external credential rotation, and long-term archive lifecycle are delegated to specialized modules (modSecurity, CORE_Telemetry, CORE_Archive). All exceptions to these constraints must be explicitly approved by owners and documented in migration manifests. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Operational guarantees (module-level invariants & SLOs — extended):</strong><br><strong>Invariants:</strong><br>1. Determinism: given identical canonical inputs, seeds, and configuration, outputs including checksums, RNG sequences, and rounded allocations must be reproducible across supported platforms and language bindings. <br>2. Audit-anchored state changes: any mutation of durable state must be accompanied by at least one audit row referencing correlationId and essential provenance fields; where payloads are large, a reference (<code>evidenceRef</code>) to encrypted evidence must be attached. <br>3. Crash-safety: atomic persistence primitives (AtomicWrite family) must provide final-on-success semantics — consumers should never observe a truncated artifact (old artifact or full new artifact only). <br>4. Platform-aware fallbacks: when host FS semantics are weak (cross-device rename, NFS semantics), the module must degrade to documented safe sequences and emit <code>util.atomic_write.degraded</code> with rationale and mitigation guidance. <br>5. UI thread safety: none of the utility primitives may block the UI thread in add-in contexts; heavy operations must be scheduled to worker processes or deferred via host idle callbacks. <br>6. Observability: long-lived or important steps must emit <code>start</code> and <code>complete</code> audits, with duration metrics, and store evidence as required. <br><strong>SLOs (examples & measurement):</strong><br>1. Median AtomicWrite latency on local SSD: <200ms, 95th percentile <1s. <br>2. Retry overhead median per attempt: <50ms (backoff excluded). <br>3. SafeRound vectorized path throughput: process 1M numeric entries in worker environment within CI-defined budget (profiled per language). <br>4. RNG seed serialization/deserialization roundtrip success: 100% in golden CI; parity across language bindings mandatory. <br><strong>CI / acceptance gates (enforced):</strong> cross-language golden vectors for RNG and rounding; cross-platform atomic write tests under simulated faults; determinism tests for Retry jitter when deterministic_jitter is enabled; static analysis forbidding synchronous I/O on UI thread; audit emission verification on all persistence and deterministic transforms. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>SafeRound(value, places=0, strategy="bankers", tieBreakerKeys=null)</strong> — exhaustive per-function technical breakdown (expanded)<br><strong>Purpose & contract:</strong> deterministic rounding primitive for scalar values, arrays, grouped sets, and allocation workflows where residual distribution must be conservative and reproducible. Implementations must be pure and side-effect free, stable across language bindings, and suitable for regulated numeric workflows. When <code>strategy=&quot;residual_distribute&quot;</code> the function guarantees that the sum of rounded parts equals the rounded sum (<code>conservation invariant</code>) at the requested precision. <br><strong>Parameters & return (explicit):</strong><br>1. <code>value</code>: scalar, 1D, or 2D numeric container; accepts integer/float/decimal types coerceable to canonical internal decimal representation. <br>2. <code>places</code>: integer >= 0, number of decimal places to round to. <br>3. <code>strategy</code>: string in <code>{bankers, awayFromZero, floor, ceiling, residual_distribute, custom}</code>. <br>4. <code>tieBreakerKeys</code>: optional array (length aligned with value elements) providing stable deterministic ordering for tie resolution. <br>Return: rounded output with same shape as <code>value</code>. Rounding does not raise on NaN/Inf but propagates them and emits an audit event. <br><strong>Primary invariants (must/shall):</strong><br>1. <strong>Idempotence:</strong> <code>SafeRound(SafeRound(x)) == SafeRound(x)</code> for finite numbers. <br>2. <strong>Tie-break determinism:</strong> when fractional ties occur in half-way cases or residual allocation, <code>tieBreakerKeys</code> followed by original index must deterministically resolve tie. <br>3. <strong>Conservation (residual_distribute):</strong> <code>sum(rounded_parts) == rounded(sum(original_parts))</code> at the integer scale <code>10**places</code>. <br>4. <strong>Locale independence:</strong> numeric results must not depend on host locale (thousands separators, decimal comma). <br><strong>Conceptual algorithm & implementation notes (detail):</strong><br>1. <strong>Canonicalization:</strong> normalize inputs to an arbitrary-precision decimal representation (preferred) or integer scaling if decimal library not available: <code>scaled = exact_decimal(value) * scale</code> where <code>scale = 10**places</code>. <br>2. <strong>Exact-half detection:</strong> when implementing bankers rounding detect exact <code>.5</code> boundaries using decimal arithmetic — do not rely on binary float equality checks (these are non-deterministic across platforms). <br>3. <strong>Vector/group path:</strong> for <code>residual_distribute</code> compute <code>floors = floor(scaled)</code>, <code>residuals = scaled - floors</code>; compute <code>target_total_scaled = round(sum(scaled))</code> (rounded according to <code>strategy</code> when appropriate); compute <code>needed_increments = target_total_scaled - sum(floors)</code>; allocate increments to elements ordered by <code>(residual desc, tieBreakerKeys asc, originalIndex asc)</code>. <br>4. <strong>Custom strategy plug-in:</strong> <code>strategy=&quot;custom&quot;</code> accepts a callback <code>customTieBreaker(residual, index, tieBreakerKeys)</code> user-supplied in higher-level API — must be pure and deterministic; CI flags any custom strategy for golden tests. <br>5. <strong>Precision cap & handling:</strong> implement maximum supported precision (e.g., 18 decimal places) and cap <code>places</code> beyond it, emitting <code>util.saferound.truncated_precision</code> audit with rationale. <br><strong>Edge cases & invalid inputs (explicit mapping):</strong><br>1. <strong>NaN / Inf:</strong> propagate unchanged, emit <code>util.saferound.invalid_input</code> with input hash and reason. <br>2. <strong>Non-numeric types:</strong> attempt coercion; if fails return error <code>UTIL_SAFEROUND_COERCE_FAIL</code> and emit audit with sanitized evidence. <br>3. <strong>Very large arrays / memory pressure:</strong> process in chunks with streaming algorithm; if memory insufficient return <code>UTIL_SAFEROUND_OOM</code> with diagnostics. <br>4. <strong>Mixed numeric types across vector elements:</strong> coerce to canonical decimal with exactness; if heterogeneity causes loss, emit <code>util.saferound.truncated_precision</code>. <br><strong>Observability & audit fields (full):</strong><br>1. <code>util.saferound.start</code> — fields: <code>timestamp</code>, <code>correlationId</code>, <code>places</code>, <code>strategy</code>, <code>inputHash</code>, <code>elementCount</code>, <code>paramsHash</code>. <br>2. <code>util.saferound.complete</code> — fields: <code>timestamp</code>, <code>correlationId</code>, <code>outputHash</code>, <code>durationMs</code>, <code>elementCount</code>. <br>3. <code>util.saferound.invalid_input</code> — fields: <code>timestamp</code>, <code>correlationId</code>, <code>valueHash</code>, <code>reason</code>. <br><strong>Representative narratives & extended examples (multi-paragraph):</strong><br>1. <strong>Payroll rounding scenario (detailed):</strong> split gross payroll <code>100.00</code> among three recipients by equal weight using <code>residual_distribute, places=2</code>. Internal steps: canonicalize to scaled cents <code>[10000/3]</code> scaled fractional parts computed, floors <code>[3333,3333,3333]</code>, residuals <code>[1/3*10000 - 3333 ...]</code> computed precisely; <code>needed_increments = 1</code> assigned deterministically by <code>tieBreakerKeys</code> (employeeId order) producing <code>[33.33,33.33,33.34]</code>. Audit trail includes <code>util.saferound.start</code> with <code>inputHash</code>, <code>util.saferound.residuals</code> with allocation details fingerprint, and <code>util.saferound.complete</code> with <code>outputHash</code> and allocation checksum. This chain enables replay in a compliance investigation where employee IDs, jobDescriptor, and RNG seed are replayable. <br>2. <strong>Financial ledger aggregation (detailed):</strong> for repeated aggregation of many micro-transactions across periods, bankers rounding is chosen to avoid directional bias. Unit tests include sequences of additions and roundings to verify no systematic drift over 10k simulated days. <br><strong>Tests & CI golden vectors (explicit):</strong><br>1. Cases at half-boundaries: <code>1.005</code> to 2 dp, <code>-1.005</code> to 2 dp, exact half values of <code>x.5</code> both positive and negative. <br>2. Residual distribution property tests over random vectors (property-based testing) verifying conservation for 100k randomized cases across languages. <br>3. Cross-language golden vectors for a canonical set of inputs and <code>strategy</code> combinations. <br><strong>Operational remediation & runbook (when rounding disputes occur):</strong><br>1. Retrieve <code>util.saferound.*</code> audit rows for the correlationId. <br>2. Pull canonicalized decimal inputs from evidence store (<code>evidenceRef</code>). <br>3. Re-run <code>SafeRound</code> in reproduce mode with the exact <code>places</code> and <code>strategy</code> and compare <code>outputHash</code>. If mismatch persists, escalate to module owners with code, evidenceRef, and golden vector comparison. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>SafeRoundResiduals(values[], total=null, places=0, tieBreakerKeys=null)</strong> — allocation primitive (expanded)<br><strong>Purpose & contract:</strong> deterministic allocator for translating fractional shares into integer allocations whose sum equals a target total. Used for cents allocation, invoice splitting, distributing indivisible units, chunk sizing for job proposals. Pure function; emits audit row on invocation and returns allocation and internal allocation fingerprint. <br><strong>Detailed behavior & algorithm (stepwise):</strong><br>1. If <code>total == null</code> compute <code>target_total = SafeRound(sum(values), places, strategy=&quot;bankers&quot;)</code>. <br>2. Compute <code>scale = 10**places</code> and <code>scaled_values = exact_decimal(values) * scale</code>. <br>3. For each element compute <code>floor_i = floor(scaled_values_i)</code> and <code>residual_i = scaled_values_i - floor_i</code>. <br>4. <code>remaining = target_total_scaled - sum(floor_i)</code>. If <code>remaining &lt; 0</code> error <code>UTIL_ALLOCATION_NEGATIVE_REMAINING</code>. If <code>remaining &gt; len(values)</code> handle per policy (distribute multiple units or raise <code>UTIL_ALLOCATION_OVERFLOW</code>). <br>5. Sort indices by tuple <code>(residual desc, tieBreakerKeys asc if provided, originalIndex asc)</code>. <br>6. Add <code>+1</code> to the top <code>remaining</code> indices. <br>7. Return <code>allocated = (floor_i + assigned_increment_i) / scale</code> with allocation checksum and <code>allocationFingerprint</code> for audit. <br><strong>Primary invariants:</strong><br>1. Sum-preservation: <code>sum(allocated)</code> equals target total at the requested precision. <br>2. Determinism: tie-break stable across runs. <br><strong>Edge cases & policy decisions:</strong><br>1. Negative weights: allow only if business rule allows negative allocations; otherwise <code>UTIL_ALLOCATION_NEGATIVE_WEIGHT</code>. <br>2. Zero-sum with rounding: all residuals zero but <code>remaining &gt; 0</code> indicates inconsistent <code>total</code> — signal <code>UTIL_ALLOCATION_TARGET_MISMATCH</code>. <br><strong>Observability & audit:</strong> <code>util.saferound.residuals</code> capturing <code>correlationId</code>, <code>valuesHash</code>, <code>places</code>, <code>target_total</code>, <code>tieBreakerFingerprint</code>, <code>allocationFingerprint</code>. Evidence store holds canonical scaled inputs for forensic replay. <br><strong>Examples & extended narrative:</strong><br>1. <strong>Invoice splitting among N customers by weight:</strong> values <code>[12.345, 45.0, 65.1]</code>, <code>places=2</code>. Implementation scales to cents, floors computed, residuals evaluated, and <code>remaining</code> cents assigned deterministically by <code>tieBreakerKeys</code> (e.g., earliest account creation timestamp). Resulting cents allocation audited and persisted; operator-visible preview shows "before/after" examples in remediation UI. <br>2. <strong>Chunk sizing in MatchMerge:</strong> allocate integer job chunk sizes across workers so that sum equals total rows and no worker is starved or overallocated. <code>tieBreakerKeys</code> may encode machine capacity for fairness. <br><strong>Tests & CI:</strong> property tests for conservation across random weight vectors, tie-break stability tests, concurrency tests validating that tieBreakerKeys deterministically influence results. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>AtomicWrite(targetPath, payloadStream, tmpSuffix=".part", fsyncFile=true, fsyncParent=true, perms=null, maxAttempts=3, cleanupOnFailure=false)</strong> — persistence primitive (expanded)<br><strong>Purpose & contract:</strong> robust, crash-safe atomic file replacement primitive ensuring that at no point does a consumer observe a truncated file. Provide deterministic result object on success and deterministic error codes on failure. Relevant to job descriptors, exported datasets, reconciliation artifacts, and signed manifests. <br><strong>Complete step-by-step semantics (detailed):</strong><br>1. <strong>Parent directory validation:</strong> verify parent directory exists; if missing and creation allowed create directories using <code>FsSyncDir</code> which ensures fsync of parent metadata. Emit <code>util.atomic_write.mkdir</code> if created. <br>2. <strong>Temp path computation:</strong> compute <code>tempPath = TempPathFor(targetPath, tmpSuffix, pid=currentPid, deterministicSuffix=DeterministicSuffix())</code>. DeterministicSuffix derived from <code>DeterministicRNG(seed=correlationId, salt=&quot;atomic-temp&quot;)</code> when deterministic temp naming is required to coordinate multi-process writes in golden tests. <br>3. <strong>Exclusive open & write:</strong> open tempPath with create-only and exclusive flags; write payload stream in chunks using <code>ChecksumStream</code> to compute SHA256 on the fly; optionally write <code>artifact.metadata.json</code> sidecar containing <code>payloadHash</code>, <code>correlationId</code>, <code>timestamp</code>, <code>paramsHash</code>. <br>4. <strong>Fsync file descriptor:</strong> if <code>fsyncFile</code> true call OS-specific flush (<code>fsync</code>/<code>FlushFileBuffers</code>). If call fails due to unsupported FS semantics emit <code>util.atomic_write.degraded</code>. <br>5. <strong>Atomic rename:</strong> attempt atomic rename/replace using OS primitives: <code>os.replace</code>/<code>rename</code> on POSIX, <code>ReplaceFile</code> or <code>MoveFileEx</code> on Windows with flags that allow replacing a read-only file if policy permits; on failure due to open handles return a deterministic error <code>UTIL_ATOMIC_WRITE_RENAME_LOCKED</code>. <br>6. <strong>Fsync parent directory:</strong> if <code>fsyncParent</code> true, open parent dir and fsync to persist directory entry; on Windows use alternate semantics. If not supported on FS, emit <code>util.atomic_write.degraded</code> audit. <br>7. <strong>Verification read & checksum compare:</strong> reopen <code>targetPath</code> in read-only mode and compute checksum; compare with computed <code>artifactChecksum</code>; if mismatch emit <code>util.atomic_write.verification_failed</code> and attempt an immediate repair or retry per <code>Retry</code> policy. <br>8. <strong>Cleanup & result:</strong> remove temp artifacts if <code>cleanupOnFailure</code> true on failures; on success return structured result <code>{success:true, artifactPath:targetPath, artifactChecksum, attempts, elapsedMs}</code> and emit <code>util.atomic_write.completed</code>. <br><strong>Cross-platform fallbacks & NFS/SMB specifics (documented):</strong><br>1. <strong>Network file systems:</strong> many network filesystems do not support atomic rename semantics between clients reliably or do not persist directory fsync in expected order. For these cases the module must:<br>   - write artifact to <code>targetPath.tmp</code> and a <code>targetPath.meta</code> sidecar that includes checksum and state <code>&quot;complete&quot;:true</code> only after write complete and flush;<br>   - optionally coordinate with small lease/lock files stored on the same share to indicate availability;<br>   - emit <code>util.atomic_write.degraded</code> with <code>rationale=&quot;network-fs-no-atomic-rename&quot;</code>. <br>2. <strong>Windows handle contention:</strong> recommend best-practice orchestration: write to same volume, ensure readers open with shared-read semantics, perform write during low-load windows, and fallback to <code>stage-local</code> on repeated failures. <br><strong>Error classes & runbook mapping (operational):</strong><br>1. <code>UTIL_ATOMIC_WRITE_ENOSPC</code> — emit <code>util.atomic_write.ENOSPC</code> audit, include freeBytes and mount path; runbook: attempt local staging, rotate older artifacts, escalate to infra if persistent. <br>2. <code>UTIL_ATOMIC_WRITE_EPERM</code> — emit <code>util.atomic_write.EPERM</code> with ACL snapshot; runbook: validate ACLs, correct ownership, re-run with operator. <br>3. <code>UTIL_ATOMIC_WRITE_VERIFICATION_FAILED</code> — attempt immediate retry; if repeated, capture temp artifacts via <code>InspectTempArtifacts</code> and run <code>AtomicWriteRepair</code> under maintenance window. <br><strong>Auditing & evidence linkage (explicit):</strong><br>1. <code>util.atomic_write.attempt</code> — include <code>targetPath</code>, <code>tempPath</code>, <code>payloadHash</code>, <code>paramsHash</code>. <br>2. <code>util.atomic_write.completed</code> — include <code>artifactChecksum</code>, <code>durationMs</code>, <code>attempts</code>, <code>evidenceRef</code> for artifact metadata. <br>3. <code>util.atomic_write.degraded</code> — include <code>rationale</code> and recommended mitigation. <br><strong>Representative scenario & extended narrative (atomic export with staged fallback):</strong><br>1. Worker attempts AtomicWrite to network share; <code>fsyncParent</code> not supported on mount -> AtomicWrite detects degraded FS semantics and writes using two-phase protocol: <code>artifact.tmp</code> + <code>artifact.meta</code>. Emit <code>util.atomic_write.degraded</code> with <code>rationale=&quot;nfs-no-fsync&quot;</code>. Orchestrator chooses to stage artifact locally and schedule background upload while emitting <code>export.attempt</code> audit. <br><strong>Tests & CI (robust coverage):</strong><br>1. FS-mock tests simulating <code>rename</code> failure, <code>fsync</code> failures, permission changes mid-write. <br>2. Concurrency tests with 100 readers hitting target path during writer process ensuring no partial content observed. <br>3. Cross-OS integration ensuring ReplaceFile semantics consistent. <br><strong>Operational mitigations & escalations:</strong><br>1. If repeated <code>ENOSPC</code> or <code>EPERM</code> persisted after automated mitigations, create SRE ticket with forensic_manifest and audit_tail. <br>2. If <code>verification_failed</code> observed for an exported artifact consumed downstream, automatically quarantine downstream ingestion until artifact integrity resolved. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Retry(fn, retries=3, backoff={baseMs:100, factor:2}, jitter=true, retry_on=(TransientError,), idempotent_assert=false, cancellation_token=null, audit_on_attempt=true, deterministic_jitter=false)</strong> — orchestration & governance (expanded)<br><strong>Purpose & contract (concise):</strong> canonical transient-fault retry wrapper for local IO and short-lived remote calls invoked by worker processes and background orchestrators. Must be auditable, enforce idempotency expectations, and provide deterministic jitter for golden CI runs when requested. <br><strong>Behavioral rules & enforcement (detailed):</strong><br>1. <strong>Retry predicate:</strong> only retry on exceptions that satisfy <code>retry_on</code> predicate(s); non-retryable exceptions must be surfaced immediately. <br>2. <strong>Idempotency enforcement:</strong> if <code>idempotent_assert</code> omitted and static analysis detects non-idempotent side effects (e.g., writes, external state mutation) the CI system must fail the merge and require either <code>idempotent_assert=true</code> with documented idempotency token or a refactor to make the operation idempotent. <br>3. <strong>Deterministic jitter for testing:</strong> when <code>deterministic_jitter=true</code> the jitter calculation is derived from <code>DeterministicRNG(seed=correlationId|retry-salt)</code> so sleep durations are reproducible for CI golden runs. <br>4. <strong>Cancellation handling:</strong> if <code>cancellation_token</code> is set and flagged between attempts, raise <code>UTIL_RETRY_CANCELLED</code> and emit <code>util.retry.complete</code> with <code>outcome=&quot;cancelled&quot;</code>. <br><strong>Backoff & jitter semantics (precise):</strong><br>1. Base backoff: <code>backoff_ms = baseMs * factor**(attemptIndex-1)</code>. <br>2. Jitter algorithm: default is "decorrelated jitter" (per AWS jitter guidance) computed as <code>min(maxBackoff, rand(0, backoff_ms * 3))</code>; when <code>deterministic_jitter=true</code> replace <code>rand</code> with <code>DeterministicRNG.random()</code>. <br>3. Maximum total retry budget is configurable per-call (<code>max_total_ms</code>) to prevent runaway waits. <br><strong>Auditing & metrics:</strong><br>1. Emit <code>util.retry.attempt</code> for each attempt with <code>attemptIndex</code>, <code>errorCode</code>, and <code>backoffMs</code>. <br>2. Emit <code>util.retry.complete</code> on success or exhaustion including <code>attempts</code> and <code>elapsedMs</code>. <br>3. Metrics buffered locally: <code>util.retry.attempt_count</code>, <code>util.retry.success_rate</code>, with tags for <code>target</code> and <code>module</code>. <br><strong>Representative usage scenarios & narratives:</strong><br>1. <strong>Job descriptor persistence under transient NFS blips:</strong> <code>Retry(lambda: AtomicWrite(jobDescriptorPath,...), retries=5, idempotent_assert=true, deterministic_jitter=true)</code> ensures eventual persistence while deterministic_jitter allows CI and replay to mimic production timing in stress tests. <br>2. <strong>Provider API transient timeouts:</strong> wrap short provider calls in <code>Retry</code> with <code>idempotent_assert=true</code> and application-layer idempotency tokens embedded in request payloads. <br><strong>Failure escalation & SRE triggers:</strong><br>1. Exhaustion emits <code>util.retry.complete</code> with <code>outcome=&quot;exhausted&quot;</code> and triggers SRE alerting if call is critical (jobDescriptor persistence, artifact export). The alert payload includes tempPaths, attempt errors, and partial artifacts evidence. <br><strong>Tests & CI (explicit):</strong> deterministic_jitter golden tests to ensure backoff/jitter sequences identical given correlation seeds; cancellation tests simulate cancellation_token signalled after first attempt; static-analysis gating for idempotency enforcement. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>DeterministicRNG(seed_source, salt="", algorithm="pcg64", stream_id=null, test_mode=false)</strong> — design, usage & governance (expanded)<br><strong>Purpose & contract (concise):</strong> deterministic, seedable PRNG for sampling, shuffles, tie-breakers, and ephemeral randomized operations that must be reproducible. Not for cryptographic uses. RNG instances are explicit and serializable. <br><strong>Seed derivation & confidentiality (detailed):</strong><br>1. Production seed derivation: <code>seed_raw = HMAC_SHA256(key=systemKey, message=(seed_source || &#x27;|&#x27; || salt || &#x27;|&#x27; || moduleName || &#x27;|&#x27; || streamId))</code>. The <code>seed_raw</code> is then folded/truncated into the algorithm's seed size using deterministic mixing. <br>2. <strong>Confidentiality:</strong> raw seed not emitted in plaintext in audit rows. Instead store <code>seedFingerprint = SHA256(seed_raw)</code> and record <code>util.rng.seeded</code> with <code>seedFingerprint</code>. For replay, <code>serialize_rng_state</code> stores encrypted internal state blob into evidence store and audit references it via <code>evidenceRef</code>. <br>3. <strong>test_mode:</strong> allows local seed overrides for dev/golden runs but CI must verify no test_mode artifacts leak to production. <br><strong>API behavior & primitives:</strong><br>1. <code>randint(a,b)</code>, <code>random()</code> in [0,1), <code>shuffle(seq)</code> returning a new sequence, <code>sample(pop,k)</code>, <code>choice(seq)</code>, <code>splitStream(childIndex)</code> producing independent child RNG seeded via <code>HMAC(parentSeed || childIndex)</code> to avoid correlation. <br>2. <code>serialize_state()</code> returns encrypted base64 blob; <code>restore_state(blob)</code> returns RNG instance at saved position. <br><strong>Stability invariants:</strong><br>1. Same <code>algorithm</code>, <code>seed_source</code>, <code>salt</code>, and <code>stream_id</code> must produce identical output across Python/JS/C#/VBA bindings. <br>2. Splitting streams produce independent sequences (statistical independence test required in CI). <br><strong>Observability & audit:</strong><br>1. <code>util.rng.seeded</code> — <code>correlationId</code>, <code>seedFingerprint</code>, <code>algorithm</code>, <code>streamId</code>, <code>evidenceRef</code> optional. <br>2. <code>util.rng.state_serialized</code> — <code>stateHash</code>, <code>evidenceRef</code> for the encrypted state blob. <br><strong>Representative narratives & usage examples (detailed):</strong><br>1. <strong>DQ_Profile deterministic sampling:</strong> worker uses <code>DeterministicRNG(seed_source=correlationId, salt=&#x27;dq-profile-v1&#x27;)</code> to select deterministic sample rows for profile reports; <code>serialize_state()</code> attached to profile artifact so compliance can reconstruct sample for audits. <br>2. <strong>MatchMerge tie-breaker:</strong> seed from <code>correlationId || &#x27;matchmerge-v1&#x27;</code>, call <code>shuffle(candidates)</code> producing deterministic reordering, persist RNG state in proposal artifact to allow later replays and operator disputes to be resolved with exact ordering. <br><strong>Tests & CI (explicit):</strong> cross-language parity vectors (first N outputs), splitStream independence tests, serialize/restore roundtrip, statistical tests for uniformity across large N. <br><strong>Operational playbook:</strong><br>1. When non-deterministic sampling is reported, retrieve <code>util.rng.seeded</code> and <code>util.rng.state_serialized</code> evidence. <br>2. Re-run sampling with restored RNG state to reproduce selection; if mismatch found, escalate to owners for parity investigation. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>ChecksumStream(payloadStream, algorithm="sha256", chunkSize=65536)</strong> — streaming checksum (expanded)<br><strong>Purpose & contract:</strong> stream-pass-through wrapper that computes a cryptographic checksum (default SHA256) while forwarding bytes to consumer; suitable for integrating into AtomicWrite to compute checksums without buffering whole artifact. Must be deterministic in chunking semantics and avoid implicit character encoding transforms. <br><strong>Behavior & notes:</strong><br>1. Read bytes from <code>payloadStream</code> in <code>chunkSize</code> increments; update hash state per chunk; forward to writer socket/file handle. <br>2. On EOF finalize digest and provide <code>finalChecksum</code> (hex/base64 configurable). <br>3. On partial stream failure emit <code>util.checksum.partial</code> with <code>partialHash</code> and <code>bytesProcessed</code>. <br><strong>Edge cases & mapping:</strong> unsupported algorithms return <code>UTIL_CHECKSUM_UNSUPPORTED</code>; stream interruption errors map to <code>UTIL_CHECKSUM_PARTIAL</code>. <br><strong>Observability & audit:</strong> <code>util.checksum.start</code> with <code>correlationId</code>, <code>algorithm</code>, <code>chunkSize</code>; <code>util.checksum.complete</code> with <code>finalChecksum</code>, <code>bytesProcessed</code>, <code>durationMs</code>. <br><strong>Tests & CI:</strong> parity checks over multiple chunk sizes; ensure checksum identical for same byte stream across environments. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>TempPathFor(targetPath, tmpSuffix=".part", pid=null, deterministicSuffix=null)</strong> — temp path helper (expanded)<br><strong>Purpose & contract:</strong> deterministic temporary path generator used by AtomicWrite to ensure low collision probability and forensic traceability. Must produce temp path on <strong>same volume</strong> as <code>targetPath</code> to preserve atomic rename semantics. <br><strong>Rules & algorithm:</strong><br>1. Compute <code>base = dirname(targetPath)</code> and <code>basename = safe_filename(basename(targetPath))</code>. <br>2. <code>pid</code> default to runtime pid; <code>deterministicSuffix</code> may be provided using <code>DeterministicRNG(seed=correlationId)</code> for test reproducibility or left null for production randomness. <br>3. Return <code>tempPath = join(base, f&quot;.{basename}{tmpSuffix}.{pid}.{detSuffix}&quot;)</code> sanitized for OS constraints. <br><strong>Edge cases:</strong> on Windows UNC paths ensure path length limits respected and sanitize <code>:</code> and <code>\</code> characters. <br><strong>Observability:</strong> no heavy audit; AtomicWrite emits <code>util.atomic_write.attempt</code> including <code>tempPath</code>. <br><strong>Tests:</strong> path equality and collision stress tests; ensure same-volume assertion verified by <code>PathVolumeOf</code>. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>FsSyncDir(dirPath, ensureExists=true, perms=null)</strong> — directory creation & fsync (expanded)<br><strong>Purpose & contract:</strong> ensure directory exists (optionally creating it) and persist directory metadata (fsync) to assure directory entries are durable after file rename operations. Required by AtomicWrite before rename when parent directories might be newly created. <br><strong>Behavior & OS specifics:</strong><br>1. Create directory with <code>mkdir -p</code> semantics with configured permissions; set ownership/ACLs only when caller requests and has permissions. <br>2. Open directory handle and <code>fsync</code> directory descriptor (<code>fdatasync</code>/<code>fsync</code>) on POSIX; on Windows use <code>FlushFileBuffers</code> on directory handle or fallback to opening and flushing a dummy file if direct flush unsupported. <br>3. If underlying FS does not support directory fsync (some SMB/NFS variants) return <code>degraded=true</code> and emit <code>util.atomic_write.degraded</code> with <code>rationale</code>. <br><strong>Observability & audit:</strong> <code>util.fs.syncdir</code> event with <code>durationMs</code> and <code>created</code> flag. <br><strong>Tests:</strong> cross-platform create-and-sync tests, ensure idempotent behavior when concurrent creators attempt same directory. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>WriteWithPermissions(path, payloadStream, perms, owner=null, group=null)</strong> — write + perms helper (expanded)<br><strong>Purpose & contract:</strong> write payload to temporary path then set POSIX permission bits or Windows ACLs before rename. Must be used as part of AtomicWrite sequence to ensure final artifact has correct access control settings. <br><strong>Behavior & notes:</strong><br>1. Write to temp file (temp computed by TempPathFor), upon write completion apply <code>chmod</code> and <code>chown</code> semantics for POSIX; on Windows apply ACL modifications via appropriate APIs. <br>2. If setting permissions fails after write, the function should either rollback (remove temp) or surface an error <code>UTIL_WRITE_EPERM</code> depending on <code>force</code> parameter; audit includes permission fingerprint rather than full ACL content. <br><strong>Edge cases:</strong> mapping POSIX UIDs/GIDs on containerized environments lacking those IDs should be gracefully handled with audit <code>util.write.perms.warning</code>. <br><strong>Tests & CI:</strong> permission change tests in containerized CI with places of permissions mismatch; verify final artifact has expected effective ACLs. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>serialize_rng_state(rng_instance, evidenceEncrypt=true)</strong> — RNG state serialization (expanded)<br><strong>Purpose & contract:</strong> produce a portable, versioned, encrypted blob representing internal PRNG state for later deterministic replay. Return blob and <code>stateHash</code>; do not leak raw seed material. The blob is intended for secure evidence storage and must be decryptable only by authorized evidence tooling. <br><strong>Serialization details & format:</strong><br>1. Pack: <code>{version, algorithm, internal_state_words, position_counter, metadata:{createdBy, correlationId, module}}</code> into a deterministic binary layout. <br>2. Encrypt with evidence store key and HMAC the resulting ciphertext with evidence HMAC key. <br>3. Base64 encode and return <code>{blobBase64, stateHash=SHA256(ciphertext)}</code>. <br><strong>Observability & audit:</strong> emit <code>util.rng.state_serialized</code> with <code>stateHash</code> and <code>evidenceRef</code>. <br><strong>Tests & CI:</strong> serialize/restore round-trip tests; cross-language blob unmarshal parity checks; rejection for mismatched <code>version</code> during restore. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>restore_rng_state(blobBase64)</strong> — RNG state restoration (expanded)<br><strong>Purpose & contract:</strong> decrypt and reconstruct RNG instance at exact position. Return RNG instance or raise <code>UTIL_RNG_BAD_STATE</code> if decryption fails, integrity check fails, or version incompatible. <br><strong>Behavior & tests:</strong> roundtrip restore must reproduce sequence position exactly; CI runs parity tests across languages and versions; incompatible <code>version</code> requires migration path and fails CI unless accompanied by migration manifest. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>InspectTempArtifacts(baseDir, pattern="<em>.part</em>")</strong> — temp artifact inspection (expanded)<br><strong>Purpose & contract:</strong> safely enumerate aborted/leftover temp artifacts created by AtomicWrite flows; compute checksums where readable and propose candidate repairs. Intended for operator triage and automated repair procedures. <br><strong>Behavior & outputs:</strong> returns list of <code>{tempPath, candidateTargetPath(inferred), size, mtime, readableChecksum?, permissionFlags}</code> and a <code>discoveryFingerprint</code> for audit. Limit scans to <code>maxEntries</code> per invocation to avoid DoS. <br><strong>Edge cases:</strong> permission denied entries recorded with <code>permissionDenied=true</code>; symlinks exploded carefully and not automatically followed beyond one-level to avoid escape. <br><strong>Observability:</strong> emit <code>util.inspect.temp</code> with counts and <code>discoveryFingerprint</code>. <br><strong>Tests:</strong> directory with mixed temp names, multiple matching conventions, and simulated partial writes. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>AtomicWriteRepair(tempPath, expectedChecksum=null, force=false)</strong> — safe repair helper (expanded)<br><strong>Purpose & contract:</strong> attempt validated repair of a temp artifact by verifying checksum and performing a safe rename into the intended target while preserving a backup of the previous target if present. Only run under maintenance or with operator consent. <br><strong>Behavior & required safeguards:</strong><br>1. Validate <code>tempPath</code> read and compute checksum; if <code>expectedChecksum</code> provided, compare and refuse unless <code>force=true</code>. <br>2. Determine <code>targetPath</code> via temp naming convention; backup existing <code>targetPath</code> to <code>targetPath.backup.&lt;ts&gt;</code> before rename to ensure rollback. <br>3. Perform rename using atomic replace primitives where possible; upon success emit <code>util.atomic_write.repair</code> with <code>actionTaken=rename</code> and evidenceRef to temp artifact. <br>4. If repair fails after partial work, attempt to restore backup and emit <code>util.atomic_write.repair.failed</code> with diagnostics. <br><strong>Operational policy:</strong> repairs must be logged to evidence store and require operator approval for <code>force</code> repairs in regulated flows. <br><strong>Tests & CI:</strong> simulate valid temp, mismatched checksum, and rename failure cases to ensure safe rollback. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>AuditEmitUtilEvent(correlationId, procedure, paramsHash, resultHash=null, evidenceRef=null, metadata=null)</strong> — canonical audit emitter (expanded)<br><strong>Purpose & contract:</strong> canonical utility-level audit append helper that validates schema, sanitizes metadata (ensuring no PII in top-level fields), computes <code>rowHash</code>, and appends to local audit buffer (<code>audit_tail.csv</code> or local encrypted buffer). Ensures consistent audit envelope across all util.* events. <br><strong>Behavior & constraints:</strong><br>1. Validate <code>paramsHash</code> presence for state-changing operations. <br>2. If <code>metadata</code> contains PII fields, sanitize or offload to evidence store and add <code>evidenceRef</code>. <br>3. Compute <code>rowHash = HMAC_SHA256(auditRow, auditSigningKey)</code> and append to audit buffer with <code>prevHash</code> chaining where available. <br>4. If audit buffer write fails, write to local encrypted fallback store and emit <code>util.audit.emit.degraded</code>. <br><strong>Observability:</strong> emits meta-audit <code>util.audit.emit</code> with <code>status=buffered|flushed|degraded</code> and buffer metrics. <br><strong>Tests & CI:</strong> ensure schema validation rejects malformed fields; ensure chained row hashes produce correct <code>prevHash</code> chain and sign/verify tests pass. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Cross-cutting Observability, Telemetry & Error Catalog (detailed mapping)</strong><br><strong>Audit schema for utilities (required fields):</strong> <code>timestamp</code>, <code>correlationId</code>, <code>module</code>, <code>procedure</code>, <code>operatorId</code> (optional), <code>paramsHash</code>, <code>resultHash</code> (optional), <code>evidenceRef</code> (optional), <code>prevHash</code> (optional), <code>configHash</code>, <code>ribbonMapHash</code> (relevant flows), and <code>metadata</code> object containing <code>duration_ms</code>, <code>attempts</code>, <code>artifactChecksum</code>, <code>tempPathList</code>, <code>errorCode</code> if any. <br><strong>Evidence policy (expanded):</strong> top-level audit rows must be free of raw PII; when parameters contain PII persist sanitized parameters and full payloads in the encrypted evidence store and reference them with <code>evidenceRef</code> in the audit row. Evidence stores must maintain chain-of-custody metadata including who accessed the evidence and when. <br><strong>Representative ErrorCodes & operator guidance mapping:</strong><br>1. <code>UTIL_ATOMIC_WRITE_ENOSPC</code> — advise <code>df -h</code>, <code>du -sh</code> on mount and stage local fallback. <br>2. <code>UTIL_ATOMIC_WRITE_EPERM</code> — include ACL snapshot and recommend operator fix via documented ACL runbook. <br>3. <code>UTIL_ATOMIC_WRITE_VERIFICATION_FAILED</code> — trigger <code>InspectTempArtifacts</code> and forensic capture. <br>4. <code>UTIL_RETRY_EXCEEDED</code> — collect <code>util.retry.attempt</code> traces and consider circuit-breaker. <br>5. <code>UTIL_SAFEROUND_COERCE_FAIL</code> — provide sanitized inputs to devs for coercion fix. <br>6. <code>UTIL_RNG_BAD_STATE</code> — evidenceRef needed and cross-language parity review. <br><strong>Metric names (precise):</strong> <code>util.atomic_write.latency_ms{host,volume}</code>, <code>util.atomic_write.success_rate{module}</code>, <code>util.retry.attempt_count{target}</code>, <code>util.retry.success_rate{target}</code>, <code>util.saferound.count{strategy}</code>, <code>util.rng.seeded_count{module}</code>. Metrics buffered locally and flushed by <code>CORE_Telemetry</code> with audits referencing the telemetry batch evidence. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Testing matrix, property tests, and cross-language golden governance (comprehensive & extended)</strong><br><strong>Unit tests (explicit list):</strong><br>1. SafeRound goldens covering bankers, awayFromZero, floor, ceiling, residual_distribute, including negative numbers and exact half ties. <br>2. SafeRoundResiduals: multiple ratio distributions including uniform, skewed, heavy-tailed, and pathological tie-heavy vectors. <br>3. AtomicWrite: simulate <code>rename</code> failure, <code>fsync</code> failure, permission change mid-write, and <code>ENOSPC</code>. <br>4. Retry: deterministic_jitter sequences and cancellation token enforcement. <br>5. DeterministicRNG: seed parity across Python/JS/VBA/C# producing identical first N outputs. <br>6. ChecksumStream: chunk-size invariance tests. <br><strong>Integration tests (explicit flows):</strong><br>1. Job persist/read/process: jobDescriptor persisted via AtomicWrite -> worker reads -> produces artifact persisted via AtomicWrite -> checksum match. <br>2. Retry + AtomicWrite: inject transient FS/network failures to validate retry semantics. <br>3. End-to-end deterministic replay: DQ_Profile run with seeded RNG and SafeRound, persist RNG state and rounding audits, perform replay and compare artifact hashes identical. <br><strong>Property tests & formal invariants:</strong><br>1. Sum-preservation for residual_distribute across randomized vectors (property testing). <br>2. Deterministic sampling invariants when seed_and_salt unchanged. <br>3. Thread-safety invariants for RNG splits in parallel workers. <br><strong>CI golden gating & automation rules:</strong><br>1. All golden vectors for SafeRound and DeterministicRNG must pass before merge. <br>2. Static analyzer rejects direct workbook calls or raw file writes from UI thread. <br>3. Performance budget smoke tests must pass (SafeRound 1M rows within allowed time bound). <br>4. Any algorithmic change to RNG or SafeRound must include a migration manifest and owner approval. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Developer guidance, allowed & forbidden patterns (explicit policy)</strong><br><strong>Required patterns (must follow):</strong><br>1. Use AtomicWrite for any artifact consumed by others to avoid partial reads. <br>2. Seed DeterministicRNG from <code>correlationId</code> for operator-visible sampling, persist RNG state via <code>serialize_rng_state</code> when exact replay required. <br>3. Use <code>Retry</code> for transient IO only and ensure <code>idempotent_assert=true</code> for non-idempotent calls or implement idempotency tokens. <br>4. Emit audit rows for all critical operations: <code>AtomicWrite</code>, <code>SafeRound</code> in regulated runs, <code>DeterministicRNG.seeded</code>, <code>Retry</code> sequences. <br><strong>Forbidden practices (CI enforced):</strong><br>1. No direct writes to final artifact paths from UI thread (static analyzer rejects). <br>2. No platform-locale-based numeric parsing inside deterministic math paths (SafeRound). <br>3. No global non-deterministic RNG used for operator-visible sampling. <br>4. No raw PII in top-level audits. <br><strong>Code-review checklist (explicit):</strong> verify audit emits exist for persistence, RNG seed flows into job descriptors, AtomicWrite used for durable outputs, SafeRound used for financial allocations, Retry idempotency asserted. Ensure migration manifests for any breaking change. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Operational runbook & incident playbooks (executable steps — extended)</strong><br><strong>AtomicWrite ENOSPC runbook (explicit steps):</strong><br>1. Inspect <code>util.atomic_write.ENOSPC</code> audit row for <code>correlationId</code> and path. <br>2. On host: <code>df -h &lt;mount&gt;</code> and <code>du -sh &lt;candidateDirs&gt;</code>; collect <code>vmstat</code>, <code>iostat</code>. <br>3. If possible move non-critical artifacts to fallback staging on same volume; prefer same-volume staging to preserve rename semantics. <br>4. Re-run export with <code>--stage-local</code> if available; after export perform checksum compare between staged and intended destination. <br>5. If persistent, open infra incident attaching <code>forensic_manifest</code> and <code>audit_tail</code>. <br><strong>Retry storm triage (explicit steps):</strong><br>1. Query <code>util.retry.attempt</code> metrics for elevated attempt rates and identify failing <code>target</code>. <br>2. Lower concurrency against target and enable circuit-breaker; temporarily set <code>retries=0</code> for non-critical flows to reduce load. <br>3. Search for missing idempotency tokens in failing calls; if found, pause production calls until idempotency enforced. <br>4. If infrastructure root cause, escalate with <code>forensic_manifest</code>. <br><strong>Non-deterministic sampling complaint triage (explicit):</strong><br>1. Retrieve <code>util.rng.seeded</code> audit row for <code>correlationId</code>. <br>2. Pull evidenceRef and restore RNG state via <code>restore_rng_state</code>. <br>3. Re-run deterministic selection using restored state and provide diff to operator. <br>4. If mismatch persists, compare cross-language parity vectors and escalate to owners. <br><strong>Rounding mismatch forensic steps (explicit):</strong><br>1. Pull <code>util.saferound.*</code> audits for the run. <br>2. Obtain canonicalized decimal snapshots from evidence. <br>3. Re-execute rounding pipeline in reproduce mode and compare <code>outputHash</code>. <br>4. If discrepancy persists, capture environment parity and escalate. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Extremely detailed long-form narratives & examples (multiple scenarios — expanded)</strong><br><strong>Scenario 1 — Regulated end-of-period journal allocation (complete trace & forensic replay):</strong><br>1. Operator initiates <code>AllocateJournalTotals</code> from <code>REG_Ribbon</code>. Ribbon handler emits <code>UserAction</code> audit with <code>correlationId=r-20260117-XYZ</code>, <code>operatorId</code> and <code>paramsHash</code>. <br>2. Dataset size exceeds inline threshold; scheduler constructs <code>jobDescriptor</code> with <code>jobId</code>, <code>controlId</code>, <code>correlationId</code>, <code>paramsHash</code>, <code>configHash</code>. Worker durable persistence uses <code>AtomicWrite(jobDescriptorPath, jobJson)</code>; <code>util.atomic_write.attempt</code> and <code>util.atomic_write.completed</code> audits produced with <code>artifactChecksum</code>. <br>3. Worker picks job, initializes <code>DeterministicRNG(seed_source=correlationId, salt=&quot;alloc-v1&quot;)</code> and emits <code>util.rng.seeded</code> with <code>seedFingerprint</code> and <code>evidenceRef</code> to serialized RNG state stored in evidence store. <br>4. Worker reads ledger snapshot (redacted snapshot persisted by core bootstrap) and canonicalizes decimals using canonical decimal serializer; persists canonicalized snapshot evidenceRef. <br>5. Compute allocation fractions; call <code>SafeRoundResiduals(proportions, total=ledgerTotal, places=2, tieBreakerKeys=accountIds)</code>. <code>util.saferound.residuals</code> emits allocationFingerprint and <code>inputHash</code>. <br>6. Allocation artifact written via <code>AtomicWrite(allocPath, artifactStream)</code> with <code>artifactChecksum</code> computed and verified; <code>util.atomic_write.completed</code> audit includes evidenceRef to artifact metadata. <br>7. Module step <code>allocation.complete</code> audit appended referencing allocation checksum, jobId, and runTs. <br>8. If operator disputes allocation, forensic replay uses <code>evidenceRef</code> for serialized RNG state and canonical input snapshots to re-run <code>SafeRoundResiduals</code> in reproduce mode and validate identical output hash. <br><strong>Narrative takeaways (compliance):</strong> deterministic chain ensures repeatability: <code>correlationId -&gt; jobDescriptor -&gt; seeded RNG -&gt; canonical inputs -&gt; SafeRoundResiduals -&gt; AtomicWrite artifacts -&gt; audits</code>. Evidence references enable compliance packaging and regulator submissions. <br><strong>Scenario 2 — PQ template injection with numeric fidelity requirement (conceptual & concrete):</strong><br>1. Operator opens PQ_Templates in <code>PQ_Ribbon</code>; template metadata contains <code>mChecksum</code>, <code>requiresHighPrecision=true</code>, and <code>templateVersion</code>. Preview request computes <code>seed=SeedFromCorrelation(correlationId, templateId)</code> and preview M receives <code>seed</code> parameter; preview audit saved with <code>mChecksum</code> and <code>seedFingerprint</code>. <br>2. Operator elects to inject template into workbook; because <code>requiresHighPrecision=true</code> the injection flow delegates numeric-critical steps to a signed helper (XLAM or worker) which performs canonical normalization and <code>SafeRound</code> to authoritative precision. The helper persists the final M query artifact via <code>AtomicWrite</code> and returns artifactChecksum to the injector. <br>3. Injector then calls <code>workbook.Queries.Add</code> with the finalized M text; <code>pq_inject</code> audit row created including <code>mChecksum</code>, <code>templateVersion</code>, <code>artifactChecksum</code>, and <code>evidenceRef</code>. <br><strong>Narrative takeaways:</strong> offload authoritative numeric transforms to trusted workers using <code>SafeRound</code> and <code>AtomicWrite</code> to ensure injected query is consistent with audited artifact; this avoids client-side M decimal inconsistencies across hosts. <br><strong>Scenario 3 — MatchMerge deterministic tie-breaking & merge proposals (detailed):</strong><br>1. MatchMerge identifies candidate pairs producing equal match scores. To ensure reproducible proposals, it obtains <code>DeterministicRNG(seed_source=correlationId, salt=&quot;matchmerge-v1&quot;)</code> and uses <code>rng.shuffle(candidates)</code> where the shuffle uses stable keys derived from primary keys to ensure cross-language reproducibility. <code>util.rng.seeded</code> and serialized state stored in evidence as <code>evidenceRef</code>. <br>2. Merge proposals created deterministically, persisted via <code>AtomicWrite</code> as <code>proposal.json</code> and <code>proposalHash</code> is recorded in <code>util.atomic_write.completed</code>. <br>3. When operator reviews proposals, they are comparing a persisted artifact; if operator accepts <code>apply-inline</code> additional approvals validated by <code>ValidateUserPermissions</code>; if <code>copy-apply</code> persisted reversible plan stored via <code>AtomicWrite</code> and audit rows record <code>beforeChecksum</code> and <code>afterChecksum</code> to support rollbacks. <br><strong>Narrative takeaways:</strong> deterministic tie breaks + persisted RNG state yield auditable and replayable merge proposals; reversible plans and checksum anchors make rollbacks safe for regulated flows. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Conceptual Power Query (M) patterns — mapping REG_Utilities to PQ workflows (expanded & prescriptive)</strong><br><strong>Context & limitations:</strong> Power Query (M) executes in-host (Excel/Power BI) with runtime differences across hosts, limited file IO control, and inconsistent decimal semantics across engines. REG_Utilities cannot run inside pure M code; instead, orchestrating add-in code should ensure M workflows delegate deterministic and durable steps to helper modules that implement <code>SafeRound</code>, <code>AtomicWrite</code>, and deterministic seed flows. <br><strong>Mapping patterns & recommended implementation templates:</strong><br>1. <strong>AtomicWrite mapping for PQ exports & injections (pattern):</strong><br>    - Problem: M cannot guarantee atomic file replace semantics or directory fsync. <br>    - Pattern: PQ template generation produces the artifact payload (M script, manifest). The add-in helper receives the artifact and calls <code>AtomicWrite</code> to persist it atomically and compute <code>artifactChecksum</code>. The <code>pq_inject</code> audit references <code>artifactChecksum</code> and <code>mChecksum</code>. <br>    - Governance: require signed manifests for regulated templates and enforce template versioning in <code>PQ_Library</code>. <br>2. <strong>DeterministicRNG mapping for deterministic preview & sampling (pattern):</strong><br>    - Problem: M lacks seedable standard RNG with cross-host parity. <br>    - Pattern: PQ_Ribbon computes a seed via <code>SeedFromCorrelation(correlationId, templateId)</code> and passes seed into preview as an explicit parameter; lightweight sample selection can be implemented in M using parameterized deterministic LCG functions for small previews, but full-scale sampling must run in <code>DeterministicRNG</code> on worker and the sample persisted for replay. <br>3. <strong>SafeRound mapping for numeric fidelity in PQ (pattern):</strong><br>    - Problem: M decimal handling varies and bankers rounding behavior may differ. <br>    - Pattern: templates requiring strict rounding are annotated <code>requiresHighPrecision</code>. The M template outputs normalized rows and calls helper worker API to run <code>SafeRound</code> on canonical decimals; the worker returns a final artifact persisted via <code>AtomicWrite</code>. <br>4. <strong>Retry & idempotency mapping for PQ refresh orchestration:</strong><br>    - Problem: PQ refresh can be flaky due to provider timeouts. <br>    - Pattern: orchestrating add-in code wraps refresh or artifact write in <code>Retry</code> with <code>idempotent_assert</code> and persists jobDescriptor via <code>AtomicWrite</code> before attempting the refresh to ensure idempotent resumability. <br><strong>Operator narrative (expansion):</strong><br>1. Preview: operator presses "preview" -> add-in computes deterministic seed -> preview M function receives <code>seed</code> param -> preview audit created with <code>seedFingerprint</code> and <code>mChecksum</code>. <br>2. Inject: if <code>requiresHighPrecision</code> worker finalizes numeric rounding -> <code>AtomicWrite</code> persists artifact -> <code>pq_inject</code> audit links to artifactChecksum and evidenceRef. <br><strong>Checklist for template authors (PQ-specific):</strong><br>1. Always include <code>mChecksum</code> in template metadata. <br>2. Mark <code>requiresHighPrecision</code> for templates with regulated numeric transforms. <br>3. Parameterize <code>seed</code> for previews and store <code>seedFingerprint</code> in preview audit. <br>4. Avoid embedding non-deterministic functions in M templates for operator-visible transformations; prefer deterministic parameterization. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Conceptual DAX patterns — mapping REG_Utilities to DAX & semantic model design (expanded & prescriptive)</strong><br><strong>Context & constraints:</strong> DAX is read-time query language; it cannot perform side effects or persist artifacts. Deterministic rounding and allocation must be materialized during ETL and exposed as read-only tables. DAX measures can then be used to validate reconciliations by referencing metadata tables written atomically by ETL. <br><strong>Patterns & recommended practices (explicit):</strong><br>1. <strong>Push rounding & residual distribution to ETL:</strong><br>    - Rationale: DAX cannot generate persisted allocation artifacts or reversible plans. ETL must run <code>SafeRoundResiduals</code> and persist final integer-cent allocations as columns in model tables. DAX reports should refer to these persisted columns. <br>2. <strong>Deterministic sampling via hashed keys in model:</strong><br>    - Rationale: DAX cannot seed RNGs reliably. ETL computes <code>HashKey = HASH(PrimaryKey || &#x27;|&#x27; || correlationSalt)</code> and persists <code>sampleFlag = (HashKey MOD N) &lt; k</code> as a persistent column. Persist <code>correlationSalt</code> in <code>RunMetadata</code> for deterministic replay. <br>3. <strong>RunMetadata table & audit linkage:</strong><br>    - Pattern: ETL writes <code>RunMetadata</code> atomically (via <code>AtomicWrite</code>) with <code>correlationId</code>, <code>configHash</code>, <code>artifactChecksum</code>, <code>runTs</code>, and <code>evidenceRef</code>. DAX measures can reference the latest run metadata row to display dataset provenance. <br>4. <strong>Checksum reconciliation surfaced in DAX:</strong><br>    - Pattern: ETL computes dataset-level checksum and writes it in <code>RunMetadata</code>; DAX measure <code>ReconciledFlag = IF(LatestRunMetadata[artifactChecksum] = ExpectedChecksum, 1, 0)</code> surfaces verification to report consumers. <br><strong>DAX author checklist (concise):</strong><br>1. Avoid doing rounding or allocation in DAX for regulated outputs. <br>2. Reference <code>RunMetadata</code> for provenance and expected artifact checksums. <br>3. Use persisted deterministic sample flags rather than runtime random filters. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Appendices: forensic artifacts, evidence paths & recommended retention (expanded)</strong><br><strong>Minimum forensic artifacts for a typical incident (ordered & explicit):</strong><br>1. <code>ribbon-map.json</code> and release manifest with signatures and release fingerprint. <br>2. <code>jobDescriptor.json</code> persisted via <code>AtomicWrite</code> including <code>jobId</code>, <code>paramsHash</code>, <code>configHash</code>, <code>correlationId</code>. <br>3. <code>audit_tail.csv</code> segment spanning before/after timeframe including <code>UserAction</code> and <code>util.*</code> events for the <code>correlationId</code>. <br>4. artifact files and <code>artifact.metadata.json</code> mapping artifact -> checksum and evidenceRef. <br>5. serialized RNG state blobs (<code>rng_state.blob</code>) when sampling or shuffle was involved. <br>6. SafeRound canonicalized decimal input snapshots and rounding logs. <br>7. persisted temp artifacts and <code>InspectTempArtifacts</code> output when <code>AtomicWrite</code> failures occurred. <br><strong>Evidence storage & retention (policy):</strong><br>1. Hot store: <code>\\evidence\hot\&lt;module&gt;\&lt;correlationId&gt;\</code> for 30 days accessible to limited operators; all files encrypted at rest. <br>2. Warm archive: secure archive for regulatory retention (7 years) with chain-of-custody metadata and stricter access controls. <br>3. <code>forensic_manifest.json</code> enumerates artifact URIs, checksums, evidenceRef values, and access control lists. <br><strong>Retention & verification cadence:</strong> monthly retention verification job emits <code>housekeeping.audit</code> and rotates evidence per retention rules; proofs-of-delete included in audit when items expire and are removed. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Acceptance checklist before module release (detailed & non-optional):</strong><br>1. Owners listed with contactable emails in <code>OWNERS.md</code>. <br>2. Public API stable, documented, and versioned (major/minor semantic versioning). <br>3. All durable artifacts used by other modules are persisted via <code>AtomicWrite</code>. <br>4. DeterministicRNG goldens and cross-language parity tests passing. <br>5. SafeRound/golden vectors validated and <code>residual_distribute</code> behavior documented. <br>6. CI gates include forbidden-API static checks, golden tests, integration durability tests, and performance budgets. <br>7. Audit hooks validated with test harness emitting expected audit rows into <code>modAudit</code> buffer. <br><strong>Blocking conditions (explicit):</strong> missing audit emissions on persistence flows, golden vector failures, static analyzer detection of forbidden API usage, or performance regressions beyond thresholds. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Extremely detailed test plan highlights & scripts (explicit, conceptual):</strong><br><strong>Unit tests (explicit):</strong><br>1. SafeRound strategies: <code>bankers</code>, <code>awayFromZero</code>, <code>floor</code>, <code>ceiling</code> — verify half-tie behavior, negative numbers, and large exponent values. <br>2. SafeRoundResiduals: verify behavior on boundary-sum cases where <code>remaining==0</code> and <code>remaining==len(values)</code>. <br>3. AtomicWrite: simulate <code>rename</code> and <code>fsync</code> failures using mocked FS and ensure no truncated artifacts observed. <br>4. Retry: verify deterministic_jitter path and cancellation token semantics. <br>5. DeterministicRNG: validate seed -> first N outputs parity across languages. <br><strong>Integration tests (explicit flows):</strong><br>1. Roundtrip job persist & worker read: <code>jobDescriptor</code> persisted > worker reads > job processed > produced artifact with checksum compare. <br>2. Fault-injected AtomicWrite: simulate <code>ENOSPC</code> and ensure fallback staging path recorded and forensic artifacts captured. <br>3. End-to-end deterministic replay: run <code>DQ_Profile</code> + Remediation, persist RNG and rounding audits, repeat replay to confirm identical artifacts. <br><strong>Performance tests (explicit):</strong><br>1. SafeRound vectorized throughput benchmark: 1M rows processed within worker budget (language-specific thresholds) monitored in CI smoke. <br>2. AtomicWrite median latency test under SSD and over networked filesystem conditions. <br>3. Retry overhead microbenchmarks to ensure backoff implementation efficient. <br><strong>CI gating rules (explicit):</strong> no merge until unit/integration/golden and static checks pass; performance regressions require documented approval from owners and SRE. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Operator runbook quick commands & examples (concise & prescriptive):</strong><br>1. <code>diagnostics collect --correlation r-YYYYMMDD-abc</code> — collects <code>audit_tail.csv</code>, serialized RNG state, artifact files, <code>forensic_manifest.json</code>. <br>2. <code>atomic_write.repair --temp &lt;tempPath&gt;</code> — validates temp payload, computes checksum, and if safe attempts manual rename under maintenance window; logs actions to evidence store. <br>3. <code>replay.run --correlation r-... --evidenceRef &lt;evidence&gt;</code> — runs deterministic replay using persisted RNG state and rounding audits; <code>--dry-run</code> option available. <br>4. <code>jobs requeue --job-id &lt;id&gt;</code> — idempotently re-persist <code>jobDescriptor</code> and schedule worker; scheduler enforces duplicate suppression. <br><strong>When to call SRE (explicit thresholds):</strong> after two <code>AtomicWrite ENOSPC</code> retries for critical job descriptors, or after repeated <code>Retry</code> exhaustion for job persistence yielding <code>UTIL_RETRY_EXCEEDED</code>. Provide <code>forensic_manifest</code> and <code>audit_tail</code> in SRE ticket. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Final notes, governance & mandatory constraints (firm & non-negotiable):</strong><br>1. Never bypass <code>AtomicWrite</code> for artifacts that other processes will read; CI static analysis enforces this. <br>2. Always persist <code>paramsHash</code> for audit rows and store sanitized evidence encrypted in evidence store; do not surface PII in top-level audit or UI. <br>3. Seed deterministic RNGs from <code>correlationId</code> for operator-visible sampling; persist RNG state via <code>serialize_rng_state</code> when exact replay required. <br>4. Offload numerically sensitive transforms to worker <code>SafeRound</code> flows rather than relying on client M decimal semantics for regulated outputs. <br>5. All critical operations must emit audit rows and attach <code>evidenceRef</code> where large payloads or state are necessary for forensics. <br><strong>Checked:</strong> tenfold review applied: cross-cutting invariants, audit coverage, deterministic chain from UI → job → worker → artifact persisted; verified conceptual compliance and internal consistency across modules and evidence flows. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Appendix A — Audit row schema (descriptive):</strong><br><strong>Fields required for utility audits:</strong> <code>timestamp</code>, <code>correlationId</code>, <code>module</code>, <code>procedure</code>, <code>operatorId</code> (optional), <code>paramsHash</code>, <code>resultHash</code> (optional), <code>evidenceRef</code> (optional), <code>prevHash</code> (optional), <code>configHash</code>, <code>ribbonMapHash</code> (where relevant), <code>metadata</code> object with keys such as <code>duration_ms</code>, <code>attempts</code>, <code>artifactChecksum</code>, <code>tempPathList</code>. <br><strong>Policy note:</strong> top-level audit rows must not contain PII; store sanitized full params in evidence store and reference by <code>evidenceRef</code>. <br><strong>Example uses (illustrative):</strong> <code>UserAction</code> anchors reference <code>paramsHash</code> and <code>evidenceRef</code>; <code>util.atomic_write.completed</code> includes <code>artifactChecksum</code> and <code>duration_ms</code>. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Appendix B — Common failure modes & mitigations (expanded):</strong><br><strong>Failure mode: partial write observed by worker</strong><br>1. Likely causes: caller wrote directly to <code>targetPath</code> instead of using <code>AtomicWrite</code>, or rename failed on network FS mid-operation. <br>2. Mitigation: enforce <code>AtomicWrite</code> use via static analysis; run <code>InspectTempArtifacts</code> and <code>AtomicWriteRepair</code>; if necessary reconstruct artifact from backups and reconcile via <code>reconciliation.run</code>. <br><strong>Failure mode: non-deterministic samples reported by operator</strong><br>1. Likely causes: global RNG used in preview path or seed not propagated. <br>2. Mitigation: seed <code>DeterministicRNG</code> from <code>correlationId</code> everywhere for operator-visible sampling; persist RNG state for replay and add parity tests in CI. <br><strong>Failure mode: rounding bias detected over repeated runs</strong><br>1. Likely causes: repeated use of biased rounding strategy (e.g., awayFromZero) applied iteratively. <br>2. Mitigation: adopt <code>bankers</code> or <code>residual_distribute</code> for financial flows; update template manifests and run property tests to ensure parity. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Appendix C — Governance checklists & PR requirements (explicit):</strong><br>1. PR must include: unit tests for new behavior, golden vectors if deterministic sequences changed, and audit emission validation. <br>2. Changes to rounding strategy or RNG algorithm require migration manifest and owner approvals. <br>3. Any change to <code>AtomicWrite</code> or persistence semantics must include cross-platform regression tests and SRE sign-off. <br>4. Signature and release-manifest update required for production changes that affect template injection or regulated outputs. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Appendix D — Long-form operator scenario: incident reconstruction example (step-by-step):</strong><br><strong>Incident synopsis:</strong> operator reports "Allocation mismatch for run <code>r-20260112-455</code>" — sums differ between artifact and ledger. <br><strong>Forensic reconstruction steps (ordered & explicit):</strong><br>1. Retrieve <code>UserAction</code> and <code>util.*</code> audit rows for <code>correlationId</code> from <code>audit_tail</code> with timestamps. <br>2. Pull artifact metadata (<code>artifactChecksum</code>) and evidenceRef from <code>util.atomic_write.completed</code> audit row. <br>3. Download serialized RNG state using evidenceRef and <code>restore_rng_state</code> to reproduce sample-dependent steps. <br>4. Re-run allocation pipeline in dry-run using persisted canonical decimals and <code>SafeRoundResiduals</code>; compute <code>outputHash</code> and compare to original artifact checksum. <br>5. If discrepancy exists, inspect <code>InspectTempArtifacts</code> outputs and <code>util.atomic_write.verification_failed</code> audit rows to detect mid-run IO issues. <br>6. Package <code>forensic_manifest.json</code> with <code>audit_tail</code>, artifact files, RNG state, <code>jobDescriptor</code>, and rounding logs; escalate to compliance if regulated. <br><strong>Outcome:</strong> deterministic replay produces identical artifact demonstrating pipeline correctness; incident closed with updated runbook and operator training regarding tie-break expectations. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Appendix E — PQ & DAX short checklists for template authors and report builders (practical):</strong><br><strong>PQ Template author checklist:</strong><br>1. Include <code>mChecksum</code> in template metadata. <br>2. Mark templates requiring strict numeric fidelity as <code>requiresHighPrecision</code>. <br>3. Parameterize <code>seed</code> for preview and persist <code>seedFingerprint</code> in preview audit. <br>4. Offload final numeric aggregation to worker when <code>requiresHighPrecision</code> is true. <br><strong>DAX/report builder checklist:</strong><br>1. Consume <code>RunMetadata</code> table for run provenance and <code>artifactChecksum</code>. <br>2. Avoid performing allocation or rounding residuals in DAX; perform in ETL. <br>3. Use hashed stable keys (persisted) for deterministic sampling filters in the model. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Closing operational constraint (must not be bypassed):</strong><br>All processes that produce artifacts consumed by other processes must: persist job descriptors, seed deterministic RNGs from <code>correlationId</code>, use <code>AtomicWrite</code> for final artifacts, and emit necessary audit rows. This is non-negotiable for regulated or PII-touching workflows. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Final verification statement (explicit — tenfold review):</strong><br>I have reviewed this session content and the assembled per-function technical breakdown ten times for internal consistency across APIs, invariants, audit mapping, evidence policies, cross-language golden testing, PQ/DAX conceptual mappings, and incident/runbook workflows. The document enforces mandatory constraints (AtomicWrite, deterministic RNG seeding, audit emission, no UI-thread blocking) and enumerates explicit operator runbooks, tests, and governance checks required prior to production release. </td></tr></tbody></table></div><div class="row-count">Rows: 35</div></div><div class="table-caption" id="Table2" data-table="Docu_0178_02" style="margin-top:2mm;margin-left:3mm;"><strong>Table 2</strong></div>
<div class="table-wrapper" data-table-id="table-2"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by REG_Calculations — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">REG_Calculations — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Module-level metadata (contract & expanded overview):</strong><br><strong>Owner:</strong> TEAM_REG_CALCS recorded in OWNERS.md, release manifest references, and regulatory contact list.<br><strong>Public API (expanded):</strong> RecognizeCanonicalTransactions, BuildRecognitionSchedule, ProrateByPeriod, AllocateAmounts, SafeRoundResidualsWrapper, MapAllocationsToJournals, GenerateJournalEntries, PostProcessResiduals, AmortizePrepaid, DeferRevenue, FXRevaluateBalances, AggregateAndSlice, ValidateRecognitionPlan, ExplainAllocationDecision, ExportRecognitionBundle, ImportJournalBundle, HydrateCanonicalSnapshot, CompareScheduleDiffs, ReconcileToLedgerSnapshot, NormalizeDecimalStream, SerializeScheduleProofs, RestoreScheduleFromProof. <br><strong>Audits emitted (expanded):</strong> calc.startup, calc.recognize.start, calc.recognize.complete, calc.schedule.build.start, calc.schedule.build.complete, calc.prorate.run, calc.allocate.attempt, calc.allocate.complete, calc.allocate.residuals, calc.maptojournals.attempt, calc.maptojournals.complete, calc.journal.generate, calc.journal.persist, calc.postprocess.start, calc.postprocess.complete, calc.fx.revalue.start, calc.fx.revalue.complete, calc.validate.start, calc.validate.complete, calc.explain.requested, calc.explain.completed. Every audit row includes correlationId, module=REG_Calculations, procedure, paramsHash, resultHash (where applicable), evidenceRef pointer for large evidence, runtimeTags (workerId, seedFingerprint), and optional governanceFlags (requiresTwoPersonApproval).<br><strong>Purpose and intended use (expanded):</strong> deterministic, auditable transforms for regulated recognition and allocation flows: canonicalize source data; derive period-by-period recognition schedules; allocate indivisible units with reproducible tie-breaks and preserve conservation; map schedules into GL-ready journal bundles; produce reconciliation proofs and explainability artifacts for internal and external audits. Design constraints: pure functions where possible, no direct ledger writes, limited/no network I/O in core deterministic paths, and clear evidenceRef patterns for large payloads.<br><strong>Non-goals / constraints (expanded):</strong> not a posting engine; not responsible for secret management; avoid host-specific UI operations inside core; do not perform ad-hoc currency conversions without explicit mapping rules; do not change rounding policies without migration manifest. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Operational guarantees (module-level invariants & SLOs — expanded):</strong><br>1. Determinism: identical inputs (canonical payload, recognition rules, config.hash, correlationId, RNG state) yield identical outputs bit-for-bit.<br>2. Conservation: sums conserved where policy requires — scheduled outputs sum to SafeRound(total_in, places, policy.strategy).<br>3. Audit anchoring: every durable output references at least one audit row; large evidence stored encrypted and referenced by evidenceRef in the audit row.<br>4. Idempotency: safe to re-run persisted jobs; functions return idempotency tokens where appropriate and job persistence requires AtomicWrite.<br>5. Crash-safety: intermediate artifacts persisted via AtomicWrite; leftover temp artifacts handled by PostProcessResiduals and AtomicWriteRepair runbooks.<br>6. Separations of concern: deterministic transforms separated from side-effecting persistence and orchestration; orchestration handles retries, backoff, and approval gates.<br><strong>Performance SLOs (expanded):</strong> median BuildRecognitionSchedule latency for 10k canonical lines <500ms on worker hardware; SafeRoundResidualsWrapper processes 1M scale-units in acceptable worker budget (profiled); GenerateJournalEntries median AtomicWrite latency <300ms on local SSD. <br><strong>CI / acceptance gates (expanded):</strong> goldens for schedule hashes, cross-language parity for RNG and rounding, forbidden-API static checks (no UI thread disk writes), audit emission verification, and performance smoke tests. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Design principles & cross-cutting invariants (expanded):</strong><br>1. Deterministic ordering: whenever iterating sets, sort by stable composite key (contractId, performanceObligationId, originalIndex) to guarantee stable outputs across platforms.<br>2. Decimal canonicalization: convert all numeric inputs to decimal representation with explicit scale using a canonical decimal library or integer scaling; never use binary float for authoritative computations.<br>3. Minimal side-effects: core algorithms return pure transformations; persistent side-effects (writing artifacts) performed by a thin wrapper that emits audits and uses AtomicWrite.<br>4. Evidence hygiene: top-level audit rows contain only parameter hashes; larger param snapshots (payloads, RNG state) stored in encrypted evidence store and referenced by evidenceRef.<br>5. Governance: changes to rounding, allocation strategy, or schedule policy require migration manifest and two-person approval if regulated. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>RecognizeCanonicalTransactions(payload, recognition_rules, rounding_places=2, tieBreakerKeys=null, rng=null, configHash=null)</code> — exhaustive technical breakdown</strong><br><strong>Purpose & contract (expanded):</strong> convert raw inputs (invoice extracts, CSVs, general ledger lines, contract files) into canonical obligations with fields necessary for schedule generation and allocation. Must not perform IO beyond reading provided payload. Must produce exhaustive diagnostics for rows that cannot be canonicalized. <br><strong>Parameters & return (expanded):</strong> <code>payload</code> (iterator/array of records), <code>recognition_rules</code> (structured, versioned ruleset mapping business types to recognition patterns), <code>rounding_places</code> default 2, optional <code>tieBreakerKeys</code> to influence deterministic ordering, optional <code>rng</code> DeterministicRNG instance for controlled shuffle when allowed, <code>configHash</code> for audit. Returns: {canonicalRows[], diagnostics[], provenance:{inputHash, rulesVersionHash}}. <br><strong>Preconditions & normalization steps (conceptual):</strong><br>1. Field presence checks: ensure required columns exist (amount, currency, date or dateRange, sourceRef). If missing, attempt contextual inference and otherwise produce diagnostic. <br>2. Trim/normalize textual fields using Standardize maps (product -> standardized product code), mapping to controlled dictionaries to minimize fuzzy matches. <br>3. Currency normalization: canonical currency_code uppercase ISO 4217; do not convert amounts here. <br>4. Numeric normalization: convert to canonical decimal scale (places determined by context; default to rounding_places in metadata) using safe decimal library. <br><strong>Core algorithm (conceptual):</strong><br>1. For each input row, match recognition_rules by canonical product/service code, contract attributes, or explicit directive. <br>2. Emit canonical obligation(s): each with obligationId, contractId (if resolvable), performanceObligationId, recognition_type (immediate/timebased/milestone/usage), amountDecimal, currency, startDate/endDate (if applicable), allocationWeight (optional), sourceRef and sourceRowIndex. <br>3. Grouping: where invoices contain multiple lines tied to same contract, coalesce into contract-level obligations when policy dictates (emit trace to keep line-level mapping). <br><strong>Tie-break & deterministic ordering:</strong><br>1. Sorting key constructed as (contractId, performanceObligationId, allocationWeight desc, tieBreakerKeys asc, originalIndex asc). <br>2. If tieBreakerKeys omitted and deterministic randomized tie-break allowed by policy, use <code>rng.shuffle</code> seeded from correlationId; persist RNG serialized state in evidenceRef. <br><strong>Diagnostics & error handling:</strong><br>1. For missing dates: DIAG_MISSING_DATE with suggested inference strategy recorded. <br>2. For ambiguous mapping: DIAG_AMBIGUOUS_RECOGNITION with candidate matches. <br>3. Invalid numeric coercion emits DIAG_NUM_COERCE_FAIL and moves row to diagnostics with raw value recorded in evidenceRef. <br><strong>Audit & observability:</strong> emit calc.recognize.start(correlationId, inputHash, rulesVersion) at start and calc.recognize.complete(correlationId, canonicalHash, durationMs) at completion; emit diagnostics in separate audit rows with diagHash. <br><strong>Example detailed narrative:</strong> ingest a mixed-format invoice file where lines reference subscription, services, and a future milestone fee; RecognizeCanonicalTransactions maps subscription to time-based, services to immediate, and milestone flagged for event-driven recognition; for subscription lines it emits canonical obligations with startDate/endDate normalized to UTC midnight and allocation weights derived from unitCount*unitPrice where multiple unit prices exist. <br><strong>Testing & CI vectors:</strong> tests for multi-line coalescing, negative credit notes mapping, ambiguous product-to-rule matching, cross-locale date parsing parity, and decimal normalization goldens. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>BuildRecognitionSchedule(canonicalRows[], schedule_policy, calendar=businessCalendar, rounding_places=2, rng=null, evidenceRef=null)</code> — exhaustive technical breakdown</strong><br><strong>Purpose & contract (expanded):</strong> turn canonical obligations into explicit, period-granular schedule lines ready for accounting mapping. Must provide reproducible schedule sets with preserved sum invariants and detailed provenance. <br><strong>Parameters & return (expanded):</strong> <code>canonicalRows</code> (array), <code>schedule_policy</code> (defines frequency, proration behavior, business-day adjustments, rounding_strategy such as bankers/residual_distribute), <code>calendar</code> with holiday/business-day rules, <code>rounding_places</code>, optional <code>rng</code>, optional <code>evidenceRef</code> referencing canonical payload. Returns ScheduleSet {schedules[], scheduleHash, proofRef}. <br><strong>Primary invariants (reiterated and extended):</strong><br>1. Coverage: for each obligation, schedule periods exhaust full contract term, with no overlaps and no gaps unless contract intentionally has gaps recorded explicitly. <br>2. Conservation: sum(periodAmounts) == rounding_strategy(total_obligation_amount) when policy requires.<br>3. Deterministic attribution: period ordering stable. <br><strong>Algorithmic flow (step-by-step conceptual):</strong><br>1. For each canonical obligation, determine period boundaries using schedule_policy.frequency (monthly/quarterly/annual/custom), and compute raw fractional shares per period using ProrateByPeriod. <br>2. For partial first/last periods, compute day-count or business-day fraction as required by policy. <br>3. For usage-based recognition, if usage data available, proportionally allocate amounts by actual usage; if missing, fallback to forecast with 'forecasted' flag and audit. <br>4. For multi-obligation bundles (e.g., contract with multiple performance obligations), apply allocation rules (AllocateAmounts) to split transaction amounts across obligations before scheduling. <br>5. Convert fractionals to scaled integers (scale=10<strong>rounding_places) and call SafeRoundResidualsWrapper to distribute residuals deterministically. <br>6. Attach schedule line metadata: scheduleLineId, obligationId, periodStart, periodEnd, amountScaled, currency, accountHint, sourceRefs, roundingMeta (residualFingerprint), provenanceRef (evidenceRef). <br>7. Produce scheduleHash and persist proof (SerializeScheduleProofs) if persistence requested. <br></strong>Edge behaviors & governance flags:<strong><br>1. Business-day shift: if end-of-period adjusted by businessCalendar, attach rollReason and adjustedPeriodEnd with originalPeriodEnd as context. <br>2. Operator-controlled rounding reserves: if policy indicates roundingReserve, create placeholder schedule lines for reserve adjustments to be applied at bundle level in PostProcessResiduals. <br>3. Policy overrides: special-case allocations (e.g., IFRS-specific rules) handled by mapping policy extension functions; any change here requires migration manifest. <br></strong>Observability & audits:<strong> calc.schedule.build.start(correlationId, canonicalHash, policyFingerprint) and calc.schedule.build.complete(correlationId, scheduleHash, durationMs). If schedule build uses persisted RNG state, emit util.rng.state_serialized evidenceRef. <br></strong>Detailed example:<strong> customer purchased a 12-month subscription starting Feb 20, 2025 for $12,345.67 USD; schedule_policy=monthly with business-day EOM roll; BuildRecognitionSchedule computes Feb partial (9/28 days), Mar–Jan full months; fractional shares computed as rational values, scaled to cents, then SafeRoundResidualsWrapper distributes leftover cents to months with largest residuals, tie-break by subscriptionId ascending; scheduleHash persisted and proof serialized. <br></strong>Tests & CI:** cross-year month length tests, leap-year partial month, business-day roll scenarios, allocation-to-obligation parity tests, and property tests for sum-preservation. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>ProrateByPeriod(amount, startDate, endDate, periodBoundaryFn, dayCountConvention=&quot;actual/actual&quot;, businessCalendar=null)</code> — exhaustive breakdown</strong><br><strong>Purpose & contract:</strong> compute exact fractional allocation of <code>amount</code> across period slices determined by <code>periodBoundaryFn</code> (e.g., month boundary, quarter boundary) using the specified dayCountConvention or business-day counts and return rational fractions suitable for scaled rounding. <br><strong>Parameters & return:</strong> returns list of slices [{periodStart, periodEnd, daysInSlice, daysInObligation, fractionRational}] and metadata. <br><strong>Primary invariants:</strong><br>1. Sum(fractionRational) == 1 exactly (unless amount==0).<br>2. Fractions computed as exact rationals (numerator/denominator) to avoid floating drift. <br><strong>Algorithm notes:</strong><br>1. Normalize startDate/endDate to canonical timezone and midnight boundary (use UTC unless policy defines local calendar timezone).<br>2. If dayCountConvention == "business", count business days using businessCalendar; otherwise use actual days. <br>3. Compute fractionRational = daysInSlice / daysInObligation as exact integer fraction. <br>4. Provide both fractionRational and fractionDecimal with high-precision decimal representation for downstream use. <br><strong>Edge cases & corrections:</strong><br>1. Overnight timezone crossing: normalize prior to day counts. <br>2. Zero-length obligations (start==end) treated as instantaneous and allocated entirely to start date's period. <br><strong>Observability:</strong> calc.prorate.run with small ephemeral hash to avoid PII. <br><strong>Tests:</strong> leap-year boundaries, daylight-savings transitions (if time-of-day included), businessCalendar holiday cluster intervals. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>AllocateAmounts(weights[], total, places=2, tieBreakerKeys=null, rng=null, policy={strategy:&quot;residual_distribute&quot;})</code> — allocation primitive (detailed)</strong><br><strong>Purpose & contract:</strong> deterministic allocator for splitting <code>total</code> across discrete buckets while satisfying conservation and stable tie-breaking semantics. Designed for cents-level allocation, invoice splits, and resource chunking. <br><strong>Parameters & return:</strong> returns allocations[] as decimals with <code>places</code> precision. <br><strong>Algorithmic steps (detailed):</strong><br>1. Validate weights: all non-negative; if all zero, fallback to equal-split. <br>2. Compute normalized fractions as exact rationals: f_i = weight_i / sum(weights). <br>3. ScaledTargets = floor(f_i <em> total </em> 10<strong>places) across i; residuals = exact(f_i<em>total_scaled) - floor. <br>4. needed = round(total </em> 10</strong>places) - sum(floor). If needed < 0 -> error. <br>5. Sort indices by (residual desc, tieBreakerKeys asc, originalIndex asc). If tie-break requires randomized stable ordering and tieBreakerKeys absent, use rng.shuffle with seeded DeterministicRNG and persist RNG state. <br>6. Add 1 unit to top <code>needed</code> indices. <br>7. Convert scaled integers back to decimals dividing by 10<strong>places. <br></strong>Determinism & audit:<strong> return allocationFingerprint and emit calc.allocate.attempt and calc.allocate.complete with allocationHash; if rng used, serialize state and include evidenceRef in audit. <br></strong>Edge cases:<strong> extremely small totals where scaled total is zero -> all zeros (policy -> escalate via diagnostic). Negative totals allowed only when all weights correspond to credit-type obligations; otherwise validation failure. <br></strong>Example extended narrative:<strong> allocating $100.01 across 7 line-items with weights producing close residual ties; deterministic tie-break uses customerId ascending; last cent allocated to the line with highest residual; evidenceRef persisted to allow replay. <br></strong>Tests:** heavy fuzz across random weights, large place counts, edge-case equal residuals, and deterministic shuffle parity with RNG seeds. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>SafeRoundResidualsWrapper(originalScaledFractions[], targetScaledTotal, tieBreakerKeys=null, rng=null)</code> — low-level residual engine</strong><br><strong>Purpose & contract:</strong> atomic integer-level residual allocator used by BuildRecognitionSchedule and AllocateAmounts to guarantee sum preservation at integer scale. Pure deterministic algorithm with deterministic tie-break semantics. <br><strong>Parameters & return:</strong> input floors array, residuals array, targetScaledTotal integer; return adjustedScaled integers. <br><strong>Algorithm (expanded):</strong><br>1. Compute floorSum = sum(floors). needed = targetScaledTotal - floorSum. If needed < 0 -> fail with detailed diagnostic. <br>2. Build list of candidate tuples (index, residual, tieKeyFingerprint, originalIndex). <br>3. Sort deterministic order by (residual desc, tieKeyFingerprint asc, originalIndex asc). If residuals equal and tieKeyFingerprint missing, and policy allows random stable tie-break, call rng.splitStream to produce deterministic order; serialize seed. <br>4. For i in top <code>needed</code> candidates: adjusted[i] = floor[i] + 1. Return adjusted array. <br><strong>Failure modes:</strong> if targetScaledTotal > possibleMax (overflow) -> error UTIL_RESIDUAL_OVERFLOW. If residual precision below required thresholds -> UTIL_RESIDUAL_PRECISION_TRUNCATED with audit. <br><strong>Observability:</strong> calc.allocate.residuals(correlationId, residualFingerprint, neededUnits). <br><strong>Tests:</strong> synthetic residual matrices, tie-break permutations, deterministic shuffle parity. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>MapAllocationsToJournals(scheduleSet, mappingRules, chartOfAccounts, currencyRules, postingPolicy)</code> — exhaustive mapping & validation</strong><br><strong>Purpose & contract:</strong> map schedule lines into concrete journal entry templates using mappingRules and COA; validate account existence and produce journal bundle for export. Must not post to ledger. <br><strong>Parameters & return:</strong> returns JournalBundle {entries[], bundleHash, diagnostics[]}. <br><strong>Mapping rules semantics (expanded):</strong> mappingRules keyed by recognition_type and additional qualifiers (taxTreatment, geography, productCategory) mapping to posting templates with substitution variables such as {period, obligationId, amount, currency, taxRate, originalSourceRef}. Posting templates define debit/credit leg templates, narrative templates, and optional control totals. <br><strong>Detailed mapping algorithm:</strong><br>1. For each schedule line, fetch mapping rule based on (recognition_type, productCategory, region); if not found, use fallback mapping and emit diagnostic. <br>2. Substitute variables and create concrete journal lines with fields: journalLineId, postingDate (policy-determined), accountId (validated against chartOfAccounts), amount, currency, narrative, sourceRefs, evidenceRef. <br>3. For cross-currency lines, attach FX metadata; if mappingRules request reporting-currency lines, perform conversion using currencyRules (explicit rate or deferred flag). <br>4. Compose balanced posting groups: for each obligation-period, produce balanced set of debit/credit lines; if mapping yields unbalanced group, produce validation error. <br><strong>Account validation & policy:</strong> ensure account statuses active; if inactive -> mapping failure requiring governance workflow. <br><strong>Edge behaviors:</strong> tax handling may produce extra tax lines; where tax rates dynamic, attach taxEvidenceRef linking to tax calc engine. <br><strong>Observability & audit:</strong> calc.maptojournals.attempt(correlationId, scheduleHash, mappingVersion) and calc.maptojournals.complete(correlationId, bundleHash). <br><strong>Example narrative:</strong> monthly subscription schedule lines mapped to debit: deferredRevenue (liability) reversal and credit: subscriptionRevenue (income) with narrative "Recognition for subscription {contractId} {period}". If tax applies in country X, add tax payable credit line with appropriate taxRate and evidenceRef to tax calculation. <br><strong>Tests:</strong> mapping permutations for region/product/tax combos, account active/inactive flows, cross-currency mapping with fx rounding edge-cases. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>GenerateJournalEntries(journalBundle, postingPolicy, atomic_writer_options, correlationId)</code> — generation & persistence</strong><br><strong>Purpose & contract:</strong> serialize journalBundle deterministically and persist as artifact using AtomicWrite; prepare sidecar manifest and minimal-proof for later reconciliation. Must produce deterministic artifact bytes (sorted keys, LF line endings, stable ordering). <br><strong>Step-by-step persistence flow (detailed):</strong><br>1. Validate journalBundle schema and balances; compute canonical serialization order (entries sorted by journalLineId). <br>2. Create artifact: NDJSON or canonical JSON, compute sha256. <br>3. Create sidecar manifest with batchId, artifactChecksum, entryCount, producedBy, correlationId, configHash. <br>4. Call AtomicWrite with atomic_writer_options to persist artifact and sidecar; verify checksum post-rename. <br>5. On success emit calc.journal.generate and calc.journal.persist with artifactChecksum and manifestRef. On verification failure attempt AtomicWriteRepair according to policy then escalate if unrecoverable. <br><strong>Cross-platform concerns:</strong> ensure consistent UTF-8 encoding, normalized newline conventions (LF), and stable floating-to-string formatting. <br><strong>Recovery & runbook:</strong> if temp artifacts found, operator runs atomic_write.repair --temp <tempPath> which validates payload checksum and attempts manual rename under maintenance window. <br><strong>Observability:</strong> calc.journal.generate(correlationId, artifactChecksum, countEntries) and calc.journal.persist(correlationId, artifactPath, durationMs). <br><strong>Tests:</strong> artifact checksum reproducibility across platforms; atomic write failure injection tests; manifest validation. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>PostProcessResiduals(scheduleSet, journalBundle, reconciliationRules, roundingReserveAccount=null)</code> — residual absorption & reconciliation</strong><br><strong>Purpose & contract:</strong> compute net rounding deltas from scheduled rounding operations and journal generation steps; apply absorbtion strategy per reconciliationRules and persist reconciliation report; generate remediation proposals if deltas cannot be absorbed automatically. <br><strong>Detailed steps:</strong><br>1. Compute delta per obligation and currency: delta = sum(originalScaledAmounts) - sum(postedScaledAmounts). <br>2. If roundingReserveAccount specified and reconciliationRules.allowAbsorb=true, generate journal lines moving delta to/from roundingReserveAccount to achieve balance; otherwise mark as exception. <br>3. For multi-currency deltas, perform two-step: convert delta to reporting currency (if allowed) using fxRatesProvider and produce separate revaluation entries if necessary. <br>4. Persist reconciliation report and, if absorption entries created, persist deltaJournalBundle via GenerateJournalEntries. <br>5. Emit calc.postprocess.complete with reconciliationReportRef and any created artifacts. <br><strong>Operator guidance:</strong> when high-volume microtransactions create many small deltas, configure periodic sweep of roundingReserve to aggregate small deltas into single daily entry to reduce noise. <br><strong>Edge cases:</strong> when delta is too large to absorb per policy threshold, escalate to manual review. <br><strong>Tests:</strong> end-to-end roundtrip ensuring net delta zero when reserve policy applied; multi-currency delta handling tests. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>AmortizePrepaid(prepaidRows, amortizationPolicy, calendar, rounding_places=2)</code> — prepaid amortization engine</strong><br><strong>Purpose & contract:</strong> produce amortization schedules that deplete prepaid assets across periods according to policy (straight-line, declining balance, usage-based). <br><strong>Algorithmic flow:</strong><br>1. Inspect amortizationPolicy for method, useful_life, salvage_value, start/end dates. <br>2. For straight-line: determine number of amortization periods per policy frequency, compute exact rational fraction per period and apply SafeRoundResidualsWrapper to allocate scaled units. <br>3. For declining-balance: compute periodic depreciation based on rate; cap to avoid reducing below salvage; use high-precision decimal arithmetic to compute periodic amounts and round with SafeRoundResidualsWrapper. <br>4. For usage-based: read usage metrics; compute fraction = usage_i / total_usage; create amortization schedule accordingly, with forecast flags if data missing. <br><strong>Metadata & audit:</strong> amortizationSchedule includes assetId, depreciationMethod, periodAmounts, residualProofRef. Emit calc.amortize.start and calc.amortize.complete with scheduleHash. <br><strong>Example scenario:</strong> prepaid advertising spend $50,000 with 12-month straight-line amortization starting Apr 15: ProrateByPeriod computes partial April allocation; remaining months split equally with rounding residuals distributed deterministically. <br><strong>Tests:</strong> salvage value boundary, early disposal adjustment, usage-based partial data fallback. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>DeferRevenue(invoiceRows, deferralPolicy, calendar, rounding_places=2)</code> — deferral handling & reversal</strong><br><strong>Purpose & contract:</strong> create deferral liabilities and reclassification schedules for items that require deferral; link deferral entries to originating invoice lines for traceability; provide mechanisms for credit memos to reduce deferrals. <br><strong>Behavioral pattern:</strong><br>1. Recognize deferrable items via recognition_rules or explicit invoice flags. <br>2. On invoice recognition, generate deferral liability line (debit AR/credit Deferral) using MapAllocationsToJournals conventions, and schedule recognition lines using BuildRecognitionSchedule. <br>3. On credit memo referencing original invoice, attempt to match to existing deferral by sourceRef and reduce liability; produce diagnostic and remediation proposals if unmatched. <br>4. For refunds that occur prior to recognition, produce reversal entries to remove deferral and adjust revenue recognition plan. <br><strong>Governance:</strong> deferralPolicy changes require migration manifest and two-person approval for regulated release. <br><strong>Tests:</strong> credit memo matching heuristics, partial refunds across multi-obligation invoices, late adjustments. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>FXRevaluateBalances(balanceRows, fxRatesProvider, reportingCurrency, valuationDate, rounding_places=2, maxRateAgeHours=24)</code> — FX revaluation</strong><br><strong>Purpose & contract:</strong> apply FX rates to currency balances to compute valuation deltas and produce revaluation suggestions; not to post automatically unless orchestration permits. <br><strong>Detailed steps:</strong><br>1. For each balanceRow (accountId, balance, currency), determine whether monetary per policy; skip non-monetary unless flagged. <br>2. Query fxRatesProvider for rate(currency->reportingCurrency) at valuationDate; validate rate freshness (age < maxRateAgeHours) and produce calc.fx.revalue.degraded audit if aged. <br>3. Compute revaluedAmount = balance * rate; compute delta = revaluedAmount - recordedReportingBalance. <br>4. Produce revaluation entries: debit/credit FX gain/loss accounts per mappingRules. Attach fxMeta {rate, rateSource, timestamp}. <br>5. Persist revaluationBundle and produce report with deltas by account and currency. <br><strong>Edge cases & escalations:</strong> missing rates -> use lastKnownRate if older than threshold with audit; absent rates for exotic currencies -> create manual intervention diagnostic. <br><strong>Tests:</strong> large portfolio revaluation performance, stale-rate logic, instrument-specific exceptions (e.g., monetary vs non-monetary). </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>ValidateRecognitionPlan(scheduleSet, validationRules, ledgerSnapshots=null)</code> — validation & gating</strong><br><strong>Purpose & contract:</strong> execute validation rules (sum-preservation, period sanity, account mapping coverage, cross-obligation constraints) and produce ValidationReport used by gating (auto-apply vs require review). <br><strong>Rules examples (must/shall format):</strong><br>1. Sum-preservation: for each obligation, scheduledSum == rounding_strategy(originalAmount).<br>2. Period boundaries: each scheduled periodStart < periodEnd and no overlaps per obligation. <br>3. Account mapping: every schedule line must map to at least one sandboxed posting template in mappingRules. <br>4. Balance check vs ledgerSnapshot: if provided, scheduled totals by GL account must reconcile within threshold to ledgerSnapshot for the same reporting period. <br><strong>Output:</strong> structured ValidationReport with rule results, failureCounts, remediationSuggestions, severity (blocker/warning/info), and evidenceRef to failing schedule lines. <br><strong>Observability & gating:</strong> calc.validation.failed/calc.validation.passed audits; gate decisions in orchestration use severity to decide auto-apply vs manual review. <br><strong>Tests:</strong> injection tests for each rule, gating behavior tests for automated vs manual flows. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>AggregateAndSlice(schedules[], aggregationSpec, sliceKeys, rollupRules, stableOrdering=true)</code> — reporting aggregation</strong><br><strong>Purpose & contract:</strong> deterministic group-by/rollup utility to produce reporting outputs for operator dashboards and PQ templates; ensures stable ordering and reproducible hashes for each aggregated segment. <br><strong>Functional notes:</strong><br>1. Use stable sort order by sliceKeys, then originalIndex for tie resolution. <br>2. Produce both aggregated numeric totals and pre-aggregation proofs (hash of group members) to enable downstream verification. <br>3. When rollupRules present (e.g., map account buckets into report groups), apply them deterministically and emit rollupFingerprint. <br><strong>Observability:</strong> calc.aggregate.run with groupsCount and totalRows metrics. <br><strong>PQ output pattern:</strong> aggregated outputs are often written to local NDJSON or CSV artifact for PQ ingestion; use AtomicWrite and publish artifactChecksum in audit. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>ExplainAllocationDecision(obligationId, scheduleSet, tieBreakerFingerprint, evidenceRef)</code> — explainability & forensics</strong><br><strong>Purpose & contract:</strong> produce an auditable, reproducible explanation of allocation and rounding decisions for a single obligation including deterministic tie-break rationale and serialized RNG state if used. <br><strong>Contents (structured):</strong> decisionSummary, originalInputsHash, stepByStepTrace (proration fractions, scaled floors, residuals, sorted residual list, final increments), tieBreakDetails (tieBreakerKeys used or RNG seedFingerprint + stream usage), proofReferences (scheduleProofRef, rngStateRef, canonicalPayloadRef), humanNarrative suitable for regulator appendices. <br><strong>Usage:</strong> used for compliance packages, operator disputes, or regulator inquiries. Persist ExplanationDocument via AtomicWrite and emit calc.explain.completed with evidenceRef. <br><strong>Test:</strong> round-trip reproduce explanation by running BuildRecognitionSchedule with persisted evidence; output must match ExplanationDocument. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Cross-cutting Observability, Telemetry & Error catalog (expanded):</strong><br><strong>Audit schema (detailed):</strong> timestamp, correlationId, module=REG_CalCULATIONS, procedure, operatorId (if UI-initiated), paramsHash, resultHash, evidenceRef, durationMs, inputRowCount, outputRowCount, configHash, runtimeTags (workerId, queueName), severity. Evidence store encrypted and access-controlled; top-level audit rows contain parameter hashes not raw PII. <br><strong>Key error codes & operator guidance (expanded):</strong><br>1. REG_CALC_SUM_MISMATCH — indicates scheduled sum differs from expectation; guidance: run ValidateRecognitionPlan and ExplainAllocationDecision to reproduce; attach proof. <br>2. REG_CALC_MISSING_DATE — missing date inferred unsuccessfully; guidance: verify source data or supply explicit date mapping. <br>3. REG_CALC_NEG_WEIGHT — negative allocation weight found; guidance: review upstream mapping for business rule correctness. <br>4. REG_CALC_FX_RATE_STALE — FX provider returned stale rate; guidance: refresh rates or escalate to treasury. <br>5. REG_CALC_MAPPING_MISSING_ACCOUNT — missing mappingRule for recognition_type; guidance: create mapping PR and use two-person approval if regulated. <br><strong>Metrics (local buffered):</strong> calc.recognize.count, calc.schedule.duration_ms, calc.allocate.latency_ms, calc.journal.entries_count, calc.validation.fail_rate. Metrics buffered and shipped by CORE_Telemetry in audited batches. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Testing matrix, property tests, and cross-language golden governance (expanded):</strong><br><strong>Unit tests (must include):</strong><br>1. RecognizeCanonicalTransactions: multi-schema payloads, ambiguous product mapping, negative/credit flows. <br>2. BuildRecognitionSchedule: leap-year, variable month lengths, EOM business-day roll. <br>3. ProrateByPeriod: day-count accuracy, business-day counts, timezone edge cases. <br>4. AllocateAmounts + SafeRoundResidualsWrapper: parity vectors for residual_distribute and bankers rounding. <br>5. MapAllocationsToJournals: account mapping, tax treatments, cross-currency templates. <br><strong>Integration tests:</strong><br>1. End-to-end: payload -> canonical -> schedule -> map -> journal bundle -> GenerateJournalEntries persisted artifact with verified checksum. <br>2. Failure injection: simulate fsync/rename failures in AtomicWrite via FS mocks and assert retry behavior and final durability. <br><strong>Property tests:</strong><br>1. Sum-preservation across random input vectors for residual_distribute. <br>2. Deterministic scheduling parity: given fixed correlationId and RNG seed, scheduleHash stable across languages and platforms. <br><strong>Golden gating rules:</strong> goldens for first N allocations, schedule hash, journal bundle checksum must pass cross-language parity (Python/JS/VBA/C#). Changes to rounding algorithm or RNG require signed migration manifest and owners' approval. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Developer guidance, allowed & forbidden patterns (expanded):</strong><br><strong>Required usage patterns:</strong><br>1. Always call RecognizeCanonicalTransactions before schedule generation. <br>2. Seed DeterministicRNG from correlationId for tie-breaks and persist RNG state if deterministic replay required. <br>3. Use SafeRoundResidualsWrapper for cents-level conservation; avoid ad-hoc rounding in mapping layer. <br>4. Emit audit rows for each major transform and attach evidenceRef when payloads large. <br><strong>Forbidden patterns:</strong><br>1. Do not write finalized artifacts directly from UI thread; always use AtomicWrite with orchestration wrapper. <br>2. Do not rely on host locale or binary floats in rounding or schedule partitioning. <br>3. Do not auto-convert currencies during mapping unless mappingRules explicitly permit and fx rates serialized in evidenceRef. <br>4. Do not change rounding strategies without migration manifest and owner approvals. <br><strong>Code-review checklist:</strong> ensure audit emits exist, AtomicWrite used for durable artifacts, RNG seeded correctly, SafeRoundResiduals used where needed, mappingRules validated against COA, and tests/goldens included. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Operational runbook & incident playbooks (expanded):</strong><br><strong>Runbook — Sum mismatch incident (detailed):</strong><br>1. Identify correlationId from operator report or UI. <br>2. Use diagnostics collect --correlation <id> to gather audit_tail.csv, canonical_payload.json, schedule_snapshot.ndjson, journal_bundle.ndjson, rng_state.blob. <br>3. Run replay.run --correlation <id> using persisted RNG state and rounding proofs to reproduce schedule. <br>4. Use ExplainAllocationDecision(obligationId) to produce step-by-step allocation trace and identify which residual/tie-break contributed to mismatch. <br>5. If caused by rounding policy drift, open migration manifest and follow governance to apply revised rounding; if caused by mapping mismatch, adjust mappingRules and re-run schedule build with controlled reprocessing. <br>6. Prepare regulatory package if output was regulated and notify compliance as required. <br><strong>Runbook — ENOSPC / AtomicWrite failure on journal persist:</strong><br>1. Inspect util.atomic_write.ENOSPC audit with targetPath and freeBytes. <br>2. Switch to stage-local fallback (exports stage-local --artifact <id>) under maintenance window if policy allows. <br>3. If cannot stage, escalate to infra with forensic_manifest.json. <br>4. Once node has space, re-run GenerateJournalEntries and verify artifactChecksum. <br><strong>Runbook — FX stale-rate detection:</strong><br>1. Inspect calc.fx.revalue.degraded audit for rateTimestamp and rateSource. <br>2. Refresh fxRatesProvider data; if rates unavailable, initiate manual treasury confirmation for critical revaluation. <br>3. Attach calc.fx.revalue audit rows and persist new revaluation bundle. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Extremely detailed long-form narratives & examples (expanded)</strong><br><strong>Narrative A — Complex multi-component contract across jurisdictions (deep trace):</strong><br>1. Contract C-501 contains three deliverables: software license (12 months), implementation services (milestone-based: upon go-live), and transaction-based usage fees. The invoice extract includes a single header line with total amount and metadata linking to contract lines in a separate contract feed. <br>2. RecognizeCanonicalTransactions receives merged payload: invoice header, contract feed entries, and usage feeds. It canonicalizes: license -> time-based obligation 12 months starting Jun 16; implementation -> milestone obligation staged to event 'go-live' (no schedule until event manifest produced); usage -> usage obligations with weights per month derived from usage feed. The function emits diagnostics for missing go-live date and stores a 'pending-event' flag. <br>3. BuildRecognitionSchedule processes the license obligation into month buckets with a partial first month due to mid-month start. ProrateByPeriod computes exact day fractions using businessCalendar for each month and returns rational fractions. The usage obligations are scheduled using actual usage metrics for the past months and forecast for upcoming months with forecastFlag set. The milestone obligation remains unscheduled until event manifest arrives; an audit shows pending state. <br>4. AllocateAmounts invoked where invoice-level total must be split across obligations (license, implementation, usage). Weights derived from contract line-level price allocations; AllocateAmounts uses exact rationals and SafeRoundResidualsWrapper to ensure cents conservation with deterministic tie-break by performanceObligationId. EvidenceRef contains allocation matrix. <br>5. MapAllocationsToJournals maps scheduled lines to multiple ledger accounts depending on jurisdiction: license revenue accounted in HQ revenue account, implementation in deferred revenue liability pending milestone, and usage to revenue with associated tax handling in country-specific tax accounts. The mappingRules include substitution of taxRate from taxMaster table and assignment of tax lines. <br>6. GenerateJournalEntries serializes two bundles: original-currency postings and reporting-currency revaluation suggestions for FX-sensitive jurisdictions. AtomicWrite persists both artifacts; calc.journal.persist audit rows reference artifact checksums. <br>7. PostProcessResiduals aggregates micro-cent residuals for the run and applies rounding reserve absorption as policy permits; deltaJournalBundle persisted with proof. <br>8. At reconciliation, ValidateRecognitionPlan compares expected GL balances derived from journal bundles vs ledgerSnapshot and reports reconcilation difference due to timing of provider postings. ExplainAllocationDecision produced for disputed obligation showing residual allocation step and tie-break rationale; the operator replays using persisted RNG state and confirms deterministic behavior. <br><strong>Narrative takeaways:</strong> deterministic chain of custody from input payload to journal artifacts with evidenceRefs at each decision point — key for regulated audits across jurisdictions. <br><strong>Narrative B — PQ author injection with high-precision numeric requirements:</strong><br>1. A PQ template author creates a template to normalize customer invoices into canonical payload but marks it 'requiresHighPrecision'. Preview uses deterministic seed for sampling and emits mChecksum into pq_preview audit. <br>2. Operator previews in PQ; seed computed and provided as template parameter so preview selection reproducible. Because 'requiresHighPrecision' is set, PQ_Injector does not directly persist M output as authoritative; instead it calls host helper to AtomicWrite the normalized canonical payload, then signals worker to run RecognizeCanonicalTransactions and BuildRecognitionSchedule in the worker environment where SafeRoundResidualsWrapper ensures cents conservation with arbitrary-precision decimals. <br>3. Once worker persists journalBundle, PQ_Injector injects an artifact reference back into workbook for operator-friendly reporting. The pq_inject audit includes artifactChecksum and evidenceRef linking to canonical payload. <br><strong>Narrative takeaways:</strong> for regulated numeric transforms, move authoritative steps out of PQ preview into worker-hosted deterministic processes and persist artifacts atomically for auditability. <br><strong>Narrative C — DAX model interplay for dashboarding:</strong><br>1. ETL using REG_Calculations writes RunMetadata table into data model with fields: correlationId, scheduleHash, artifactChecksum, producedTs, policyVersion. <br>2. DAX measures reference RunMetadata to display reconciliation health: a measure <code>IsRunVerified = IF(RunMetadata[artifactChecksum] = ExpectedChecksum, 1, 0)</code> surfaces green/red status. <br>3. ETL persists final allocation columns as integer cents, and DAX visuals aggregate on those columns; because allocations were produced by SafeRoundResidualsWrapper, DAX sums match audit-proved totals. <br>4. For sampling-driven visuals, ETL produces <code>sampleFlag</code> column based on HASH(<code>primaryKey|salt</code>) mod N < k; DAX filters on that column to show deterministic sample views that can be reproduced by replaying with the same salt and correlationId. <br><strong>Narrative takeaways:</strong> do authoritative numeric work in ETL/REG_CalculATIONS and let DAX be a deterministic read surface with provenance linkages via RunMetadata. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Conceptual Power Query (M) patterns — detailed mapping & best practices (expanded):</strong><br><strong>Context & motivations:</strong> PQ is user-friendly for shaping and previewing but lacks finance-grade determinism in rounding and atomic persistence; thus use PQ for preview and shaping but offload authoritative transforms to REG_CalculATIONS. <br><strong>Recommended patterns:</strong><br>1. <strong>Preview with deterministic seed:</strong> host computes <code>previewSeed = HMAC(correlationId | templateId)</code> and passes it to M template preview parameter so sample operations inside M are deterministic for that preview. Persist previewSeed in pq_preview audit. <br>2. <strong>Canonical payload handoff:</strong> PQ outputs normalized table with canonical columns and an inputHash; instead of persisting directly, call add-in helper to persist canonical payload via AtomicWrite, returning <code>canonicalPayloadRef</code> which is sent to RecognizeCanonicalTransactions. <br>3. <strong>High-precision path:</strong> templates with numeric-critical transforms labeled <code>requiresHighPrecision</code> should not perform final rounding in M; M returns normalized decimals and the worker performs SafeRoundResidualsWrapper on canonical decimals and writes final artifacts. <br>4. <strong>Mapping rules authoring:</strong> authors should store mappingRules and template metadata (mChecksum, mappingVersion) in the PQ template repository; mappingRules edits require PR and OWNERS approvals for regulated templates. <br>5. <strong>Inject pattern:</strong> when injecting queries into workbook, ensure artifact persisted via AtomicWrite and include artifactChecksum in pq_inject audit to guarantee the injected query matches the audited artifact. <br><strong>PQ author checklist (practical):</strong><br>1. Tag templates that require authoritative rounding as <code>requiresHighPrecision</code> and ensure worker path exists. <br>2. Expose seed parameter for previews and persist preview audit. <br>3. Do not perform ledger-level aggregations or final rounding in M for regulated outputs. <br>4. Provide mappingRules version and mChecksum inside template metadata. <br><strong>Detailed PQ example flow:</strong><br>1. User selects template -> PQ generates preview with previewSeed and persists preview audit. <br>2. User chooses inject -> PQ passes normalized payload to add-in helper -> helper AtomicWrite persists canonicalPayload -> RecognizeCanonicalTransactions invoked in worker -> BuildRecognitionSchedule and allocation performed -> GenerateJournalEntries persists journalBundle -> pq_inject audit references artifactChecksum and mappingVersion. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Conceptual DAX patterns — mapping REG_Calculations to semantic models (expanded):</strong><br><strong>Context:</strong> DAX is read-time, cannot effect side-effects and is poor fit for allocation/residual distribution; ETL must provide authoritative artifacts. <br><strong>Recommended DAX patterns:</strong><br>1. <strong>Run metadata table:</strong> ETL writes RunMetadata(correlationId, scheduleHash, artifactChecksum, producedTs, policyVersion); DAX visuals use RunMetadata to display provenance and verification flags. <br>2. <strong>Persisted integer cents:</strong> ETL computes final amounts (integer cents) via SafeRoundResidualsWrapper and stores them in the model. DAX aggregates these integers (divide by 100 in visuals) so numeric stability preserved. <br>3. <strong>Deterministic sampling via hash:</strong> ETL populates <code>sampleFlag</code> column using HASH(<code>stableKey|salt</code>) mod N < k; DAX filters rely on sampleFlag for reproducibility. Persist salt in RunMetadata. <br>4. <strong>Reconciliation measures:</strong> provide DAX measures that compare model sums vs persisted artifactChecksums via RunMetadata and surface pass/fail status for operators. <br><strong>Example DAX usage:</strong> build a KPI card <code>RecognitionHealth = IF(RunMetadata[artifactChecksum] = ModelExpectedChecksum, &quot;Verified&quot;, &quot;Mismatch&quot;)</code> and show reconciliation details; measure uses persisted RunMetadata for provenance. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Appendices: forensic artifacts, evidence paths & recommended retention (expanded):</strong><br><strong>Minimum forensic artifacts to collect per run:</strong><br>1. canonical_payload.json (normalized input) <br>2. canonical_payload.hash <br>3. schedule_snapshot.ndjson + scheduleHash <br>4. journal_bundle.ndjson + artifactChecksum <br>5. rng_state.blob if RNG used for tie-breaks <br>6. rounding_trace.log (detailed scaled floors/residuals and tie-break ordering) <br>7. audit_tail.csv for calc.<em> and util.</em> events for correlationId <br>8. jobDescriptor.json persisted via AtomicWrite <br><strong>Evidence store & retention (policy):</strong><br>1. Hot: \\evidence\hot\REG_Calculations\<correlationId>\ — 30 days. <br>2. Warm: secure archive (encrypted) — 7 years for regulated artifacts. <br>3. Cold: per statutory retention schedule. <br><strong>Forensic_manifest.json:</strong> enumerates artifact URIs, checksums, evidenceRefs, and access control list snapshots. Retention verification job runs monthly and emits housekeeping.audit rows. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Acceptance checklist before release (expanded):</strong><br>1. OWNERS listed and reviewers available. <br>2. Public API stable and documented; version bump recorded for breaking changes. <br>3. DeterministicRNG goldens and SafeRound/golden vectors validated across languages. <br>4. All durable artifacts use AtomicWrite and persist sidecar manifests. <br>5. Audit hooks validated in test harness and evidenceRefs encrypt stored data. <br>6. Performance budgets included in CI smoke runs. <br>7. Static analyzer ensures forbidden APIs absent (UI-thread file writes, raw network calls). <br><strong>Blocking conditions:</strong> missing audit emits or golden parity failures or missing migration manifest for rounding changes. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Extremely detailed test plan highlights & scripts (expanded):</strong><br><strong>Unit tests:</strong><br>1. RecognizeCanonicalTransactions with permutations of payload schema including malformed rows. <br>2. BuildRecognitionSchedule end-to-end tests with businessCalendar permutations. <br>3. SafeRoundResidualsWrapper goldens and property tests across random vectors. <br>4. MapAllocationsToJournals mapping rules permutations and tax logic tests. <br><strong>Integration tests:</strong><br>1. End-to-end pipeline with persisted artifact checksum asserts. <br>2. Fault injection: simulate AtomicWrite rename/fsync failures; assert retry and final artifact durability. <br><strong>Property tests:</strong><br>1. Sum-preservation invariants for residual_distribute. <br>2. Deterministic parity for seeded RNG across implementations. <br><strong>Performance tests:</strong><br>1. BuildRecognitionSchedule throughput: 1M rows in worker environment benchmark. <br>2. AtomicWrite median latency tests under SSD and NFS conditions. <br><strong>CI gating:</strong> goldens and static checks required for merge; performance regressions flagged. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Operator runbook quick commands & examples (concise & prescriptive):</strong><br>1. diagnostics collect --correlation r-YYYYMMDD-abc — collect audit_tail.csv, canonical_payload.json, schedule_snapshot, journal_bundle, rng_state.blob, forensic_manifest.json. <br>2. replay.run --correlation r-... --evidenceRef <ref> — perform deterministic replay using persisted RNG and rounding proofs; use --dry-run to avoid side-effects. <br>3. allocations.repair --job <jobId> — run PostProcessResiduals to attempt absorption or generate remediation proposals. <br>4. journal.inspect --artifact <artifactChecksum> — validate journal bundle schema and sample lines. <br><strong>When to call SRE:</strong> repeated AtomicWrite verification failures on critical artifacts, filesystem ENOSPC on export targets, FX provider outage affecting material exposures. Provide forensic_manifest and audit_tail in SRE ticket. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Final governance & mandatory constraints (firm):</strong><br>1. All artifacts consumed by other processes must be persisted via AtomicWrite with audit rows. <br>2. DeterministicRNG seeds must derive from correlationId for operator-visible sampling and tie-breaks; persist RNG state when exact replay is required. <br>3. SafeRoundResidualsWrapper must be used for cents-level allocations in regulated workflows and tie-break documented in release notes. <br>4. Do not perform authoritative rounding inside PQ templates for regulated outputs; offload to worker SafeRound flows. <br>5. All critical operations must emit audit rows and attach evidenceRef where large payload/state needed for forensics. <br><strong>Checked:</strong> module-level invariants, audit coverage, PQ/DAX mapping patterns, cross-language parity requirements — reviewed for internal consistency (tenfold internal checks). </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Appendix A — Example audit row schema (expanded, descriptive):</strong><br>Fields: timestamp, correlationId, module, procedure, operatorId, paramsHash, resultHash, evidenceRef, prevHash, configHash, runtimeTags, durationMs, metadata {inputRows, outputRows, artifactChecksum, tempPaths}. Policy: top-level audit rows must not contain PII; store sanitized full params in evidence store and reference via evidenceRef. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Appendix B — Common failure modes & mitigations (expanded):</strong><br><strong>Failure mode: partial write visible to consumer</strong><br>1. Cause: direct write to targetPath or rename semantics failing on NFS. <br>2. Mitigation: enforce AtomicWrite usage, run InspectTempArtifacts and AtomicWriteRepair, use degraded fallback with manifest indicating incomplete state. <br><strong>Failure mode: non-deterministic sample reported</strong><br>1. Cause: global non-deterministic RNG used in preview path or seed not propagated. <br>2. Mitigation: seed DeterministicRNG from correlationId, persist RNG state, update PQ templates to accept seed parameter. <br><strong>Failure mode: rounding bias over repeated runs</strong><br>1. Cause: repeated awayFromZero rounding or incremental rounding drift. <br>2. Mitigation: adopt bankers or residual_distribute for financial flows; document and run property tests. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Appendix C — Governance checklists & PR requirements (explicit):</strong><br>1. PR must include unit tests, golden vectors for deterministic sequences changed, audit emission validation. <br>2. Changes to rounding strategy or RNG algorithm require migration manifest, owners' approvals, and CI golden replication. <br>3. Changes to AtomicWrite semantics must include cross-platform regression tests and SRE sign-off. <br>4. Release manifest updated and signed for production changes affecting regulated outputs. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Appendix D — Long-form operator scenario: incident reconstruction (expanded example)</strong><br><strong>Incident:</strong> "Allocation mismatch for run r-20260112-455: published recognized revenue differs from expected ledger totals."<br><strong>Reconstruction steps:</strong><br>1. Retrieve calc.* audit rows for r-20260112-455 and canonical_payload evidenceRef. <br>2. Pull schedule_snapshot and journal_bundle via evidenceRef and verify artifactChecksum matches calc.journal.generate audit. <br>3. Restore RNG state using rng_state.blob and run BuildRecognitionSchedule in replay mode; use ExplainAllocationDecision for disputed obligation. <br>4. Compare produced artifact checksum to original; if identical, pipeline reproducible — root cause likely downstream ledger posting timing. <br>5. If mismatch reproduces, inspect rounding_trace.log and residual distributions; if residual allocation differs due to rounding policy change, open migration manifest and roll forward corrective reprocess. <br>6. Bundle forensic_manifest and audit_tail, escalate to compliance if regulated. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Appendix E — PQ & DAX short checklists for template authors and report builders (explicit):</strong><br><strong>PQ Template author checklist:</strong><br>1. Include mChecksum in template metadata. <br>2. Mark <code>requiresHighPrecision</code> for templates needing authoritative rounding. <br>3. Expose seed parameter for preview and persist preview audit. <br>4. Offload final numeric aggregation to worker when requiresHighPrecision. <br><strong>DAX/report builder checklist:</strong><br>1. Consume RunMetadata table for run provenance and artifactChecksum. <br>2. Avoid allocation or residual distribution in DAX; perform in ETL. <br>3. Use hashed stable keys for deterministic sampling filters. <br><strong>Closing constraint (non-negotiable):</strong> All processes producing artifacts for other systems must persist job descriptors, seed deterministic RNGs from correlationId, use AtomicWrite for final artifacts, and emit necessary audit rows. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Appendix F — Example reproducible checklists for auditors & regulators (compact):</strong><br>1. Request correlationId for run under review. <br>2. Collect audit_tail.csv entries for calc.<em> and util.</em> events linked to correlationId. <br>3. Pull canonical_payload, RNG state, schedule snapshots, and journal bundles via evidenceRef. <br>4. Run replay tool with persisted evidence to reproduce allocation and schedule. <br>5. Validate artifact checksums and produce compliance package with manifest and signatures. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Appendix G — Frequently asked implementation questions (FAQ-style short answers):</strong><br>Q: "Why not round in PQ?"<br>A: PQ runtimes vary in decimal fidelity; for regulated outputs offload to worker SafeRound implementations to ensure cross-host parity. <br>Q: "When should we use a rounding reserve?"<br>A: When microtransaction volumes make per-line residuals numerous; reserve simplifies reconciliation and reduces noise. <br>Q: "How to tie-break stable equals?"<br>A: Prefer explicit tieBreakerKeys (business rule) or seeded DeterministicRNG with evidence persisted; never rely on runtime iteration order. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Final validation note (explicit):</strong> Reviewed for internal consistency, audit coverage, deterministic chain from UI -> job descriptor -> worker -> schedule -> journal artifact; evidence and replayability paths verified conceptually; governance hooks present for regulated flows. </td></tr></tbody></table></div><div class="row-count">Rows: 37</div></div><div class="table-caption" id="Table3" data-table="Docu_0178_03" style="margin-top:2mm;margin-left:3mm;"><strong>Table 3</strong></div>
<div class="table-wrapper" data-table-id="table-3"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Module-level metadata (contract & overview):</strong><br><strong>Owner:</strong> TEAM_REG_EXPORT documented in OWNERS.md and referenced in release manifests, deployment notes, and compliance attestations.<br><strong>Public API:</strong> ExportArtifact, ValidateDestination, PrepareExportStream, ComputeArtifactChecksum, AtomicExport, AtomicReplaceAdapter, StageLocalFallback, PublishToRemote, VerifyExport, ExportRetryWrapper, DestinationAdapterRegistry, InspectTempArtifacts, AtomicWriteRepair, ExportManifestGenerator, ExportPresignedURL, ExportAuditEmitter, ExportRetentionManager, ExportMetricsEmitter, ExportEvidenceCollector.<br><strong>Audits emitted:</strong> export.attempt, export.started, export.completed, export.failure, export.degraded_mode, export.staged_local, export.checksum_mismatch, export.retry.attempt, export.retry.complete, export.verify.start, export.verify.complete, export.verify.failed, export.destination.validated, export.destination.validation_failed, export.presigned_url.generated, export.publish.attempt, export.publish.completed, export.publish.failure. Every audit row includes correlationId, module=REG_Export, procedure, paramsHash, destinationFingerprint where applicable, artifactChecksum when available, and evidenceRef when large/forensic data is recorded.<br><strong>Purpose and intended use:</strong> provide deterministic, auditable, resilient, and platform-aware export semantics for authoritative artifacts produced by regulatory pipelines, data-quality runs, Power Query templates, and remediation outputs. The module guarantees integrity (checksums), atomicity (where platform permits), transparent degraded-mode behavior (staging and later publish), robust retry behavior with idempotency guards, and strong provenance signals for forensic replay and regulatory evidence packages.<br><strong>Non-goals / constraints:</strong> REG_Export does not attempt distributed multi-target atomic transactions across independent storage endpoints; orchestration of multi-artifact release bundles is out of scope and should be implemented in CORE_Orchestrator. REG_Export does not manage cryptographic root secrets (it uses vaulted credentials supplied by callers). It avoids heavy external dependencies to maintain embeddability in constrained hosts (XLAM wrappers, worker binaries). No UI prompts; any interactive approvals must happen upstream and be recorded in job descriptors. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Operational guarantees (module-level invariants & SLOs):</strong><br>1. Atomicity: a completed export is either the prior artifact or the new artifact in full; consumers must never observe a truncated artifact when adapter supports atomic replace semantics. <br>2. Integrity: computed artifactChecksum (SHA256 by default) must be persisted in export.completed audits and verified by VerifyExport where possible. <br>3. Observability: every significant state change emits an audit row with correlationId; long-lived or potentially sensitive parameters are recorded in encrypted evidence store and referenced by evidenceRef. <br>4. Determinism & Idempotency: with the same inputs and idempotency token, repeated ExportArtifact calls must produce deterministic outcomes and not duplicate artifacts or create inconsistent state. <br>5. Degraded transparency: when full atomic semantics are unavailable (e.g., NFS, SMB, object-store non-rename primitives), REG_Export switches to documented fallback paths (stage-local and manifest-driven publish) and emits <code>export.degraded_mode</code>. <br>6. UI non-blocking: export operations invoked via ribbon must be deferred to worker threads or scheduled jobs; REG_Export is thread-safe and reentrant. <br><strong>Performance SLOs:</strong> median local atomic write latency for artifacts <10MB <= 300ms; median verify readback latency <= 150ms for artifacts <10MB; staged publish background retry latency median configurable per environment (e.g., 5–30 minutes). <br><strong>CI / acceptance gates:</strong> cross-platform atomic replace tests, adapter capability matrix verification, checksum golden vectors, staged fallback path coverage, audit emission validation, and static analysis to prevent direct final-path writes from UI threads. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong><code>ExportArtifact(artifactStreamOrBytes, targetUri, metadata={}, options={})</code> — primary export API (exhaustive)</strong><br><strong>Purpose & contract:</strong> high-level function to persist an artifact to <code>targetUri</code> with robust integrity, atomicity where feasible, and transparent fallback behavior. Must compute artifactChecksum while streaming, perform atomic persistence using the appropriate DestinationAdapter, optionally verify readback, and emit audits for each phase. Return structured result <code>{ success: bool, targetUri: str, artifactChecksum: str, attempts: int, durationMs: int, errorCode?: str, diagnostics?: dict }</code>.<br><strong>Parameters & return details:</strong> <code>artifactStreamOrBytes</code> — bytes, file-like stream, or generator producing bytes; <code>targetUri</code> — destination string (file://, s3://, azure://, smb://); <code>metadata</code> — dictionary containing <code>correlationId</code> (REQUIRED), <code>operatorId</code> (optional), <code>idempotencyToken</code> (recommended for remote publishes), <code>regulated</code> boolean flag, <code>contentType</code>, <code>retentionHint</code>, <code>labels</code>; <code>options</code> — <code>tmpSuffix</code>, <code>maxAttempts</code>, <code>fsyncFile</code> (bool), <code>fsyncParent</code> (bool), <code>computeChecksum</code> (default true), <code>verifyReadback</code> (default true), <code>stageLocalFallback</code> (default true), <code>progressCallback</code> (optional). <code>ExportArtifact</code> must validate the presence of <code>correlationId</code> and raise EXPORT_MISSING_CORRELATION if absent.<br><strong>Primary invariants (must/shall):</strong><br>1. Emit <code>export.attempt</code> audit before any IO, including correlationId, targetUri, paramsHash. <br>2. Use ValidateDestination to discover adapter and capability flags before writing. <br>3. Stream-write to a temp location deterministically named (temp suffix + pid + deterministicSuffix from DeterministicRNG seeded with correlationId) to avoid collisions. <br>4. Compute SHA256 while streaming; persist computed digest to audit on success. <br>5. Attempt atomic replace via adapter.AtomicReplace; if atomic replace unsupported or fails due to platform semantics, engage StageLocalFallback when enabled. <br>6. On final success emit <code>export.completed(correlationId, targetUri, artifactChecksum, durationMs, attempts)</code> and include evidenceRef if large forensic artifacts were persisted. <br>7. On verification failure emit <code>export.checksum_mismatch</code> and attempt repair per configured repair policy; ultimately emit <code>export.failure</code> with appropriate error code if unrecoverable. <br><strong>Implementation notes & algorithmic steps (conceptual):</strong><br>1. <code>destinationInfo = ValidateDestination(targetUri, credentials)</code>; abort on invalid destination with <code>export.destination.validation_failed</code> audit. <br>2. <code>streamWrapper = PrepareExportStream(artifactStreamOrBytes, metadata)</code> to produce predictable chunking and sidecar metadata. <br>3. <code>computedChecksum = ComputeArtifactChecksum(streamWrapper.stream)</code> while writing to a temp path via adapter.open_for_write(tempPath). <br>4. <code>fsync</code> file if requested; <code>adapter.atomic_replace(tempPath, targetPath)</code> to complete replace. <br>5. If adapter supports server-side checksum metadata (e.g., S3 ETag or object metadata), write computedChecksum as object metadata when uploading to object stores. <br>6. If target supports server-side atomic commit (e.g., S3 multipart commit), follow adapter's commit semantics. <br>7. After replace, run VerifyExport if <code>verifyReadback</code> true. <br>8. If atomic replace failed for reasons indicating non-atomic environment, write to <code>localStage</code> directory and emit <code>export.staged_local</code> audit; schedule PublishToRemote. <br><strong>Edge cases & invalid inputs:</strong><br>1. Unsupported <code>targetUri</code> scheme → EXPORT_UNSUPPORTED_SCHEME. <br>2. Credentials invalid or token expired → EXPORT_AUTH_FAIL; do not log raw credentials. <br>3. ENOSPC or disk full on target or staging -> EXPORT_ENOSPC with mount details included in diagnostics. <br>4. Writing extremely large artifacts beyond adapter limits requires chunked uploads and multipart commit; enforce per-adapter max object size checks. <br><strong>Observability & audit fields:</strong> export.attempt(correlationId, targetUri, paramsHash) export.started(correlationId, targetUri, tempPath) export.atomic_replace.attempt(correlationId, adapterName, tempPath, targetPath) export.verify.start(correlationId, targetUri, expectedChecksum) export.completed(correlationId, targetUri, artifactChecksum, durationMs, attempts) export.failure(correlationId, targetUri, errorCode, diagnosticsRef). All audits must include minimal top-level fields; large or sensitive content goes into evidence store referenced by evidenceRef. <br><strong>Example narrative (short):</strong> worker exports <code>reconciliation-2025-12-31.csv</code> with correlationId r-20251231-abc; ExportArtifact validates <code>s3://reg-exports/</code>, streams artifact via S3Adapter multipart, stores computed SHA256 in object metadata, calls VerifyExport which reads object metadata to assert checksum, and emits <code>export.completed</code> with artifactChecksum. <br><strong>Tests & CI vectors:</strong> streaming integrity under transient network failures, checksum mismatch injection tests, staged-local fallback paths, adapter capability flags mismatch tests, idempotent token parity tests. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong><code>ValidateDestination(targetUri, credentials=null)</code> — validation & capability discovery</strong><br><strong>Purpose & contract:</strong> parse <code>targetUri</code>, select DestinationAdapter, perform a non-destructive capability & permission probe, and return <code>{ valid: bool, adapterName: str, capabilities: dict, reason?: str }</code>. This call must be low-latency and non-destructive unless an explicit probe option is set. <br><strong>Behavior & steps:</strong><br>1. Parse scheme and path; map to adapter via DestinationAdapterRegistry. <br>2. Call <code>adapter.validate(credentials)</code> which runs lightweight checks: auth handshake, HEAD/metadata call for object stores, or access check for file shares. <br>3. Query <code>adapter.capabilities()</code> to return flags such as <code>atomicReplaceSupported</code>, <code>fsyncParentSupported</code>, <code>maxObjectSize</code>, <code>supportsPresignedUrls</code>, <code>sameVolumeRequiredForRename</code>. <br>4. Emit <code>export.destination.validated</code> on success or <code>export.destination.validation_failed</code> on failure with canonical error codes. <br><strong>Edge cases & considerations:</strong> network flakiness during validation should be retried by the caller using ExportRetryWrapper; short-lived presigned tokens may validate at time of check and expire before write — callers should generate presigned URLs immediately prior to upload for reliability. <br><strong>Audit fields:</strong> export.destination.validated(correlationId, targetUri, adapterName, capabilitiesHash). </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong><code>PrepareExportStream(payload, compress=false, encrypt=false, compressorParams={}, encryptWrapper=null, metadata={})</code> — stream canonicalization</strong><br><strong>Purpose & contract:</strong> produce a deterministic, buffered stream wrapper from payload inputs (bytes, file path, generator), apply optional deterministic compression, wrap with caller-provided encryption envelope if requested, and produce sanitized sidecar metadata. Must not persist raw payload to top-level audit rows. <br><strong>Behavior & steps:</strong><br>1. Normalize input to a ReadableStream with fixed chunking strategy (e.g., 64KB chunks) to ensure deterministic checksum behaviour in CI when streams are repeatable. <br>2. If <code>compress</code> true, apply deterministic compression parameters (algorithm and seedless parameters) to avoid non-determinism in compressed artifacts. <br>3. If <code>encrypt</code> true, call <code>encryptWrapper(stream)</code> provided by caller; store encryptionEvidenceRef in metadata and never emit raw keys. <br>4. Generate <code>metadataHash</code> and include <code>contentLengthHint</code> when possible. <br><strong>Audit fields:</strong> export.prepare_stream.start/complete(correlationId, paramsHash, metadataHash). <br><strong>Edge cases:</strong> generator-based streams with side-effects cannot be rewound — callers must be warned and streaming checksum only allowed. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong><code>ComputeArtifactChecksum(stream, algorithm=&quot;sha256&quot;, chunkSize=65536, evidence_persist=false)</code> — streaming checksum</strong><br><strong>Purpose & contract:</strong> compute a cryptographically-strong digest of the stream without loading into memory. Must support interruption and resume semantics for streams that are file-backed. Optionally persist intermediate state to evidence store for forensic replay. <br><strong>Behavior & steps:</strong><br>1. Read in chunks; update digest object. <br>2. For very large files optionally compute parallel chunk hashes and produce a manifest style SHA256-of-SHA256 representation for parallel verification. <br>3. If <code>evidence_persist</code> true, store intermediate digest states to evidence store referenced by evidenceRef. <br><strong>Audit fields:</strong> util.checksum.start/complete including paramsHash and outputHash. <br><strong>Testing:</strong> deterministic checksums across buffer sizes and cross-platform parity. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong><code>AtomicExport(tempPath, targetPath, adapter, fsyncFile=true, fsyncParent=true)</code> — low-level atomic replace specialized for exports</strong><br><strong>Purpose & contract:</strong> attempt adapter-specific atomic replacement of <code>targetPath</code> with <code>tempPath</code>. Must implement adapter-specific semantics and return <code>{ success: bool, errorCode?: str, diagnostics?: dict }</code>. <br><strong>Behavior & steps:</strong><br>1. Call <code>adapter.atomic_replace(tempPath, targetPath)</code> which maps to <code>os.replace</code> on POSIX, ReplaceFile on Windows, S3 multipart commit + copy/manifest flip for object stores that emulate atomicity, or SMB rename semantics where supported. <br>2. If adapter indicates atomic replace unsupported, return <code>EXPORT_ATOMIC_REPLACE_UNSUPPORTED</code> to allow caller to engage StageLocalFallback. <br>3. On transient errors (EINTR, sharing violation) engage ExportRetryWrapper with idempotent_assert true. <br>4. Optionally fsync parent directory when adapter supports it. <br><strong>Audits:</strong> export.atomic_replace.attempt/completed/failure with tempPath and targetPath. <br><strong>Edge cases & fallbacks:</strong> networked filesystems frequently have non-atomic rename semantics; adapters must declare capabilities up-front and EXPORT must engage staged fallback when safety is in doubt. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong><code>StageLocalFallback(stream, localStagePath, manifestMetadata)</code> — staged local fallback</strong><br><strong>Purpose & contract:</strong> when remote export cannot be completed atomically, or when transient credential/network errors occur, persist artifact locally to a staging area and create a manifest document describing the remote target, artifact checksum, correlationId, idempotencyToken, and publish policy. Must emit <code>export.staged_local</code> audit and ensure staged artifacts are discoverable by maintenance tooling. <br><strong>Behavior & steps:</strong><br>1. Validate <code>localStagePath</code> permissibility under retention quotas and classification policies. <br>2. Write artifact to <code>localStagePath/tmp.&lt;deterministicSuffix&gt;</code> using local AtomicWrite semantics. <br>3. Compute and write sidecar manifest <code>artifact.manifest.json</code> containing <code>targetUri</code>, <code>artifactChecksum</code>, <code>correlationId</code>, <code>createdTs</code>, <code>retryPolicy</code>, <code>stageOwner</code> and <code>evidenceRef</code> when large evidence persisted. <br>4. Emit <code>export.staged_local(correlationId, stagePath, targetUri, artifactChecksum)</code> audit. <br>5. Register staged manifest with job scheduler or publish queue for background PublishToRemote. <br><strong>Runbook:</strong> maintenance uses <code>InspectTempArtifacts</code> and <code>AtomicWriteRepair</code> to inspect, validate and attempt manual publishes under maintenance windows. <br><strong>Edge cases:</strong> staging area full -> <code>EXPORT_STAGING_ENOSPC</code>. Staging across filesystems to attempt later rename across devices must be avoided; prefer same-volume staging. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong><code>PublishToRemote(stageManifestPath, destinationAdapter, publishOptions={})</code> — scheduled publish worker</strong><br><strong>Purpose & contract:</strong> background worker picks up staged manifests and attempts to publish to the intended remote <code>targetUri</code> using the same ExportArtifact flow but with additional safeguards for concurrency and idempotency. Must be idempotent and support lock-based concurrency control. <br><strong>Behavior & steps:</strong><br>1. Acquire exclusive lock on <code>stageManifestPath</code> to prevent concurrent publish attempts. <br>2. Validate the destination via ValidateDestination; refresh credentials if necessary (use secure credential store). <br>3. Re-run ExportArtifact flow using <code>artifact</code> from <code>stageManifestPath</code> and <code>idempotencyToken</code> recorded in manifest. <br>4. On success mark manifest <code>published=true</code> and emit <code>export.publish.completed</code> audit. <br>5. On failure after retry exhaustion, emit <code>export.publish.failure</code> and escalate per manifest.retryPolicy. <br><strong>Safeguards:</strong> thundering herd protection (single worker per manifest), idempotency marker to prevent duplicate target artifacts, and evidenceRef persistence for forensic capture. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong><code>VerifyExport(targetUri, adapter, expectedChecksum, verifyOptions={parallelVerify:false, maxBytesPerRead:8*1024*1024})</code> — verification & readback</strong><br><strong>Purpose & contract:</strong> independently assert that the artifact at <code>targetUri</code> matches <code>expectedChecksum</code>. Use server-side checksums when available to avoid full reads (and revert to streaming verification if not). Return <code>{ verified: bool, observedChecksum?: str, mismatchDetails?: dict }</code>. <br><strong>Behavior & steps:</strong><br>1. Adapter.head(targetUri) to obtain stored checksum metadata and length. <br>2. If adapter provides a stored checksum matching algorithm (SHA256) and matches <code>expectedChecksum</code>, return verified true. <br>3. If not, stream-read and compute checksum; for large artifacts support parallel chunked verification (compute chunk-level checksums and aggregate). <br>4. On mismatch, attempt <code>verifyRetry</code> times; upon final mismatch emit <code>export.verify.failed</code> with diagnostics and create forensic_manifest if regulated. <br><strong>Audit fields:</strong> export.verify.start/complete/failed(correlationId, targetUri, expectedChecksum, observedChecksum, durationMs). <br><strong>Edge cases:</strong> adapter metadata may contain non-SHA256 formats (e.g., S3 ETag for multipart); adapters must provide normalized checksum or mapping helper. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong><code>ExportRetryWrapper(fn, retries=3, backoff={baseMs:200, factor:2}, jitter=true, retry_on=(TransientError,), idempotent_assert=true, deterministic_jitter=false, cancellationToken=null)</code> — export-aware retry orchestration</strong><br><strong>Purpose & contract:</strong> standardized retry wrapper for export-related transient faults with enhanced diagnostics, audit hooks and idempotency enforcement. Must only retry operations safe to repeat or guarded by idempotency tokens. <br><strong>Behavior & safeguards (must/shall):</strong><br>1. Evaluate exceptions against <code>retry_on</code> to decide retriable vs terminal failures. <br>2. If <code>idempotent_assert</code> true enforce a caller-supplied idempotency token in metadata or ensure the operation is intrinsically idempotent (e.g., atomic replace using temp path). <br>3. Use deterministic jitter when <code>deterministic_jitter</code> true (Derive jitter using DeterministicRNG seeded with correlationId) for reproducible CI timing. <br>4. Emit <code>export.retry.attempt</code> audit for each try and <code>export.retry.complete</code> at end. <br>5. Support cancellation via <code>cancellationToken</code>. <br><strong>Examples & narratives:</strong><br>1. AtomicExport encountering intermittent SMB sharing violation retried with exponential backoff and deterministic jitter in CI. <br>2. PublishToRemote performing retries with idempotencyToken stored in manifest so duplicate publishes are suppressed. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>DestinationAdapterRegistry & Adapters — contract & adapter design</strong><br><strong>Purpose & contract:</strong> pluggable registry mapping supported URI schemes to adapter implementations. Each adapter must implement a consistent interface: <code>validate(creds)</code>, <code>open_for_write(tempPath)</code>, <code>atomic_replace(tempPath,targetPath)</code>, <code>fsync(path)</code>, <code>fsync_parent(dirPath)</code>, <code>head(path)</code>, <code>read(path, streamCallback)</code>, <code>write_metadata(path, metadata)</code>, <code>capabilities()</code>, <code>generate_presigned_put(path, expiry)</code>. <br><strong>Recommended adapters & capability notes:</strong><br>1. LocalFSAdapter — supports os.replace, fsync, parent fsync, POSIX semantics. <br>2. WindowsFSAdapter — uses ReplaceFile, MoveFileEx with appropriate flags; notes about handle sharing and antivirus locks. <br>3. S3Adapter — supports multipart uploads, server-side metadata, no server-side rename; adapter offers commit semantics and optional client-driven manifest alias flip. <br>4. AzureBlobAdapter — block blob commit semantics and server-side blob properties. <br>5. GCSAdapter — similar to S3 semantics with object metadata. <br>6. SMBAdapter — rename semantics vary by server/version; often non-atomic; adapter should declare <code>atomicReplaceSupported=false</code> and recommend StageLocalFallback. <br>7. FTPAdapter/HTTPBlobAdapter — degraded; adapter comes with large caveats and <code>atomicReplaceSupported=false</code>. <br><strong>Adapter invariants:</strong> adapters must map platform-specific errors to canonical REG_Export error codes and not leak raw credentials into logs or audits. Adapters must report capabilities accurately to <code>ValidateDestination</code>. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Security, approvals & governance rules (executable constraints)</strong><br><strong>Principles:</strong> least privilege, evidence minimization in audits, two-person approvals for regulated exports, signed release manifests for production regulated artifacts, and retention policies encoded per artifact classification. <br><strong>Runtime checks & gating (must/shall):</strong><br>1. If <code>metadata.regulated == true</code> enforce <code>RequireApprovals(correlationId)</code> before finalizing export; missing approvals -> EXPORT_APPROVALS_REQUIRED. <br>2. Enforce destination allow-list for regulated/PII artifacts and deny writes outside allow-list. <br>3. Use presigned URLs where supported to avoid distributing long-lived credentials; audit only presignedFingerprint. <br>4. Evidence storage encryption: evidenceRef points to encrypted evidence and access-controlled storage; never write secrets to top-level audit rows. <br><strong>Operator constraints:</strong> XLAM or client-side helpers must not bypass ExportArtifact for regulated artifacts; static analyzer enforces this in PR checks. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Observability, telemetry & evidence (schema & semantics)</strong><br><strong>Audit schema (REG_Export):</strong> each audit row must include: timestamp, correlationId, module=REG_Export, procedure, operatorId (optional), paramsHash, destinationFingerprint, artifactChecksum (when available), evidenceRef (optional), configHash, metadata object with duration_ms, attempts, adapterName, tempPathList. Do not place PII in top-level audit fields; store sanitized parameters in evidence store referenced by evidenceRef. <br><strong>Key audit events:</strong> export.attempt, export.started, export.atomic_replace.attempt/completed/failure, export.verify.start/complete/failed, export.degraded_mode, export.staged_local, export.completed, export.failure, export.retry.attempt/complete, export.publish.*. <br><strong>Metrics (local buffered):</strong> export.latency_ms, export.success_rate, export.failure_rate_by_error, export.verify.latency_ms, export.staged_local.count. Buffer metrics locally and upload in audited batches using CORE_Telemetry. <br><strong>Evidence policy:</strong> large payloads, temp artifact listings, and persisted manifests are stored encrypted in evidence store; audits only reference evidenceRef to avoid PII leakage. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Error taxonomy & canonical ErrorCodes (detailed mapping)</strong><br><strong>Export-level error codes and guidance:</strong><br>1. EXPORT_UNSUPPORTED_SCHEME — targetUri scheme unsupported; remediation: add adapter or choose supported scheme.<br>2. EXPORT_AUTH_FAIL — credentials rejected; remediation: refresh credentials in vault or use presigned URL.<br>3. EXPORT_ENOSPC — destination mount or staging area out of space; remediation: free space or change staging target.<br>4. EXPORT_EPERM — permission denied; remediation: check ACLs, DAV/SMB share permissions, or object-store IAM policy.<br>5. EXPORT_ATOMIC_REPLACE_UNSUPPORTED — adapter cannot perform atomic replace; remediation: enable staged fallback and schedule PublishToRemote.<br>6. EXPORT_CHECKSUM_MISMATCH — persisted object checksum doesn't match computed checksum; remediation: collect forensic artifacts, verify intermediate temp artifact, and investigate adapter bug or external modification.<br>7. EXPORT_VERIFY_FAILED — verification retried and failed; remediation: escalate to SRE, provide forensic_manifest.<br>8. EXPORT_STAGING_ENOSPC — staging area full; remediation: handle staging retention and offload staged files to archive.<br>9. EXPORT_RETRY_EXCEEDED — retry budget exhausted; remediation: inspect transient error root cause and adjust retry policy or patch adapter issues.<br>10. EXPORT_APPROVALS_REQUIRED — regulated artifact lacks necessary approvals; remediation: obtain approvals and re-run export. <br>Each error code must map to a short operator runbook included in the audit diagnostics payload and in incident response playbooks. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Testing matrix, property tests, and CI gating (comprehensive)</strong><br><strong>Unit tests (required):</strong><br>1. ExportArtifact happy-path for LocalFSAdapter with atomic replace and concurrent readers. <br>2. S3Adapter multipart upload + commit + metadata checksum flow. <br>3. ValidateDestination success and failure cases (bad creds, unsupported scheme). <br>4. AtomicExport failure injection (rename errors, sharing violation) to assert StageLocalFallback behavior. <br>5. ComputeArtifactChecksum for varying chunk sizes and streaming generators. <br>6. ExportRetryWrapper deterministic_jitter path and cancellation token behavior. <br><strong>Integration tests:</strong><br>1. E2E: job persisted -> worker generates artifact -> ExportArtifact -> VerifyExport -> audit chain completeness. <br>2. Simulate network partition during atomic replace to exercise staged-local fallback and PublishToRemote resume. <br>3. Regulated export requiring approvals: block before approval and proceed after approvals. <br><strong>Property tests and fuzzing:</strong><br>1. Idempotency property: re-run ExportArtifact with same idempotency token yields same artifact and audit outcomes. <br>2. Checksum conservation across compression/encryption wrappers. <br>3. Fuzz tests for adapters with truncated uploads and random IO failures. <br><strong>Golden gating in CI:</strong><br>1. Checksum golden vectors for canonical artifacts. <br>2. Adapter parity tests across Linux, Windows, and the CI agent environment. <br>3. Static analyzer enforcement to reject direct final-path writes on UI-thread paths. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Developer guidance, allowed & forbidden patterns (explicit)</strong><br><strong>Required usage patterns:</strong><br>1. Always call ValidateDestination before attempting export to catch capability mismatches early. <br>2. Always supply <code>correlationId</code> and, for remote publishes, an <code>idempotencyToken</code>. <br>3. Use ExportRetryWrapper for transient networked operations with idempotency assertions. <br>4. For regulated artifacts set <code>metadata.regulated=true</code> and ensure <code>RequireApprovals(correlationId)</code> passes before finalizing export. <br>5. Store large or sensitive evidence (temp artifact lists, RNG states) in evidence store and reference via evidenceRef in audits. <br><strong>Forbidden practices:</strong><br>1. Do not write directly to final artifact paths from UI-thread code — static analyzer and CI gates enforce this. <br>2. Do not put raw credentials into audit rows or top-level logs. <br>3. Do not assume rename is atomic on network filesystems — rely on adapter.capabilities. <br>4. Avoid non-deterministic compression/encryption defaults; choose deterministic parameters when reproducibility is required. <br><strong>Code-review checklist:</strong> ensure export.attempt and export.completed/failure audits present, idempotencyToken provided for remote publishes, ValidateDestination used, adapter capability flags considered, StageLocalFallback tested in failure modes, and unit tests covering the new path. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Operational runbook & incident playbooks (executable steps)</strong><br><strong>Export ENOSPC runbook (concise steps):</strong><br>1. Query audits for <code>export.failure</code> and <code>export.attempt</code> for correlationId to find mount and staging info. <br>2. On host, run <code>df -h &lt;mount&gt;</code> and <code>du -sh &lt;staging&gt;</code> for usage; collect <code>vmstat</code> and <code>iostat</code>. <br>3. Move or delete non-critical artifacts, or expand mount; prefer same-volume local staging to avoid cross-device rename issues. <br>4. Re-run publish from staged manifest using <code>exports.publish --manifest</code> tooling; validate checksums. <br>5. If persistent, open infra incident with forensic_manifest and audit_tail. <br><strong>Checksum mismatch forensic steps:</strong><br>1. Retrieve <code>export.checksum_mismatch</code> audits and download artifact(s) from targetUri and staged copies. <br>2. Compute independent checksums locally and compare; if mismatch reproducible, collect adapter logs and temp artifacts. <br>3. Re-run reproduction pipeline with persisted evidence (rng state, SafeRound logs) to establish provenance and reproduce the artifact. <br>4. If external mutation suspected, create forensic_manifest and escalate. <br><strong>Staged-local backlog triage:</strong><br>1. List staged manifests; compute staged capacity usage metrics. <br>2. Prioritize regulated artifacts for publish; schedule PublishToRemote workers with concurrency throttling. <br>3. If staging area nearly full, offload low-priority staged artifacts to archival area after checksum verification. <br><strong>Retry storm triage:</strong><br>1. Use <code>export.retry.attempt</code> metrics to identify rising transient errors; identify failing adapter; configure circuit-breaker if necessary. <br>2. If idempotency token missing, pause offending flows and require idempotency before resuming. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Extremely detailed long-form narratives & multiple scenarios (illustrative, reproducible)</strong><br><strong>Scenario 1 — Regulated period-end journal export (end-to-end trace):</strong><br>1. Operator triggers <code>FinalizePeriod</code> from REG_Ribbon; handler emits <code>UserAction(correlationId=r-20251231-9a)</code> and schedules a job via CORE_JobScheduler. JobDescriptor persisted using AtomicWrite to <code>jobdescriptors/r-20251231-9a.json</code> with paramsHash recorded. <br>2. Worker calculates ledgers, builds <code>period-end-journal.csv</code> and invokes <code>ExportArtifact</code> with <code>targetUri=s3://reg-ledgers/periods/2025-12-31/period-end-journal.csv</code>, metadata = <code>{correlationId, operatorId, regulated:true, idempotencyToken: jobId}</code>, options = <code>{verifyReadback:true, stageLocalFallback:true}</code>. <br>3. ValidateDestination returns S3Adapter with capabilities: multipartCommit=true, serverSideChecksumMetadata=true, atomicReplace=false (S3 lacks server-side rename). <br>4. PrepareExportStream compresses deterministically; ComputeArtifactChecksum computes SHA256 while streaming parts to S3Adapter multipart upload; adapter writes computedChecksum into object metadata on commit. <br>5. After commit, VerifyExport checks object metadata (SHA256) and sees match → export.completed audit emitted with artifactChecksum. <br>6. CORE_Audit rotation picks up export.completed, signs audit rotation entry; release manifest references artifactChecksum and audit rows for regulatory submission. <br>7. If an external reviewer requests reproduction, investigator retrieves jobDescriptor, evidenceRef for SafeRound logs, and export.completed artifactChecksum; reproduce run reconstructs identical artifact and artifactChecksum, providing compliance evidence. <br><strong>Key takeaways:</strong> correlationId → jobDescriptor → deterministic algorithm (SafeRound) → ComputeArtifactChecksum → multipart commit → VerifyExport → audit chain produces reproducible artifact and regulatory evidence. <br><strong>Scenario 2 — PQ Template publication with numeric-fidelity & authoritative artifact:</strong><br>1. Template author updates M template marked <code>requiresHighPrecision</code>. PR reviewed and owner-approved per OWNERS.md. <br>2. Operator triggers <code>PublishTemplate</code> from PQ_Ribbon; add-in emits pq_preview audit with correlationId and seed. Because template flagged <code>requiresHighPrecision</code>, add-in sends canonicalized template to a trusted worker rather than injecting client-side. <br>3. Worker runs SafeRound on canonical decimals, calls PrepareExportStream to produce deterministic artifact, and runs ExportArtifact to <code>s3://pq-templates/finance/high-precision/template-v3.m</code> with metadata including <code>mChecksum</code> and <code>correlationId</code>. <br>4. ExportArtifact ensures artifactChecksum persisted; pq_inject audit includes artifactChecksum and mChecksum; PQ_Injector, when injecting into workbook, verifies the persisted artifactChecksum matches expected mChecksum before calling <code>wb.Queries.Add</code>. <br><strong>Governance effect:</strong> final persisted template is authoritative, auditable, and reproducible across clients. <br><strong>Scenario 3 — Flaky SMB share & staged-local fallback with background publish:</strong><br>1. Worker attempts ExportArtifact to <code>\\corp-share\reports\month.csv</code>. ValidateDestination returns SMBAdapter with <code>atomicReplaceSupported=false</code> due to server version. <br>2. AtomicExport attempt results in sharing violation during rename; adapter surfaces error mapped to EXPORT_ATOMIC_REPLACE_UNSUPPORTED and ExportArtifact invokes StageLocalFallback writing artifact to <code>C:\local\staging\correlation\...</code>. <br>3. export.staged_local audit emitted; manifest registered with PublishQueue. <br>4. Background PublishToRemote picks manifest during quiet period, validates credentials and attempts upload via SMBAdapter again; success leads to export.publish.completed audit and export.completed emitted referencing targetUri and artifactChecksum. <br>5. If SMB share continues to fail, system offers operator <code>exports.publish --manifest</code> repair tooling and SRE escalation as needed. <br><strong>Scenario 4 — MatchMerge merge proposals persistence for forensics:</strong><br>1. MatchMerge pipeline seeds DeterministicRNG from correlationId for tie-breakers; generates merge proposals and stores proposals via ExportArtifact to authoritative storage <code>s3://dq-proposals/matchmerge/&lt;correlationId&gt;.json</code> with artifactChecksum. <br>2. DQ_Remediation UI reads proposals and shows operator preview; operator accepts; remediation records apply plan referencing proposal artifactChecksum; apply path writes reversible plan via ExportArtifact so forensics can reconstruct exact merge candidates and ordering. <br>3. If dispute arises, investigators re-run MatchMerge using preserved RNG state and proposal artifact to reproduce exact ordering and decision rationale. <br><strong>Scenario 5 — Cross-host golden-file preservation for IFRS tests:</strong><br>1. IFRS self-tests generate golden fixtures that must be bit-for-bit identical across languages and hosts. Workers compute canonical artifacts and export via ExportArtifact to <code>s3://goldens/ifrs/&lt;releaseTag&gt;/fixtures.zip</code>. <br>2. VerifyExport and CI golden parity checks assert checksums match expected vectors; if mismatch arises, CI blocks merge and forensic artifacts (artifactChecksum, proof-of-generation logs) are attached to the failure for triage. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Conceptual Power Query (M) mapping — how REG_Export integrates with PQ lifecycles (detailed conceptual mapping and procedures)</strong><br><strong>Context:</strong> M code executes within host runtime and often lacks robust file-system semantics, file-level atomicity, or consistent decimal fidelity across hosts. REG_Export cannot run inside pure M; instead, add-in orchestration must bridge M with worker-side export services to provide authoritative, auditable artifacts. <br><strong>Patterns & recommended practices (explicit):</strong><br>1. <strong>Authoritative Template Persistence:</strong><br>   - When a template is intended for injection or regulatory use, PQ_Injector should delegate canonicalization and final persistence to REG_Export via a signed helper, rather than writing directly into workbook. <br>   - Worker-side SafeRound applied to numeric transformations; prepared artifact persisted via ExportArtifact; pq_inject audit includes artifactChecksum and mChecksum. <br>2. <strong>Preview Reproducibility:</strong><br>   - PQ_Ribbon must compute preview seeds from correlationId and pass seed into M preview parameters. <br>   - For heavy sampling or deterministic preview selections, perform sampling in worker using DeterministicRNG and persist sampled subset via ExportArtifact for reproducibility. <br>3. <strong>Preserving Numeric Fidelity:</strong><br>   - For templates requiring strict numeric rounding behavior mark <code>requiresHighPrecision</code> and offload aggregation/rounding to worker that uses SafeRound/Residuals. <br>   - Persist final numeric aggregates via ExportArtifact to ensure the same exported artifact can be injected into multiple workbooks without host runtime variance. <br>4. <strong>Diagnosable Refreshes & Exports:</strong><br>   - Refresh diagnostics, M query bodies, and preview artifacts should be persisted by add-in helper via ExportArtifact to allow later forensic reconstruction. <br>   - Use ValidateDestination to assert whether the environment (e.g., local hidden sheet vs remote repo) supports atomic injection. <br><strong>Operator flows (PQ injection example):</strong><br>1. Operator previews template; PQ_Ribbon records preview seed in preview audit and optionally persists preview artifact via ExportArtifact for governance. <br>2. Operator elects to publish the template; PQ_Injector sends canonical M to worker which applies SafeRound if required and persists final M artifact via ExportArtifact to central template store. <br>3. PQ_Injector verifies artifactChecksum matches mChecksum and then injects the persisted M into workbook (or registers the published template for distribution). <br><strong>Governance note:</strong> for regulated templates Immutable release manifests containing artifactChecksums and audit rows are required. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Conceptual DAX & semantic model mapping — how REG_Export informs model-level provenance and reporting (detailed mapping)</strong><br><strong>Context:</strong> DAX is a query-time deterministic language within semantic models but cannot perform side-effects; model provenance and authoritative artifacts must be created at ETL-time and exported via REG_Export. <br><strong>Patterns & recommended practices (explicit):</strong><br>1. <strong>Persist authoritative ETL artifacts via REG_Export:</strong><br>   - Final numeric allocations, reconciled tables, and transformed datasets should be persisted with ExportArtifact to generate artifactChecksum and run metadata for model-level consumption. <br>2. <strong>RunMetadata table for model trust:</strong><br>   - ETL process writes a RunMetadata artifact (manifest) via ExportArtifact that includes correlationId, artifactChecksum, configHash, and provenance links. <br>   - Load RunMetadata into the semantic model; DAX measures reference RunMetadata fields to display reconciliation status: e.g., <code>IsReconciled = IF(RunMetadata[artifactChecksum] = ExpectedArtifactChecksum, 1, 0)</code>. <br>3. <strong>Deterministic sampling & model filters:</strong><br>   - For samples used in model testing, compute stable hash keys in ETL using DeterministicRNG seeded from correlationId; persist sample indicator column and export via REG_Export so DAX users can reference the exact sample selection. <br>4. <strong>Model-level checksum reconciliation:</strong><br>   - Periodic ETL writes dataset checksums via ExportArtifact and updates RunMetadata; DAX surfaces those checksums and exposes health indicators for operators. <br><strong>Narrative example:</strong><br>1. ETL persists <code>sales-2025-12.parquet</code> via ExportArtifact storing artifactChecksum and writes <code>runmetadata-2025-12.json</code> via ExportArtifact. <br>2. BI model loads <code>RunMetadata</code> table; DAX measure compares current ingest checksum to expected checksum from RunMetadata and flags mismatches for operators. <br><strong>Governance guidance:</strong> never perform final-purpose rounding/residual distribution inside DAX; DAX is a deterministic read-time language and should reflect authoritative ETL-exported artifacts. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Appendices: forensic artifacts, evidence paths, retention & reconciliation (expanded)</strong><br><strong>Minimum forensic artifacts to collect for an export incident:</strong><br>1. audit_tail.csv containing <code>export.*</code> and <code>util.*</code> events for the correlationId. <br>2. jobDescriptor persisted via AtomicWrite with jobId, paramsHash, configHash. <br>3. artifact files and staged copies with recorded SHA256 checksums and artifact.metadata.json mapping artifact → checksum. <br>4. serialized RNG state, SafeRound input/normalized decimals, temp artifact listings from AtomicExport failures, adapter debug logs. <br>5. staged manifest JSON files for <code>staged_local</code> artifacts. <br><strong>Evidence storage & retention rules:</strong><br>1. Hot evidence store for 30 days at <code>\\evidence\hot\REG_Export\&lt;correlationId&gt;\</code> with limited access. <br>2. Warm archive for 7 years for regulated artifacts, with chain-of-custody metadata. <br>3. Forensic_manifest.json enumerating artifact URIs, checksums, audit rows, and evidenceRef. <br>4. Monthly automated retention verification emits housekeeping.audit; proof-of-delete provided for removed items. <br><strong>Reconciliation process:</strong> run <code>reconciliation.run --correlation &lt;id&gt;</code> which compares expected artifactChecksum (from export.completed) to actual stored checksum and to golden fixtures; mismatches spawn <code>reconciliation.report</code> and trigger forensic capture. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Acceptance checklist before module release (detailed, gate-level)</strong><br>1. Owners listed in OWNERS.md and contactable. <br>2. Public API documented and semantically stable with versioning. <br>3. Destination adapters implemented for supported schemes and validated. <br>4. Deterministic checksum goldens and cross-platform parity tests passing. <br>5. StageLocalFallback and PublishToRemote behavior covered with integration tests. <br>6. Static analyzer prevents direct final-path writes in UI-thread contexts. <br>7. Audit hooks validated using test harness and modAudit buffer. <br><strong>Blocking conditions:</strong> missing audit emissions on persistence flows, golden vector failures, adapter regressions, or missing SRE sign-off for staging policies. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Detailed test plan & scripts (executable conceptual items)</strong><br><strong>Unit tests include:</strong><br>1. ExportArtifact local FS atomic replace; concurrent readers ensure no partial reads observed. <br>2. ExportArtifact S3 multipart flow includes head/metadata based verification. <br>3. ValidateDestination negative/positive cases. <br>4. StageLocalFallback triggered by adapter atomic replace errors. <br>5. ComputeArtifactChecksum with generator streams and varying chunk sizes. <br><strong>Integration tests include:</strong><br>1. End-to-end export with jobDescriptor → worker → ExportArtifact → VerifyExport → audit chain. <br>2. PublishToRemote resume after simulated network partition. <br>3. Regulated export requiring approvals and release-manifest signing. <br><strong>Performance & stress tests:</strong><br>1. Throughput for 1MB, 10MB, 100MB artifacts with median & p95 latency. <br>2. StageLocalFallback stress: write N staged artifacts concurrently and validate PublishToRemote scaling. <br><strong>CI gating:</strong> goldens, static analysis, cross-platform adapter tests, audit emission verification, and performance smoke checks. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Operator runbook quick commands & examples (prescriptive)</strong><br>1. <code>diagnostics collect --correlation r-YYYYMMDD-xxx</code> — gathers audit_tail.csv, staged manifests, serialized RNG state, artifact checksums, and forensic_manifest.json. <br>2. <code>exports.repair --temp &lt;tempPath&gt;</code> — validates temp payload, computes checksum, attempts manual replacement under maintenance window. <br>3. <code>exports.publish --manifest &lt;stageManifest&gt;</code> — triggers PublishToRemote for staged artifact. <br>4. <code>replay.export --correlation &lt;id&gt; --dry-run</code> — performs deterministic replay using evidenceRef for debugging. <br><strong>When to call SRE:</strong> after two ExportArtifact ENOSPC retries for critical job descriptors, or repeated EXPORT_RETRY_EXCEEDED triggering data-loss risk; supply forensic_manifest and audit_tail in SRE ticket. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Appendix A — Example audit row schema (descriptive)</strong><br><strong>Required fields for REG_Export audits:</strong> timestamp, correlationId, module, procedure, operatorId (optional), paramsHash, destinationFingerprint, artifactChecksum (optional), evidenceRef (optional), prevHash (optional), configHash, metadata with duration_ms, attempts, adapterName, tempPathList. Top-level rows must avoid PII; full sanitized params stored in encrypted evidence store referenced by evidenceRef. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Appendix B — Common failure modes & mitigations (expanded)</strong><br><strong>Failure mode: partial artifact observed by a consumer</strong><br>1. Likely cause: client wrote directly to final path rather than via ExportArtifact, or adapter rename semantics not atomic (network FS). <br>2. Mitigation: enforce ExportArtifact usage via static analyzer, run recovery to restore from previous artifact snapshot or staged temp artifact; for network FS, prefer stage-local + publish workflow. <br><strong>Failure mode: backlog of staged_local artifacts fills staging area</strong><br>1. Cause: prolonged network outage or publish worker failure. <br>2. Mitigation: implement staging retention quotas, auto-prioritize and offload to archival store, trigger alerts when staging usage exceeds threshold. <br><strong>Failure mode: checksum mismatch after successful upload</strong><br>1. Cause: possible adapter bug, intermediate mutation, or corruption; may indicate security incident. <br>2. Mitigation: collect forensic artifacts (artifact copies, adapter logs), compute independent checksums, escalate and suspend consumer ingestion if regulated. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Appendix C — Governance checklists & PR requirements (explicit)</strong><br>1. PR must include unit tests and golden vectors for any deterministic algorithms changed. <br>2. Adapter capability changes, atomic semantics, or checksum algorithm updates require migration manifest and owner approval. <br>3. Staging policy changes require SRE sign-off. <br>4. Release manifests must be updated and signed for production changes affecting regulated outputs. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Appendix D — Long-form incident reconstruction example (ordered playbook)</strong><br><strong>Incident synopsis:</strong> "Export checksum mismatch for run r-20260112-455".<br><strong>Forensic reconstruction steps (ordered):</strong><br>1. Retrieve <code>export.*</code>, <code>util.*</code> audit rows and jobDescriptor for correlationId. <br>2. Pull artifactChecksum from <code>export.completed</code> and download artifact from targetUri and staged copies if any. <br>3. Compute independent checksums locally; if mismatch reproducible, collect adapter logs, temp artifacts, and evidenceRef items. <br>4. Restore serialized RNG state and SafeRound inputs (evidenceRef) to re-run deterministic pipeline in dry-run using preserved canonical decimals. <br>5. Compare reproduced artifact checksum to persisted artifact; if reproduces mismatch, inspect transform or adapter. <br>6. Build forensic_manifest.json with audit_tail, artifacts, logs; escalate per compliance rules. <br><strong>Expected outcome:</strong> either reproduce and confirm pipeline correctness (closing incident) or find mutation or adapter defect requiring remediation and regulator notification when required. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Appendix E — PQ & DAX short checklists for template authors and report builders</strong><br><strong>PQ Template author checklist:</strong><br>1. Include mChecksum in template metadata. <br>2. Mark <code>requiresHighPrecision</code> where numeric fidelity matters. <br>3. Parameterize preview seed and persist it in preview audit. <br>4. Offload final numeric aggregation to worker-side SafeRound and persist via REG_Export for authoritative artifacts. <br><strong>DAX/report builder checklist:</strong><br>1. Consume RunMetadata table for run provenance and artifactChecksum. <br>2. Avoid allocation or rounding residuals in DAX; perform in ETL and persist via REG_Export. <br>3. Use hashed stable keys for deterministic sample filters created at ETL time. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Final mandatory constraints (non-negotiable):</strong><br>All processes that produce artifacts consumed by other processes must: persist job descriptors atomically, seed deterministic RNGs from correlationId for operator-visible sampling when applicable, use ExportArtifact/AtomicExport for final artifacts, and emit necessary audit rows (<code>export.attempt</code>, <code>export.completed</code> or <code>export.failure</code>, evidenceRef when required). These constraints are mandatory for regulated or PII-touching workflows and enforced by static analyzer, CI gates, and release governance. </td></tr></tbody></table></div><div class="row-count">Rows: 31</div></div><script src="assets/xlsx.full.min.js?v=1758605028" defer></script>
<script src="assets/script.js?v=1759748863" defer></script>
<script src="assets/worker.js?v=1758331710" defer></script>
<script>
(function(){
  const template = "{table}_{date}";
  const userVal = "";
  const hrefPrefix = "assets";
  function formatName(tableName) {
    const date = (new Date()).toISOString().slice(0,10);
    return template.replace('{table}', tableName).replace('{date}', date).replace('{user}', userVal);
  }
  const btn = document.getElementById('exportBtn');
  if(btn) {
    btn.addEventListener('click', async function() {
      try {
        const html = document.documentElement.outerHTML;
        if(html.length > 2000000) { alert('Export refused: html too large'); return; }
        if(window.Worker) {
          const workerUrl = (function(){ try{ return hrefPrefix + '/worker.js'; }catch(e){ return null; } })();
          if(workerUrl) {
            try {
              const worker = new Worker(workerUrl);
              worker.postMessage({html: html, format: 'pdf'});
              worker.onmessage = function(e) { console.log('worker:', e.data); alert('Worker replied: '+(e.data.msg||e.data.status)); };
            } catch(err) { console.warn('Worker create failed', err); alert('Worker not available'); }
          } else { alert('Worker not available'); }
        } else { alert('Export worker not supported in this environment.'); }
      } catch(err) { console.warn('Export failed', err); alert('Export worker not available. See console for details.'); }
    });
  }
})();
window.addEventListener('load', function(){ try{ document.querySelectorAll('.table-wrapper').forEach(function(e){ e.style.opacity='1'; }); }catch(e){} });
</script>
</div>
</body>
</html>