<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='utf-8'><meta name='viewport' content='width=device-width,initial-scale=1'>
<title>Tables Viewer v2.1</title>
<style>:root{--main-header-height:56px;} .table-wrapper{opacity:0;min-height:24px;transition:opacity .12s linear}</style>
<link rel="stylesheet" href="assets/style.css?v=1759925496">
<link rel="stylesheet" href="assets/overrides.css?v=1768662828">
</head><body>
<div id="tables-viewer" role="region" aria-label="Tables viewer">
<div id="stickyMainHeader">
<div id="tv-header">
<div><h1>Tables Viewer v2.1</h1></div>
<div style="display:flex;gap:8px;align-items:center;">
<input id="searchBox" class="tv-search" type="search" placeholder="Search" aria-label="Search tables" />
<button id="modeBtn" type="button" onclick="toggleMode()" aria-label="Toggle theme">Theme</button>
<button id="toggleAllBtn" type="button" aria-label="Toggle all" onclick="toggleAllTables()">Collapse All Tables</button>
<button id="copyAllPlainBtn" class="copy-btn" type="button" onclick="copyAllTablesPlain()" aria-label="Copy all tables as plain text">Copy All Tables (Plain Text)</button>
<button id="copyAllTablesBtn" class="copy-all-btn" type="button" onclick="copyAllTablesMarkdown()" aria-label="Copy All Tables (Markdown)">Copy All Tables (Markdown)</button>
<button id="copyAllMdBtn" style="display:none" aria-hidden="true"></button>
<button id="resetAllBtn" type="button" onclick="resetAllTables()" aria-label="Reset All Tables">Reset All Tables</button>
</div></div>
<script>
(function(){
  function ensureDelegation(){
    try {
      var vis = document.getElementById('copyAllTablesBtn');
      var alias = document.getElementById('copyAllMdBtn');
      if(!vis || !alias) return;
      alias.addEventListener = function(type, listener, options){
        vis.addEventListener(type, listener, options);
      };
      alias.removeEventListener = function(type, listener, options){
        vis.removeEventListener(type, listener, options);
      };
      Object.defineProperty(alias, 'onclick', {
        set: function(fn){ vis.onclick = fn; },
        get: function(){ return vis.onclick; },
        configurable: true
      });
      alias.focus = function(){ vis.focus(); };
      alias.blur = function(){ vis.blur(); };
    } catch(e) {
      try{ console && console.warn && console.warn('alias delegation failed', e); }catch(_){} 
    }
  }
  if(document.readyState === 'loading'){
    document.addEventListener('DOMContentLoaded', ensureDelegation);
  } else {
    ensureDelegation();
  }
})();
</script>
<noscript><div style='color:#b91c1c'>JavaScript is disabled. Tables will be shown statically. For large tables enable JS for virtualization.</div></noscript>
<div id="tocBar" role="navigation" aria-label="Table of contents"><ul><li class="toc-item"><a class="toc-link" href="#Table1">Table 1</a></li>
<li class="toc-item"><a class="toc-link" href="#Table2">Table 2</a></li>
<li class="toc-item"><a class="toc-link" href="#Table3">Table 3</a></li>
<li class="toc-item"><a class="toc-link" href="#Table4">Table 4</a></li>
<li class="toc-item"><a class="toc-link" href="#Table5">Table 5</a></li></ul></div></div>
<div class="table-caption" id="Table1" data-table="Docu_0178_01" style="margin-top:2mm;margin-left:3mm;"><strong>Table 1</strong></div>
<div class="table-wrapper" data-table-id="table-1"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by REG_Utilities — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">REG_Utilities — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Module-level metadata (contract & extended overview):</strong><br><strong>Owner:</strong> TEAM_REG_UTILS (recorded in <code>OWNERS.md</code>, cross-referenced in release manifests and deployment notes).<br><strong>Public API (canonical list):</strong> SafeRound, SafeRoundResiduals, AtomicWrite, Retry, DeterministicRNG, ChecksumStream, TempPathFor, FsSyncDir, WriteWithPermissions, serialize_rng_state, restore_rng_state, InspectTempArtifacts, AtomicWriteRepair, AuditEmitUtilEvent, plus small helpers: PathVolumeOf, EnsureSameVolume, EvidenceFingerprint. <br><strong>Audits emitted (representative):</strong> util.startup, util.atomic_write.attempt, util.atomic_write.completed, util.atomic_write.failure, util.atomic_write.repair, util.atomic_write.degraded, util.retry.attempt, util.retry.complete, util.saferound.start, util.saferound.complete, util.saferound.invalid_input, util.saferound.truncated_precision, util.saferound.residuals, util.rng.seeded, util.rng.state_serialized, util.checksum.start, util.checksum.complete. Every audit row includes <code>correlationId</code>, <code>module=REG_Utilities</code>, <code>procedure</code>, <code>paramsHash</code>, and <code>resultHash</code> when appropriate; long/large payload evidence is referenced via <code>evidenceRef</code> rather than embedded in the audit row itself. <br><strong>Purpose and intended use (expanded):</strong> provide deterministic, auditable, cross-platform primitives that higher-level modules depend on for compliance, replayability, and crash-safety. These primitives are intentionally small-surface, side-effect-minimal, and embeddable in host environments ranging from worker processes to signed XLAM helpers. Fast paths must not perform network I/O, must not expose unencrypted secrets, and must avoid behavior that would prevent safe embedding into UDFs or host add-ins. <br><strong>Non-goals / enforced constraints (expanded):</strong> the utilities do not attempt to manage multi-file distributed transactions, do not assume a single consistent distributed filesystem semantics, and do not hold locks indefinitely across process restarts. Secret management, external credential rotation, and long-term archive lifecycle are delegated to specialized modules (modSecurity, CORE_Telemetry, CORE_Archive). All exceptions to these constraints must be explicitly approved by owners and documented in migration manifests. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Operational guarantees (module-level invariants & SLOs — extended):</strong><br><strong>Invariants:</strong><br>1. Determinism: given identical canonical inputs, seeds, and configuration, outputs including checksums, RNG sequences, and rounded allocations must be reproducible across supported platforms and language bindings. <br>2. Audit-anchored state changes: any mutation of durable state must be accompanied by at least one audit row referencing correlationId and essential provenance fields; where payloads are large, a reference (<code>evidenceRef</code>) to encrypted evidence must be attached. <br>3. Crash-safety: atomic persistence primitives (AtomicWrite family) must provide final-on-success semantics — consumers should never observe a truncated artifact (old artifact or full new artifact only). <br>4. Platform-aware fallbacks: when host FS semantics are weak (cross-device rename, NFS semantics), the module must degrade to documented safe sequences and emit <code>util.atomic_write.degraded</code> with rationale and mitigation guidance. <br>5. UI thread safety: none of the utility primitives may block the UI thread in add-in contexts; heavy operations must be scheduled to worker processes or deferred via host idle callbacks. <br>6. Observability: long-lived or important steps must emit <code>start</code> and <code>complete</code> audits, with duration metrics, and store evidence as required. <br><strong>SLOs (examples & measurement):</strong><br>1. Median AtomicWrite latency on local SSD: <200ms, 95th percentile <1s. <br>2. Retry overhead median per attempt: <50ms (backoff excluded). <br>3. SafeRound vectorized path throughput: process 1M numeric entries in worker environment within CI-defined budget (profiled per language). <br>4. RNG seed serialization/deserialization roundtrip success: 100% in golden CI; parity across language bindings mandatory. <br><strong>CI / acceptance gates (enforced):</strong> cross-language golden vectors for RNG and rounding; cross-platform atomic write tests under simulated faults; determinism tests for Retry jitter when deterministic_jitter is enabled; static analysis forbidding synchronous I/O on UI thread; audit emission verification on all persistence and deterministic transforms. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>SafeRound(value, places=0, strategy="bankers", tieBreakerKeys=null)</strong> — exhaustive per-function technical breakdown (expanded)<br><strong>Purpose & contract:</strong> deterministic rounding primitive for scalar values, arrays, grouped sets, and allocation workflows where residual distribution must be conservative and reproducible. Implementations must be pure and side-effect free, stable across language bindings, and suitable for regulated numeric workflows. When <code>strategy=&quot;residual_distribute&quot;</code> the function guarantees that the sum of rounded parts equals the rounded sum (<code>conservation invariant</code>) at the requested precision. <br><strong>Parameters & return (explicit):</strong><br>1. <code>value</code>: scalar, 1D, or 2D numeric container; accepts integer/float/decimal types coerceable to canonical internal decimal representation. <br>2. <code>places</code>: integer >= 0, number of decimal places to round to. <br>3. <code>strategy</code>: string in <code>{bankers, awayFromZero, floor, ceiling, residual_distribute, custom}</code>. <br>4. <code>tieBreakerKeys</code>: optional array (length aligned with value elements) providing stable deterministic ordering for tie resolution. <br>Return: rounded output with same shape as <code>value</code>. Rounding does not raise on NaN/Inf but propagates them and emits an audit event. <br><strong>Primary invariants (must/shall):</strong><br>1. <strong>Idempotence:</strong> <code>SafeRound(SafeRound(x)) == SafeRound(x)</code> for finite numbers. <br>2. <strong>Tie-break determinism:</strong> when fractional ties occur in half-way cases or residual allocation, <code>tieBreakerKeys</code> followed by original index must deterministically resolve tie. <br>3. <strong>Conservation (residual_distribute):</strong> <code>sum(rounded_parts) == rounded(sum(original_parts))</code> at the integer scale <code>10**places</code>. <br>4. <strong>Locale independence:</strong> numeric results must not depend on host locale (thousands separators, decimal comma). <br><strong>Conceptual algorithm & implementation notes (detail):</strong><br>1. <strong>Canonicalization:</strong> normalize inputs to an arbitrary-precision decimal representation (preferred) or integer scaling if decimal library not available: <code>scaled = exact_decimal(value) * scale</code> where <code>scale = 10**places</code>. <br>2. <strong>Exact-half detection:</strong> when implementing bankers rounding detect exact <code>.5</code> boundaries using decimal arithmetic — do not rely on binary float equality checks (these are non-deterministic across platforms). <br>3. <strong>Vector/group path:</strong> for <code>residual_distribute</code> compute <code>floors = floor(scaled)</code>, <code>residuals = scaled - floors</code>; compute <code>target_total_scaled = round(sum(scaled))</code> (rounded according to <code>strategy</code> when appropriate); compute <code>needed_increments = target_total_scaled - sum(floors)</code>; allocate increments to elements ordered by <code>(residual desc, tieBreakerKeys asc, originalIndex asc)</code>. <br>4. <strong>Custom strategy plug-in:</strong> <code>strategy=&quot;custom&quot;</code> accepts a callback <code>customTieBreaker(residual, index, tieBreakerKeys)</code> user-supplied in higher-level API — must be pure and deterministic; CI flags any custom strategy for golden tests. <br>5. <strong>Precision cap & handling:</strong> implement maximum supported precision (e.g., 18 decimal places) and cap <code>places</code> beyond it, emitting <code>util.saferound.truncated_precision</code> audit with rationale. <br><strong>Edge cases & invalid inputs (explicit mapping):</strong><br>1. <strong>NaN / Inf:</strong> propagate unchanged, emit <code>util.saferound.invalid_input</code> with input hash and reason. <br>2. <strong>Non-numeric types:</strong> attempt coercion; if fails return error <code>UTIL_SAFEROUND_COERCE_FAIL</code> and emit audit with sanitized evidence. <br>3. <strong>Very large arrays / memory pressure:</strong> process in chunks with streaming algorithm; if memory insufficient return <code>UTIL_SAFEROUND_OOM</code> with diagnostics. <br>4. <strong>Mixed numeric types across vector elements:</strong> coerce to canonical decimal with exactness; if heterogeneity causes loss, emit <code>util.saferound.truncated_precision</code>. <br><strong>Observability & audit fields (full):</strong><br>1. <code>util.saferound.start</code> — fields: <code>timestamp</code>, <code>correlationId</code>, <code>places</code>, <code>strategy</code>, <code>inputHash</code>, <code>elementCount</code>, <code>paramsHash</code>. <br>2. <code>util.saferound.complete</code> — fields: <code>timestamp</code>, <code>correlationId</code>, <code>outputHash</code>, <code>durationMs</code>, <code>elementCount</code>. <br>3. <code>util.saferound.invalid_input</code> — fields: <code>timestamp</code>, <code>correlationId</code>, <code>valueHash</code>, <code>reason</code>. <br><strong>Representative narratives & extended examples (multi-paragraph):</strong><br>1. <strong>Payroll rounding scenario (detailed):</strong> split gross payroll <code>100.00</code> among three recipients by equal weight using <code>residual_distribute, places=2</code>. Internal steps: canonicalize to scaled cents <code>[10000/3]</code> scaled fractional parts computed, floors <code>[3333,3333,3333]</code>, residuals <code>[1/3*10000 - 3333 ...]</code> computed precisely; <code>needed_increments = 1</code> assigned deterministically by <code>tieBreakerKeys</code> (employeeId order) producing <code>[33.33,33.33,33.34]</code>. Audit trail includes <code>util.saferound.start</code> with <code>inputHash</code>, <code>util.saferound.residuals</code> with allocation details fingerprint, and <code>util.saferound.complete</code> with <code>outputHash</code> and allocation checksum. This chain enables replay in a compliance investigation where employee IDs, jobDescriptor, and RNG seed are replayable. <br>2. <strong>Financial ledger aggregation (detailed):</strong> for repeated aggregation of many micro-transactions across periods, bankers rounding is chosen to avoid directional bias. Unit tests include sequences of additions and roundings to verify no systematic drift over 10k simulated days. <br><strong>Tests & CI golden vectors (explicit):</strong><br>1. Cases at half-boundaries: <code>1.005</code> to 2 dp, <code>-1.005</code> to 2 dp, exact half values of <code>x.5</code> both positive and negative. <br>2. Residual distribution property tests over random vectors (property-based testing) verifying conservation for 100k randomized cases across languages. <br>3. Cross-language golden vectors for a canonical set of inputs and <code>strategy</code> combinations. <br><strong>Operational remediation & runbook (when rounding disputes occur):</strong><br>1. Retrieve <code>util.saferound.*</code> audit rows for the correlationId. <br>2. Pull canonicalized decimal inputs from evidence store (<code>evidenceRef</code>). <br>3. Re-run <code>SafeRound</code> in reproduce mode with the exact <code>places</code> and <code>strategy</code> and compare <code>outputHash</code>. If mismatch persists, escalate to module owners with code, evidenceRef, and golden vector comparison. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>SafeRoundResiduals(values[], total=null, places=0, tieBreakerKeys=null)</strong> — allocation primitive (expanded)<br><strong>Purpose & contract:</strong> deterministic allocator for translating fractional shares into integer allocations whose sum equals a target total. Used for cents allocation, invoice splitting, distributing indivisible units, chunk sizing for job proposals. Pure function; emits audit row on invocation and returns allocation and internal allocation fingerprint. <br><strong>Detailed behavior & algorithm (stepwise):</strong><br>1. If <code>total == null</code> compute <code>target_total = SafeRound(sum(values), places, strategy=&quot;bankers&quot;)</code>. <br>2. Compute <code>scale = 10**places</code> and <code>scaled_values = exact_decimal(values) * scale</code>. <br>3. For each element compute <code>floor_i = floor(scaled_values_i)</code> and <code>residual_i = scaled_values_i - floor_i</code>. <br>4. <code>remaining = target_total_scaled - sum(floor_i)</code>. If <code>remaining &lt; 0</code> error <code>UTIL_ALLOCATION_NEGATIVE_REMAINING</code>. If <code>remaining &gt; len(values)</code> handle per policy (distribute multiple units or raise <code>UTIL_ALLOCATION_OVERFLOW</code>). <br>5. Sort indices by tuple <code>(residual desc, tieBreakerKeys asc if provided, originalIndex asc)</code>. <br>6. Add <code>+1</code> to the top <code>remaining</code> indices. <br>7. Return <code>allocated = (floor_i + assigned_increment_i) / scale</code> with allocation checksum and <code>allocationFingerprint</code> for audit. <br><strong>Primary invariants:</strong><br>1. Sum-preservation: <code>sum(allocated)</code> equals target total at the requested precision. <br>2. Determinism: tie-break stable across runs. <br><strong>Edge cases & policy decisions:</strong><br>1. Negative weights: allow only if business rule allows negative allocations; otherwise <code>UTIL_ALLOCATION_NEGATIVE_WEIGHT</code>. <br>2. Zero-sum with rounding: all residuals zero but <code>remaining &gt; 0</code> indicates inconsistent <code>total</code> — signal <code>UTIL_ALLOCATION_TARGET_MISMATCH</code>. <br><strong>Observability & audit:</strong> <code>util.saferound.residuals</code> capturing <code>correlationId</code>, <code>valuesHash</code>, <code>places</code>, <code>target_total</code>, <code>tieBreakerFingerprint</code>, <code>allocationFingerprint</code>. Evidence store holds canonical scaled inputs for forensic replay. <br><strong>Examples & extended narrative:</strong><br>1. <strong>Invoice splitting among N customers by weight:</strong> values <code>[12.345, 45.0, 65.1]</code>, <code>places=2</code>. Implementation scales to cents, floors computed, residuals evaluated, and <code>remaining</code> cents assigned deterministically by <code>tieBreakerKeys</code> (e.g., earliest account creation timestamp). Resulting cents allocation audited and persisted; operator-visible preview shows "before/after" examples in remediation UI. <br>2. <strong>Chunk sizing in MatchMerge:</strong> allocate integer job chunk sizes across workers so that sum equals total rows and no worker is starved or overallocated. <code>tieBreakerKeys</code> may encode machine capacity for fairness. <br><strong>Tests & CI:</strong> property tests for conservation across random weight vectors, tie-break stability tests, concurrency tests validating that tieBreakerKeys deterministically influence results. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>AtomicWrite(targetPath, payloadStream, tmpSuffix=".part", fsyncFile=true, fsyncParent=true, perms=null, maxAttempts=3, cleanupOnFailure=false)</strong> — persistence primitive (expanded)<br><strong>Purpose & contract:</strong> robust, crash-safe atomic file replacement primitive ensuring that at no point does a consumer observe a truncated file. Provide deterministic result object on success and deterministic error codes on failure. Relevant to job descriptors, exported datasets, reconciliation artifacts, and signed manifests. <br><strong>Complete step-by-step semantics (detailed):</strong><br>1. <strong>Parent directory validation:</strong> verify parent directory exists; if missing and creation allowed create directories using <code>FsSyncDir</code> which ensures fsync of parent metadata. Emit <code>util.atomic_write.mkdir</code> if created. <br>2. <strong>Temp path computation:</strong> compute <code>tempPath = TempPathFor(targetPath, tmpSuffix, pid=currentPid, deterministicSuffix=DeterministicSuffix())</code>. DeterministicSuffix derived from <code>DeterministicRNG(seed=correlationId, salt=&quot;atomic-temp&quot;)</code> when deterministic temp naming is required to coordinate multi-process writes in golden tests. <br>3. <strong>Exclusive open & write:</strong> open tempPath with create-only and exclusive flags; write payload stream in chunks using <code>ChecksumStream</code> to compute SHA256 on the fly; optionally write <code>artifact.metadata.json</code> sidecar containing <code>payloadHash</code>, <code>correlationId</code>, <code>timestamp</code>, <code>paramsHash</code>. <br>4. <strong>Fsync file descriptor:</strong> if <code>fsyncFile</code> true call OS-specific flush (<code>fsync</code>/<code>FlushFileBuffers</code>). If call fails due to unsupported FS semantics emit <code>util.atomic_write.degraded</code>. <br>5. <strong>Atomic rename:</strong> attempt atomic rename/replace using OS primitives: <code>os.replace</code>/<code>rename</code> on POSIX, <code>ReplaceFile</code> or <code>MoveFileEx</code> on Windows with flags that allow replacing a read-only file if policy permits; on failure due to open handles return a deterministic error <code>UTIL_ATOMIC_WRITE_RENAME_LOCKED</code>. <br>6. <strong>Fsync parent directory:</strong> if <code>fsyncParent</code> true, open parent dir and fsync to persist directory entry; on Windows use alternate semantics. If not supported on FS, emit <code>util.atomic_write.degraded</code> audit. <br>7. <strong>Verification read & checksum compare:</strong> reopen <code>targetPath</code> in read-only mode and compute checksum; compare with computed <code>artifactChecksum</code>; if mismatch emit <code>util.atomic_write.verification_failed</code> and attempt an immediate repair or retry per <code>Retry</code> policy. <br>8. <strong>Cleanup & result:</strong> remove temp artifacts if <code>cleanupOnFailure</code> true on failures; on success return structured result <code>{success:true, artifactPath:targetPath, artifactChecksum, attempts, elapsedMs}</code> and emit <code>util.atomic_write.completed</code>. <br><strong>Cross-platform fallbacks & NFS/SMB specifics (documented):</strong><br>1. <strong>Network file systems:</strong> many network filesystems do not support atomic rename semantics between clients reliably or do not persist directory fsync in expected order. For these cases the module must:<br>   - write artifact to <code>targetPath.tmp</code> and a <code>targetPath.meta</code> sidecar that includes checksum and state <code>&quot;complete&quot;:true</code> only after write complete and flush;<br>   - optionally coordinate with small lease/lock files stored on the same share to indicate availability;<br>   - emit <code>util.atomic_write.degraded</code> with <code>rationale=&quot;network-fs-no-atomic-rename&quot;</code>. <br>2. <strong>Windows handle contention:</strong> recommend best-practice orchestration: write to same volume, ensure readers open with shared-read semantics, perform write during low-load windows, and fallback to <code>stage-local</code> on repeated failures. <br><strong>Error classes & runbook mapping (operational):</strong><br>1. <code>UTIL_ATOMIC_WRITE_ENOSPC</code> — emit <code>util.atomic_write.ENOSPC</code> audit, include freeBytes and mount path; runbook: attempt local staging, rotate older artifacts, escalate to infra if persistent. <br>2. <code>UTIL_ATOMIC_WRITE_EPERM</code> — emit <code>util.atomic_write.EPERM</code> with ACL snapshot; runbook: validate ACLs, correct ownership, re-run with operator. <br>3. <code>UTIL_ATOMIC_WRITE_VERIFICATION_FAILED</code> — attempt immediate retry; if repeated, capture temp artifacts via <code>InspectTempArtifacts</code> and run <code>AtomicWriteRepair</code> under maintenance window. <br><strong>Auditing & evidence linkage (explicit):</strong><br>1. <code>util.atomic_write.attempt</code> — include <code>targetPath</code>, <code>tempPath</code>, <code>payloadHash</code>, <code>paramsHash</code>. <br>2. <code>util.atomic_write.completed</code> — include <code>artifactChecksum</code>, <code>durationMs</code>, <code>attempts</code>, <code>evidenceRef</code> for artifact metadata. <br>3. <code>util.atomic_write.degraded</code> — include <code>rationale</code> and recommended mitigation. <br><strong>Representative scenario & extended narrative (atomic export with staged fallback):</strong><br>1. Worker attempts AtomicWrite to network share; <code>fsyncParent</code> not supported on mount -> AtomicWrite detects degraded FS semantics and writes using two-phase protocol: <code>artifact.tmp</code> + <code>artifact.meta</code>. Emit <code>util.atomic_write.degraded</code> with <code>rationale=&quot;nfs-no-fsync&quot;</code>. Orchestrator chooses to stage artifact locally and schedule background upload while emitting <code>export.attempt</code> audit. <br><strong>Tests & CI (robust coverage):</strong><br>1. FS-mock tests simulating <code>rename</code> failure, <code>fsync</code> failures, permission changes mid-write. <br>2. Concurrency tests with 100 readers hitting target path during writer process ensuring no partial content observed. <br>3. Cross-OS integration ensuring ReplaceFile semantics consistent. <br><strong>Operational mitigations & escalations:</strong><br>1. If repeated <code>ENOSPC</code> or <code>EPERM</code> persisted after automated mitigations, create SRE ticket with forensic_manifest and audit_tail. <br>2. If <code>verification_failed</code> observed for an exported artifact consumed downstream, automatically quarantine downstream ingestion until artifact integrity resolved. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Retry(fn, retries=3, backoff={baseMs:100, factor:2}, jitter=true, retry_on=(TransientError,), idempotent_assert=false, cancellation_token=null, audit_on_attempt=true, deterministic_jitter=false)</strong> — orchestration & governance (expanded)<br><strong>Purpose & contract (concise):</strong> canonical transient-fault retry wrapper for local IO and short-lived remote calls invoked by worker processes and background orchestrators. Must be auditable, enforce idempotency expectations, and provide deterministic jitter for golden CI runs when requested. <br><strong>Behavioral rules & enforcement (detailed):</strong><br>1. <strong>Retry predicate:</strong> only retry on exceptions that satisfy <code>retry_on</code> predicate(s); non-retryable exceptions must be surfaced immediately. <br>2. <strong>Idempotency enforcement:</strong> if <code>idempotent_assert</code> omitted and static analysis detects non-idempotent side effects (e.g., writes, external state mutation) the CI system must fail the merge and require either <code>idempotent_assert=true</code> with documented idempotency token or a refactor to make the operation idempotent. <br>3. <strong>Deterministic jitter for testing:</strong> when <code>deterministic_jitter=true</code> the jitter calculation is derived from <code>DeterministicRNG(seed=correlationId|retry-salt)</code> so sleep durations are reproducible for CI golden runs. <br>4. <strong>Cancellation handling:</strong> if <code>cancellation_token</code> is set and flagged between attempts, raise <code>UTIL_RETRY_CANCELLED</code> and emit <code>util.retry.complete</code> with <code>outcome=&quot;cancelled&quot;</code>. <br><strong>Backoff & jitter semantics (precise):</strong><br>1. Base backoff: <code>backoff_ms = baseMs * factor**(attemptIndex-1)</code>. <br>2. Jitter algorithm: default is "decorrelated jitter" (per AWS jitter guidance) computed as <code>min(maxBackoff, rand(0, backoff_ms * 3))</code>; when <code>deterministic_jitter=true</code> replace <code>rand</code> with <code>DeterministicRNG.random()</code>. <br>3. Maximum total retry budget is configurable per-call (<code>max_total_ms</code>) to prevent runaway waits. <br><strong>Auditing & metrics:</strong><br>1. Emit <code>util.retry.attempt</code> for each attempt with <code>attemptIndex</code>, <code>errorCode</code>, and <code>backoffMs</code>. <br>2. Emit <code>util.retry.complete</code> on success or exhaustion including <code>attempts</code> and <code>elapsedMs</code>. <br>3. Metrics buffered locally: <code>util.retry.attempt_count</code>, <code>util.retry.success_rate</code>, with tags for <code>target</code> and <code>module</code>. <br><strong>Representative usage scenarios & narratives:</strong><br>1. <strong>Job descriptor persistence under transient NFS blips:</strong> <code>Retry(lambda: AtomicWrite(jobDescriptorPath,...), retries=5, idempotent_assert=true, deterministic_jitter=true)</code> ensures eventual persistence while deterministic_jitter allows CI and replay to mimic production timing in stress tests. <br>2. <strong>Provider API transient timeouts:</strong> wrap short provider calls in <code>Retry</code> with <code>idempotent_assert=true</code> and application-layer idempotency tokens embedded in request payloads. <br><strong>Failure escalation & SRE triggers:</strong><br>1. Exhaustion emits <code>util.retry.complete</code> with <code>outcome=&quot;exhausted&quot;</code> and triggers SRE alerting if call is critical (jobDescriptor persistence, artifact export). The alert payload includes tempPaths, attempt errors, and partial artifacts evidence. <br><strong>Tests & CI (explicit):</strong> deterministic_jitter golden tests to ensure backoff/jitter sequences identical given correlation seeds; cancellation tests simulate cancellation_token signalled after first attempt; static-analysis gating for idempotency enforcement. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>DeterministicRNG(seed_source, salt="", algorithm="pcg64", stream_id=null, test_mode=false)</strong> — design, usage & governance (expanded)<br><strong>Purpose & contract (concise):</strong> deterministic, seedable PRNG for sampling, shuffles, tie-breakers, and ephemeral randomized operations that must be reproducible. Not for cryptographic uses. RNG instances are explicit and serializable. <br><strong>Seed derivation & confidentiality (detailed):</strong><br>1. Production seed derivation: <code>seed_raw = HMAC_SHA256(key=systemKey, message=(seed_source || &#x27;|&#x27; || salt || &#x27;|&#x27; || moduleName || &#x27;|&#x27; || streamId))</code>. The <code>seed_raw</code> is then folded/truncated into the algorithm's seed size using deterministic mixing. <br>2. <strong>Confidentiality:</strong> raw seed not emitted in plaintext in audit rows. Instead store <code>seedFingerprint = SHA256(seed_raw)</code> and record <code>util.rng.seeded</code> with <code>seedFingerprint</code>. For replay, <code>serialize_rng_state</code> stores encrypted internal state blob into evidence store and audit references it via <code>evidenceRef</code>. <br>3. <strong>test_mode:</strong> allows local seed overrides for dev/golden runs but CI must verify no test_mode artifacts leak to production. <br><strong>API behavior & primitives:</strong><br>1. <code>randint(a,b)</code>, <code>random()</code> in [0,1), <code>shuffle(seq)</code> returning a new sequence, <code>sample(pop,k)</code>, <code>choice(seq)</code>, <code>splitStream(childIndex)</code> producing independent child RNG seeded via <code>HMAC(parentSeed || childIndex)</code> to avoid correlation. <br>2. <code>serialize_state()</code> returns encrypted base64 blob; <code>restore_state(blob)</code> returns RNG instance at saved position. <br><strong>Stability invariants:</strong><br>1. Same <code>algorithm</code>, <code>seed_source</code>, <code>salt</code>, and <code>stream_id</code> must produce identical output across Python/JS/C#/VBA bindings. <br>2. Splitting streams produce independent sequences (statistical independence test required in CI). <br><strong>Observability & audit:</strong><br>1. <code>util.rng.seeded</code> — <code>correlationId</code>, <code>seedFingerprint</code>, <code>algorithm</code>, <code>streamId</code>, <code>evidenceRef</code> optional. <br>2. <code>util.rng.state_serialized</code> — <code>stateHash</code>, <code>evidenceRef</code> for the encrypted state blob. <br><strong>Representative narratives & usage examples (detailed):</strong><br>1. <strong>DQ_Profile deterministic sampling:</strong> worker uses <code>DeterministicRNG(seed_source=correlationId, salt=&#x27;dq-profile-v1&#x27;)</code> to select deterministic sample rows for profile reports; <code>serialize_state()</code> attached to profile artifact so compliance can reconstruct sample for audits. <br>2. <strong>MatchMerge tie-breaker:</strong> seed from <code>correlationId || &#x27;matchmerge-v1&#x27;</code>, call <code>shuffle(candidates)</code> producing deterministic reordering, persist RNG state in proposal artifact to allow later replays and operator disputes to be resolved with exact ordering. <br><strong>Tests & CI (explicit):</strong> cross-language parity vectors (first N outputs), splitStream independence tests, serialize/restore roundtrip, statistical tests for uniformity across large N. <br><strong>Operational playbook:</strong><br>1. When non-deterministic sampling is reported, retrieve <code>util.rng.seeded</code> and <code>util.rng.state_serialized</code> evidence. <br>2. Re-run sampling with restored RNG state to reproduce selection; if mismatch found, escalate to owners for parity investigation. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>ChecksumStream(payloadStream, algorithm="sha256", chunkSize=65536)</strong> — streaming checksum (expanded)<br><strong>Purpose & contract:</strong> stream-pass-through wrapper that computes a cryptographic checksum (default SHA256) while forwarding bytes to consumer; suitable for integrating into AtomicWrite to compute checksums without buffering whole artifact. Must be deterministic in chunking semantics and avoid implicit character encoding transforms. <br><strong>Behavior & notes:</strong><br>1. Read bytes from <code>payloadStream</code> in <code>chunkSize</code> increments; update hash state per chunk; forward to writer socket/file handle. <br>2. On EOF finalize digest and provide <code>finalChecksum</code> (hex/base64 configurable). <br>3. On partial stream failure emit <code>util.checksum.partial</code> with <code>partialHash</code> and <code>bytesProcessed</code>. <br><strong>Edge cases & mapping:</strong> unsupported algorithms return <code>UTIL_CHECKSUM_UNSUPPORTED</code>; stream interruption errors map to <code>UTIL_CHECKSUM_PARTIAL</code>. <br><strong>Observability & audit:</strong> <code>util.checksum.start</code> with <code>correlationId</code>, <code>algorithm</code>, <code>chunkSize</code>; <code>util.checksum.complete</code> with <code>finalChecksum</code>, <code>bytesProcessed</code>, <code>durationMs</code>. <br><strong>Tests & CI:</strong> parity checks over multiple chunk sizes; ensure checksum identical for same byte stream across environments. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>TempPathFor(targetPath, tmpSuffix=".part", pid=null, deterministicSuffix=null)</strong> — temp path helper (expanded)<br><strong>Purpose & contract:</strong> deterministic temporary path generator used by AtomicWrite to ensure low collision probability and forensic traceability. Must produce temp path on <strong>same volume</strong> as <code>targetPath</code> to preserve atomic rename semantics. <br><strong>Rules & algorithm:</strong><br>1. Compute <code>base = dirname(targetPath)</code> and <code>basename = safe_filename(basename(targetPath))</code>. <br>2. <code>pid</code> default to runtime pid; <code>deterministicSuffix</code> may be provided using <code>DeterministicRNG(seed=correlationId)</code> for test reproducibility or left null for production randomness. <br>3. Return <code>tempPath = join(base, f&quot;.{basename}{tmpSuffix}.{pid}.{detSuffix}&quot;)</code> sanitized for OS constraints. <br><strong>Edge cases:</strong> on Windows UNC paths ensure path length limits respected and sanitize <code>:</code> and <code>\</code> characters. <br><strong>Observability:</strong> no heavy audit; AtomicWrite emits <code>util.atomic_write.attempt</code> including <code>tempPath</code>. <br><strong>Tests:</strong> path equality and collision stress tests; ensure same-volume assertion verified by <code>PathVolumeOf</code>. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>FsSyncDir(dirPath, ensureExists=true, perms=null)</strong> — directory creation & fsync (expanded)<br><strong>Purpose & contract:</strong> ensure directory exists (optionally creating it) and persist directory metadata (fsync) to assure directory entries are durable after file rename operations. Required by AtomicWrite before rename when parent directories might be newly created. <br><strong>Behavior & OS specifics:</strong><br>1. Create directory with <code>mkdir -p</code> semantics with configured permissions; set ownership/ACLs only when caller requests and has permissions. <br>2. Open directory handle and <code>fsync</code> directory descriptor (<code>fdatasync</code>/<code>fsync</code>) on POSIX; on Windows use <code>FlushFileBuffers</code> on directory handle or fallback to opening and flushing a dummy file if direct flush unsupported. <br>3. If underlying FS does not support directory fsync (some SMB/NFS variants) return <code>degraded=true</code> and emit <code>util.atomic_write.degraded</code> with <code>rationale</code>. <br><strong>Observability & audit:</strong> <code>util.fs.syncdir</code> event with <code>durationMs</code> and <code>created</code> flag. <br><strong>Tests:</strong> cross-platform create-and-sync tests, ensure idempotent behavior when concurrent creators attempt same directory. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>WriteWithPermissions(path, payloadStream, perms, owner=null, group=null)</strong> — write + perms helper (expanded)<br><strong>Purpose & contract:</strong> write payload to temporary path then set POSIX permission bits or Windows ACLs before rename. Must be used as part of AtomicWrite sequence to ensure final artifact has correct access control settings. <br><strong>Behavior & notes:</strong><br>1. Write to temp file (temp computed by TempPathFor), upon write completion apply <code>chmod</code> and <code>chown</code> semantics for POSIX; on Windows apply ACL modifications via appropriate APIs. <br>2. If setting permissions fails after write, the function should either rollback (remove temp) or surface an error <code>UTIL_WRITE_EPERM</code> depending on <code>force</code> parameter; audit includes permission fingerprint rather than full ACL content. <br><strong>Edge cases:</strong> mapping POSIX UIDs/GIDs on containerized environments lacking those IDs should be gracefully handled with audit <code>util.write.perms.warning</code>. <br><strong>Tests & CI:</strong> permission change tests in containerized CI with places of permissions mismatch; verify final artifact has expected effective ACLs. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>serialize_rng_state(rng_instance, evidenceEncrypt=true)</strong> — RNG state serialization (expanded)<br><strong>Purpose & contract:</strong> produce a portable, versioned, encrypted blob representing internal PRNG state for later deterministic replay. Return blob and <code>stateHash</code>; do not leak raw seed material. The blob is intended for secure evidence storage and must be decryptable only by authorized evidence tooling. <br><strong>Serialization details & format:</strong><br>1. Pack: <code>{version, algorithm, internal_state_words, position_counter, metadata:{createdBy, correlationId, module}}</code> into a deterministic binary layout. <br>2. Encrypt with evidence store key and HMAC the resulting ciphertext with evidence HMAC key. <br>3. Base64 encode and return <code>{blobBase64, stateHash=SHA256(ciphertext)}</code>. <br><strong>Observability & audit:</strong> emit <code>util.rng.state_serialized</code> with <code>stateHash</code> and <code>evidenceRef</code>. <br><strong>Tests & CI:</strong> serialize/restore round-trip tests; cross-language blob unmarshal parity checks; rejection for mismatched <code>version</code> during restore. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>restore_rng_state(blobBase64)</strong> — RNG state restoration (expanded)<br><strong>Purpose & contract:</strong> decrypt and reconstruct RNG instance at exact position. Return RNG instance or raise <code>UTIL_RNG_BAD_STATE</code> if decryption fails, integrity check fails, or version incompatible. <br><strong>Behavior & tests:</strong> roundtrip restore must reproduce sequence position exactly; CI runs parity tests across languages and versions; incompatible <code>version</code> requires migration path and fails CI unless accompanied by migration manifest. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>InspectTempArtifacts(baseDir, pattern="<em>.part</em>")</strong> — temp artifact inspection (expanded)<br><strong>Purpose & contract:</strong> safely enumerate aborted/leftover temp artifacts created by AtomicWrite flows; compute checksums where readable and propose candidate repairs. Intended for operator triage and automated repair procedures. <br><strong>Behavior & outputs:</strong> returns list of <code>{tempPath, candidateTargetPath(inferred), size, mtime, readableChecksum?, permissionFlags}</code> and a <code>discoveryFingerprint</code> for audit. Limit scans to <code>maxEntries</code> per invocation to avoid DoS. <br><strong>Edge cases:</strong> permission denied entries recorded with <code>permissionDenied=true</code>; symlinks exploded carefully and not automatically followed beyond one-level to avoid escape. <br><strong>Observability:</strong> emit <code>util.inspect.temp</code> with counts and <code>discoveryFingerprint</code>. <br><strong>Tests:</strong> directory with mixed temp names, multiple matching conventions, and simulated partial writes. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>AtomicWriteRepair(tempPath, expectedChecksum=null, force=false)</strong> — safe repair helper (expanded)<br><strong>Purpose & contract:</strong> attempt validated repair of a temp artifact by verifying checksum and performing a safe rename into the intended target while preserving a backup of the previous target if present. Only run under maintenance or with operator consent. <br><strong>Behavior & required safeguards:</strong><br>1. Validate <code>tempPath</code> read and compute checksum; if <code>expectedChecksum</code> provided, compare and refuse unless <code>force=true</code>. <br>2. Determine <code>targetPath</code> via temp naming convention; backup existing <code>targetPath</code> to <code>targetPath.backup.&lt;ts&gt;</code> before rename to ensure rollback. <br>3. Perform rename using atomic replace primitives where possible; upon success emit <code>util.atomic_write.repair</code> with <code>actionTaken=rename</code> and evidenceRef to temp artifact. <br>4. If repair fails after partial work, attempt to restore backup and emit <code>util.atomic_write.repair.failed</code> with diagnostics. <br><strong>Operational policy:</strong> repairs must be logged to evidence store and require operator approval for <code>force</code> repairs in regulated flows. <br><strong>Tests & CI:</strong> simulate valid temp, mismatched checksum, and rename failure cases to ensure safe rollback. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>AuditEmitUtilEvent(correlationId, procedure, paramsHash, resultHash=null, evidenceRef=null, metadata=null)</strong> — canonical audit emitter (expanded)<br><strong>Purpose & contract:</strong> canonical utility-level audit append helper that validates schema, sanitizes metadata (ensuring no PII in top-level fields), computes <code>rowHash</code>, and appends to local audit buffer (<code>audit_tail.csv</code> or local encrypted buffer). Ensures consistent audit envelope across all util.* events. <br><strong>Behavior & constraints:</strong><br>1. Validate <code>paramsHash</code> presence for state-changing operations. <br>2. If <code>metadata</code> contains PII fields, sanitize or offload to evidence store and add <code>evidenceRef</code>. <br>3. Compute <code>rowHash = HMAC_SHA256(auditRow, auditSigningKey)</code> and append to audit buffer with <code>prevHash</code> chaining where available. <br>4. If audit buffer write fails, write to local encrypted fallback store and emit <code>util.audit.emit.degraded</code>. <br><strong>Observability:</strong> emits meta-audit <code>util.audit.emit</code> with <code>status=buffered|flushed|degraded</code> and buffer metrics. <br><strong>Tests & CI:</strong> ensure schema validation rejects malformed fields; ensure chained row hashes produce correct <code>prevHash</code> chain and sign/verify tests pass. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Cross-cutting Observability, Telemetry & Error Catalog (detailed mapping)</strong><br><strong>Audit schema for utilities (required fields):</strong> <code>timestamp</code>, <code>correlationId</code>, <code>module</code>, <code>procedure</code>, <code>operatorId</code> (optional), <code>paramsHash</code>, <code>resultHash</code> (optional), <code>evidenceRef</code> (optional), <code>prevHash</code> (optional), <code>configHash</code>, <code>ribbonMapHash</code> (relevant flows), and <code>metadata</code> object containing <code>duration_ms</code>, <code>attempts</code>, <code>artifactChecksum</code>, <code>tempPathList</code>, <code>errorCode</code> if any. <br><strong>Evidence policy (expanded):</strong> top-level audit rows must be free of raw PII; when parameters contain PII persist sanitized parameters and full payloads in the encrypted evidence store and reference them with <code>evidenceRef</code> in the audit row. Evidence stores must maintain chain-of-custody metadata including who accessed the evidence and when. <br><strong>Representative ErrorCodes & operator guidance mapping:</strong><br>1. <code>UTIL_ATOMIC_WRITE_ENOSPC</code> — advise <code>df -h</code>, <code>du -sh</code> on mount and stage local fallback. <br>2. <code>UTIL_ATOMIC_WRITE_EPERM</code> — include ACL snapshot and recommend operator fix via documented ACL runbook. <br>3. <code>UTIL_ATOMIC_WRITE_VERIFICATION_FAILED</code> — trigger <code>InspectTempArtifacts</code> and forensic capture. <br>4. <code>UTIL_RETRY_EXCEEDED</code> — collect <code>util.retry.attempt</code> traces and consider circuit-breaker. <br>5. <code>UTIL_SAFEROUND_COERCE_FAIL</code> — provide sanitized inputs to devs for coercion fix. <br>6. <code>UTIL_RNG_BAD_STATE</code> — evidenceRef needed and cross-language parity review. <br><strong>Metric names (precise):</strong> <code>util.atomic_write.latency_ms{host,volume}</code>, <code>util.atomic_write.success_rate{module}</code>, <code>util.retry.attempt_count{target}</code>, <code>util.retry.success_rate{target}</code>, <code>util.saferound.count{strategy}</code>, <code>util.rng.seeded_count{module}</code>. Metrics buffered locally and flushed by <code>CORE_Telemetry</code> with audits referencing the telemetry batch evidence. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Testing matrix, property tests, and cross-language golden governance (comprehensive & extended)</strong><br><strong>Unit tests (explicit list):</strong><br>1. SafeRound goldens covering bankers, awayFromZero, floor, ceiling, residual_distribute, including negative numbers and exact half ties. <br>2. SafeRoundResiduals: multiple ratio distributions including uniform, skewed, heavy-tailed, and pathological tie-heavy vectors. <br>3. AtomicWrite: simulate <code>rename</code> failure, <code>fsync</code> failure, permission change mid-write, and <code>ENOSPC</code>. <br>4. Retry: deterministic_jitter sequences and cancellation token enforcement. <br>5. DeterministicRNG: seed parity across Python/JS/VBA/C# producing identical first N outputs. <br>6. ChecksumStream: chunk-size invariance tests. <br><strong>Integration tests (explicit flows):</strong><br>1. Job persist/read/process: jobDescriptor persisted via AtomicWrite -> worker reads -> produces artifact persisted via AtomicWrite -> checksum match. <br>2. Retry + AtomicWrite: inject transient FS/network failures to validate retry semantics. <br>3. End-to-end deterministic replay: DQ_Profile run with seeded RNG and SafeRound, persist RNG state and rounding audits, perform replay and compare artifact hashes identical. <br><strong>Property tests & formal invariants:</strong><br>1. Sum-preservation for residual_distribute across randomized vectors (property testing). <br>2. Deterministic sampling invariants when seed_and_salt unchanged. <br>3. Thread-safety invariants for RNG splits in parallel workers. <br><strong>CI golden gating & automation rules:</strong><br>1. All golden vectors for SafeRound and DeterministicRNG must pass before merge. <br>2. Static analyzer rejects direct workbook calls or raw file writes from UI thread. <br>3. Performance budget smoke tests must pass (SafeRound 1M rows within allowed time bound). <br>4. Any algorithmic change to RNG or SafeRound must include a migration manifest and owner approval. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Developer guidance, allowed & forbidden patterns (explicit policy)</strong><br><strong>Required patterns (must follow):</strong><br>1. Use AtomicWrite for any artifact consumed by others to avoid partial reads. <br>2. Seed DeterministicRNG from <code>correlationId</code> for operator-visible sampling, persist RNG state via <code>serialize_rng_state</code> when exact replay required. <br>3. Use <code>Retry</code> for transient IO only and ensure <code>idempotent_assert=true</code> for non-idempotent calls or implement idempotency tokens. <br>4. Emit audit rows for all critical operations: <code>AtomicWrite</code>, <code>SafeRound</code> in regulated runs, <code>DeterministicRNG.seeded</code>, <code>Retry</code> sequences. <br><strong>Forbidden practices (CI enforced):</strong><br>1. No direct writes to final artifact paths from UI thread (static analyzer rejects). <br>2. No platform-locale-based numeric parsing inside deterministic math paths (SafeRound). <br>3. No global non-deterministic RNG used for operator-visible sampling. <br>4. No raw PII in top-level audits. <br><strong>Code-review checklist (explicit):</strong> verify audit emits exist for persistence, RNG seed flows into job descriptors, AtomicWrite used for durable outputs, SafeRound used for financial allocations, Retry idempotency asserted. Ensure migration manifests for any breaking change. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Operational runbook & incident playbooks (executable steps — extended)</strong><br><strong>AtomicWrite ENOSPC runbook (explicit steps):</strong><br>1. Inspect <code>util.atomic_write.ENOSPC</code> audit row for <code>correlationId</code> and path. <br>2. On host: <code>df -h &lt;mount&gt;</code> and <code>du -sh &lt;candidateDirs&gt;</code>; collect <code>vmstat</code>, <code>iostat</code>. <br>3. If possible move non-critical artifacts to fallback staging on same volume; prefer same-volume staging to preserve rename semantics. <br>4. Re-run export with <code>--stage-local</code> if available; after export perform checksum compare between staged and intended destination. <br>5. If persistent, open infra incident attaching <code>forensic_manifest</code> and <code>audit_tail</code>. <br><strong>Retry storm triage (explicit steps):</strong><br>1. Query <code>util.retry.attempt</code> metrics for elevated attempt rates and identify failing <code>target</code>. <br>2. Lower concurrency against target and enable circuit-breaker; temporarily set <code>retries=0</code> for non-critical flows to reduce load. <br>3. Search for missing idempotency tokens in failing calls; if found, pause production calls until idempotency enforced. <br>4. If infrastructure root cause, escalate with <code>forensic_manifest</code>. <br><strong>Non-deterministic sampling complaint triage (explicit):</strong><br>1. Retrieve <code>util.rng.seeded</code> audit row for <code>correlationId</code>. <br>2. Pull evidenceRef and restore RNG state via <code>restore_rng_state</code>. <br>3. Re-run deterministic selection using restored state and provide diff to operator. <br>4. If mismatch persists, compare cross-language parity vectors and escalate to owners. <br><strong>Rounding mismatch forensic steps (explicit):</strong><br>1. Pull <code>util.saferound.*</code> audits for the run. <br>2. Obtain canonicalized decimal snapshots from evidence. <br>3. Re-execute rounding pipeline in reproduce mode and compare <code>outputHash</code>. <br>4. If discrepancy persists, capture environment parity and escalate. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Extremely detailed long-form narratives & examples (multiple scenarios — expanded)</strong><br><strong>Scenario 1 — Regulated end-of-period journal allocation (complete trace & forensic replay):</strong><br>1. Operator initiates <code>AllocateJournalTotals</code> from <code>REG_Ribbon</code>. Ribbon handler emits <code>UserAction</code> audit with <code>correlationId=r-20260117-XYZ</code>, <code>operatorId</code> and <code>paramsHash</code>. <br>2. Dataset size exceeds inline threshold; scheduler constructs <code>jobDescriptor</code> with <code>jobId</code>, <code>controlId</code>, <code>correlationId</code>, <code>paramsHash</code>, <code>configHash</code>. Worker durable persistence uses <code>AtomicWrite(jobDescriptorPath, jobJson)</code>; <code>util.atomic_write.attempt</code> and <code>util.atomic_write.completed</code> audits produced with <code>artifactChecksum</code>. <br>3. Worker picks job, initializes <code>DeterministicRNG(seed_source=correlationId, salt=&quot;alloc-v1&quot;)</code> and emits <code>util.rng.seeded</code> with <code>seedFingerprint</code> and <code>evidenceRef</code> to serialized RNG state stored in evidence store. <br>4. Worker reads ledger snapshot (redacted snapshot persisted by core bootstrap) and canonicalizes decimals using canonical decimal serializer; persists canonicalized snapshot evidenceRef. <br>5. Compute allocation fractions; call <code>SafeRoundResiduals(proportions, total=ledgerTotal, places=2, tieBreakerKeys=accountIds)</code>. <code>util.saferound.residuals</code> emits allocationFingerprint and <code>inputHash</code>. <br>6. Allocation artifact written via <code>AtomicWrite(allocPath, artifactStream)</code> with <code>artifactChecksum</code> computed and verified; <code>util.atomic_write.completed</code> audit includes evidenceRef to artifact metadata. <br>7. Module step <code>allocation.complete</code> audit appended referencing allocation checksum, jobId, and runTs. <br>8. If operator disputes allocation, forensic replay uses <code>evidenceRef</code> for serialized RNG state and canonical input snapshots to re-run <code>SafeRoundResiduals</code> in reproduce mode and validate identical output hash. <br><strong>Narrative takeaways (compliance):</strong> deterministic chain ensures repeatability: <code>correlationId -&gt; jobDescriptor -&gt; seeded RNG -&gt; canonical inputs -&gt; SafeRoundResiduals -&gt; AtomicWrite artifacts -&gt; audits</code>. Evidence references enable compliance packaging and regulator submissions. <br><strong>Scenario 2 — PQ template injection with numeric fidelity requirement (conceptual & concrete):</strong><br>1. Operator opens PQ_Templates in <code>PQ_Ribbon</code>; template metadata contains <code>mChecksum</code>, <code>requiresHighPrecision=true</code>, and <code>templateVersion</code>. Preview request computes <code>seed=SeedFromCorrelation(correlationId, templateId)</code> and preview M receives <code>seed</code> parameter; preview audit saved with <code>mChecksum</code> and <code>seedFingerprint</code>. <br>2. Operator elects to inject template into workbook; because <code>requiresHighPrecision=true</code> the injection flow delegates numeric-critical steps to a signed helper (XLAM or worker) which performs canonical normalization and <code>SafeRound</code> to authoritative precision. The helper persists the final M query artifact via <code>AtomicWrite</code> and returns artifactChecksum to the injector. <br>3. Injector then calls <code>workbook.Queries.Add</code> with the finalized M text; <code>pq_inject</code> audit row created including <code>mChecksum</code>, <code>templateVersion</code>, <code>artifactChecksum</code>, and <code>evidenceRef</code>. <br><strong>Narrative takeaways:</strong> offload authoritative numeric transforms to trusted workers using <code>SafeRound</code> and <code>AtomicWrite</code> to ensure injected query is consistent with audited artifact; this avoids client-side M decimal inconsistencies across hosts. <br><strong>Scenario 3 — MatchMerge deterministic tie-breaking & merge proposals (detailed):</strong><br>1. MatchMerge identifies candidate pairs producing equal match scores. To ensure reproducible proposals, it obtains <code>DeterministicRNG(seed_source=correlationId, salt=&quot;matchmerge-v1&quot;)</code> and uses <code>rng.shuffle(candidates)</code> where the shuffle uses stable keys derived from primary keys to ensure cross-language reproducibility. <code>util.rng.seeded</code> and serialized state stored in evidence as <code>evidenceRef</code>. <br>2. Merge proposals created deterministically, persisted via <code>AtomicWrite</code> as <code>proposal.json</code> and <code>proposalHash</code> is recorded in <code>util.atomic_write.completed</code>. <br>3. When operator reviews proposals, they are comparing a persisted artifact; if operator accepts <code>apply-inline</code> additional approvals validated by <code>ValidateUserPermissions</code>; if <code>copy-apply</code> persisted reversible plan stored via <code>AtomicWrite</code> and audit rows record <code>beforeChecksum</code> and <code>afterChecksum</code> to support rollbacks. <br><strong>Narrative takeaways:</strong> deterministic tie breaks + persisted RNG state yield auditable and replayable merge proposals; reversible plans and checksum anchors make rollbacks safe for regulated flows. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Conceptual Power Query (M) patterns — mapping REG_Utilities to PQ workflows (expanded & prescriptive)</strong><br><strong>Context & limitations:</strong> Power Query (M) executes in-host (Excel/Power BI) with runtime differences across hosts, limited file IO control, and inconsistent decimal semantics across engines. REG_Utilities cannot run inside pure M code; instead, orchestrating add-in code should ensure M workflows delegate deterministic and durable steps to helper modules that implement <code>SafeRound</code>, <code>AtomicWrite</code>, and deterministic seed flows. <br><strong>Mapping patterns & recommended implementation templates:</strong><br>1. <strong>AtomicWrite mapping for PQ exports & injections (pattern):</strong><br>    - Problem: M cannot guarantee atomic file replace semantics or directory fsync. <br>    - Pattern: PQ template generation produces the artifact payload (M script, manifest). The add-in helper receives the artifact and calls <code>AtomicWrite</code> to persist it atomically and compute <code>artifactChecksum</code>. The <code>pq_inject</code> audit references <code>artifactChecksum</code> and <code>mChecksum</code>. <br>    - Governance: require signed manifests for regulated templates and enforce template versioning in <code>PQ_Library</code>. <br>2. <strong>DeterministicRNG mapping for deterministic preview & sampling (pattern):</strong><br>    - Problem: M lacks seedable standard RNG with cross-host parity. <br>    - Pattern: PQ_Ribbon computes a seed via <code>SeedFromCorrelation(correlationId, templateId)</code> and passes seed into preview as an explicit parameter; lightweight sample selection can be implemented in M using parameterized deterministic LCG functions for small previews, but full-scale sampling must run in <code>DeterministicRNG</code> on worker and the sample persisted for replay. <br>3. <strong>SafeRound mapping for numeric fidelity in PQ (pattern):</strong><br>    - Problem: M decimal handling varies and bankers rounding behavior may differ. <br>    - Pattern: templates requiring strict rounding are annotated <code>requiresHighPrecision</code>. The M template outputs normalized rows and calls helper worker API to run <code>SafeRound</code> on canonical decimals; the worker returns a final artifact persisted via <code>AtomicWrite</code>. <br>4. <strong>Retry & idempotency mapping for PQ refresh orchestration:</strong><br>    - Problem: PQ refresh can be flaky due to provider timeouts. <br>    - Pattern: orchestrating add-in code wraps refresh or artifact write in <code>Retry</code> with <code>idempotent_assert</code> and persists jobDescriptor via <code>AtomicWrite</code> before attempting the refresh to ensure idempotent resumability. <br><strong>Operator narrative (expansion):</strong><br>1. Preview: operator presses "preview" -> add-in computes deterministic seed -> preview M function receives <code>seed</code> param -> preview audit created with <code>seedFingerprint</code> and <code>mChecksum</code>. <br>2. Inject: if <code>requiresHighPrecision</code> worker finalizes numeric rounding -> <code>AtomicWrite</code> persists artifact -> <code>pq_inject</code> audit links to artifactChecksum and evidenceRef. <br><strong>Checklist for template authors (PQ-specific):</strong><br>1. Always include <code>mChecksum</code> in template metadata. <br>2. Mark <code>requiresHighPrecision</code> for templates with regulated numeric transforms. <br>3. Parameterize <code>seed</code> for previews and store <code>seedFingerprint</code> in preview audit. <br>4. Avoid embedding non-deterministic functions in M templates for operator-visible transformations; prefer deterministic parameterization. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Conceptual DAX patterns — mapping REG_Utilities to DAX & semantic model design (expanded & prescriptive)</strong><br><strong>Context & constraints:</strong> DAX is read-time query language; it cannot perform side effects or persist artifacts. Deterministic rounding and allocation must be materialized during ETL and exposed as read-only tables. DAX measures can then be used to validate reconciliations by referencing metadata tables written atomically by ETL. <br><strong>Patterns & recommended practices (explicit):</strong><br>1. <strong>Push rounding & residual distribution to ETL:</strong><br>    - Rationale: DAX cannot generate persisted allocation artifacts or reversible plans. ETL must run <code>SafeRoundResiduals</code> and persist final integer-cent allocations as columns in model tables. DAX reports should refer to these persisted columns. <br>2. <strong>Deterministic sampling via hashed keys in model:</strong><br>    - Rationale: DAX cannot seed RNGs reliably. ETL computes <code>HashKey = HASH(PrimaryKey || &#x27;|&#x27; || correlationSalt)</code> and persists <code>sampleFlag = (HashKey MOD N) &lt; k</code> as a persistent column. Persist <code>correlationSalt</code> in <code>RunMetadata</code> for deterministic replay. <br>3. <strong>RunMetadata table & audit linkage:</strong><br>    - Pattern: ETL writes <code>RunMetadata</code> atomically (via <code>AtomicWrite</code>) with <code>correlationId</code>, <code>configHash</code>, <code>artifactChecksum</code>, <code>runTs</code>, and <code>evidenceRef</code>. DAX measures can reference the latest run metadata row to display dataset provenance. <br>4. <strong>Checksum reconciliation surfaced in DAX:</strong><br>    - Pattern: ETL computes dataset-level checksum and writes it in <code>RunMetadata</code>; DAX measure <code>ReconciledFlag = IF(LatestRunMetadata[artifactChecksum] = ExpectedChecksum, 1, 0)</code> surfaces verification to report consumers. <br><strong>DAX author checklist (concise):</strong><br>1. Avoid doing rounding or allocation in DAX for regulated outputs. <br>2. Reference <code>RunMetadata</code> for provenance and expected artifact checksums. <br>3. Use persisted deterministic sample flags rather than runtime random filters. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Appendices: forensic artifacts, evidence paths & recommended retention (expanded)</strong><br><strong>Minimum forensic artifacts for a typical incident (ordered & explicit):</strong><br>1. <code>ribbon-map.json</code> and release manifest with signatures and release fingerprint. <br>2. <code>jobDescriptor.json</code> persisted via <code>AtomicWrite</code> including <code>jobId</code>, <code>paramsHash</code>, <code>configHash</code>, <code>correlationId</code>. <br>3. <code>audit_tail.csv</code> segment spanning before/after timeframe including <code>UserAction</code> and <code>util.*</code> events for the <code>correlationId</code>. <br>4. artifact files and <code>artifact.metadata.json</code> mapping artifact -> checksum and evidenceRef. <br>5. serialized RNG state blobs (<code>rng_state.blob</code>) when sampling or shuffle was involved. <br>6. SafeRound canonicalized decimal input snapshots and rounding logs. <br>7. persisted temp artifacts and <code>InspectTempArtifacts</code> output when <code>AtomicWrite</code> failures occurred. <br><strong>Evidence storage & retention (policy):</strong><br>1. Hot store: <code>\\evidence\hot\&lt;module&gt;\&lt;correlationId&gt;\</code> for 30 days accessible to limited operators; all files encrypted at rest. <br>2. Warm archive: secure archive for regulatory retention (7 years) with chain-of-custody metadata and stricter access controls. <br>3. <code>forensic_manifest.json</code> enumerates artifact URIs, checksums, evidenceRef values, and access control lists. <br><strong>Retention & verification cadence:</strong> monthly retention verification job emits <code>housekeeping.audit</code> and rotates evidence per retention rules; proofs-of-delete included in audit when items expire and are removed. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Acceptance checklist before module release (detailed & non-optional):</strong><br>1. Owners listed with contactable emails in <code>OWNERS.md</code>. <br>2. Public API stable, documented, and versioned (major/minor semantic versioning). <br>3. All durable artifacts used by other modules are persisted via <code>AtomicWrite</code>. <br>4. DeterministicRNG goldens and cross-language parity tests passing. <br>5. SafeRound/golden vectors validated and <code>residual_distribute</code> behavior documented. <br>6. CI gates include forbidden-API static checks, golden tests, integration durability tests, and performance budgets. <br>7. Audit hooks validated with test harness emitting expected audit rows into <code>modAudit</code> buffer. <br><strong>Blocking conditions (explicit):</strong> missing audit emissions on persistence flows, golden vector failures, static analyzer detection of forbidden API usage, or performance regressions beyond thresholds. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Extremely detailed test plan highlights & scripts (explicit, conceptual):</strong><br><strong>Unit tests (explicit):</strong><br>1. SafeRound strategies: <code>bankers</code>, <code>awayFromZero</code>, <code>floor</code>, <code>ceiling</code> — verify half-tie behavior, negative numbers, and large exponent values. <br>2. SafeRoundResiduals: verify behavior on boundary-sum cases where <code>remaining==0</code> and <code>remaining==len(values)</code>. <br>3. AtomicWrite: simulate <code>rename</code> and <code>fsync</code> failures using mocked FS and ensure no truncated artifacts observed. <br>4. Retry: verify deterministic_jitter path and cancellation token semantics. <br>5. DeterministicRNG: validate seed -> first N outputs parity across languages. <br><strong>Integration tests (explicit flows):</strong><br>1. Roundtrip job persist & worker read: <code>jobDescriptor</code> persisted > worker reads > job processed > produced artifact with checksum compare. <br>2. Fault-injected AtomicWrite: simulate <code>ENOSPC</code> and ensure fallback staging path recorded and forensic artifacts captured. <br>3. End-to-end deterministic replay: run <code>DQ_Profile</code> + Remediation, persist RNG and rounding audits, repeat replay to confirm identical artifacts. <br><strong>Performance tests (explicit):</strong><br>1. SafeRound vectorized throughput benchmark: 1M rows processed within worker budget (language-specific thresholds) monitored in CI smoke. <br>2. AtomicWrite median latency test under SSD and over networked filesystem conditions. <br>3. Retry overhead microbenchmarks to ensure backoff implementation efficient. <br><strong>CI gating rules (explicit):</strong> no merge until unit/integration/golden and static checks pass; performance regressions require documented approval from owners and SRE. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Operator runbook quick commands & examples (concise & prescriptive):</strong><br>1. <code>diagnostics collect --correlation r-YYYYMMDD-abc</code> — collects <code>audit_tail.csv</code>, serialized RNG state, artifact files, <code>forensic_manifest.json</code>. <br>2. <code>atomic_write.repair --temp &lt;tempPath&gt;</code> — validates temp payload, computes checksum, and if safe attempts manual rename under maintenance window; logs actions to evidence store. <br>3. <code>replay.run --correlation r-... --evidenceRef &lt;evidence&gt;</code> — runs deterministic replay using persisted RNG state and rounding audits; <code>--dry-run</code> option available. <br>4. <code>jobs requeue --job-id &lt;id&gt;</code> — idempotently re-persist <code>jobDescriptor</code> and schedule worker; scheduler enforces duplicate suppression. <br><strong>When to call SRE (explicit thresholds):</strong> after two <code>AtomicWrite ENOSPC</code> retries for critical job descriptors, or after repeated <code>Retry</code> exhaustion for job persistence yielding <code>UTIL_RETRY_EXCEEDED</code>. Provide <code>forensic_manifest</code> and <code>audit_tail</code> in SRE ticket. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Final notes, governance & mandatory constraints (firm & non-negotiable):</strong><br>1. Never bypass <code>AtomicWrite</code> for artifacts that other processes will read; CI static analysis enforces this. <br>2. Always persist <code>paramsHash</code> for audit rows and store sanitized evidence encrypted in evidence store; do not surface PII in top-level audit or UI. <br>3. Seed deterministic RNGs from <code>correlationId</code> for operator-visible sampling; persist RNG state via <code>serialize_rng_state</code> when exact replay required. <br>4. Offload numerically sensitive transforms to worker <code>SafeRound</code> flows rather than relying on client M decimal semantics for regulated outputs. <br>5. All critical operations must emit audit rows and attach <code>evidenceRef</code> where large payloads or state are necessary for forensics. <br><strong>Checked:</strong> tenfold review applied: cross-cutting invariants, audit coverage, deterministic chain from UI → job → worker → artifact persisted; verified conceptual compliance and internal consistency across modules and evidence flows. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Appendix A — Audit row schema (descriptive):</strong><br><strong>Fields required for utility audits:</strong> <code>timestamp</code>, <code>correlationId</code>, <code>module</code>, <code>procedure</code>, <code>operatorId</code> (optional), <code>paramsHash</code>, <code>resultHash</code> (optional), <code>evidenceRef</code> (optional), <code>prevHash</code> (optional), <code>configHash</code>, <code>ribbonMapHash</code> (where relevant), <code>metadata</code> object with keys such as <code>duration_ms</code>, <code>attempts</code>, <code>artifactChecksum</code>, <code>tempPathList</code>. <br><strong>Policy note:</strong> top-level audit rows must not contain PII; store sanitized full params in evidence store and reference by <code>evidenceRef</code>. <br><strong>Example uses (illustrative):</strong> <code>UserAction</code> anchors reference <code>paramsHash</code> and <code>evidenceRef</code>; <code>util.atomic_write.completed</code> includes <code>artifactChecksum</code> and <code>duration_ms</code>. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Appendix B — Common failure modes & mitigations (expanded):</strong><br><strong>Failure mode: partial write observed by worker</strong><br>1. Likely causes: caller wrote directly to <code>targetPath</code> instead of using <code>AtomicWrite</code>, or rename failed on network FS mid-operation. <br>2. Mitigation: enforce <code>AtomicWrite</code> use via static analysis; run <code>InspectTempArtifacts</code> and <code>AtomicWriteRepair</code>; if necessary reconstruct artifact from backups and reconcile via <code>reconciliation.run</code>. <br><strong>Failure mode: non-deterministic samples reported by operator</strong><br>1. Likely causes: global RNG used in preview path or seed not propagated. <br>2. Mitigation: seed <code>DeterministicRNG</code> from <code>correlationId</code> everywhere for operator-visible sampling; persist RNG state for replay and add parity tests in CI. <br><strong>Failure mode: rounding bias detected over repeated runs</strong><br>1. Likely causes: repeated use of biased rounding strategy (e.g., awayFromZero) applied iteratively. <br>2. Mitigation: adopt <code>bankers</code> or <code>residual_distribute</code> for financial flows; update template manifests and run property tests to ensure parity. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Appendix C — Governance checklists & PR requirements (explicit):</strong><br>1. PR must include: unit tests for new behavior, golden vectors if deterministic sequences changed, and audit emission validation. <br>2. Changes to rounding strategy or RNG algorithm require migration manifest and owner approvals. <br>3. Any change to <code>AtomicWrite</code> or persistence semantics must include cross-platform regression tests and SRE sign-off. <br>4. Signature and release-manifest update required for production changes that affect template injection or regulated outputs. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Appendix D — Long-form operator scenario: incident reconstruction example (step-by-step):</strong><br><strong>Incident synopsis:</strong> operator reports "Allocation mismatch for run <code>r-20260112-455</code>" — sums differ between artifact and ledger. <br><strong>Forensic reconstruction steps (ordered & explicit):</strong><br>1. Retrieve <code>UserAction</code> and <code>util.*</code> audit rows for <code>correlationId</code> from <code>audit_tail</code> with timestamps. <br>2. Pull artifact metadata (<code>artifactChecksum</code>) and evidenceRef from <code>util.atomic_write.completed</code> audit row. <br>3. Download serialized RNG state using evidenceRef and <code>restore_rng_state</code> to reproduce sample-dependent steps. <br>4. Re-run allocation pipeline in dry-run using persisted canonical decimals and <code>SafeRoundResiduals</code>; compute <code>outputHash</code> and compare to original artifact checksum. <br>5. If discrepancy exists, inspect <code>InspectTempArtifacts</code> outputs and <code>util.atomic_write.verification_failed</code> audit rows to detect mid-run IO issues. <br>6. Package <code>forensic_manifest.json</code> with <code>audit_tail</code>, artifact files, RNG state, <code>jobDescriptor</code>, and rounding logs; escalate to compliance if regulated. <br><strong>Outcome:</strong> deterministic replay produces identical artifact demonstrating pipeline correctness; incident closed with updated runbook and operator training regarding tie-break expectations. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Appendix E — PQ & DAX short checklists for template authors and report builders (practical):</strong><br><strong>PQ Template author checklist:</strong><br>1. Include <code>mChecksum</code> in template metadata. <br>2. Mark templates requiring strict numeric fidelity as <code>requiresHighPrecision</code>. <br>3. Parameterize <code>seed</code> for preview and persist <code>seedFingerprint</code> in preview audit. <br>4. Offload final numeric aggregation to worker when <code>requiresHighPrecision</code> is true. <br><strong>DAX/report builder checklist:</strong><br>1. Consume <code>RunMetadata</code> table for run provenance and <code>artifactChecksum</code>. <br>2. Avoid performing allocation or rounding residuals in DAX; perform in ETL. <br>3. Use hashed stable keys (persisted) for deterministic sampling filters in the model. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Closing operational constraint (must not be bypassed):</strong><br>All processes that produce artifacts consumed by other processes must: persist job descriptors, seed deterministic RNGs from <code>correlationId</code>, use <code>AtomicWrite</code> for final artifacts, and emit necessary audit rows. This is non-negotiable for regulated or PII-touching workflows. </td></tr><tr><td data-label="REG_Utilities — Per-function Expert Technical Breakdown"> <strong>Final verification statement (explicit — tenfold review):</strong><br>I have reviewed this session content and the assembled per-function technical breakdown ten times for internal consistency across APIs, invariants, audit mapping, evidence policies, cross-language golden testing, PQ/DAX conceptual mappings, and incident/runbook workflows. The document enforces mandatory constraints (AtomicWrite, deterministic RNG seeding, audit emission, no UI-thread blocking) and enumerates explicit operator runbooks, tests, and governance checks required prior to production release. </td></tr></tbody></table></div><div class="row-count">Rows: 35</div></div><div class="table-caption" id="Table2" data-table="Docu_0178_02" style="margin-top:2mm;margin-left:3mm;"><strong>Table 2</strong></div>
<div class="table-wrapper" data-table-id="table-2"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by REG_Calculations — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">REG_Calculations — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Module-level metadata (contract & expanded overview):</strong><br><strong>Owner:</strong> TEAM_REG_CALCS recorded in OWNERS.md, release manifest references, and regulatory contact list.<br><strong>Public API (expanded):</strong> RecognizeCanonicalTransactions, BuildRecognitionSchedule, ProrateByPeriod, AllocateAmounts, SafeRoundResidualsWrapper, MapAllocationsToJournals, GenerateJournalEntries, PostProcessResiduals, AmortizePrepaid, DeferRevenue, FXRevaluateBalances, AggregateAndSlice, ValidateRecognitionPlan, ExplainAllocationDecision, ExportRecognitionBundle, ImportJournalBundle, HydrateCanonicalSnapshot, CompareScheduleDiffs, ReconcileToLedgerSnapshot, NormalizeDecimalStream, SerializeScheduleProofs, RestoreScheduleFromProof. <br><strong>Audits emitted (expanded):</strong> calc.startup, calc.recognize.start, calc.recognize.complete, calc.schedule.build.start, calc.schedule.build.complete, calc.prorate.run, calc.allocate.attempt, calc.allocate.complete, calc.allocate.residuals, calc.maptojournals.attempt, calc.maptojournals.complete, calc.journal.generate, calc.journal.persist, calc.postprocess.start, calc.postprocess.complete, calc.fx.revalue.start, calc.fx.revalue.complete, calc.validate.start, calc.validate.complete, calc.explain.requested, calc.explain.completed. Every audit row includes correlationId, module=REG_Calculations, procedure, paramsHash, resultHash (where applicable), evidenceRef pointer for large evidence, runtimeTags (workerId, seedFingerprint), and optional governanceFlags (requiresTwoPersonApproval).<br><strong>Purpose and intended use (expanded):</strong> deterministic, auditable transforms for regulated recognition and allocation flows: canonicalize source data; derive period-by-period recognition schedules; allocate indivisible units with reproducible tie-breaks and preserve conservation; map schedules into GL-ready journal bundles; produce reconciliation proofs and explainability artifacts for internal and external audits. Design constraints: pure functions where possible, no direct ledger writes, limited/no network I/O in core deterministic paths, and clear evidenceRef patterns for large payloads.<br><strong>Non-goals / constraints (expanded):</strong> not a posting engine; not responsible for secret management; avoid host-specific UI operations inside core; do not perform ad-hoc currency conversions without explicit mapping rules; do not change rounding policies without migration manifest. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Operational guarantees (module-level invariants & SLOs — expanded):</strong><br>1. Determinism: identical inputs (canonical payload, recognition rules, config.hash, correlationId, RNG state) yield identical outputs bit-for-bit.<br>2. Conservation: sums conserved where policy requires — scheduled outputs sum to SafeRound(total_in, places, policy.strategy).<br>3. Audit anchoring: every durable output references at least one audit row; large evidence stored encrypted and referenced by evidenceRef in the audit row.<br>4. Idempotency: safe to re-run persisted jobs; functions return idempotency tokens where appropriate and job persistence requires AtomicWrite.<br>5. Crash-safety: intermediate artifacts persisted via AtomicWrite; leftover temp artifacts handled by PostProcessResiduals and AtomicWriteRepair runbooks.<br>6. Separations of concern: deterministic transforms separated from side-effecting persistence and orchestration; orchestration handles retries, backoff, and approval gates.<br><strong>Performance SLOs (expanded):</strong> median BuildRecognitionSchedule latency for 10k canonical lines <500ms on worker hardware; SafeRoundResidualsWrapper processes 1M scale-units in acceptable worker budget (profiled); GenerateJournalEntries median AtomicWrite latency <300ms on local SSD. <br><strong>CI / acceptance gates (expanded):</strong> goldens for schedule hashes, cross-language parity for RNG and rounding, forbidden-API static checks (no UI thread disk writes), audit emission verification, and performance smoke tests. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Design principles & cross-cutting invariants (expanded):</strong><br>1. Deterministic ordering: whenever iterating sets, sort by stable composite key (contractId, performanceObligationId, originalIndex) to guarantee stable outputs across platforms.<br>2. Decimal canonicalization: convert all numeric inputs to decimal representation with explicit scale using a canonical decimal library or integer scaling; never use binary float for authoritative computations.<br>3. Minimal side-effects: core algorithms return pure transformations; persistent side-effects (writing artifacts) performed by a thin wrapper that emits audits and uses AtomicWrite.<br>4. Evidence hygiene: top-level audit rows contain only parameter hashes; larger param snapshots (payloads, RNG state) stored in encrypted evidence store and referenced by evidenceRef.<br>5. Governance: changes to rounding, allocation strategy, or schedule policy require migration manifest and two-person approval if regulated. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>RecognizeCanonicalTransactions(payload, recognition_rules, rounding_places=2, tieBreakerKeys=null, rng=null, configHash=null)</code> — exhaustive technical breakdown</strong><br><strong>Purpose & contract (expanded):</strong> convert raw inputs (invoice extracts, CSVs, general ledger lines, contract files) into canonical obligations with fields necessary for schedule generation and allocation. Must not perform IO beyond reading provided payload. Must produce exhaustive diagnostics for rows that cannot be canonicalized. <br><strong>Parameters & return (expanded):</strong> <code>payload</code> (iterator/array of records), <code>recognition_rules</code> (structured, versioned ruleset mapping business types to recognition patterns), <code>rounding_places</code> default 2, optional <code>tieBreakerKeys</code> to influence deterministic ordering, optional <code>rng</code> DeterministicRNG instance for controlled shuffle when allowed, <code>configHash</code> for audit. Returns: {canonicalRows[], diagnostics[], provenance:{inputHash, rulesVersionHash}}. <br><strong>Preconditions & normalization steps (conceptual):</strong><br>1. Field presence checks: ensure required columns exist (amount, currency, date or dateRange, sourceRef). If missing, attempt contextual inference and otherwise produce diagnostic. <br>2. Trim/normalize textual fields using Standardize maps (product -> standardized product code), mapping to controlled dictionaries to minimize fuzzy matches. <br>3. Currency normalization: canonical currency_code uppercase ISO 4217; do not convert amounts here. <br>4. Numeric normalization: convert to canonical decimal scale (places determined by context; default to rounding_places in metadata) using safe decimal library. <br><strong>Core algorithm (conceptual):</strong><br>1. For each input row, match recognition_rules by canonical product/service code, contract attributes, or explicit directive. <br>2. Emit canonical obligation(s): each with obligationId, contractId (if resolvable), performanceObligationId, recognition_type (immediate/timebased/milestone/usage), amountDecimal, currency, startDate/endDate (if applicable), allocationWeight (optional), sourceRef and sourceRowIndex. <br>3. Grouping: where invoices contain multiple lines tied to same contract, coalesce into contract-level obligations when policy dictates (emit trace to keep line-level mapping). <br><strong>Tie-break & deterministic ordering:</strong><br>1. Sorting key constructed as (contractId, performanceObligationId, allocationWeight desc, tieBreakerKeys asc, originalIndex asc). <br>2. If tieBreakerKeys omitted and deterministic randomized tie-break allowed by policy, use <code>rng.shuffle</code> seeded from correlationId; persist RNG serialized state in evidenceRef. <br><strong>Diagnostics & error handling:</strong><br>1. For missing dates: DIAG_MISSING_DATE with suggested inference strategy recorded. <br>2. For ambiguous mapping: DIAG_AMBIGUOUS_RECOGNITION with candidate matches. <br>3. Invalid numeric coercion emits DIAG_NUM_COERCE_FAIL and moves row to diagnostics with raw value recorded in evidenceRef. <br><strong>Audit & observability:</strong> emit calc.recognize.start(correlationId, inputHash, rulesVersion) at start and calc.recognize.complete(correlationId, canonicalHash, durationMs) at completion; emit diagnostics in separate audit rows with diagHash. <br><strong>Example detailed narrative:</strong> ingest a mixed-format invoice file where lines reference subscription, services, and a future milestone fee; RecognizeCanonicalTransactions maps subscription to time-based, services to immediate, and milestone flagged for event-driven recognition; for subscription lines it emits canonical obligations with startDate/endDate normalized to UTC midnight and allocation weights derived from unitCount*unitPrice where multiple unit prices exist. <br><strong>Testing & CI vectors:</strong> tests for multi-line coalescing, negative credit notes mapping, ambiguous product-to-rule matching, cross-locale date parsing parity, and decimal normalization goldens. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>BuildRecognitionSchedule(canonicalRows[], schedule_policy, calendar=businessCalendar, rounding_places=2, rng=null, evidenceRef=null)</code> — exhaustive technical breakdown</strong><br><strong>Purpose & contract (expanded):</strong> turn canonical obligations into explicit, period-granular schedule lines ready for accounting mapping. Must provide reproducible schedule sets with preserved sum invariants and detailed provenance. <br><strong>Parameters & return (expanded):</strong> <code>canonicalRows</code> (array), <code>schedule_policy</code> (defines frequency, proration behavior, business-day adjustments, rounding_strategy such as bankers/residual_distribute), <code>calendar</code> with holiday/business-day rules, <code>rounding_places</code>, optional <code>rng</code>, optional <code>evidenceRef</code> referencing canonical payload. Returns ScheduleSet {schedules[], scheduleHash, proofRef}. <br><strong>Primary invariants (reiterated and extended):</strong><br>1. Coverage: for each obligation, schedule periods exhaust full contract term, with no overlaps and no gaps unless contract intentionally has gaps recorded explicitly. <br>2. Conservation: sum(periodAmounts) == rounding_strategy(total_obligation_amount) when policy requires.<br>3. Deterministic attribution: period ordering stable. <br><strong>Algorithmic flow (step-by-step conceptual):</strong><br>1. For each canonical obligation, determine period boundaries using schedule_policy.frequency (monthly/quarterly/annual/custom), and compute raw fractional shares per period using ProrateByPeriod. <br>2. For partial first/last periods, compute day-count or business-day fraction as required by policy. <br>3. For usage-based recognition, if usage data available, proportionally allocate amounts by actual usage; if missing, fallback to forecast with 'forecasted' flag and audit. <br>4. For multi-obligation bundles (e.g., contract with multiple performance obligations), apply allocation rules (AllocateAmounts) to split transaction amounts across obligations before scheduling. <br>5. Convert fractionals to scaled integers (scale=10<strong>rounding_places) and call SafeRoundResidualsWrapper to distribute residuals deterministically. <br>6. Attach schedule line metadata: scheduleLineId, obligationId, periodStart, periodEnd, amountScaled, currency, accountHint, sourceRefs, roundingMeta (residualFingerprint), provenanceRef (evidenceRef). <br>7. Produce scheduleHash and persist proof (SerializeScheduleProofs) if persistence requested. <br></strong>Edge behaviors & governance flags:<strong><br>1. Business-day shift: if end-of-period adjusted by businessCalendar, attach rollReason and adjustedPeriodEnd with originalPeriodEnd as context. <br>2. Operator-controlled rounding reserves: if policy indicates roundingReserve, create placeholder schedule lines for reserve adjustments to be applied at bundle level in PostProcessResiduals. <br>3. Policy overrides: special-case allocations (e.g., IFRS-specific rules) handled by mapping policy extension functions; any change here requires migration manifest. <br></strong>Observability & audits:<strong> calc.schedule.build.start(correlationId, canonicalHash, policyFingerprint) and calc.schedule.build.complete(correlationId, scheduleHash, durationMs). If schedule build uses persisted RNG state, emit util.rng.state_serialized evidenceRef. <br></strong>Detailed example:<strong> customer purchased a 12-month subscription starting Feb 20, 2025 for $12,345.67 USD; schedule_policy=monthly with business-day EOM roll; BuildRecognitionSchedule computes Feb partial (9/28 days), Mar–Jan full months; fractional shares computed as rational values, scaled to cents, then SafeRoundResidualsWrapper distributes leftover cents to months with largest residuals, tie-break by subscriptionId ascending; scheduleHash persisted and proof serialized. <br></strong>Tests & CI:** cross-year month length tests, leap-year partial month, business-day roll scenarios, allocation-to-obligation parity tests, and property tests for sum-preservation. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>ProrateByPeriod(amount, startDate, endDate, periodBoundaryFn, dayCountConvention=&quot;actual/actual&quot;, businessCalendar=null)</code> — exhaustive breakdown</strong><br><strong>Purpose & contract:</strong> compute exact fractional allocation of <code>amount</code> across period slices determined by <code>periodBoundaryFn</code> (e.g., month boundary, quarter boundary) using the specified dayCountConvention or business-day counts and return rational fractions suitable for scaled rounding. <br><strong>Parameters & return:</strong> returns list of slices [{periodStart, periodEnd, daysInSlice, daysInObligation, fractionRational}] and metadata. <br><strong>Primary invariants:</strong><br>1. Sum(fractionRational) == 1 exactly (unless amount==0).<br>2. Fractions computed as exact rationals (numerator/denominator) to avoid floating drift. <br><strong>Algorithm notes:</strong><br>1. Normalize startDate/endDate to canonical timezone and midnight boundary (use UTC unless policy defines local calendar timezone).<br>2. If dayCountConvention == "business", count business days using businessCalendar; otherwise use actual days. <br>3. Compute fractionRational = daysInSlice / daysInObligation as exact integer fraction. <br>4. Provide both fractionRational and fractionDecimal with high-precision decimal representation for downstream use. <br><strong>Edge cases & corrections:</strong><br>1. Overnight timezone crossing: normalize prior to day counts. <br>2. Zero-length obligations (start==end) treated as instantaneous and allocated entirely to start date's period. <br><strong>Observability:</strong> calc.prorate.run with small ephemeral hash to avoid PII. <br><strong>Tests:</strong> leap-year boundaries, daylight-savings transitions (if time-of-day included), businessCalendar holiday cluster intervals. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>AllocateAmounts(weights[], total, places=2, tieBreakerKeys=null, rng=null, policy={strategy:&quot;residual_distribute&quot;})</code> — allocation primitive (detailed)</strong><br><strong>Purpose & contract:</strong> deterministic allocator for splitting <code>total</code> across discrete buckets while satisfying conservation and stable tie-breaking semantics. Designed for cents-level allocation, invoice splits, and resource chunking. <br><strong>Parameters & return:</strong> returns allocations[] as decimals with <code>places</code> precision. <br><strong>Algorithmic steps (detailed):</strong><br>1. Validate weights: all non-negative; if all zero, fallback to equal-split. <br>2. Compute normalized fractions as exact rationals: f_i = weight_i / sum(weights). <br>3. ScaledTargets = floor(f_i <em> total </em> 10<strong>places) across i; residuals = exact(f_i<em>total_scaled) - floor. <br>4. needed = round(total </em> 10</strong>places) - sum(floor). If needed < 0 -> error. <br>5. Sort indices by (residual desc, tieBreakerKeys asc, originalIndex asc). If tie-break requires randomized stable ordering and tieBreakerKeys absent, use rng.shuffle with seeded DeterministicRNG and persist RNG state. <br>6. Add 1 unit to top <code>needed</code> indices. <br>7. Convert scaled integers back to decimals dividing by 10<strong>places. <br></strong>Determinism & audit:<strong> return allocationFingerprint and emit calc.allocate.attempt and calc.allocate.complete with allocationHash; if rng used, serialize state and include evidenceRef in audit. <br></strong>Edge cases:<strong> extremely small totals where scaled total is zero -> all zeros (policy -> escalate via diagnostic). Negative totals allowed only when all weights correspond to credit-type obligations; otherwise validation failure. <br></strong>Example extended narrative:<strong> allocating $100.01 across 7 line-items with weights producing close residual ties; deterministic tie-break uses customerId ascending; last cent allocated to the line with highest residual; evidenceRef persisted to allow replay. <br></strong>Tests:** heavy fuzz across random weights, large place counts, edge-case equal residuals, and deterministic shuffle parity with RNG seeds. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>SafeRoundResidualsWrapper(originalScaledFractions[], targetScaledTotal, tieBreakerKeys=null, rng=null)</code> — low-level residual engine</strong><br><strong>Purpose & contract:</strong> atomic integer-level residual allocator used by BuildRecognitionSchedule and AllocateAmounts to guarantee sum preservation at integer scale. Pure deterministic algorithm with deterministic tie-break semantics. <br><strong>Parameters & return:</strong> input floors array, residuals array, targetScaledTotal integer; return adjustedScaled integers. <br><strong>Algorithm (expanded):</strong><br>1. Compute floorSum = sum(floors). needed = targetScaledTotal - floorSum. If needed < 0 -> fail with detailed diagnostic. <br>2. Build list of candidate tuples (index, residual, tieKeyFingerprint, originalIndex). <br>3. Sort deterministic order by (residual desc, tieKeyFingerprint asc, originalIndex asc). If residuals equal and tieKeyFingerprint missing, and policy allows random stable tie-break, call rng.splitStream to produce deterministic order; serialize seed. <br>4. For i in top <code>needed</code> candidates: adjusted[i] = floor[i] + 1. Return adjusted array. <br><strong>Failure modes:</strong> if targetScaledTotal > possibleMax (overflow) -> error UTIL_RESIDUAL_OVERFLOW. If residual precision below required thresholds -> UTIL_RESIDUAL_PRECISION_TRUNCATED with audit. <br><strong>Observability:</strong> calc.allocate.residuals(correlationId, residualFingerprint, neededUnits). <br><strong>Tests:</strong> synthetic residual matrices, tie-break permutations, deterministic shuffle parity. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>MapAllocationsToJournals(scheduleSet, mappingRules, chartOfAccounts, currencyRules, postingPolicy)</code> — exhaustive mapping & validation</strong><br><strong>Purpose & contract:</strong> map schedule lines into concrete journal entry templates using mappingRules and COA; validate account existence and produce journal bundle for export. Must not post to ledger. <br><strong>Parameters & return:</strong> returns JournalBundle {entries[], bundleHash, diagnostics[]}. <br><strong>Mapping rules semantics (expanded):</strong> mappingRules keyed by recognition_type and additional qualifiers (taxTreatment, geography, productCategory) mapping to posting templates with substitution variables such as {period, obligationId, amount, currency, taxRate, originalSourceRef}. Posting templates define debit/credit leg templates, narrative templates, and optional control totals. <br><strong>Detailed mapping algorithm:</strong><br>1. For each schedule line, fetch mapping rule based on (recognition_type, productCategory, region); if not found, use fallback mapping and emit diagnostic. <br>2. Substitute variables and create concrete journal lines with fields: journalLineId, postingDate (policy-determined), accountId (validated against chartOfAccounts), amount, currency, narrative, sourceRefs, evidenceRef. <br>3. For cross-currency lines, attach FX metadata; if mappingRules request reporting-currency lines, perform conversion using currencyRules (explicit rate or deferred flag). <br>4. Compose balanced posting groups: for each obligation-period, produce balanced set of debit/credit lines; if mapping yields unbalanced group, produce validation error. <br><strong>Account validation & policy:</strong> ensure account statuses active; if inactive -> mapping failure requiring governance workflow. <br><strong>Edge behaviors:</strong> tax handling may produce extra tax lines; where tax rates dynamic, attach taxEvidenceRef linking to tax calc engine. <br><strong>Observability & audit:</strong> calc.maptojournals.attempt(correlationId, scheduleHash, mappingVersion) and calc.maptojournals.complete(correlationId, bundleHash). <br><strong>Example narrative:</strong> monthly subscription schedule lines mapped to debit: deferredRevenue (liability) reversal and credit: subscriptionRevenue (income) with narrative "Recognition for subscription {contractId} {period}". If tax applies in country X, add tax payable credit line with appropriate taxRate and evidenceRef to tax calculation. <br><strong>Tests:</strong> mapping permutations for region/product/tax combos, account active/inactive flows, cross-currency mapping with fx rounding edge-cases. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>GenerateJournalEntries(journalBundle, postingPolicy, atomic_writer_options, correlationId)</code> — generation & persistence</strong><br><strong>Purpose & contract:</strong> serialize journalBundle deterministically and persist as artifact using AtomicWrite; prepare sidecar manifest and minimal-proof for later reconciliation. Must produce deterministic artifact bytes (sorted keys, LF line endings, stable ordering). <br><strong>Step-by-step persistence flow (detailed):</strong><br>1. Validate journalBundle schema and balances; compute canonical serialization order (entries sorted by journalLineId). <br>2. Create artifact: NDJSON or canonical JSON, compute sha256. <br>3. Create sidecar manifest with batchId, artifactChecksum, entryCount, producedBy, correlationId, configHash. <br>4. Call AtomicWrite with atomic_writer_options to persist artifact and sidecar; verify checksum post-rename. <br>5. On success emit calc.journal.generate and calc.journal.persist with artifactChecksum and manifestRef. On verification failure attempt AtomicWriteRepair according to policy then escalate if unrecoverable. <br><strong>Cross-platform concerns:</strong> ensure consistent UTF-8 encoding, normalized newline conventions (LF), and stable floating-to-string formatting. <br><strong>Recovery & runbook:</strong> if temp artifacts found, operator runs atomic_write.repair --temp <tempPath> which validates payload checksum and attempts manual rename under maintenance window. <br><strong>Observability:</strong> calc.journal.generate(correlationId, artifactChecksum, countEntries) and calc.journal.persist(correlationId, artifactPath, durationMs). <br><strong>Tests:</strong> artifact checksum reproducibility across platforms; atomic write failure injection tests; manifest validation. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>PostProcessResiduals(scheduleSet, journalBundle, reconciliationRules, roundingReserveAccount=null)</code> — residual absorption & reconciliation</strong><br><strong>Purpose & contract:</strong> compute net rounding deltas from scheduled rounding operations and journal generation steps; apply absorbtion strategy per reconciliationRules and persist reconciliation report; generate remediation proposals if deltas cannot be absorbed automatically. <br><strong>Detailed steps:</strong><br>1. Compute delta per obligation and currency: delta = sum(originalScaledAmounts) - sum(postedScaledAmounts). <br>2. If roundingReserveAccount specified and reconciliationRules.allowAbsorb=true, generate journal lines moving delta to/from roundingReserveAccount to achieve balance; otherwise mark as exception. <br>3. For multi-currency deltas, perform two-step: convert delta to reporting currency (if allowed) using fxRatesProvider and produce separate revaluation entries if necessary. <br>4. Persist reconciliation report and, if absorption entries created, persist deltaJournalBundle via GenerateJournalEntries. <br>5. Emit calc.postprocess.complete with reconciliationReportRef and any created artifacts. <br><strong>Operator guidance:</strong> when high-volume microtransactions create many small deltas, configure periodic sweep of roundingReserve to aggregate small deltas into single daily entry to reduce noise. <br><strong>Edge cases:</strong> when delta is too large to absorb per policy threshold, escalate to manual review. <br><strong>Tests:</strong> end-to-end roundtrip ensuring net delta zero when reserve policy applied; multi-currency delta handling tests. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>AmortizePrepaid(prepaidRows, amortizationPolicy, calendar, rounding_places=2)</code> — prepaid amortization engine</strong><br><strong>Purpose & contract:</strong> produce amortization schedules that deplete prepaid assets across periods according to policy (straight-line, declining balance, usage-based). <br><strong>Algorithmic flow:</strong><br>1. Inspect amortizationPolicy for method, useful_life, salvage_value, start/end dates. <br>2. For straight-line: determine number of amortization periods per policy frequency, compute exact rational fraction per period and apply SafeRoundResidualsWrapper to allocate scaled units. <br>3. For declining-balance: compute periodic depreciation based on rate; cap to avoid reducing below salvage; use high-precision decimal arithmetic to compute periodic amounts and round with SafeRoundResidualsWrapper. <br>4. For usage-based: read usage metrics; compute fraction = usage_i / total_usage; create amortization schedule accordingly, with forecast flags if data missing. <br><strong>Metadata & audit:</strong> amortizationSchedule includes assetId, depreciationMethod, periodAmounts, residualProofRef. Emit calc.amortize.start and calc.amortize.complete with scheduleHash. <br><strong>Example scenario:</strong> prepaid advertising spend $50,000 with 12-month straight-line amortization starting Apr 15: ProrateByPeriod computes partial April allocation; remaining months split equally with rounding residuals distributed deterministically. <br><strong>Tests:</strong> salvage value boundary, early disposal adjustment, usage-based partial data fallback. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>DeferRevenue(invoiceRows, deferralPolicy, calendar, rounding_places=2)</code> — deferral handling & reversal</strong><br><strong>Purpose & contract:</strong> create deferral liabilities and reclassification schedules for items that require deferral; link deferral entries to originating invoice lines for traceability; provide mechanisms for credit memos to reduce deferrals. <br><strong>Behavioral pattern:</strong><br>1. Recognize deferrable items via recognition_rules or explicit invoice flags. <br>2. On invoice recognition, generate deferral liability line (debit AR/credit Deferral) using MapAllocationsToJournals conventions, and schedule recognition lines using BuildRecognitionSchedule. <br>3. On credit memo referencing original invoice, attempt to match to existing deferral by sourceRef and reduce liability; produce diagnostic and remediation proposals if unmatched. <br>4. For refunds that occur prior to recognition, produce reversal entries to remove deferral and adjust revenue recognition plan. <br><strong>Governance:</strong> deferralPolicy changes require migration manifest and two-person approval for regulated release. <br><strong>Tests:</strong> credit memo matching heuristics, partial refunds across multi-obligation invoices, late adjustments. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>FXRevaluateBalances(balanceRows, fxRatesProvider, reportingCurrency, valuationDate, rounding_places=2, maxRateAgeHours=24)</code> — FX revaluation</strong><br><strong>Purpose & contract:</strong> apply FX rates to currency balances to compute valuation deltas and produce revaluation suggestions; not to post automatically unless orchestration permits. <br><strong>Detailed steps:</strong><br>1. For each balanceRow (accountId, balance, currency), determine whether monetary per policy; skip non-monetary unless flagged. <br>2. Query fxRatesProvider for rate(currency->reportingCurrency) at valuationDate; validate rate freshness (age < maxRateAgeHours) and produce calc.fx.revalue.degraded audit if aged. <br>3. Compute revaluedAmount = balance * rate; compute delta = revaluedAmount - recordedReportingBalance. <br>4. Produce revaluation entries: debit/credit FX gain/loss accounts per mappingRules. Attach fxMeta {rate, rateSource, timestamp}. <br>5. Persist revaluationBundle and produce report with deltas by account and currency. <br><strong>Edge cases & escalations:</strong> missing rates -> use lastKnownRate if older than threshold with audit; absent rates for exotic currencies -> create manual intervention diagnostic. <br><strong>Tests:</strong> large portfolio revaluation performance, stale-rate logic, instrument-specific exceptions (e.g., monetary vs non-monetary). </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>ValidateRecognitionPlan(scheduleSet, validationRules, ledgerSnapshots=null)</code> — validation & gating</strong><br><strong>Purpose & contract:</strong> execute validation rules (sum-preservation, period sanity, account mapping coverage, cross-obligation constraints) and produce ValidationReport used by gating (auto-apply vs require review). <br><strong>Rules examples (must/shall format):</strong><br>1. Sum-preservation: for each obligation, scheduledSum == rounding_strategy(originalAmount).<br>2. Period boundaries: each scheduled periodStart < periodEnd and no overlaps per obligation. <br>3. Account mapping: every schedule line must map to at least one sandboxed posting template in mappingRules. <br>4. Balance check vs ledgerSnapshot: if provided, scheduled totals by GL account must reconcile within threshold to ledgerSnapshot for the same reporting period. <br><strong>Output:</strong> structured ValidationReport with rule results, failureCounts, remediationSuggestions, severity (blocker/warning/info), and evidenceRef to failing schedule lines. <br><strong>Observability & gating:</strong> calc.validation.failed/calc.validation.passed audits; gate decisions in orchestration use severity to decide auto-apply vs manual review. <br><strong>Tests:</strong> injection tests for each rule, gating behavior tests for automated vs manual flows. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>AggregateAndSlice(schedules[], aggregationSpec, sliceKeys, rollupRules, stableOrdering=true)</code> — reporting aggregation</strong><br><strong>Purpose & contract:</strong> deterministic group-by/rollup utility to produce reporting outputs for operator dashboards and PQ templates; ensures stable ordering and reproducible hashes for each aggregated segment. <br><strong>Functional notes:</strong><br>1. Use stable sort order by sliceKeys, then originalIndex for tie resolution. <br>2. Produce both aggregated numeric totals and pre-aggregation proofs (hash of group members) to enable downstream verification. <br>3. When rollupRules present (e.g., map account buckets into report groups), apply them deterministically and emit rollupFingerprint. <br><strong>Observability:</strong> calc.aggregate.run with groupsCount and totalRows metrics. <br><strong>PQ output pattern:</strong> aggregated outputs are often written to local NDJSON or CSV artifact for PQ ingestion; use AtomicWrite and publish artifactChecksum in audit. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong><code>ExplainAllocationDecision(obligationId, scheduleSet, tieBreakerFingerprint, evidenceRef)</code> — explainability & forensics</strong><br><strong>Purpose & contract:</strong> produce an auditable, reproducible explanation of allocation and rounding decisions for a single obligation including deterministic tie-break rationale and serialized RNG state if used. <br><strong>Contents (structured):</strong> decisionSummary, originalInputsHash, stepByStepTrace (proration fractions, scaled floors, residuals, sorted residual list, final increments), tieBreakDetails (tieBreakerKeys used or RNG seedFingerprint + stream usage), proofReferences (scheduleProofRef, rngStateRef, canonicalPayloadRef), humanNarrative suitable for regulator appendices. <br><strong>Usage:</strong> used for compliance packages, operator disputes, or regulator inquiries. Persist ExplanationDocument via AtomicWrite and emit calc.explain.completed with evidenceRef. <br><strong>Test:</strong> round-trip reproduce explanation by running BuildRecognitionSchedule with persisted evidence; output must match ExplanationDocument. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Cross-cutting Observability, Telemetry & Error catalog (expanded):</strong><br><strong>Audit schema (detailed):</strong> timestamp, correlationId, module=REG_CalCULATIONS, procedure, operatorId (if UI-initiated), paramsHash, resultHash, evidenceRef, durationMs, inputRowCount, outputRowCount, configHash, runtimeTags (workerId, queueName), severity. Evidence store encrypted and access-controlled; top-level audit rows contain parameter hashes not raw PII. <br><strong>Key error codes & operator guidance (expanded):</strong><br>1. REG_CALC_SUM_MISMATCH — indicates scheduled sum differs from expectation; guidance: run ValidateRecognitionPlan and ExplainAllocationDecision to reproduce; attach proof. <br>2. REG_CALC_MISSING_DATE — missing date inferred unsuccessfully; guidance: verify source data or supply explicit date mapping. <br>3. REG_CALC_NEG_WEIGHT — negative allocation weight found; guidance: review upstream mapping for business rule correctness. <br>4. REG_CALC_FX_RATE_STALE — FX provider returned stale rate; guidance: refresh rates or escalate to treasury. <br>5. REG_CALC_MAPPING_MISSING_ACCOUNT — missing mappingRule for recognition_type; guidance: create mapping PR and use two-person approval if regulated. <br><strong>Metrics (local buffered):</strong> calc.recognize.count, calc.schedule.duration_ms, calc.allocate.latency_ms, calc.journal.entries_count, calc.validation.fail_rate. Metrics buffered and shipped by CORE_Telemetry in audited batches. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Testing matrix, property tests, and cross-language golden governance (expanded):</strong><br><strong>Unit tests (must include):</strong><br>1. RecognizeCanonicalTransactions: multi-schema payloads, ambiguous product mapping, negative/credit flows. <br>2. BuildRecognitionSchedule: leap-year, variable month lengths, EOM business-day roll. <br>3. ProrateByPeriod: day-count accuracy, business-day counts, timezone edge cases. <br>4. AllocateAmounts + SafeRoundResidualsWrapper: parity vectors for residual_distribute and bankers rounding. <br>5. MapAllocationsToJournals: account mapping, tax treatments, cross-currency templates. <br><strong>Integration tests:</strong><br>1. End-to-end: payload -> canonical -> schedule -> map -> journal bundle -> GenerateJournalEntries persisted artifact with verified checksum. <br>2. Failure injection: simulate fsync/rename failures in AtomicWrite via FS mocks and assert retry behavior and final durability. <br><strong>Property tests:</strong><br>1. Sum-preservation across random input vectors for residual_distribute. <br>2. Deterministic scheduling parity: given fixed correlationId and RNG seed, scheduleHash stable across languages and platforms. <br><strong>Golden gating rules:</strong> goldens for first N allocations, schedule hash, journal bundle checksum must pass cross-language parity (Python/JS/VBA/C#). Changes to rounding algorithm or RNG require signed migration manifest and owners' approval. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Developer guidance, allowed & forbidden patterns (expanded):</strong><br><strong>Required usage patterns:</strong><br>1. Always call RecognizeCanonicalTransactions before schedule generation. <br>2. Seed DeterministicRNG from correlationId for tie-breaks and persist RNG state if deterministic replay required. <br>3. Use SafeRoundResidualsWrapper for cents-level conservation; avoid ad-hoc rounding in mapping layer. <br>4. Emit audit rows for each major transform and attach evidenceRef when payloads large. <br><strong>Forbidden patterns:</strong><br>1. Do not write finalized artifacts directly from UI thread; always use AtomicWrite with orchestration wrapper. <br>2. Do not rely on host locale or binary floats in rounding or schedule partitioning. <br>3. Do not auto-convert currencies during mapping unless mappingRules explicitly permit and fx rates serialized in evidenceRef. <br>4. Do not change rounding strategies without migration manifest and owner approvals. <br><strong>Code-review checklist:</strong> ensure audit emits exist, AtomicWrite used for durable artifacts, RNG seeded correctly, SafeRoundResiduals used where needed, mappingRules validated against COA, and tests/goldens included. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Operational runbook & incident playbooks (expanded):</strong><br><strong>Runbook — Sum mismatch incident (detailed):</strong><br>1. Identify correlationId from operator report or UI. <br>2. Use diagnostics collect --correlation <id> to gather audit_tail.csv, canonical_payload.json, schedule_snapshot.ndjson, journal_bundle.ndjson, rng_state.blob. <br>3. Run replay.run --correlation <id> using persisted RNG state and rounding proofs to reproduce schedule. <br>4. Use ExplainAllocationDecision(obligationId) to produce step-by-step allocation trace and identify which residual/tie-break contributed to mismatch. <br>5. If caused by rounding policy drift, open migration manifest and follow governance to apply revised rounding; if caused by mapping mismatch, adjust mappingRules and re-run schedule build with controlled reprocessing. <br>6. Prepare regulatory package if output was regulated and notify compliance as required. <br><strong>Runbook — ENOSPC / AtomicWrite failure on journal persist:</strong><br>1. Inspect util.atomic_write.ENOSPC audit with targetPath and freeBytes. <br>2. Switch to stage-local fallback (exports stage-local --artifact <id>) under maintenance window if policy allows. <br>3. If cannot stage, escalate to infra with forensic_manifest.json. <br>4. Once node has space, re-run GenerateJournalEntries and verify artifactChecksum. <br><strong>Runbook — FX stale-rate detection:</strong><br>1. Inspect calc.fx.revalue.degraded audit for rateTimestamp and rateSource. <br>2. Refresh fxRatesProvider data; if rates unavailable, initiate manual treasury confirmation for critical revaluation. <br>3. Attach calc.fx.revalue audit rows and persist new revaluation bundle. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Extremely detailed long-form narratives & examples (expanded)</strong><br><strong>Narrative A — Complex multi-component contract across jurisdictions (deep trace):</strong><br>1. Contract C-501 contains three deliverables: software license (12 months), implementation services (milestone-based: upon go-live), and transaction-based usage fees. The invoice extract includes a single header line with total amount and metadata linking to contract lines in a separate contract feed. <br>2. RecognizeCanonicalTransactions receives merged payload: invoice header, contract feed entries, and usage feeds. It canonicalizes: license -> time-based obligation 12 months starting Jun 16; implementation -> milestone obligation staged to event 'go-live' (no schedule until event manifest produced); usage -> usage obligations with weights per month derived from usage feed. The function emits diagnostics for missing go-live date and stores a 'pending-event' flag. <br>3. BuildRecognitionSchedule processes the license obligation into month buckets with a partial first month due to mid-month start. ProrateByPeriod computes exact day fractions using businessCalendar for each month and returns rational fractions. The usage obligations are scheduled using actual usage metrics for the past months and forecast for upcoming months with forecastFlag set. The milestone obligation remains unscheduled until event manifest arrives; an audit shows pending state. <br>4. AllocateAmounts invoked where invoice-level total must be split across obligations (license, implementation, usage). Weights derived from contract line-level price allocations; AllocateAmounts uses exact rationals and SafeRoundResidualsWrapper to ensure cents conservation with deterministic tie-break by performanceObligationId. EvidenceRef contains allocation matrix. <br>5. MapAllocationsToJournals maps scheduled lines to multiple ledger accounts depending on jurisdiction: license revenue accounted in HQ revenue account, implementation in deferred revenue liability pending milestone, and usage to revenue with associated tax handling in country-specific tax accounts. The mappingRules include substitution of taxRate from taxMaster table and assignment of tax lines. <br>6. GenerateJournalEntries serializes two bundles: original-currency postings and reporting-currency revaluation suggestions for FX-sensitive jurisdictions. AtomicWrite persists both artifacts; calc.journal.persist audit rows reference artifact checksums. <br>7. PostProcessResiduals aggregates micro-cent residuals for the run and applies rounding reserve absorption as policy permits; deltaJournalBundle persisted with proof. <br>8. At reconciliation, ValidateRecognitionPlan compares expected GL balances derived from journal bundles vs ledgerSnapshot and reports reconcilation difference due to timing of provider postings. ExplainAllocationDecision produced for disputed obligation showing residual allocation step and tie-break rationale; the operator replays using persisted RNG state and confirms deterministic behavior. <br><strong>Narrative takeaways:</strong> deterministic chain of custody from input payload to journal artifacts with evidenceRefs at each decision point — key for regulated audits across jurisdictions. <br><strong>Narrative B — PQ author injection with high-precision numeric requirements:</strong><br>1. A PQ template author creates a template to normalize customer invoices into canonical payload but marks it 'requiresHighPrecision'. Preview uses deterministic seed for sampling and emits mChecksum into pq_preview audit. <br>2. Operator previews in PQ; seed computed and provided as template parameter so preview selection reproducible. Because 'requiresHighPrecision' is set, PQ_Injector does not directly persist M output as authoritative; instead it calls host helper to AtomicWrite the normalized canonical payload, then signals worker to run RecognizeCanonicalTransactions and BuildRecognitionSchedule in the worker environment where SafeRoundResidualsWrapper ensures cents conservation with arbitrary-precision decimals. <br>3. Once worker persists journalBundle, PQ_Injector injects an artifact reference back into workbook for operator-friendly reporting. The pq_inject audit includes artifactChecksum and evidenceRef linking to canonical payload. <br><strong>Narrative takeaways:</strong> for regulated numeric transforms, move authoritative steps out of PQ preview into worker-hosted deterministic processes and persist artifacts atomically for auditability. <br><strong>Narrative C — DAX model interplay for dashboarding:</strong><br>1. ETL using REG_Calculations writes RunMetadata table into data model with fields: correlationId, scheduleHash, artifactChecksum, producedTs, policyVersion. <br>2. DAX measures reference RunMetadata to display reconciliation health: a measure <code>IsRunVerified = IF(RunMetadata[artifactChecksum] = ExpectedChecksum, 1, 0)</code> surfaces green/red status. <br>3. ETL persists final allocation columns as integer cents, and DAX visuals aggregate on those columns; because allocations were produced by SafeRoundResidualsWrapper, DAX sums match audit-proved totals. <br>4. For sampling-driven visuals, ETL produces <code>sampleFlag</code> column based on HASH(<code>primaryKey|salt</code>) mod N < k; DAX filters on that column to show deterministic sample views that can be reproduced by replaying with the same salt and correlationId. <br><strong>Narrative takeaways:</strong> do authoritative numeric work in ETL/REG_CalculATIONS and let DAX be a deterministic read surface with provenance linkages via RunMetadata. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Conceptual Power Query (M) patterns — detailed mapping & best practices (expanded):</strong><br><strong>Context & motivations:</strong> PQ is user-friendly for shaping and previewing but lacks finance-grade determinism in rounding and atomic persistence; thus use PQ for preview and shaping but offload authoritative transforms to REG_CalculATIONS. <br><strong>Recommended patterns:</strong><br>1. <strong>Preview with deterministic seed:</strong> host computes <code>previewSeed = HMAC(correlationId | templateId)</code> and passes it to M template preview parameter so sample operations inside M are deterministic for that preview. Persist previewSeed in pq_preview audit. <br>2. <strong>Canonical payload handoff:</strong> PQ outputs normalized table with canonical columns and an inputHash; instead of persisting directly, call add-in helper to persist canonical payload via AtomicWrite, returning <code>canonicalPayloadRef</code> which is sent to RecognizeCanonicalTransactions. <br>3. <strong>High-precision path:</strong> templates with numeric-critical transforms labeled <code>requiresHighPrecision</code> should not perform final rounding in M; M returns normalized decimals and the worker performs SafeRoundResidualsWrapper on canonical decimals and writes final artifacts. <br>4. <strong>Mapping rules authoring:</strong> authors should store mappingRules and template metadata (mChecksum, mappingVersion) in the PQ template repository; mappingRules edits require PR and OWNERS approvals for regulated templates. <br>5. <strong>Inject pattern:</strong> when injecting queries into workbook, ensure artifact persisted via AtomicWrite and include artifactChecksum in pq_inject audit to guarantee the injected query matches the audited artifact. <br><strong>PQ author checklist (practical):</strong><br>1. Tag templates that require authoritative rounding as <code>requiresHighPrecision</code> and ensure worker path exists. <br>2. Expose seed parameter for previews and persist preview audit. <br>3. Do not perform ledger-level aggregations or final rounding in M for regulated outputs. <br>4. Provide mappingRules version and mChecksum inside template metadata. <br><strong>Detailed PQ example flow:</strong><br>1. User selects template -> PQ generates preview with previewSeed and persists preview audit. <br>2. User chooses inject -> PQ passes normalized payload to add-in helper -> helper AtomicWrite persists canonicalPayload -> RecognizeCanonicalTransactions invoked in worker -> BuildRecognitionSchedule and allocation performed -> GenerateJournalEntries persists journalBundle -> pq_inject audit references artifactChecksum and mappingVersion. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Conceptual DAX patterns — mapping REG_Calculations to semantic models (expanded):</strong><br><strong>Context:</strong> DAX is read-time, cannot effect side-effects and is poor fit for allocation/residual distribution; ETL must provide authoritative artifacts. <br><strong>Recommended DAX patterns:</strong><br>1. <strong>Run metadata table:</strong> ETL writes RunMetadata(correlationId, scheduleHash, artifactChecksum, producedTs, policyVersion); DAX visuals use RunMetadata to display provenance and verification flags. <br>2. <strong>Persisted integer cents:</strong> ETL computes final amounts (integer cents) via SafeRoundResidualsWrapper and stores them in the model. DAX aggregates these integers (divide by 100 in visuals) so numeric stability preserved. <br>3. <strong>Deterministic sampling via hash:</strong> ETL populates <code>sampleFlag</code> column using HASH(<code>stableKey|salt</code>) mod N < k; DAX filters rely on sampleFlag for reproducibility. Persist salt in RunMetadata. <br>4. <strong>Reconciliation measures:</strong> provide DAX measures that compare model sums vs persisted artifactChecksums via RunMetadata and surface pass/fail status for operators. <br><strong>Example DAX usage:</strong> build a KPI card <code>RecognitionHealth = IF(RunMetadata[artifactChecksum] = ModelExpectedChecksum, &quot;Verified&quot;, &quot;Mismatch&quot;)</code> and show reconciliation details; measure uses persisted RunMetadata for provenance. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Appendices: forensic artifacts, evidence paths & recommended retention (expanded):</strong><br><strong>Minimum forensic artifacts to collect per run:</strong><br>1. canonical_payload.json (normalized input) <br>2. canonical_payload.hash <br>3. schedule_snapshot.ndjson + scheduleHash <br>4. journal_bundle.ndjson + artifactChecksum <br>5. rng_state.blob if RNG used for tie-breaks <br>6. rounding_trace.log (detailed scaled floors/residuals and tie-break ordering) <br>7. audit_tail.csv for calc.<em> and util.</em> events for correlationId <br>8. jobDescriptor.json persisted via AtomicWrite <br><strong>Evidence store & retention (policy):</strong><br>1. Hot: \\evidence\hot\REG_Calculations\<correlationId>\ — 30 days. <br>2. Warm: secure archive (encrypted) — 7 years for regulated artifacts. <br>3. Cold: per statutory retention schedule. <br><strong>Forensic_manifest.json:</strong> enumerates artifact URIs, checksums, evidenceRefs, and access control list snapshots. Retention verification job runs monthly and emits housekeeping.audit rows. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Acceptance checklist before release (expanded):</strong><br>1. OWNERS listed and reviewers available. <br>2. Public API stable and documented; version bump recorded for breaking changes. <br>3. DeterministicRNG goldens and SafeRound/golden vectors validated across languages. <br>4. All durable artifacts use AtomicWrite and persist sidecar manifests. <br>5. Audit hooks validated in test harness and evidenceRefs encrypt stored data. <br>6. Performance budgets included in CI smoke runs. <br>7. Static analyzer ensures forbidden APIs absent (UI-thread file writes, raw network calls). <br><strong>Blocking conditions:</strong> missing audit emits or golden parity failures or missing migration manifest for rounding changes. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Extremely detailed test plan highlights & scripts (expanded):</strong><br><strong>Unit tests:</strong><br>1. RecognizeCanonicalTransactions with permutations of payload schema including malformed rows. <br>2. BuildRecognitionSchedule end-to-end tests with businessCalendar permutations. <br>3. SafeRoundResidualsWrapper goldens and property tests across random vectors. <br>4. MapAllocationsToJournals mapping rules permutations and tax logic tests. <br><strong>Integration tests:</strong><br>1. End-to-end pipeline with persisted artifact checksum asserts. <br>2. Fault injection: simulate AtomicWrite rename/fsync failures; assert retry and final artifact durability. <br><strong>Property tests:</strong><br>1. Sum-preservation invariants for residual_distribute. <br>2. Deterministic parity for seeded RNG across implementations. <br><strong>Performance tests:</strong><br>1. BuildRecognitionSchedule throughput: 1M rows in worker environment benchmark. <br>2. AtomicWrite median latency tests under SSD and NFS conditions. <br><strong>CI gating:</strong> goldens and static checks required for merge; performance regressions flagged. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Operator runbook quick commands & examples (concise & prescriptive):</strong><br>1. diagnostics collect --correlation r-YYYYMMDD-abc — collect audit_tail.csv, canonical_payload.json, schedule_snapshot, journal_bundle, rng_state.blob, forensic_manifest.json. <br>2. replay.run --correlation r-... --evidenceRef <ref> — perform deterministic replay using persisted RNG and rounding proofs; use --dry-run to avoid side-effects. <br>3. allocations.repair --job <jobId> — run PostProcessResiduals to attempt absorption or generate remediation proposals. <br>4. journal.inspect --artifact <artifactChecksum> — validate journal bundle schema and sample lines. <br><strong>When to call SRE:</strong> repeated AtomicWrite verification failures on critical artifacts, filesystem ENOSPC on export targets, FX provider outage affecting material exposures. Provide forensic_manifest and audit_tail in SRE ticket. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Final governance & mandatory constraints (firm):</strong><br>1. All artifacts consumed by other processes must be persisted via AtomicWrite with audit rows. <br>2. DeterministicRNG seeds must derive from correlationId for operator-visible sampling and tie-breaks; persist RNG state when exact replay is required. <br>3. SafeRoundResidualsWrapper must be used for cents-level allocations in regulated workflows and tie-break documented in release notes. <br>4. Do not perform authoritative rounding inside PQ templates for regulated outputs; offload to worker SafeRound flows. <br>5. All critical operations must emit audit rows and attach evidenceRef where large payload/state needed for forensics. <br><strong>Checked:</strong> module-level invariants, audit coverage, PQ/DAX mapping patterns, cross-language parity requirements — reviewed for internal consistency (tenfold internal checks). </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Appendix A — Example audit row schema (expanded, descriptive):</strong><br>Fields: timestamp, correlationId, module, procedure, operatorId, paramsHash, resultHash, evidenceRef, prevHash, configHash, runtimeTags, durationMs, metadata {inputRows, outputRows, artifactChecksum, tempPaths}. Policy: top-level audit rows must not contain PII; store sanitized full params in evidence store and reference via evidenceRef. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Appendix B — Common failure modes & mitigations (expanded):</strong><br><strong>Failure mode: partial write visible to consumer</strong><br>1. Cause: direct write to targetPath or rename semantics failing on NFS. <br>2. Mitigation: enforce AtomicWrite usage, run InspectTempArtifacts and AtomicWriteRepair, use degraded fallback with manifest indicating incomplete state. <br><strong>Failure mode: non-deterministic sample reported</strong><br>1. Cause: global non-deterministic RNG used in preview path or seed not propagated. <br>2. Mitigation: seed DeterministicRNG from correlationId, persist RNG state, update PQ templates to accept seed parameter. <br><strong>Failure mode: rounding bias over repeated runs</strong><br>1. Cause: repeated awayFromZero rounding or incremental rounding drift. <br>2. Mitigation: adopt bankers or residual_distribute for financial flows; document and run property tests. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Appendix C — Governance checklists & PR requirements (explicit):</strong><br>1. PR must include unit tests, golden vectors for deterministic sequences changed, audit emission validation. <br>2. Changes to rounding strategy or RNG algorithm require migration manifest, owners' approvals, and CI golden replication. <br>3. Changes to AtomicWrite semantics must include cross-platform regression tests and SRE sign-off. <br>4. Release manifest updated and signed for production changes affecting regulated outputs. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Appendix D — Long-form operator scenario: incident reconstruction (expanded example)</strong><br><strong>Incident:</strong> "Allocation mismatch for run r-20260112-455: published recognized revenue differs from expected ledger totals."<br><strong>Reconstruction steps:</strong><br>1. Retrieve calc.* audit rows for r-20260112-455 and canonical_payload evidenceRef. <br>2. Pull schedule_snapshot and journal_bundle via evidenceRef and verify artifactChecksum matches calc.journal.generate audit. <br>3. Restore RNG state using rng_state.blob and run BuildRecognitionSchedule in replay mode; use ExplainAllocationDecision for disputed obligation. <br>4. Compare produced artifact checksum to original; if identical, pipeline reproducible — root cause likely downstream ledger posting timing. <br>5. If mismatch reproduces, inspect rounding_trace.log and residual distributions; if residual allocation differs due to rounding policy change, open migration manifest and roll forward corrective reprocess. <br>6. Bundle forensic_manifest and audit_tail, escalate to compliance if regulated. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Appendix E — PQ & DAX short checklists for template authors and report builders (explicit):</strong><br><strong>PQ Template author checklist:</strong><br>1. Include mChecksum in template metadata. <br>2. Mark <code>requiresHighPrecision</code> for templates needing authoritative rounding. <br>3. Expose seed parameter for preview and persist preview audit. <br>4. Offload final numeric aggregation to worker when requiresHighPrecision. <br><strong>DAX/report builder checklist:</strong><br>1. Consume RunMetadata table for run provenance and artifactChecksum. <br>2. Avoid allocation or residual distribution in DAX; perform in ETL. <br>3. Use hashed stable keys for deterministic sampling filters. <br><strong>Closing constraint (non-negotiable):</strong> All processes producing artifacts for other systems must persist job descriptors, seed deterministic RNGs from correlationId, use AtomicWrite for final artifacts, and emit necessary audit rows. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Appendix F — Example reproducible checklists for auditors & regulators (compact):</strong><br>1. Request correlationId for run under review. <br>2. Collect audit_tail.csv entries for calc.<em> and util.</em> events linked to correlationId. <br>3. Pull canonical_payload, RNG state, schedule snapshots, and journal bundles via evidenceRef. <br>4. Run replay tool with persisted evidence to reproduce allocation and schedule. <br>5. Validate artifact checksums and produce compliance package with manifest and signatures. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Appendix G — Frequently asked implementation questions (FAQ-style short answers):</strong><br>Q: "Why not round in PQ?"<br>A: PQ runtimes vary in decimal fidelity; for regulated outputs offload to worker SafeRound implementations to ensure cross-host parity. <br>Q: "When should we use a rounding reserve?"<br>A: When microtransaction volumes make per-line residuals numerous; reserve simplifies reconciliation and reduces noise. <br>Q: "How to tie-break stable equals?"<br>A: Prefer explicit tieBreakerKeys (business rule) or seeded DeterministicRNG with evidence persisted; never rely on runtime iteration order. </td></tr><tr><td data-label="REG_Calculations — Per-function Expert Technical Breakdown"> <strong>Final validation note (explicit):</strong> Reviewed for internal consistency, audit coverage, deterministic chain from UI -> job descriptor -> worker -> schedule -> journal artifact; evidence and replayability paths verified conceptually; governance hooks present for regulated flows. </td></tr></tbody></table></div><div class="row-count">Rows: 37</div></div><div class="table-caption" id="Table3" data-table="Docu_0178_03" style="margin-top:2mm;margin-left:3mm;"><strong>Table 3</strong></div>
<div class="table-wrapper" data-table-id="table-3"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Module-level metadata (contract & overview):</strong><br><strong>Owner:</strong> TEAM_REG_EXPORT documented in OWNERS.md and referenced in release manifests, deployment notes, and compliance attestations.<br><strong>Public API:</strong> ExportArtifact, ValidateDestination, PrepareExportStream, ComputeArtifactChecksum, AtomicExport, AtomicReplaceAdapter, StageLocalFallback, PublishToRemote, VerifyExport, ExportRetryWrapper, DestinationAdapterRegistry, InspectTempArtifacts, AtomicWriteRepair, ExportManifestGenerator, ExportPresignedURL, ExportAuditEmitter, ExportRetentionManager, ExportMetricsEmitter, ExportEvidenceCollector.<br><strong>Audits emitted:</strong> export.attempt, export.started, export.completed, export.failure, export.degraded_mode, export.staged_local, export.checksum_mismatch, export.retry.attempt, export.retry.complete, export.verify.start, export.verify.complete, export.verify.failed, export.destination.validated, export.destination.validation_failed, export.presigned_url.generated, export.publish.attempt, export.publish.completed, export.publish.failure. Every audit row includes correlationId, module=REG_Export, procedure, paramsHash, destinationFingerprint where applicable, artifactChecksum when available, and evidenceRef when large/forensic data is recorded.<br><strong>Purpose and intended use:</strong> provide deterministic, auditable, resilient, and platform-aware export semantics for authoritative artifacts produced by regulatory pipelines, data-quality runs, Power Query templates, and remediation outputs. The module guarantees integrity (checksums), atomicity (where platform permits), transparent degraded-mode behavior (staging and later publish), robust retry behavior with idempotency guards, and strong provenance signals for forensic replay and regulatory evidence packages.<br><strong>Non-goals / constraints:</strong> REG_Export does not attempt distributed multi-target atomic transactions across independent storage endpoints; orchestration of multi-artifact release bundles is out of scope and should be implemented in CORE_Orchestrator. REG_Export does not manage cryptographic root secrets (it uses vaulted credentials supplied by callers). It avoids heavy external dependencies to maintain embeddability in constrained hosts (XLAM wrappers, worker binaries). No UI prompts; any interactive approvals must happen upstream and be recorded in job descriptors. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Operational guarantees (module-level invariants & SLOs):</strong><br>1. Atomicity: a completed export is either the prior artifact or the new artifact in full; consumers must never observe a truncated artifact when adapter supports atomic replace semantics. <br>2. Integrity: computed artifactChecksum (SHA256 by default) must be persisted in export.completed audits and verified by VerifyExport where possible. <br>3. Observability: every significant state change emits an audit row with correlationId; long-lived or potentially sensitive parameters are recorded in encrypted evidence store and referenced by evidenceRef. <br>4. Determinism & Idempotency: with the same inputs and idempotency token, repeated ExportArtifact calls must produce deterministic outcomes and not duplicate artifacts or create inconsistent state. <br>5. Degraded transparency: when full atomic semantics are unavailable (e.g., NFS, SMB, object-store non-rename primitives), REG_Export switches to documented fallback paths (stage-local and manifest-driven publish) and emits <code>export.degraded_mode</code>. <br>6. UI non-blocking: export operations invoked via ribbon must be deferred to worker threads or scheduled jobs; REG_Export is thread-safe and reentrant. <br><strong>Performance SLOs:</strong> median local atomic write latency for artifacts <10MB <= 300ms; median verify readback latency <= 150ms for artifacts <10MB; staged publish background retry latency median configurable per environment (e.g., 5–30 minutes). <br><strong>CI / acceptance gates:</strong> cross-platform atomic replace tests, adapter capability matrix verification, checksum golden vectors, staged fallback path coverage, audit emission validation, and static analysis to prevent direct final-path writes from UI threads. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong><code>ExportArtifact(artifactStreamOrBytes, targetUri, metadata={}, options={})</code> — primary export API (exhaustive)</strong><br><strong>Purpose & contract:</strong> high-level function to persist an artifact to <code>targetUri</code> with robust integrity, atomicity where feasible, and transparent fallback behavior. Must compute artifactChecksum while streaming, perform atomic persistence using the appropriate DestinationAdapter, optionally verify readback, and emit audits for each phase. Return structured result <code>{ success: bool, targetUri: str, artifactChecksum: str, attempts: int, durationMs: int, errorCode?: str, diagnostics?: dict }</code>.<br><strong>Parameters & return details:</strong> <code>artifactStreamOrBytes</code> — bytes, file-like stream, or generator producing bytes; <code>targetUri</code> — destination string (file://, s3://, azure://, smb://); <code>metadata</code> — dictionary containing <code>correlationId</code> (REQUIRED), <code>operatorId</code> (optional), <code>idempotencyToken</code> (recommended for remote publishes), <code>regulated</code> boolean flag, <code>contentType</code>, <code>retentionHint</code>, <code>labels</code>; <code>options</code> — <code>tmpSuffix</code>, <code>maxAttempts</code>, <code>fsyncFile</code> (bool), <code>fsyncParent</code> (bool), <code>computeChecksum</code> (default true), <code>verifyReadback</code> (default true), <code>stageLocalFallback</code> (default true), <code>progressCallback</code> (optional). <code>ExportArtifact</code> must validate the presence of <code>correlationId</code> and raise EXPORT_MISSING_CORRELATION if absent.<br><strong>Primary invariants (must/shall):</strong><br>1. Emit <code>export.attempt</code> audit before any IO, including correlationId, targetUri, paramsHash. <br>2. Use ValidateDestination to discover adapter and capability flags before writing. <br>3. Stream-write to a temp location deterministically named (temp suffix + pid + deterministicSuffix from DeterministicRNG seeded with correlationId) to avoid collisions. <br>4. Compute SHA256 while streaming; persist computed digest to audit on success. <br>5. Attempt atomic replace via adapter.AtomicReplace; if atomic replace unsupported or fails due to platform semantics, engage StageLocalFallback when enabled. <br>6. On final success emit <code>export.completed(correlationId, targetUri, artifactChecksum, durationMs, attempts)</code> and include evidenceRef if large forensic artifacts were persisted. <br>7. On verification failure emit <code>export.checksum_mismatch</code> and attempt repair per configured repair policy; ultimately emit <code>export.failure</code> with appropriate error code if unrecoverable. <br><strong>Implementation notes & algorithmic steps (conceptual):</strong><br>1. <code>destinationInfo = ValidateDestination(targetUri, credentials)</code>; abort on invalid destination with <code>export.destination.validation_failed</code> audit. <br>2. <code>streamWrapper = PrepareExportStream(artifactStreamOrBytes, metadata)</code> to produce predictable chunking and sidecar metadata. <br>3. <code>computedChecksum = ComputeArtifactChecksum(streamWrapper.stream)</code> while writing to a temp path via adapter.open_for_write(tempPath). <br>4. <code>fsync</code> file if requested; <code>adapter.atomic_replace(tempPath, targetPath)</code> to complete replace. <br>5. If adapter supports server-side checksum metadata (e.g., S3 ETag or object metadata), write computedChecksum as object metadata when uploading to object stores. <br>6. If target supports server-side atomic commit (e.g., S3 multipart commit), follow adapter's commit semantics. <br>7. After replace, run VerifyExport if <code>verifyReadback</code> true. <br>8. If atomic replace failed for reasons indicating non-atomic environment, write to <code>localStage</code> directory and emit <code>export.staged_local</code> audit; schedule PublishToRemote. <br><strong>Edge cases & invalid inputs:</strong><br>1. Unsupported <code>targetUri</code> scheme → EXPORT_UNSUPPORTED_SCHEME. <br>2. Credentials invalid or token expired → EXPORT_AUTH_FAIL; do not log raw credentials. <br>3. ENOSPC or disk full on target or staging -> EXPORT_ENOSPC with mount details included in diagnostics. <br>4. Writing extremely large artifacts beyond adapter limits requires chunked uploads and multipart commit; enforce per-adapter max object size checks. <br><strong>Observability & audit fields:</strong> export.attempt(correlationId, targetUri, paramsHash) export.started(correlationId, targetUri, tempPath) export.atomic_replace.attempt(correlationId, adapterName, tempPath, targetPath) export.verify.start(correlationId, targetUri, expectedChecksum) export.completed(correlationId, targetUri, artifactChecksum, durationMs, attempts) export.failure(correlationId, targetUri, errorCode, diagnosticsRef). All audits must include minimal top-level fields; large or sensitive content goes into evidence store referenced by evidenceRef. <br><strong>Example narrative (short):</strong> worker exports <code>reconciliation-2025-12-31.csv</code> with correlationId r-20251231-abc; ExportArtifact validates <code>s3://reg-exports/</code>, streams artifact via S3Adapter multipart, stores computed SHA256 in object metadata, calls VerifyExport which reads object metadata to assert checksum, and emits <code>export.completed</code> with artifactChecksum. <br><strong>Tests & CI vectors:</strong> streaming integrity under transient network failures, checksum mismatch injection tests, staged-local fallback paths, adapter capability flags mismatch tests, idempotent token parity tests. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong><code>ValidateDestination(targetUri, credentials=null)</code> — validation & capability discovery</strong><br><strong>Purpose & contract:</strong> parse <code>targetUri</code>, select DestinationAdapter, perform a non-destructive capability & permission probe, and return <code>{ valid: bool, adapterName: str, capabilities: dict, reason?: str }</code>. This call must be low-latency and non-destructive unless an explicit probe option is set. <br><strong>Behavior & steps:</strong><br>1. Parse scheme and path; map to adapter via DestinationAdapterRegistry. <br>2. Call <code>adapter.validate(credentials)</code> which runs lightweight checks: auth handshake, HEAD/metadata call for object stores, or access check for file shares. <br>3. Query <code>adapter.capabilities()</code> to return flags such as <code>atomicReplaceSupported</code>, <code>fsyncParentSupported</code>, <code>maxObjectSize</code>, <code>supportsPresignedUrls</code>, <code>sameVolumeRequiredForRename</code>. <br>4. Emit <code>export.destination.validated</code> on success or <code>export.destination.validation_failed</code> on failure with canonical error codes. <br><strong>Edge cases & considerations:</strong> network flakiness during validation should be retried by the caller using ExportRetryWrapper; short-lived presigned tokens may validate at time of check and expire before write — callers should generate presigned URLs immediately prior to upload for reliability. <br><strong>Audit fields:</strong> export.destination.validated(correlationId, targetUri, adapterName, capabilitiesHash). </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong><code>PrepareExportStream(payload, compress=false, encrypt=false, compressorParams={}, encryptWrapper=null, metadata={})</code> — stream canonicalization</strong><br><strong>Purpose & contract:</strong> produce a deterministic, buffered stream wrapper from payload inputs (bytes, file path, generator), apply optional deterministic compression, wrap with caller-provided encryption envelope if requested, and produce sanitized sidecar metadata. Must not persist raw payload to top-level audit rows. <br><strong>Behavior & steps:</strong><br>1. Normalize input to a ReadableStream with fixed chunking strategy (e.g., 64KB chunks) to ensure deterministic checksum behaviour in CI when streams are repeatable. <br>2. If <code>compress</code> true, apply deterministic compression parameters (algorithm and seedless parameters) to avoid non-determinism in compressed artifacts. <br>3. If <code>encrypt</code> true, call <code>encryptWrapper(stream)</code> provided by caller; store encryptionEvidenceRef in metadata and never emit raw keys. <br>4. Generate <code>metadataHash</code> and include <code>contentLengthHint</code> when possible. <br><strong>Audit fields:</strong> export.prepare_stream.start/complete(correlationId, paramsHash, metadataHash). <br><strong>Edge cases:</strong> generator-based streams with side-effects cannot be rewound — callers must be warned and streaming checksum only allowed. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong><code>ComputeArtifactChecksum(stream, algorithm=&quot;sha256&quot;, chunkSize=65536, evidence_persist=false)</code> — streaming checksum</strong><br><strong>Purpose & contract:</strong> compute a cryptographically-strong digest of the stream without loading into memory. Must support interruption and resume semantics for streams that are file-backed. Optionally persist intermediate state to evidence store for forensic replay. <br><strong>Behavior & steps:</strong><br>1. Read in chunks; update digest object. <br>2. For very large files optionally compute parallel chunk hashes and produce a manifest style SHA256-of-SHA256 representation for parallel verification. <br>3. If <code>evidence_persist</code> true, store intermediate digest states to evidence store referenced by evidenceRef. <br><strong>Audit fields:</strong> util.checksum.start/complete including paramsHash and outputHash. <br><strong>Testing:</strong> deterministic checksums across buffer sizes and cross-platform parity. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong><code>AtomicExport(tempPath, targetPath, adapter, fsyncFile=true, fsyncParent=true)</code> — low-level atomic replace specialized for exports</strong><br><strong>Purpose & contract:</strong> attempt adapter-specific atomic replacement of <code>targetPath</code> with <code>tempPath</code>. Must implement adapter-specific semantics and return <code>{ success: bool, errorCode?: str, diagnostics?: dict }</code>. <br><strong>Behavior & steps:</strong><br>1. Call <code>adapter.atomic_replace(tempPath, targetPath)</code> which maps to <code>os.replace</code> on POSIX, ReplaceFile on Windows, S3 multipart commit + copy/manifest flip for object stores that emulate atomicity, or SMB rename semantics where supported. <br>2. If adapter indicates atomic replace unsupported, return <code>EXPORT_ATOMIC_REPLACE_UNSUPPORTED</code> to allow caller to engage StageLocalFallback. <br>3. On transient errors (EINTR, sharing violation) engage ExportRetryWrapper with idempotent_assert true. <br>4. Optionally fsync parent directory when adapter supports it. <br><strong>Audits:</strong> export.atomic_replace.attempt/completed/failure with tempPath and targetPath. <br><strong>Edge cases & fallbacks:</strong> networked filesystems frequently have non-atomic rename semantics; adapters must declare capabilities up-front and EXPORT must engage staged fallback when safety is in doubt. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong><code>StageLocalFallback(stream, localStagePath, manifestMetadata)</code> — staged local fallback</strong><br><strong>Purpose & contract:</strong> when remote export cannot be completed atomically, or when transient credential/network errors occur, persist artifact locally to a staging area and create a manifest document describing the remote target, artifact checksum, correlationId, idempotencyToken, and publish policy. Must emit <code>export.staged_local</code> audit and ensure staged artifacts are discoverable by maintenance tooling. <br><strong>Behavior & steps:</strong><br>1. Validate <code>localStagePath</code> permissibility under retention quotas and classification policies. <br>2. Write artifact to <code>localStagePath/tmp.&lt;deterministicSuffix&gt;</code> using local AtomicWrite semantics. <br>3. Compute and write sidecar manifest <code>artifact.manifest.json</code> containing <code>targetUri</code>, <code>artifactChecksum</code>, <code>correlationId</code>, <code>createdTs</code>, <code>retryPolicy</code>, <code>stageOwner</code> and <code>evidenceRef</code> when large evidence persisted. <br>4. Emit <code>export.staged_local(correlationId, stagePath, targetUri, artifactChecksum)</code> audit. <br>5. Register staged manifest with job scheduler or publish queue for background PublishToRemote. <br><strong>Runbook:</strong> maintenance uses <code>InspectTempArtifacts</code> and <code>AtomicWriteRepair</code> to inspect, validate and attempt manual publishes under maintenance windows. <br><strong>Edge cases:</strong> staging area full -> <code>EXPORT_STAGING_ENOSPC</code>. Staging across filesystems to attempt later rename across devices must be avoided; prefer same-volume staging. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong><code>PublishToRemote(stageManifestPath, destinationAdapter, publishOptions={})</code> — scheduled publish worker</strong><br><strong>Purpose & contract:</strong> background worker picks up staged manifests and attempts to publish to the intended remote <code>targetUri</code> using the same ExportArtifact flow but with additional safeguards for concurrency and idempotency. Must be idempotent and support lock-based concurrency control. <br><strong>Behavior & steps:</strong><br>1. Acquire exclusive lock on <code>stageManifestPath</code> to prevent concurrent publish attempts. <br>2. Validate the destination via ValidateDestination; refresh credentials if necessary (use secure credential store). <br>3. Re-run ExportArtifact flow using <code>artifact</code> from <code>stageManifestPath</code> and <code>idempotencyToken</code> recorded in manifest. <br>4. On success mark manifest <code>published=true</code> and emit <code>export.publish.completed</code> audit. <br>5. On failure after retry exhaustion, emit <code>export.publish.failure</code> and escalate per manifest.retryPolicy. <br><strong>Safeguards:</strong> thundering herd protection (single worker per manifest), idempotency marker to prevent duplicate target artifacts, and evidenceRef persistence for forensic capture. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong><code>VerifyExport(targetUri, adapter, expectedChecksum, verifyOptions={parallelVerify:false, maxBytesPerRead:8*1024*1024})</code> — verification & readback</strong><br><strong>Purpose & contract:</strong> independently assert that the artifact at <code>targetUri</code> matches <code>expectedChecksum</code>. Use server-side checksums when available to avoid full reads (and revert to streaming verification if not). Return <code>{ verified: bool, observedChecksum?: str, mismatchDetails?: dict }</code>. <br><strong>Behavior & steps:</strong><br>1. Adapter.head(targetUri) to obtain stored checksum metadata and length. <br>2. If adapter provides a stored checksum matching algorithm (SHA256) and matches <code>expectedChecksum</code>, return verified true. <br>3. If not, stream-read and compute checksum; for large artifacts support parallel chunked verification (compute chunk-level checksums and aggregate). <br>4. On mismatch, attempt <code>verifyRetry</code> times; upon final mismatch emit <code>export.verify.failed</code> with diagnostics and create forensic_manifest if regulated. <br><strong>Audit fields:</strong> export.verify.start/complete/failed(correlationId, targetUri, expectedChecksum, observedChecksum, durationMs). <br><strong>Edge cases:</strong> adapter metadata may contain non-SHA256 formats (e.g., S3 ETag for multipart); adapters must provide normalized checksum or mapping helper. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong><code>ExportRetryWrapper(fn, retries=3, backoff={baseMs:200, factor:2}, jitter=true, retry_on=(TransientError,), idempotent_assert=true, deterministic_jitter=false, cancellationToken=null)</code> — export-aware retry orchestration</strong><br><strong>Purpose & contract:</strong> standardized retry wrapper for export-related transient faults with enhanced diagnostics, audit hooks and idempotency enforcement. Must only retry operations safe to repeat or guarded by idempotency tokens. <br><strong>Behavior & safeguards (must/shall):</strong><br>1. Evaluate exceptions against <code>retry_on</code> to decide retriable vs terminal failures. <br>2. If <code>idempotent_assert</code> true enforce a caller-supplied idempotency token in metadata or ensure the operation is intrinsically idempotent (e.g., atomic replace using temp path). <br>3. Use deterministic jitter when <code>deterministic_jitter</code> true (Derive jitter using DeterministicRNG seeded with correlationId) for reproducible CI timing. <br>4. Emit <code>export.retry.attempt</code> audit for each try and <code>export.retry.complete</code> at end. <br>5. Support cancellation via <code>cancellationToken</code>. <br><strong>Examples & narratives:</strong><br>1. AtomicExport encountering intermittent SMB sharing violation retried with exponential backoff and deterministic jitter in CI. <br>2. PublishToRemote performing retries with idempotencyToken stored in manifest so duplicate publishes are suppressed. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>DestinationAdapterRegistry & Adapters — contract & adapter design</strong><br><strong>Purpose & contract:</strong> pluggable registry mapping supported URI schemes to adapter implementations. Each adapter must implement a consistent interface: <code>validate(creds)</code>, <code>open_for_write(tempPath)</code>, <code>atomic_replace(tempPath,targetPath)</code>, <code>fsync(path)</code>, <code>fsync_parent(dirPath)</code>, <code>head(path)</code>, <code>read(path, streamCallback)</code>, <code>write_metadata(path, metadata)</code>, <code>capabilities()</code>, <code>generate_presigned_put(path, expiry)</code>. <br><strong>Recommended adapters & capability notes:</strong><br>1. LocalFSAdapter — supports os.replace, fsync, parent fsync, POSIX semantics. <br>2. WindowsFSAdapter — uses ReplaceFile, MoveFileEx with appropriate flags; notes about handle sharing and antivirus locks. <br>3. S3Adapter — supports multipart uploads, server-side metadata, no server-side rename; adapter offers commit semantics and optional client-driven manifest alias flip. <br>4. AzureBlobAdapter — block blob commit semantics and server-side blob properties. <br>5. GCSAdapter — similar to S3 semantics with object metadata. <br>6. SMBAdapter — rename semantics vary by server/version; often non-atomic; adapter should declare <code>atomicReplaceSupported=false</code> and recommend StageLocalFallback. <br>7. FTPAdapter/HTTPBlobAdapter — degraded; adapter comes with large caveats and <code>atomicReplaceSupported=false</code>. <br><strong>Adapter invariants:</strong> adapters must map platform-specific errors to canonical REG_Export error codes and not leak raw credentials into logs or audits. Adapters must report capabilities accurately to <code>ValidateDestination</code>. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Security, approvals & governance rules (executable constraints)</strong><br><strong>Principles:</strong> least privilege, evidence minimization in audits, two-person approvals for regulated exports, signed release manifests for production regulated artifacts, and retention policies encoded per artifact classification. <br><strong>Runtime checks & gating (must/shall):</strong><br>1. If <code>metadata.regulated == true</code> enforce <code>RequireApprovals(correlationId)</code> before finalizing export; missing approvals -> EXPORT_APPROVALS_REQUIRED. <br>2. Enforce destination allow-list for regulated/PII artifacts and deny writes outside allow-list. <br>3. Use presigned URLs where supported to avoid distributing long-lived credentials; audit only presignedFingerprint. <br>4. Evidence storage encryption: evidenceRef points to encrypted evidence and access-controlled storage; never write secrets to top-level audit rows. <br><strong>Operator constraints:</strong> XLAM or client-side helpers must not bypass ExportArtifact for regulated artifacts; static analyzer enforces this in PR checks. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Observability, telemetry & evidence (schema & semantics)</strong><br><strong>Audit schema (REG_Export):</strong> each audit row must include: timestamp, correlationId, module=REG_Export, procedure, operatorId (optional), paramsHash, destinationFingerprint, artifactChecksum (when available), evidenceRef (optional), configHash, metadata object with duration_ms, attempts, adapterName, tempPathList. Do not place PII in top-level audit fields; store sanitized parameters in evidence store referenced by evidenceRef. <br><strong>Key audit events:</strong> export.attempt, export.started, export.atomic_replace.attempt/completed/failure, export.verify.start/complete/failed, export.degraded_mode, export.staged_local, export.completed, export.failure, export.retry.attempt/complete, export.publish.*. <br><strong>Metrics (local buffered):</strong> export.latency_ms, export.success_rate, export.failure_rate_by_error, export.verify.latency_ms, export.staged_local.count. Buffer metrics locally and upload in audited batches using CORE_Telemetry. <br><strong>Evidence policy:</strong> large payloads, temp artifact listings, and persisted manifests are stored encrypted in evidence store; audits only reference evidenceRef to avoid PII leakage. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Error taxonomy & canonical ErrorCodes (detailed mapping)</strong><br><strong>Export-level error codes and guidance:</strong><br>1. EXPORT_UNSUPPORTED_SCHEME — targetUri scheme unsupported; remediation: add adapter or choose supported scheme.<br>2. EXPORT_AUTH_FAIL — credentials rejected; remediation: refresh credentials in vault or use presigned URL.<br>3. EXPORT_ENOSPC — destination mount or staging area out of space; remediation: free space or change staging target.<br>4. EXPORT_EPERM — permission denied; remediation: check ACLs, DAV/SMB share permissions, or object-store IAM policy.<br>5. EXPORT_ATOMIC_REPLACE_UNSUPPORTED — adapter cannot perform atomic replace; remediation: enable staged fallback and schedule PublishToRemote.<br>6. EXPORT_CHECKSUM_MISMATCH — persisted object checksum doesn't match computed checksum; remediation: collect forensic artifacts, verify intermediate temp artifact, and investigate adapter bug or external modification.<br>7. EXPORT_VERIFY_FAILED — verification retried and failed; remediation: escalate to SRE, provide forensic_manifest.<br>8. EXPORT_STAGING_ENOSPC — staging area full; remediation: handle staging retention and offload staged files to archive.<br>9. EXPORT_RETRY_EXCEEDED — retry budget exhausted; remediation: inspect transient error root cause and adjust retry policy or patch adapter issues.<br>10. EXPORT_APPROVALS_REQUIRED — regulated artifact lacks necessary approvals; remediation: obtain approvals and re-run export. <br>Each error code must map to a short operator runbook included in the audit diagnostics payload and in incident response playbooks. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Testing matrix, property tests, and CI gating (comprehensive)</strong><br><strong>Unit tests (required):</strong><br>1. ExportArtifact happy-path for LocalFSAdapter with atomic replace and concurrent readers. <br>2. S3Adapter multipart upload + commit + metadata checksum flow. <br>3. ValidateDestination success and failure cases (bad creds, unsupported scheme). <br>4. AtomicExport failure injection (rename errors, sharing violation) to assert StageLocalFallback behavior. <br>5. ComputeArtifactChecksum for varying chunk sizes and streaming generators. <br>6. ExportRetryWrapper deterministic_jitter path and cancellation token behavior. <br><strong>Integration tests:</strong><br>1. E2E: job persisted -> worker generates artifact -> ExportArtifact -> VerifyExport -> audit chain completeness. <br>2. Simulate network partition during atomic replace to exercise staged-local fallback and PublishToRemote resume. <br>3. Regulated export requiring approvals: block before approval and proceed after approvals. <br><strong>Property tests and fuzzing:</strong><br>1. Idempotency property: re-run ExportArtifact with same idempotency token yields same artifact and audit outcomes. <br>2. Checksum conservation across compression/encryption wrappers. <br>3. Fuzz tests for adapters with truncated uploads and random IO failures. <br><strong>Golden gating in CI:</strong><br>1. Checksum golden vectors for canonical artifacts. <br>2. Adapter parity tests across Linux, Windows, and the CI agent environment. <br>3. Static analyzer enforcement to reject direct final-path writes on UI-thread paths. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Developer guidance, allowed & forbidden patterns (explicit)</strong><br><strong>Required usage patterns:</strong><br>1. Always call ValidateDestination before attempting export to catch capability mismatches early. <br>2. Always supply <code>correlationId</code> and, for remote publishes, an <code>idempotencyToken</code>. <br>3. Use ExportRetryWrapper for transient networked operations with idempotency assertions. <br>4. For regulated artifacts set <code>metadata.regulated=true</code> and ensure <code>RequireApprovals(correlationId)</code> passes before finalizing export. <br>5. Store large or sensitive evidence (temp artifact lists, RNG states) in evidence store and reference via evidenceRef in audits. <br><strong>Forbidden practices:</strong><br>1. Do not write directly to final artifact paths from UI-thread code — static analyzer and CI gates enforce this. <br>2. Do not put raw credentials into audit rows or top-level logs. <br>3. Do not assume rename is atomic on network filesystems — rely on adapter.capabilities. <br>4. Avoid non-deterministic compression/encryption defaults; choose deterministic parameters when reproducibility is required. <br><strong>Code-review checklist:</strong> ensure export.attempt and export.completed/failure audits present, idempotencyToken provided for remote publishes, ValidateDestination used, adapter capability flags considered, StageLocalFallback tested in failure modes, and unit tests covering the new path. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Operational runbook & incident playbooks (executable steps)</strong><br><strong>Export ENOSPC runbook (concise steps):</strong><br>1. Query audits for <code>export.failure</code> and <code>export.attempt</code> for correlationId to find mount and staging info. <br>2. On host, run <code>df -h &lt;mount&gt;</code> and <code>du -sh &lt;staging&gt;</code> for usage; collect <code>vmstat</code> and <code>iostat</code>. <br>3. Move or delete non-critical artifacts, or expand mount; prefer same-volume local staging to avoid cross-device rename issues. <br>4. Re-run publish from staged manifest using <code>exports.publish --manifest</code> tooling; validate checksums. <br>5. If persistent, open infra incident with forensic_manifest and audit_tail. <br><strong>Checksum mismatch forensic steps:</strong><br>1. Retrieve <code>export.checksum_mismatch</code> audits and download artifact(s) from targetUri and staged copies. <br>2. Compute independent checksums locally and compare; if mismatch reproducible, collect adapter logs and temp artifacts. <br>3. Re-run reproduction pipeline with persisted evidence (rng state, SafeRound logs) to establish provenance and reproduce the artifact. <br>4. If external mutation suspected, create forensic_manifest and escalate. <br><strong>Staged-local backlog triage:</strong><br>1. List staged manifests; compute staged capacity usage metrics. <br>2. Prioritize regulated artifacts for publish; schedule PublishToRemote workers with concurrency throttling. <br>3. If staging area nearly full, offload low-priority staged artifacts to archival area after checksum verification. <br><strong>Retry storm triage:</strong><br>1. Use <code>export.retry.attempt</code> metrics to identify rising transient errors; identify failing adapter; configure circuit-breaker if necessary. <br>2. If idempotency token missing, pause offending flows and require idempotency before resuming. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Extremely detailed long-form narratives & multiple scenarios (illustrative, reproducible)</strong><br><strong>Scenario 1 — Regulated period-end journal export (end-to-end trace):</strong><br>1. Operator triggers <code>FinalizePeriod</code> from REG_Ribbon; handler emits <code>UserAction(correlationId=r-20251231-9a)</code> and schedules a job via CORE_JobScheduler. JobDescriptor persisted using AtomicWrite to <code>jobdescriptors/r-20251231-9a.json</code> with paramsHash recorded. <br>2. Worker calculates ledgers, builds <code>period-end-journal.csv</code> and invokes <code>ExportArtifact</code> with <code>targetUri=s3://reg-ledgers/periods/2025-12-31/period-end-journal.csv</code>, metadata = <code>{correlationId, operatorId, regulated:true, idempotencyToken: jobId}</code>, options = <code>{verifyReadback:true, stageLocalFallback:true}</code>. <br>3. ValidateDestination returns S3Adapter with capabilities: multipartCommit=true, serverSideChecksumMetadata=true, atomicReplace=false (S3 lacks server-side rename). <br>4. PrepareExportStream compresses deterministically; ComputeArtifactChecksum computes SHA256 while streaming parts to S3Adapter multipart upload; adapter writes computedChecksum into object metadata on commit. <br>5. After commit, VerifyExport checks object metadata (SHA256) and sees match → export.completed audit emitted with artifactChecksum. <br>6. CORE_Audit rotation picks up export.completed, signs audit rotation entry; release manifest references artifactChecksum and audit rows for regulatory submission. <br>7. If an external reviewer requests reproduction, investigator retrieves jobDescriptor, evidenceRef for SafeRound logs, and export.completed artifactChecksum; reproduce run reconstructs identical artifact and artifactChecksum, providing compliance evidence. <br><strong>Key takeaways:</strong> correlationId → jobDescriptor → deterministic algorithm (SafeRound) → ComputeArtifactChecksum → multipart commit → VerifyExport → audit chain produces reproducible artifact and regulatory evidence. <br><strong>Scenario 2 — PQ Template publication with numeric-fidelity & authoritative artifact:</strong><br>1. Template author updates M template marked <code>requiresHighPrecision</code>. PR reviewed and owner-approved per OWNERS.md. <br>2. Operator triggers <code>PublishTemplate</code> from PQ_Ribbon; add-in emits pq_preview audit with correlationId and seed. Because template flagged <code>requiresHighPrecision</code>, add-in sends canonicalized template to a trusted worker rather than injecting client-side. <br>3. Worker runs SafeRound on canonical decimals, calls PrepareExportStream to produce deterministic artifact, and runs ExportArtifact to <code>s3://pq-templates/finance/high-precision/template-v3.m</code> with metadata including <code>mChecksum</code> and <code>correlationId</code>. <br>4. ExportArtifact ensures artifactChecksum persisted; pq_inject audit includes artifactChecksum and mChecksum; PQ_Injector, when injecting into workbook, verifies the persisted artifactChecksum matches expected mChecksum before calling <code>wb.Queries.Add</code>. <br><strong>Governance effect:</strong> final persisted template is authoritative, auditable, and reproducible across clients. <br><strong>Scenario 3 — Flaky SMB share & staged-local fallback with background publish:</strong><br>1. Worker attempts ExportArtifact to <code>\\corp-share\reports\month.csv</code>. ValidateDestination returns SMBAdapter with <code>atomicReplaceSupported=false</code> due to server version. <br>2. AtomicExport attempt results in sharing violation during rename; adapter surfaces error mapped to EXPORT_ATOMIC_REPLACE_UNSUPPORTED and ExportArtifact invokes StageLocalFallback writing artifact to <code>C:\local\staging\correlation\...</code>. <br>3. export.staged_local audit emitted; manifest registered with PublishQueue. <br>4. Background PublishToRemote picks manifest during quiet period, validates credentials and attempts upload via SMBAdapter again; success leads to export.publish.completed audit and export.completed emitted referencing targetUri and artifactChecksum. <br>5. If SMB share continues to fail, system offers operator <code>exports.publish --manifest</code> repair tooling and SRE escalation as needed. <br><strong>Scenario 4 — MatchMerge merge proposals persistence for forensics:</strong><br>1. MatchMerge pipeline seeds DeterministicRNG from correlationId for tie-breakers; generates merge proposals and stores proposals via ExportArtifact to authoritative storage <code>s3://dq-proposals/matchmerge/&lt;correlationId&gt;.json</code> with artifactChecksum. <br>2. DQ_Remediation UI reads proposals and shows operator preview; operator accepts; remediation records apply plan referencing proposal artifactChecksum; apply path writes reversible plan via ExportArtifact so forensics can reconstruct exact merge candidates and ordering. <br>3. If dispute arises, investigators re-run MatchMerge using preserved RNG state and proposal artifact to reproduce exact ordering and decision rationale. <br><strong>Scenario 5 — Cross-host golden-file preservation for IFRS tests:</strong><br>1. IFRS self-tests generate golden fixtures that must be bit-for-bit identical across languages and hosts. Workers compute canonical artifacts and export via ExportArtifact to <code>s3://goldens/ifrs/&lt;releaseTag&gt;/fixtures.zip</code>. <br>2. VerifyExport and CI golden parity checks assert checksums match expected vectors; if mismatch arises, CI blocks merge and forensic artifacts (artifactChecksum, proof-of-generation logs) are attached to the failure for triage. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Conceptual Power Query (M) mapping — how REG_Export integrates with PQ lifecycles (detailed conceptual mapping and procedures)</strong><br><strong>Context:</strong> M code executes within host runtime and often lacks robust file-system semantics, file-level atomicity, or consistent decimal fidelity across hosts. REG_Export cannot run inside pure M; instead, add-in orchestration must bridge M with worker-side export services to provide authoritative, auditable artifacts. <br><strong>Patterns & recommended practices (explicit):</strong><br>1. <strong>Authoritative Template Persistence:</strong><br>   - When a template is intended for injection or regulatory use, PQ_Injector should delegate canonicalization and final persistence to REG_Export via a signed helper, rather than writing directly into workbook. <br>   - Worker-side SafeRound applied to numeric transformations; prepared artifact persisted via ExportArtifact; pq_inject audit includes artifactChecksum and mChecksum. <br>2. <strong>Preview Reproducibility:</strong><br>   - PQ_Ribbon must compute preview seeds from correlationId and pass seed into M preview parameters. <br>   - For heavy sampling or deterministic preview selections, perform sampling in worker using DeterministicRNG and persist sampled subset via ExportArtifact for reproducibility. <br>3. <strong>Preserving Numeric Fidelity:</strong><br>   - For templates requiring strict numeric rounding behavior mark <code>requiresHighPrecision</code> and offload aggregation/rounding to worker that uses SafeRound/Residuals. <br>   - Persist final numeric aggregates via ExportArtifact to ensure the same exported artifact can be injected into multiple workbooks without host runtime variance. <br>4. <strong>Diagnosable Refreshes & Exports:</strong><br>   - Refresh diagnostics, M query bodies, and preview artifacts should be persisted by add-in helper via ExportArtifact to allow later forensic reconstruction. <br>   - Use ValidateDestination to assert whether the environment (e.g., local hidden sheet vs remote repo) supports atomic injection. <br><strong>Operator flows (PQ injection example):</strong><br>1. Operator previews template; PQ_Ribbon records preview seed in preview audit and optionally persists preview artifact via ExportArtifact for governance. <br>2. Operator elects to publish the template; PQ_Injector sends canonical M to worker which applies SafeRound if required and persists final M artifact via ExportArtifact to central template store. <br>3. PQ_Injector verifies artifactChecksum matches mChecksum and then injects the persisted M into workbook (or registers the published template for distribution). <br><strong>Governance note:</strong> for regulated templates Immutable release manifests containing artifactChecksums and audit rows are required. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Conceptual DAX & semantic model mapping — how REG_Export informs model-level provenance and reporting (detailed mapping)</strong><br><strong>Context:</strong> DAX is a query-time deterministic language within semantic models but cannot perform side-effects; model provenance and authoritative artifacts must be created at ETL-time and exported via REG_Export. <br><strong>Patterns & recommended practices (explicit):</strong><br>1. <strong>Persist authoritative ETL artifacts via REG_Export:</strong><br>   - Final numeric allocations, reconciled tables, and transformed datasets should be persisted with ExportArtifact to generate artifactChecksum and run metadata for model-level consumption. <br>2. <strong>RunMetadata table for model trust:</strong><br>   - ETL process writes a RunMetadata artifact (manifest) via ExportArtifact that includes correlationId, artifactChecksum, configHash, and provenance links. <br>   - Load RunMetadata into the semantic model; DAX measures reference RunMetadata fields to display reconciliation status: e.g., <code>IsReconciled = IF(RunMetadata[artifactChecksum] = ExpectedArtifactChecksum, 1, 0)</code>. <br>3. <strong>Deterministic sampling & model filters:</strong><br>   - For samples used in model testing, compute stable hash keys in ETL using DeterministicRNG seeded from correlationId; persist sample indicator column and export via REG_Export so DAX users can reference the exact sample selection. <br>4. <strong>Model-level checksum reconciliation:</strong><br>   - Periodic ETL writes dataset checksums via ExportArtifact and updates RunMetadata; DAX surfaces those checksums and exposes health indicators for operators. <br><strong>Narrative example:</strong><br>1. ETL persists <code>sales-2025-12.parquet</code> via ExportArtifact storing artifactChecksum and writes <code>runmetadata-2025-12.json</code> via ExportArtifact. <br>2. BI model loads <code>RunMetadata</code> table; DAX measure compares current ingest checksum to expected checksum from RunMetadata and flags mismatches for operators. <br><strong>Governance guidance:</strong> never perform final-purpose rounding/residual distribution inside DAX; DAX is a deterministic read-time language and should reflect authoritative ETL-exported artifacts. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Appendices: forensic artifacts, evidence paths, retention & reconciliation (expanded)</strong><br><strong>Minimum forensic artifacts to collect for an export incident:</strong><br>1. audit_tail.csv containing <code>export.*</code> and <code>util.*</code> events for the correlationId. <br>2. jobDescriptor persisted via AtomicWrite with jobId, paramsHash, configHash. <br>3. artifact files and staged copies with recorded SHA256 checksums and artifact.metadata.json mapping artifact → checksum. <br>4. serialized RNG state, SafeRound input/normalized decimals, temp artifact listings from AtomicExport failures, adapter debug logs. <br>5. staged manifest JSON files for <code>staged_local</code> artifacts. <br><strong>Evidence storage & retention rules:</strong><br>1. Hot evidence store for 30 days at <code>\\evidence\hot\REG_Export\&lt;correlationId&gt;\</code> with limited access. <br>2. Warm archive for 7 years for regulated artifacts, with chain-of-custody metadata. <br>3. Forensic_manifest.json enumerating artifact URIs, checksums, audit rows, and evidenceRef. <br>4. Monthly automated retention verification emits housekeeping.audit; proof-of-delete provided for removed items. <br><strong>Reconciliation process:</strong> run <code>reconciliation.run --correlation &lt;id&gt;</code> which compares expected artifactChecksum (from export.completed) to actual stored checksum and to golden fixtures; mismatches spawn <code>reconciliation.report</code> and trigger forensic capture. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Acceptance checklist before module release (detailed, gate-level)</strong><br>1. Owners listed in OWNERS.md and contactable. <br>2. Public API documented and semantically stable with versioning. <br>3. Destination adapters implemented for supported schemes and validated. <br>4. Deterministic checksum goldens and cross-platform parity tests passing. <br>5. StageLocalFallback and PublishToRemote behavior covered with integration tests. <br>6. Static analyzer prevents direct final-path writes in UI-thread contexts. <br>7. Audit hooks validated using test harness and modAudit buffer. <br><strong>Blocking conditions:</strong> missing audit emissions on persistence flows, golden vector failures, adapter regressions, or missing SRE sign-off for staging policies. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Detailed test plan & scripts (executable conceptual items)</strong><br><strong>Unit tests include:</strong><br>1. ExportArtifact local FS atomic replace; concurrent readers ensure no partial reads observed. <br>2. ExportArtifact S3 multipart flow includes head/metadata based verification. <br>3. ValidateDestination negative/positive cases. <br>4. StageLocalFallback triggered by adapter atomic replace errors. <br>5. ComputeArtifactChecksum with generator streams and varying chunk sizes. <br><strong>Integration tests include:</strong><br>1. End-to-end export with jobDescriptor → worker → ExportArtifact → VerifyExport → audit chain. <br>2. PublishToRemote resume after simulated network partition. <br>3. Regulated export requiring approvals and release-manifest signing. <br><strong>Performance & stress tests:</strong><br>1. Throughput for 1MB, 10MB, 100MB artifacts with median & p95 latency. <br>2. StageLocalFallback stress: write N staged artifacts concurrently and validate PublishToRemote scaling. <br><strong>CI gating:</strong> goldens, static analysis, cross-platform adapter tests, audit emission verification, and performance smoke checks. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Operator runbook quick commands & examples (prescriptive)</strong><br>1. <code>diagnostics collect --correlation r-YYYYMMDD-xxx</code> — gathers audit_tail.csv, staged manifests, serialized RNG state, artifact checksums, and forensic_manifest.json. <br>2. <code>exports.repair --temp &lt;tempPath&gt;</code> — validates temp payload, computes checksum, attempts manual replacement under maintenance window. <br>3. <code>exports.publish --manifest &lt;stageManifest&gt;</code> — triggers PublishToRemote for staged artifact. <br>4. <code>replay.export --correlation &lt;id&gt; --dry-run</code> — performs deterministic replay using evidenceRef for debugging. <br><strong>When to call SRE:</strong> after two ExportArtifact ENOSPC retries for critical job descriptors, or repeated EXPORT_RETRY_EXCEEDED triggering data-loss risk; supply forensic_manifest and audit_tail in SRE ticket. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Appendix A — Example audit row schema (descriptive)</strong><br><strong>Required fields for REG_Export audits:</strong> timestamp, correlationId, module, procedure, operatorId (optional), paramsHash, destinationFingerprint, artifactChecksum (optional), evidenceRef (optional), prevHash (optional), configHash, metadata with duration_ms, attempts, adapterName, tempPathList. Top-level rows must avoid PII; full sanitized params stored in encrypted evidence store referenced by evidenceRef. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Appendix B — Common failure modes & mitigations (expanded)</strong><br><strong>Failure mode: partial artifact observed by a consumer</strong><br>1. Likely cause: client wrote directly to final path rather than via ExportArtifact, or adapter rename semantics not atomic (network FS). <br>2. Mitigation: enforce ExportArtifact usage via static analyzer, run recovery to restore from previous artifact snapshot or staged temp artifact; for network FS, prefer stage-local + publish workflow. <br><strong>Failure mode: backlog of staged_local artifacts fills staging area</strong><br>1. Cause: prolonged network outage or publish worker failure. <br>2. Mitigation: implement staging retention quotas, auto-prioritize and offload to archival store, trigger alerts when staging usage exceeds threshold. <br><strong>Failure mode: checksum mismatch after successful upload</strong><br>1. Cause: possible adapter bug, intermediate mutation, or corruption; may indicate security incident. <br>2. Mitigation: collect forensic artifacts (artifact copies, adapter logs), compute independent checksums, escalate and suspend consumer ingestion if regulated. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Appendix C — Governance checklists & PR requirements (explicit)</strong><br>1. PR must include unit tests and golden vectors for any deterministic algorithms changed. <br>2. Adapter capability changes, atomic semantics, or checksum algorithm updates require migration manifest and owner approval. <br>3. Staging policy changes require SRE sign-off. <br>4. Release manifests must be updated and signed for production changes affecting regulated outputs. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Appendix D — Long-form incident reconstruction example (ordered playbook)</strong><br><strong>Incident synopsis:</strong> "Export checksum mismatch for run r-20260112-455".<br><strong>Forensic reconstruction steps (ordered):</strong><br>1. Retrieve <code>export.*</code>, <code>util.*</code> audit rows and jobDescriptor for correlationId. <br>2. Pull artifactChecksum from <code>export.completed</code> and download artifact from targetUri and staged copies if any. <br>3. Compute independent checksums locally; if mismatch reproducible, collect adapter logs, temp artifacts, and evidenceRef items. <br>4. Restore serialized RNG state and SafeRound inputs (evidenceRef) to re-run deterministic pipeline in dry-run using preserved canonical decimals. <br>5. Compare reproduced artifact checksum to persisted artifact; if reproduces mismatch, inspect transform or adapter. <br>6. Build forensic_manifest.json with audit_tail, artifacts, logs; escalate per compliance rules. <br><strong>Expected outcome:</strong> either reproduce and confirm pipeline correctness (closing incident) or find mutation or adapter defect requiring remediation and regulator notification when required. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Appendix E — PQ & DAX short checklists for template authors and report builders</strong><br><strong>PQ Template author checklist:</strong><br>1. Include mChecksum in template metadata. <br>2. Mark <code>requiresHighPrecision</code> where numeric fidelity matters. <br>3. Parameterize preview seed and persist it in preview audit. <br>4. Offload final numeric aggregation to worker-side SafeRound and persist via REG_Export for authoritative artifacts. <br><strong>DAX/report builder checklist:</strong><br>1. Consume RunMetadata table for run provenance and artifactChecksum. <br>2. Avoid allocation or rounding residuals in DAX; perform in ETL and persist via REG_Export. <br>3. Use hashed stable keys for deterministic sample filters created at ETL time. </td></tr><tr><td data-label="REG_Export — Per-function Expert Technical Breakdown (Expanded, Exhaustive)"> <strong>Final mandatory constraints (non-negotiable):</strong><br>All processes that produce artifacts consumed by other processes must: persist job descriptors atomically, seed deterministic RNGs from correlationId for operator-visible sampling when applicable, use ExportArtifact/AtomicExport for final artifacts, and emit necessary audit rows (<code>export.attempt</code>, <code>export.completed</code> or <code>export.failure</code>, evidenceRef when required). These constraints are mandatory for regulated or PII-touching workflows and enforced by static analyzer, CI gates, and release governance. </td></tr></tbody></table></div><div class="row-count">Rows: 31</div></div><div class="table-caption" id="Table4" data-table="Docu_0178_04" style="margin-top:2mm;margin-left:3mm;"><strong>Table 4</strong></div>
<div class="table-wrapper" data-table-id="table-4"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by REG_Audit — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">REG_Audit — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="REG_Audit — Per-function Expert Technical Breakdown"> <strong>Module-level metadata (contract & overview):</strong><br><strong>Owner:</strong> TEAM_REG_AUDIT (documented in OWNERS.md, included in release manifests and deployment notes).<br><strong>Public API:</strong> EmitAuditEvent, AppendAuditRow, AppendChainedAuditRow, ReadAuditTail, StreamAuditTail, QueryAuditRows, RotateAuditFiles, SignAuditRotation, VerifyAuditChain, CompactAuditTail, FlushAuditBuffer, ExportAuditBundle, ImportAuditBundle, AuditPrune, AuditRestoreFromBackup, AuditRetentionEnforce, EvidenceStorePut, EvidenceStoreGet, EvidenceStoreSeal, SerializeAuditState, RestoreAuditState, AuditReplayRun, AuditTailSnapshot, AuditTailReplay, AuditTailCheckpoint, AuditTailVerifyChecksum. <br><strong>Audits emitted by REG_Audit:</strong> audit.startup, audit.emit.attempt, audit.emit.completed, audit.append.chained, audit.rotate.started, audit.rotate.completed, audit.rotate.failure, audit.rotate.signing.started, audit.rotate.signing.completed, audit.verify.chain.start, audit.verify.chain.pass, audit.verify.chain.fail, audit.export.attempt, audit.export.completed, audit.prune.started, audit.prune.completed, audit.restore.attempt, audit.restore.completed. Every audit row emitted by REG_Audit also includes correlationId, module=REG_Audit, procedure, paramsHash, and resultHash where applicable. <br><strong>Purpose and intended use:</strong> provide a robust, append-only, auditable, provable audit ledger for add-ins and workers that touch regulated or sensitive data; ensure immutability in practice (append-only), tamper-evidence (chained hashes and signatures), durability (atomic persistence and rotation), and forensic replayability. Serve as the canonical source of operator actions, system transitions, and critical configuration changes consumed by compliance, SIEM, and incident response tooling. <br><strong>Non-goals / constraints:</strong> REG_Audit does not attempt to be a blockchain: it provides append-only, chained audit rows with rotation and signing rather than distributed consensus. REG_Audit does not store raw PII in top-level audit rows; large or sensitive payloads are stored encrypted in the EvidenceStore with evidenceRef links. Fast-path emits should be non-blocking for UI threads; heavy IO operations (rotation, signing, export) run on worker threads or scheduled jobs. </td></tr><tr><td data-label="REG_Audit — Per-function Expert Technical Breakdown"> <strong>Operational guarantees (module-level invariants & SLOs):</strong><br>1. <strong>Append-only invariant:</strong> once an audit row is accepted and persisted in a committed rotation, it shall never be altered in-place; only rotation with new signed batch appends is permitted. <br>2. <strong>Chained integrity:</strong> every audit row includes prevHash chaining within the active tail; rotation seals batches with a rotation header containing first/last hashes and a rotationChecksum. <br>3. <strong>Tamper-evidence:</strong> audit rotations are signed with the release manifest key and rotation manifest contains signatures; VerifyAuditChain must detect any modification. <br>4. <strong>Crash-safety:</strong> writes to audit_tail use AtomicWrite semantics: either the old tail remains visible or the new appended tail is fully visible. <br>5. <strong>UI thread safety:</strong> EmitAuditEvent and AppendAuditRow are non-blocking in UI contexts; they queue buffered audit rows and return synchronously; flush operations are asynchronous or scheduled. <br>6. <strong>Retention & archival invariants:</strong> retention rules must be enforced deterministically and verifiably; warm/cold rotations are moved to designated archives with proofs. <br>7. <strong>Performance SLOs:</strong> median EmitAuditEvent latency <5ms for in-memory enqueue; flush latency for background persistence <200ms on local filesystem; rotation signing throughput scaled to expected audit rates in CI. <br><strong>CI / acceptance gates:</strong> golden parity for chain construction, cross-language hash vectors, rotation signing tests, audit-emit performance tests, forbidden API static checks (no direct workbook writes in OnLoad). </td></tr><tr><td data-label="REG_Audit — Per-function Expert Technical Breakdown"> <strong>Global concepts & primitives used by functions:</strong><br><strong>AuditRow canonical shape (logical):</strong> <code>{timestamp, correlationId, module, procedure, operatorId?, paramsHash, payloadRef?, prevHash, rowHash, rotationId?, level, severity?, metadata}</code>.<br><strong>RowHash derivation:</strong> rowHash = SHA256( canonicalSerialize(rowWithoutRowHash) ) where canonicalSerialize uses deterministic field ordering (ASCII-order field names), UTF-8, and normalized numeric formatting.<br><strong>Chaining:</strong> prevHash = last rowHash of previous persisted row in active tail.<br><strong>Rotation header:</strong> <code>{rotationId, firstRowHash, lastRowHash, rowCount, rotationTs, rotationChecksum, rotationSignature?}</code>.<br><strong>EvidenceRef:</strong> pointer to encrypted evidence store artifact (e.g., evidence://hot/<module>/<correlationId>/<blobId>) where sanitized full params or large payloads reside.<br><strong>Tail state:</strong> in-memory queue <code>audit_buffer[]</code>, checkpointed tail file <code>audit_tail.csv</code> (append-only), rotated sealed batches <code>audit_rotation_&lt;ts&gt;.json</code>, and signature store <code>audit_rotation_signatures.json</code>.<br><strong>Security primitives:</strong> use RSA/ECDSA for rotation signatures (release manifest key), HMAC_SHA256 for evidenceRef validation, and optional hardware key (HSM) for production signing. </td></tr><tr><td data-label="REG_Audit — Per-function Expert Technical Breakdown"> <strong><code>EmitAuditEvent(event, level=&quot;INFO&quot;, operatorId=null, immediate=false)</code> — ingestion entrypoint</strong><br><strong>Purpose & contract:</strong> lightweight API for modules and UI handlers to emit auditable events. Must be non-blocking for UI flows when <code>immediate=false</code>. Each call enqueues a normalized audit row into <code>audit_buffer</code> and returns a descriptor <code>{queued=true, tempId}</code>. If <code>immediate=true</code> the call attempts synchronous persistence but may still return without waiting for remote rotation/signing. <br><strong>Parameters & return:</strong> <code>event</code> (structured map with <code>module</code>, <code>procedure</code>, <code>params</code> (optional), <code>metadata</code>), <code>level</code> in {"DEBUG","INFO","WARN","ERROR","CRITICAL"}, <code>operatorId</code> optional, <code>immediate</code> boolean. Returns <code>{queued, tempId, queuedAt}</code> or on immediate persist <code>{success, rowHash, durationMs}</code>. <br><strong>Primary invariants (must/shall):</strong><br>1. Emitted rows must have canonical timestamp in ISO8601 UTC. <br>2. Top-level audit row must not contain raw PII; if <code>params</code> contain sensitive data, module must call EvidenceStorePut and replace <code>params</code> with <code>paramsHash</code> and <code>evidenceRef</code>. <br>3. EmitAuditEvent must compute paramsHash deterministically and include it in audit row. <br><strong>Algorithm & implementation notes:</strong><br>1. Canonicalize <code>event</code> to deterministic JSON (sorted keys, normalized numbers). <br>2. If <code>params</code> size > threshold or flagged sensitive -> EvidenceStorePut -> obtain evidenceRef. <br>3. Compute <code>paramsHash</code> = SHA256(canonicalParams) and drop raw <code>params</code> from row. <br>4. Create <code>row = {timestamp, correlationId (from context), module, procedure, operatorId, paramsHash, evidenceRef?, prevHash=null, rowHash=null}</code>. <br>5. prevHash filled by tail appender; enqueue to <code>audit_buffer</code>. <br><strong>Edge cases & invalid inputs:</strong><br>1. Missing <code>module</code>/<code>procedure</code> -> reject with error UTIL_AUDIT_INVALID_SCHEMA. <br>2. Large <code>params</code> but EvidenceStore disabled -> reject with UTIL_EVIDENCE_DISABLED. <br>3. Non-serializable <code>params</code> -> emit util.audit.emit.invalid_input and return structured failure. <br><strong>Observability & audit fields:</strong> emits <code>audit.emit.attempt(correlationId, module, procedure, paramsHash)</code> and <code>audit.emit.completed(correlationId, rowTempId, queued)</code>; if immediate persist -> <code>audit.emit.persisted(correlationId, rowHash, durationMs)</code>. <br><strong>Examples & narratives:</strong><br>1. UI control click: DQ_Ribbon handler calls EmitAuditEvent({module:"DQ_Ribbon", procedure:"profile.run", params:{table="Sheet1!A1:D100"}}, level="INFO", operatorId="alice") which enqueues row and returns tempId enabling caller to display a non-blocking confirmation. <br><strong>Tests & CI vectors:</strong> property tests for paramsHash parity; security tests ensuring PII not present; throughput tests for enqueue latency under synthetic load. </td></tr><tr><td data-label="REG_Audit — Per-function Expert Technical Breakdown"> <strong><code>AppendAuditRow(serializedRow)</code> — low-level append with chaining</strong><br><strong>Purpose & contract:</strong> accept a fully prepared canonical audit row (with <code>rowHash</code> computed or computed by the function) and append atomically to the current tail file, ensuring <code>prevHash</code> consistency and atomic persistence. Intended for background workers that flush <code>audit_buffer</code>. <br><strong>Parameters & return:</strong> <code>serializedRow</code> (canonical JSON string excluding <code>rowHash</code> or including it), optional <code>expectedPrevHash</code> for optimistic concurrency. Returns <code>{success, rowHash, persistedAt}</code> or error <code>{success:false, errorCode, diagnostics}</code>. <br><strong>Primary invariants (must/shall):</strong><br>1. If <code>expectedPrevHash</code> provided, and current tail lastHash != expectedPrevHash -> fail with UTIL_AUDIT_CONCURRENT_APPEND to avoid forked tails. <br>2. Append must be atomic; use AtomicWrite semantics for tail persistence. <br><strong>Algorithm & implementation notes:</strong><br>1. Lock tail append path using local file lock (advisory) or process-level append queue to avoid races. <br>2. Read last persisted rowHash (tail state file). <br>3. Set <code>serializedRow.prevHash = lastRowHash</code>. <br>4. Compute <code>rowHash = SHA256(serializedRow)</code> and set <code>serializedRow.rowHash = rowHash</code>. <br>5. Append to <code>audit_tail.csv</code> (append line) using AtomicWrite on a temporary tail file followed by replace to avoid partial tail visibility; alternative optimized path: open tail in append mode with O_APPEND and fsync. <br>6. Update in-memory tail state <code>lastRowHash</code> and checkpoint. <br><strong>Edge cases & invalid inputs:</strong><br>1. Disk full -> return UTIL_ATOMIC_WRITE_ENOSPC with diagnostics and keep the audit_buffer intact for retry. <br>2. Partial write detected -> run tail verification and attempt recovery from temp files using AtomicWriteRepair. <br><strong>Observability & audit fields:</strong> emits <code>audit.append.attempt(correlationId, tempId)</code> and <code>audit.append.completed(correlationId, rowHash, durationMs)</code>. <br><strong>Examples & narratives:</strong> background flush thread gathers N rows, calls AppendAuditRow for each in strict order to ensure deterministic chaining. <br><strong>Tests & CI vectors:</strong> concurrency tests simulating multiple flushers; disk-failure injection tests. </td></tr><tr><td data-label="REG_Audit — Per-function Expert Technical Breakdown"> <strong><code>AppendChainedAuditRow(payload, module, procedure, operatorId=null, level=&quot;INFO&quot;)</code> — convenience full path</strong><br><strong>Purpose & contract:</strong> higher-level helper that constructs canonical row, handles EvidenceStore interactions if needed, and appends with chaining in a single call (used by workers). Returns <code>{success, rowHash}</code>. <br><strong>Parameters & return:</strong> <code>payload</code> (structured), <code>module</code>, <code>procedure</code>, <code>operatorId</code>, <code>level</code>. Returns <code>{success, rowHash, persistedAt}</code> or error. <br><strong>Primary invariants (must/shall):</strong> identical to AppendAuditRow regarding chaining, PII policy. <br><strong>Algorithm & implementation notes:</strong> combines EmitAuditEvent and AppendAuditRow flow; used in worker flows where synchronous durability for emitted row is desired before proceeding. <br><strong>Edge cases & invalid inputs:</strong> same as EmitAuditEvent and AppendAuditRow. <br><strong>Observability & audit fields:</strong> emits <code>audit.append.chained(correlationId, module, procedure, rowHash)</code> on success. <br><strong>Examples & narratives:</strong> job scheduler persists jobDescriptor and emits AppendChainedAuditRow({jobId, paramsHash,...}, module="CORE_JobScheduler", procedure="job.persist"). This ensures the job persist is immediately auditable and chained for later forensic reconstruction. </td></tr><tr><td data-label="REG_Audit — Per-function Expert Technical Breakdown"> <strong><code>ReadAuditTail(fromHash=null, limit=1000, includePayloadRefs=false)</code> — read/query tail</strong><br><strong>Purpose & contract:</strong> read sequential audit rows from the tail starting after <code>fromHash</code> (if null, from head of tail) returning up to <code>limit</code> rows. Does not load encrypted evidence payloads unless <code>includePayloadRefs=true</code> and caller has proper authorization. Return stable page with <code>nextCursor</code> (lastRowHash). <br><strong>Parameters & return:</strong> <code>fromHash</code> (string), <code>limit</code> (int), <code>includePayloadRefs</code> (bool). Returns <code>{rows:[...], nextCursor, lastHash, count}</code>. <br><strong>Primary invariants (must/shall):</strong><br>1. Results must reflect a stable snapshot at read start; if rotation occurs during read, the function ensures consistent ordering across rotation boundary by reading sealed rotations first then active tail. <br><strong>Algorithm & implementation notes:</strong><br>1. If <code>fromHash</code> lies in older rotation, locate rotation file and stream rows from that rotation and subsequent rotations up to active tail. <br>2. For active tail, read from <code>audit_tail.csv</code> starting after <code>fromHash</code>. <br>3. If <code>includePayloadRefs</code> true, include <code>paramsHash</code> and <code>evidenceRef</code> but do not retrieve evidence payloads. Evidence payloads fetched separately with EvidenceStoreGet (requires authorization). <br><strong>Edge cases & invalid inputs:</strong><br>1. <code>fromHash</code> unknown -> return UTIL_AUDIT_CURSOR_INVALID. <br>2. Limit excessive -> cap to configured max and annotate response. <br><strong>Observability & audit fields:</strong> emits <code>audit.query(correlationId, fromHash, limit, returnedCount)</code>. <br><strong>Examples & narratives:</strong> forensic tool calls ReadAuditTail(fromHash=r-...-checkpoint, limit=10000) to stream audit rows for a full run. <br><strong>Tests & CI vectors:</strong> cross-rotation reads, cursor parity tests, ordering invariants tests. </td></tr><tr><td data-label="REG_Audit — Per-function Expert Technical Breakdown"> <strong><code>StreamAuditTail(streamHandler, startFrom=null, filter=null)</code> — real-time tail streaming</strong><br><strong>Purpose & contract:</strong> subscribe to tail events in near-real-time (long-poll/stream); deliver new rows to <code>streamHandler</code> callback in canonical order. Suitable for monitoring, live dashboards, and SIEM ingestion. <br><strong>Parameters & return:</strong> <code>streamHandler</code> (callable accepting row objects), <code>startFrom</code> optional cursor, <code>filter</code> optional (module/procedure/level). Returns subscription object <code>{subscriptionId, resumedCursor}</code>. <br><strong>Primary invariants (must/shall):</strong><br>1. Stream must deliver rows in strict tail order; no duplicates except in resume-after-fail scenarios where idempotent consumer must de-duplicate using rowHash. <br>2. Stream must support at-least-once delivery semantics; exactly-once is only achievable with consumer checkpoints and de-duplication. <br><strong>Algorithm & implementation notes:</strong><br>1. Provide SSE/WebSocket or persistent HTTP stream for host processes; on reconnection use <code>startFrom</code> cursor to resume. <br>2. Buffer tail reads and push to handlers; maintain backpressure limits and reject slow consumers after threshold with audit and metrics. <br><strong>Edge cases & invalid inputs:</strong> consumer not acknowledging checkpoints -> throttle or disconnect. <br><strong>Observability & audit fields:</strong> <code>audit.stream.subscribe(correlationId, subscriptionId, filter)</code>, <code>audit.stream.deliver(correlationId, subscriptionId, rowHash)</code>. <br><strong>Examples & narratives:</strong> SIEM connector uses StreamAuditTail(filter={module:"REG_Export"}) to ingest export.attempt/completed events for further correlation. <br><strong>Tests & CI vectors:</strong> stream reconnection and resume tests; backpressure handling tests. </td></tr><tr><td data-label="REG_Audit — Per-function Expert Technical Breakdown"> <strong><code>RotateAuditFiles(rotationPolicy, signerKeyRef=null, archiveDestination=null)</code> — rotate & seal</strong><br><strong>Purpose & contract:</strong> compact active tail into a sealed rotation file according to <code>rotationPolicy</code> (size-based, time-based, or hybrid), compute rotationChecksum, sign rotation with <code>signerKeyRef</code> if provided, and move sealed rotation to archiveDestination depending on retention stage. Rotation must be atomic and produce a <code>rotationManifest</code> referencing rotationId and signature. <br><strong>Parameters & return:</strong> <code>rotationPolicy</code> object, optional <code>signerKeyRef</code> (HSM or key reference), <code>archiveDestination</code> path. Returns <code>{rotationId, firstRowHash, lastRowHash, rowCount, rotationChecksum, signatureRef, archivePath}</code>. <br><strong>Primary invariants (must/shall):</strong><br>1. Rotation must preserve chaining: the sealed rotation's firstRowHash must match the persisted prevHash in the first row recorded. <br>2. Rotation signing must use release manifest key in regulated contexts; signature must be appended to rotationManifest and stored in signature store. <br>3. Rotation operation must be idempotent when retried; detect already-rotated windows via rotationId collision detection and return existing rotation metadata rather than creating duplicates. <br><strong>Algorithm & implementation notes:</strong><br>1. Determine rotation window using policy (e.g., all rows older than <code>now - retentionHotWindow</code> or when <code>audit_tail.size &gt; maxTailSize</code>). <br>2. Atomically snapshot active tail up to rotation cutpoint into tempRotationFile. <br>3. Compute rotationChecksum = SHA256(concat(rowHashes)) or SHA256(tempRotationFile) and produce rotationManifest including first/last row hashes and rowCount. <br>4. If <code>signerKeyRef</code> present, sign rotationManifest (timestamped) producing rotationSignature. Store signature in <code>audit_rotation_signatures.json</code> and attach signatureRef. <br>5. Move tempRotationFile -> finalRotationPath and atomically truncate active tail (keeping rows after cutpoint). <br>6. Emit audit.rotate.started and audit.rotate.completed with rotation metadata. <br><strong>Cross-platform concerns & fallbacks:</strong><br>1. On filesystems with weak rename semantics, use two-phase commit with rotation state file and emit audit.rotate.degraded. <br>2. When HSM unavailable, fallback to encrypted signerKeyRef in local keystore with appropriate audit and operator approval for regulated environments. <br><strong>Recovery & runbook:</strong><br>1. On rotation failure after partial move, use rotation temp artifacts to either complete move or roll back. <br>2. Recompute rotationChecksum and compare with partial artifacts; if mismatch, mark rotation as suspect and quarantine. <br><strong>Observability & audit fields:</strong> <code>audit.rotate.started(correlationId, rotationId, firstRowHash, lastRowHash)</code> and <code>audit.rotate.completed(correlationId, rotationId, rotationChecksum, signatureRef, archivePath)</code>. <br><strong>Examples & narratives:</strong> nightly rotation consolidates that day's audit rows into <code>audit_rotation_2026-01-17.json</code> signed with release manifest key and pushed to warm archive. <br><strong>Tests & CI vectors:</strong> simulate concurrent rotations; cross-filesystem rename injection tests; signature validation suites. </td></tr><tr><td data-label="REG_Audit — Per-function Expert Technical Breakdown"> <strong><code>SignAuditRotation(rotationId, signerKeyRef)</code> — detached signature flow</strong><br><strong>Purpose & contract:</strong> create a detached cryptographic signature over the rotationManifest for <code>rotationId</code> using <code>signerKeyRef</code>. Signature stored separately and referenced from rotation manifest; supports key-rolling processes. <br><strong>Parameters & return:</strong> <code>rotationId</code>, <code>signerKeyRef</code>. Returns <code>{signatureRef, algorithm, signerId, signedAt}</code> or error. <br><strong>Primary invariants (must/shall):</strong><br>1. Signature must include canonical rotationManifest bytes and signer metadata. <br>2. Signature keys are managed by release manifest process and key rotation requires two-person approval for production. <br><strong>Algorithm & implementation notes:</strong><br>1. Retrieve rotationManifest for rotationId; canonicalize manifest; produce signature = Sign(manifest, signerKeyRef) and create signatureRef record with signature metadata and signer fingerprint. <br>2. Store signatureRef in <code>audit_rotation_signatures.json</code> and append an audit row <code>audit.rotate.signing.completed</code>. <br><strong>Edge cases & invalid inputs:</strong> missing rotationManifest -> UTIL_AUDIT_ROTATION_MISSING. <br><strong>Observability & audit fields:</strong> <code>audit.rotate.signing.started(correlationId, rotationId, signerId)</code> and <code>audit.rotate.signing.completed(correlationId, rotationId, signatureRef)</code>. <br><strong>Examples & narratives:</strong> CI pipeline signs rotation artifacts for canary release using ephemeral signing key and stores signatureRef in release manifest for traceability. <br><strong>Tests & CI vectors:</strong> signature verify golden vectors; key-rotation acceptance tests. </td></tr><tr><td data-label="REG_Audit — Per-function Expert Technical Breakdown"> <strong><code>VerifyAuditChain(earliestRotation=null, latestRotation=null)</code> — chain verification</strong><br><strong>Purpose & contract:</strong> deterministic verification procedure that validates every rotation between <code>earliestRotation</code> and <code>latestRotation</code> (or all rotations if null) for chaining integrity, rowHash correctness, rotationChecksum correctness, and signature validity. Returns a comprehensive verification report. <br><strong>Parameters & return:</strong> <code>earliestRotation</code> optional, <code>latestRotation</code> optional. Returns <code>{verified:true/false, report:{rotationChecks:[...], errors:[...], sampleRows:[...], verifiedAt}}</code>. <br><strong>Primary invariants (must/shall):</strong><br>1. VerifyAuditChain must be deterministic and idempotent; running it twice on same artifact set yields same report. <br>2. Any mismatch in rowHash, prevHash chaining, rotationChecksum, or signature must be surfaced with exact artifact paths and diagnostics. <br><strong>Algorithm & implementation notes:</strong><br>1. Iterate sealed rotations in chronological order; for each rotation: verify that concatenated row hashes compute to rotationChecksum, verify rows' rowHash fields recompute correctly, verify firstRowHash of rotation matches prevHash linkage from prior rotation lastRowHash, verify signature using stored signer public keys (release manifest). <br>2. For active tail, verify sequential rowHash/prevHash invariants. <br>3. Produce sparse sampleRows with canonicalSerialized rows for fast debug. <br><strong>Edge cases & invalid inputs:</strong><br>1. Missing signer public keys -> fail with UTIL_VERIFY_MISSING_KEYS and optionally mark rotation as unverifiable until key available. <br>2. Partially corrupted rotation files -> include byte-offset diagnostics. <br><strong>Observability & audit fields:</strong> emits <code>audit.verify.chain.start(correlationId)</code> and either <code>audit.verify.chain.pass(correlationId, checkedRotations)</code> or <code>audit.verify.chain.fail(correlationId, errors)</code>. <br><strong>Examples & narratives:</strong> pre-release CI runs VerifyAuditChain over all rotations produced during smoke runs to ensure no accidental tampering before canary rollout. <br><strong>Tests & CI vectors:</strong> mutation tests where single byte flipped in rotation file should cause verify to fail and CI to block the merge. </td></tr><tr><td data-label="REG_Audit — Per-function Expert Technical Breakdown"> <strong><code>CompactAuditTail(compactionPolicy)</code> — compress & compact active tail</strong><br><strong>Purpose & contract:</strong> reduce the active tail footprint by compacting older audit rows into more compact representations for storage while preserving full forensic replayability via evidenceRefs and rotation indexes. Must be reversible (i.e., compacted representation + evidence store allows full reconstruction). <br><strong>Parameters & return:</strong> <code>compactionPolicy</code> (ageThreshold, compressAlgo, chunkSize). Returns <code>{compactedRows, spaceSaved, compactManifestRef}</code>. <br><strong>Primary invariants (must/shall):</strong><br>1. Compaction never removes data required to reconstruct full original audit rows; only compresses or externalizes large payloads to EvidenceStore. <br>2. Compaction steps must be idempotent and produce a compactManifest referencing original rowHashes. <br><strong>Algorithm & implementation notes:</strong><br>1. Select candidate rows older than <code>ageThreshold</code>. <br>2. For each row: move <code>evidenceRef</code> payloads to long-term EvidenceStore if not already moved; replace payload references with evidence fingerprints and compacted metadata (e.g., <code>compactedRef</code>). <br>3. Batch rows into compressed chunks using <code>compressAlgo</code> and store chunk artifacts in archive, creating <code>compactManifest</code> describing mapping from rowHashes -> chunkId + offset. <br>4. Atomically replace row block in tail with a compact pointer (preserving chaining with synthetic rowHash computed deterministically from compactManifest). <br><strong>Edge cases & invalid inputs:</strong> corruption in compacted chunk -> rollback compaction for affected chunk and mark for operator review. <br><strong>Observability & audit fields:</strong> <code>audit.compact.started(correlationId, candidateCount)</code>, <code>audit.compact.completed(correlationId, compactManifestRef, spaceSaved)</code>. <br><strong>Examples & narratives:</strong> compaction run converts six months of routine INFO-level audit rows into compressed monthly bundles while preserving high-severity rows intact. <br><strong>Tests & CI vectors:</strong> round-trip reconstruction tests; compactManifest integrity tests. </td></tr><tr><td data-label="REG_Audit — Per-function Expert Technical Breakdown"> <strong><code>FlushAuditBuffer(force=false, maxBatch=1000)</code> — immediate persistence of buffered rows</strong><br><strong>Purpose & contract:</strong> push <code>audit_buffer</code> rows to durable tail storage synchronously (used in graceful shutdown, critical checkpoints). When <code>force=false</code> the function may batch and defer non-blocking semantics. Returns <code>{flushedCount, elapsedMs}</code> or error on failure. <br><strong>Parameters & return:</strong> <code>force</code> bool, <code>maxBatch</code> int. <br><strong>Primary invariants (must/shall):</strong> flush must preserve order of rows during persistence. <br><strong>Algorithm & implementation notes:</strong><br>1. Acquire flush lock to avoid concurrent flushes. <br>2. Drain up to <code>maxBatch</code> rows from <code>audit_buffer</code>, call AppendAuditRow sequentially. <br>3. On partial failure, requeue remaining rows and return failure with diagnostics. <br><strong>Edge cases & invalid inputs:</strong> disk full -> flush returns ENOSPC with flushedCount and remainingCount. <br><strong>Observability & audit fields:</strong> <code>audit.flush.started(correlationId, batchSize)</code>, <code>audit.flush.completed(correlationId, flushedCount, elapsedMs)</code>. <br><strong>Examples & narratives:</strong> On Add-in shutdown hook, call FlushAuditBuffer(force=true, maxBatch=10000) to ensure ephemeral UI emits are persisted. <br><strong>Tests & CI vectors:</strong> shutdown flush tests, partial failure retry tests. </td></tr><tr><td data-label="REG_Audit — Per-function Expert Technical Breakdown"> <strong><code>ExportAuditBundle(rotationIds[], destination, options={compress:true, includeEvidence:false})</code> — atomic export of audit artifacts</strong><br><strong>Purpose & contract:</strong> produce an auditable export bundle containing specified rotation files, optional active tail snapshot, and optional evidence artifacts; export must be atomic and accompanied by bundleChecksum. Used for regulatory submissions and forensic transfers. <br><strong>Parameters & return:</strong> <code>rotationIds</code> array, <code>destination</code> (local path or remote URI), <code>options</code>. Returns <code>{bundlePath, bundleChecksum, artifactCount, exportedEvidenceCount}</code>. <br><strong>Primary invariants (must/shall):</strong><br>1. Export must be atomic: either the bundle is fully written and checksum verified or not visible to consumers. <br>2. When including evidence and the evidence store is encrypted, ensure proper key references included and operator access controls enforced. <br><strong>Algorithm & implementation notes:</strong><br>1. Collect requested rotation files and optionally active tail snapshot. <br>2. If <code>includeEvidence</code>, fetch evidence payloads (authorized) and include them in a sealed <code>evidence/</code> directory, ensuring PII policies are observed; otherwise include evidenceRefs only. <br>3. Generate <code>bundleManifest.json</code> with file list, checksums, rotationManifests, and include signed exportMetadata if operator authorized. <br>4. Write to temporary bundle file and move to <code>destination</code> atomically using AtomicWrite semantics; compute SHA256 checksum and optionally sign bundleManifest. <br><strong>Cross-system concerns & fallbacks:</strong> when remote URI is network mount with weak guarantees, write to local staging then attempt staged upload; emit audit.export.degraded when fallback used. <br><strong>Observability & audit fields:</strong> <code>audit.export.attempt(correlationId, rotations, destination)</code> and <code>audit.export.completed(correlationId, bundlePath, bundleChecksum)</code>. <br><strong>Examples & narratives:</strong> regulator-requested export for quarter-end includes rotations for the quarter and evidence for specific correlationIds; bundle signed by release manifest key and uploaded via secure channel. <br><strong>Tests & CI vectors:</strong> export integrity tests, staged upload fallback tests. </td></tr><tr><td data-label="REG_Audit — Per-function Expert Technical Breakdown"> <strong><code>ImportAuditBundle(bundlePath, verifyOnly=true, acceptPolicy=null)</code> — import and verify exported bundles</strong><br><strong>Purpose & contract:</strong> ingest an exported audit bundle into a verification or ingest environment. When <code>verifyOnly=true</code> perform integrity and signature checks without installing into local archive. When <code>verifyOnly=false</code>, install inspected rotations and optionally evidence. Returns verification report and installed artifact refs if accepted. <br><strong>Parameters & return:</strong> <code>bundlePath</code>, <code>verifyOnly</code>, <code>acceptPolicy</code>. Returns <code>{verified, report, installedRefs}</code>. <br><strong>Primary invariants (must/shall):</strong> do not ingest bundles that fail signature or checksum checks unless operator override and documented acceptance policy present. <br><strong>Algorithm & implementation notes:</strong><br>1. Validate bundleChecksum and compare with <code>bundleManifest.json</code>. <br>2. Validate rotationManifests and signatures. <br>3. If <code>verifyOnly=false</code> and policy allows, install rotation files into local archive ensuring no chain collisions; if collisions detected, require operator manual resolution. <br><strong>Observability & audit fields:</strong> <code>audit.import.attempt(correlationId, bundlePath)</code> and <code>audit.import.completed(correlationId, installedRefs)</code>. <br><strong>Examples & narratives:</strong> security team imports third-party bundle for cross-organization audit verification; VerifyAuditChain run post-import. <br><strong>Tests & CI vectors:</strong> tampered bundle detection tests; signature verification negative tests. </td></tr><tr><td data-label="REG_Audit — Per-function Expert Technical Breakdown"> <strong><code>EvidenceStorePut(plaintextPayload, metadata, sensitivityLevel, retentionPolicy)</code> — encrypted evidence storage</strong><br><strong>Purpose & contract:</strong> store large or sensitive parameters and payloads outside top-level audit rows, returning an <code>evidenceRef</code> pointer and <code>payloadHash</code>. Evidence store encrypts payloads with tenant keys and stores metadata separately. EvidenceStore is access-controlled and audited. <br><strong>Parameters & return:</strong> <code>plaintextPayload</code> (bytes), <code>metadata</code> (json), <code>sensitivityLevel</code> (enum LOW/MEDIUM/HIGH/PII), <code>retentionPolicy</code>. Returns <code>{evidenceRef, payloadHash, storagePath}</code>. <br><strong>Primary invariants (must/shall):</strong><br>1. Evidence payloads for PII must be encrypted at rest with keys rotated per policy and stored in the hot evidence store with tight ACLs. <br>2. <code>payloadHash</code> = SHA256(plaintextPayload) used to detect duplicates and for verification during export/import. <br><strong>Algorithm & implementation notes:</strong><br>1. Sanitize <code>metadata</code> (strip PII if necessary), compute payloadHash and evidenceFingerprint. <br>2. Encrypt payload using tenant key or HSM-wrapped KEK, store encrypted blob in evidence store path (hot/warm according to sensitivity). <br>3. Generate <code>evidenceRef = evidence://&lt;tier&gt;/&lt;module&gt;/&lt;correlationId&gt;/&lt;blobId&gt;</code> and return. <br><strong>Edge cases & invalid inputs:</strong> insufficient authorization -> UTIL_EVIDENCE_ACCESS_DENIED. <br><strong>Observability & audit fields:</strong> <code>util.rng.seeded</code> may appear in evidence flows when RNG state saved; emit <code>util.evidence.put(correlationId, evidenceRef, payloadHash)</code>. <br><strong>Examples & narratives:</strong> when EmitAuditEvent receives <code>params</code> containing full SQL text or PII, call EvidenceStorePut and replace params with paramsHash/evidenceRef in the audit row. <br><strong>Tests & CI vectors:</strong> evidence round-trip encryption/decryption tests; retention lifecycle tests. </td></tr><tr><td data-label="REG_Audit — Per-function Expert Technical Breakdown"> <strong><code>EvidenceStoreGet(evidenceRef, authorizeToken)</code> — retrieve evidence payload</strong><br><strong>Purpose & contract:</strong> fetch an encrypted evidence blob, verify authorization, and return decrypted payload in memory or stream to caller. Strict access controls and audit emission are required. <br><strong>Parameters & return:</strong> <code>evidenceRef</code>, <code>authorizeToken</code> (or operator context). Returns <code>{payload, payloadHash, metadata}</code> or error. <br><strong>Primary invariants (must/shall):</strong> access must be logged with <code>evidence.access</code> audit row showing operatorId and reason. Evidence payloads should not be returned to UI without PII masking unless operator authorized. <br><strong>Edge cases & invalid inputs:</strong> stale key -> attempt KEK rotation path or fail with UTIL_EVIDENCE_KEY_MISSING. <br><strong>Observability & audit fields:</strong> <code>audit.evidence.access(correlationId, evidenceRef, operatorId, reason)</code> emitted for every retrieval. <br><strong>Examples & narratives:</strong> Forensic investigator retrieves serialized RNG state using EvidenceStoreGet(evidenceRef) to perform deterministic replay. <br><strong>Tests & CI vectors:</strong> access control tests; encrypted key rotation tests. </td></tr><tr><td data-label="REG_Audit — Per-function Expert Technical Breakdown"> <strong><code>AuditPrune(policy)</code> — controlled deletion according to retention</strong><br><strong>Purpose & contract:</strong> evaluate retention policy and safely delete expired audit artifacts and evidence in compliance with retention windows (hot/warm/cold) while preserving forensic proofs (e.g., proof-of-delete records). Prune must be verifiable and produce <code>forensic_manifest.json</code> entries listing deleted artifacts and proofs. <br><strong>Parameters & return:</strong> <code>policy</code> object with retention windows and exclusions. Returns <code>{prunedArtifacts, proofOfDeleteRef, errors}</code>. <br><strong>Primary invariants (must/shall):</strong><br>1. Deletions for regulated data must be performed only after the appropriate retention window and with required approvals; two-person approval required for regulated PII. <br>2. Proof-of-delete must include prior artifact checksum, operator approvals, and deletion timestamp, and be appended into secure deletion ledger. <br><strong>Algorithm & implementation notes:</strong><br>1. Evaluate candidate artifacts older than policy thresholds and not excluded by retention exceptions. <br>2. For each artifact, generate proof-of-delete record including artifactChecksum, artifactPath, and deletion metadata; optionally sign the proof using rotation signing key. <br>3. Remove artifact from store and move proof-of-delete to warm/cold archive with restricted ACLs. <br>4. Emit <code>audit.prune.completed</code> with counts and proofRef. <br><strong>Edge cases & invalid inputs:</strong> legal hold overrides -> skip prune and log <code>audit.prune.skipped</code>. <br><strong>Observability & audit fields:</strong> <code>audit.prune.started(correlationId, candidateCount)</code> and <code>audit.prune.completed(correlationId, prunedCount, proofRef)</code>. <br><strong>Examples & narratives:</strong> monthly prune run removes audit rotations older than 7 years except those marked by legal hold. <br><strong>Tests & CI vectors:</strong> legal hold tests; proof-of-delete signature checks. </td></tr><tr><td data-label="REG_Audit — Per-function Expert Technical Breakdown"> <strong><code>AuditRestoreFromBackup(backupManifest, restorePolicy)</code> — full forensic restore</strong><br><strong>Purpose & contract:</strong> restore audit rotations and evidence from backup archives into the live archive for forensic replay and investigation. Restore must be auditable and must not overwrite existing rotations unless operator explicitly allows. <br><strong>Parameters & return:</strong> <code>backupManifest</code>, <code>restorePolicy</code>. Returns <code>{restoredRefs, conflicts, report}</code>. <br><strong>Primary invariants (must/shall):</strong><br>1. Restored artifacts must be verified against checksums and rotation signatures before being accepted. <br>2. On conflict (rotationId exists), create a parallel namespace and preserve both artifacts for forensic comparison. <br><strong>Algorithm & implementation notes:</strong><br>1. Validate backupManifest checksums and signatures. <br>2. Copy rotation files and evidence blobs into restore staging area and run VerifyAuditChain for the restored rotation set. <br>3. On success, import into live archive with new manifest entries and append <code>audit.restore.completed</code>. <br><strong>Edge cases & invalid inputs:</strong> corrupted backup -> abort restore and emit <code>audit.restore.failure</code>. <br><strong>Observability & audit fields:</strong> <code>audit.restore.attempt(correlationId, backupManifestRef)</code> and <code>audit.restore.completed(correlationId, restoredRefs)</code>. <br><strong>Examples & narratives:</strong> SRE restores a suspected-misbehaving rotation into staging for analysis without altering live archive. <br><strong>Tests & CI vectors:</strong> restore integrity tests; conflict resolution tests. </td></tr><tr><td data-label="REG_Audit — Per-function Expert Technical Breakdown"> <strong><code>AuditReplayRun(correlationId, evidenceRefs[], options={dryRun:true, preserveOutputs:true})</code> — deterministic replay</strong><br><strong>Purpose & contract:</strong> reconstruct sequence of audit-triggered operations for a given <code>correlationId</code> by loading audit rows, fetching evidence payloads as required, restoring RNG states, and replaying deterministic modules (SafeRound, MatchMerge tie-breakers) to reproduce artifacts. Intended for forensic verification. <br><strong>Parameters & return:</strong> <code>correlationId</code>, <code>evidenceRefs</code>, <code>options</code>. Returns <code>{replaySuccess, differences:[...], outputs:[artifactRefs], proofRef}</code>. <br><strong>Primary invariants (must/shall):</strong><br>1. Replay must be deterministic given same audit rows, evidence payloads, and RNG serialized states; any divergence must be recorded with precise diff diagnostics. <br>2. Replay must not perform side-effects on production artifacts unless <code>preserveOutputs=false</code> and operator explicitly authorizes. <br><strong>Algorithm & implementation notes:</strong><br>1. Read audit rows for correlationId using ReadAuditTail; fetch EvidenceStore payloads; restore RNG states via DeterministicRNG.restore_state for seeded runs; run pipeline modules in deterministic order using canonical inputs. <br>2. Compute output artifact checksums and compare with original artifactChecksum recorded in audit rows. <br>3. Produce <code>replayReport</code> containing step-by-step hashes and divergence points if any. <br><strong>Edge cases & invalid inputs:</strong> missing evidence or missing RNG state -> replayFail with specific missing artifacts list. <br><strong>Observability & audit fields:</strong> <code>audit.replay.started(correlationId, runId)</code>, <code>audit.replay.completed(correlationId, runId, replaySuccess)</code>. <br><strong>Examples & narratives:</strong> Compliance team requests deterministic replay of <code>r-20260117-xyz</code> allocation run to validate allocation tie-break decisions; replay produces identical artifact checksums and is attached to the compliance package. <br><strong>Tests & CI vectors:</strong> deterministic replay regression tests; missing-evidence negative tests. </td></tr><tr><td data-label="REG_Audit — Per-function Expert Technical Breakdown"> <strong>Cross-cutting Observability, Telemetry & Error catalog (concepts & mapping)</strong><br><strong>Audit schema (canonical):</strong> timestamp, correlationId, module, procedure, operatorId (optional), paramsHash, evidenceRef (optional), prevHash, rowHash, rotationId (optional), severity, level, metadata object (duration_ms, attempts, artifactChecksum, tempPathList). Top-level audit rows must not contain raw PII. Evidence store must be referenced via evidenceRef with encrypted payloads. <br><strong>Key audit events produced by REG_Audit:</strong> audit.startup, audit.emit.attempt/completed, audit.append.chained, audit.flush.started/completed, audit.rotate.started/completed/failure, audit.rotate.signing.started/completed, audit.verify.chain.start/pass/fail, audit.export.attempt/completed, audit.import.attempt/completed, audit.compact.started/completed, audit.prune.started/completed, audit.restore.attempt/completed, audit.replay.started/completed. <br><strong>Representative ErrorCodes:</strong> AUDIT_APPEND_CONFLICT, AUDIT_TAIL_CORRUPT, AUDIT_ROTATE_SIGNATURE_MISSING, AUDIT_VERIFY_FAILED, AUDIT_EXPORT_ENOSPC, AUDIT_IMPORT_SIGNATURE_MISMATCH, EVIDENCE_ACCESS_DENIED, EVIDENCE_KEY_MISSING, AUDIT_PRUNE_LEGAL_HOLD, AUDIT_REPLAY_MISSING_EVIDENCE. Each error code maps to operator guidance used by REG_Error.SafeErrorToUser. <br><strong>Metric names and semantics:</strong> reg.audit.emit.latency_ms, reg.audit.append.rate, reg.audit.rotate.count, reg.audit.verify.fail_rate, reg.audit.export.latency_ms. Metrics buffered locally and uploaded by CORE_Telemetry in audited batches; utility functions must not perform remote exports in the fast path. <br><strong>Evidence policy:</strong> top-level audit rows store <code>paramsHash</code> only; full params stored encrypted in EvidenceStore and referenced by evidenceRef. PII must never appear in top-level audit fields or UI. </td></tr><tr><td data-label="REG_Audit — Per-function Expert Technical Breakdown"> <strong>Testing matrix, property tests, and cross-language golden governance</strong><br><strong>Unit tests (must include):</strong><br>1. Append & chaining unit tests verifying rowHash/prevHash properties across single and batch appends.<br>2. Rotation tests: size/time-based rotations, signature generation, signature verification, rotation manifest parity. <br>3. ReadAuditTail cursor & resume tests covering rotation boundaries.<br>4. EvidenceStore encryption/decryption and access control tests.<br>5. Export/Import bundle integrity tests and signed bundle verification.<br>6. Prune & proof-of-delete tests including legal-hold overrides.<br><strong>Integration tests:</strong><br>1. End-to-end add-in flow: UI EmitAuditEvent -> background flush -> rotation -> sign -> VerifyAuditChain success. <br>2. Recovery scenarios: partial rotation, disk full, corrupted rotation files leading to expected alarms. <br>3. Deterministic replay: produce a pipeline artifact and verify replay produces identical artifact checksums. <br><strong>Property tests:</strong><br>1. Immutable chain property under random concurrent append scheduling. <br>2. Idempotence of rotation and signing operations under retry. <br><strong>CI golden gating:</strong><br>1. Golden vectors for rowHash computations and rotation signatures must match cross-language implementations (VBA/JS/Python/C#). <br>2. Static analyzer checks: forbid blocking IO in UI handlers, forbid inclusion of PII in top-level audit rows. <br><strong>Performance tests:</strong><br>1. EmitAuditEvent throughput and latency under stress. <br>2. Rotation signing throughput and signature verification latency. <br>3. Export bundling and staged upload overhead. </td></tr><tr><td data-label="REG_Audit — Per-function Expert Technical Breakdown"> <strong>Developer guidance, allowed & forbidden patterns (explicit)</strong><br><strong>Required usage patterns:</strong><br>1. Always call EmitAuditEvent for user actions, control clicks, and critical transitions. <br>2. Use AppendChainedAuditRow when a module requires immediate durable row persisted before proceeding. <br>3. Do not include raw PII in audit rows; use EvidenceStorePut and include evidenceRef. <br>4. Seed deterministic RNGs for operator-visible sampling and persist RNG state via EvidenceStore when needed for replay. <br><strong>Forbidden practices:</strong><br>1. Do not write audit rows directly to rotation files bypassing AppendAuditRow/RotateAuditFiles; static analyzer rejects such writes. <br>2. Do not serialize secrets or credentials into audit params; EvidenceStore only accepts sanitized payloads. <br>3. Do not skip signature verification in verifier workflows; failure to verify must abort import or flag as suspect. <br><strong>Code-review checklist:</strong> ensure audit emissions cover persistence flows, evidenceRef used for large/sensitive params, rotation signing hooks present, VerifyAuditChain tested in pipeline, and legal-hold exceptions are enforced at prune time. </td></tr><tr><td data-label="REG_Audit — Per-function Expert Technical Breakdown"> <strong>Operational runbook & incident playbooks (executable steps)</strong><br><strong>Audit tail corruption detection:</strong><br>1. Alert fires: audit.verify.chain.fail or <code>audit.append.verification_failed</code> observed in monitoring.<br>2. Immediately set audit reads to read-only and emit <code>audit.containment.action</code> with correlationId.<br>3. Collect <code>audit_tail.csv</code>, rotation files around failure window, <code>audit_rotation_signatures.json</code>, release manifest, and jobDescriptor files; compute checksums and produce <code>forensic_manifest.json</code>.<br>4. Run VerifyAuditChain in isolated forensic environment; attempt recovery using temp rotation artifacts if available. <br><strong>Atomic export ENOSPC runbook:</strong><br>1. Audit audit.export.failure with ENOSPC; inspect destination mount and df -h; identify available space and mount type. <br>2. If policy allows, stage export to local same-volume staging path and retry; compute and verify bundleChecksum. <br>3. If staging not possible, escalate to SRE with <code>forensic_manifest.json</code> and audit_tail. <br><strong>Signature mismatch triage:</strong><br>1. Identify rotationId and signerId from audit.rotate.signing.completed rows; fetch rotationManifest and signature. <br>2. Verify public key for signerId from release manifest; if missing, retrieve key material from key repository. <br>3. If signature still fails, isolate rotation and mark suspect; begin incident response with full forensic artefacts. <br><strong>Non-deterministic replay complaint triage:</strong><br>1. Pull audit.replay.* and util.rng.state_serialized audits; fetch serialized RNG state from EvidenceStore. <br>2. Replay in isolated environment using same evidence and RNG state; attach replay report to incident. <br><strong>Retention & prune disputes:</strong><br>1. If audit artifact was pruned unexpectedly, fetch proof-of-delete and review legal hold status and operator approvals in proof metadata. <br>2. If missing proof or malformed proof, escalate to compliance and SRE. </td></tr><tr><td data-label="REG_Audit — Per-function Expert Technical Breakdown"> <strong>Extremely detailed narratives & examples (multiple scenarios)</strong><br><strong>Scenario 1 — Regulated end-of-period signature & submission (complete trace):</strong><br>1. Operator triggers <code>FinalizeQuarter</code> from REG_Ribbon; ribbon handler emits <code>EmitAuditEvent({module:&quot;REG_Ribbon&quot;,procedure:&quot;finalizeQuarter&quot;,params:{quarter:&quot;Q4-2025&quot;}}, operatorId=&quot;bob&quot;)</code> which returns tempId and queued status. <br>2. JobScheduler persists jobDescriptor using AppendChainedAuditRow({jobId, paramsHash,...}, module="CORE_JobScheduler", procedure="job.persist") ensuring the job persistence row is durably chained to audit tail. <br>3. Worker processes ledger, generates artifacts, calls AppendChainedAuditRow for <code>allocation.complete</code> including artifactChecksum and evidenceRef for the ledger snapshot. <br>4. Background rotation job picks up sealed rotations and calls RotateAuditFiles with <code>signerKeyRef</code> pointing to release manifest signing key. Rotation signed and moved to warm archive; audit.rotate.completed row emitted with signatureRef. <br>5. ExportAuditBundle invoked to assemble rotations for quarter-end submission; atomic export produces bundle and audit.export.completed emitted. <br>6. Release pipeline includes VerifyAuditChain in CI before regulatory submission; VerifyAuditChain passes and signatures validated. <br>7. Audit artifacts and export bundle are packaged in regulatory submission with <code>forensic_manifest.json</code> containing rotation checksums and signatureRefs. <br><strong>Narrative takeaways:</strong> full cryptographic chain from UI action → chained audit rows → rotation → signature → export produces a reproducible, verifiable regulatory package. <br><strong>Scenario 2 — PQ template injection forensic trace:</strong><br>1. PQ_Ribbon preview executes and calls EmitAuditEvent({module:"PQ_Ribbon", procedure:"pq_preview", params:{templateId:"t-123"}}, operatorId="carol"). <br>2. When operator injects template, PQ_Injector uses AppendChainedAuditRow to persist the injection event and persists the M artifact in AtomicWrite storage; audit includes artifactChecksum and evidenceRef linking to the M text. <br>3. If PQ template marked <code>requiresHighPrecision</code>, the injector delegates final numeric transforms to worker which uses SafeRound and persists final query/formula as audited artifact; audit rows include evidenceRef and deterministic RNG state if sampling occurred during preview. <br>4. Export and signature flows ensure injected templates used in regulated contexts are accounted for in release manifest. <br><strong>Narrative takeaways:</strong> ensure artifact checksums and evidenceRefs are included in audit rows so injections and template usage are traceable. <br><strong>Scenario 3 — DQ_Remediation deterministic replay:</strong><br>1. DQ_Rules identifies remediation proposals and calls AppendChainedAuditRow for <code>dq_proposal</code> including a paramsHash and evidenceRef containing sample before/after rows and RNG serialized state used for sampling. <br>2. If operator accepts proposal, DQ_Apply persists the reversible plan using AppendChainedAuditRow and AtomicWrite. <br>3. If operator disputes results, AuditReplayRun reconstructs the exact tie-break ordering and SafeRoundResiduals behavior using persisted RNG state and evidence snapshots; replay produces identical artifact checksums proving deterministic behavior. <br><strong>Narrative takeaways:</strong> deterministic RNG and persisted evidence are essential for resolving disputes. </td></tr><tr><td data-label="REG_Audit — Per-function Expert Technical Breakdown"> <strong>Conceptual Power Query (M) patterns — how REG_Audit principles map to PQ workflows (detailed conceptual mapping)</strong><br><strong>Context:</strong> PQ (Power Query M) environments vary in runtime behavior and lack direct strong guarantees for file-system atomicity and cryptographic signing; REG_Audit cannot run inside M but must be orchestrated by host add-ins and helpers. <br><strong>Patterns and recommendations:</strong><br>1. <strong>Audit emission for PQ actions:</strong> PQ_Ribbon or PQ_Injector must call EmitAuditEvent for preview, inject, refresh, and export steps. Where PQ cannot call host directly, parameterize M templates to return <code>previewSeed</code> and <code>previewChecksum</code> which host then captures and appends an audit row with EvidenceStorePut for large previews. <br>2. <strong>AtomicWrite mapping for PQ exports & injections:</strong> PQ templates produce artifacts that the host helper (signed XLAM or worker) persists via AtomicWrite to guarantee consumers never observe partial templates. The host then emits an AppendChainedAuditRow with artifactChecksum and evidenceRef. <br>3. <strong>DeterministicRNG mapping for PQ preview sampling:</strong> seed computed by the host (SeedFromCorrelation) passed into M templates as parameter; preview audit rows record the seed and mChecksum. For heavy sampling, perform sampling in worker using DeterministicRNG and persist RNG state via EvidenceStore; store evidenceRef in the audit. <br>4. <strong>Signature & export:</strong> Final export bundles for PQ templates (M + diagnostics) must be produced by the host helper and exported using ExportAuditBundle, signed, and then optionally injected into workbook with validated artifactChecksum matching audit. <br><strong>Operator narrative (PQ injection example):</strong><br>1. Operator previews template; PQ_Ribbon records preview audit with seed & mChecksum. <br>2. Operator injects; host helper persists M artifact atomically and emits audit containing artifactChecksum; workbook injection uses the same artifact to ensure audit parity. <br><strong>Governance note:</strong> for regulated templates require signed template manifests and host-side AtomicWrite for authoritative injection. </td></tr><tr><td data-label="REG_Audit — Per-function Expert Technical Breakdown"> <strong>Conceptual DAX patterns — mapping REG_Audit to DAX & semantic model design (detailed conceptual mapping)</strong><br><strong>Context:</strong> DAX runs in analysis/visualization layer and cannot perform durable side-effects or sign artifacts. Audit responsibility remains with ETL/worker layer. <br><strong>Patterns & recommended practices:</strong><br>1. <strong>Push authoritative state to ETL:</strong> any rounding, allocation, or reconciliation decisions that must be auditable should be done in ETL and persisted as audited artifacts (with artifactChecksum) referenced by model. DAX can read these artifacts and show <code>RunMetadata</code> info for provenance. <br>2. <strong>Model metadata linkage:</strong> ETL writes a <code>RunMetadata</code> table atomically with fields <code>correlationId</code>, <code>artifactChecksum</code>, <code>rotationId</code>, <code>runTs</code>; DAX reports use that table to show dataset provenance to users. <br>3. <strong>Deterministic sampling surfaced via model:</strong> ETL persists sample selection decisions and RNG seed used; DAX can surface selection fraction and sample seed for operator reference. <br>4. <strong>DAX reconciliation indicators:</strong> ETL produces reconciliation artifacts and appends <code>audit.reconciliation.completed</code> into REG_Audit; model surfaces reconciliation status by comparing <code>RunMetadata.artifactChecksum</code> to expected checksum recorded in model. <br><strong>Narrative example (DAX interplay with REG_Audit):</strong><br>1. Worker writes reconciled table and persists audit rotation + artifactChecksum. <br>2. Model <code>RunMetadata</code> includes artifactChecksum and correlationId; DAX measure <code>IsReconciled</code> compares current artifactChecksum with expectedChecksum and provides visual validation. <br><strong>Governance guidance:</strong> do not perform irreversible allocation logic in DAX; DAX is a read-only surface for provenance and verification. </td></tr><tr><td data-label="REG_Audit — Per-function Expert Technical Breakdown"> <strong>Appendices: forensic artifacts, evidence paths & recommended retention (expanded)</strong><br><strong>Minimum forensic artifacts to collect for an incident:</strong><br>1. <code>audit_tail.csv</code> and <code>audit_rotation_*.json</code> covering time window for correlationId. <br>2. <code>audit_rotation_signatures.json</code> and release manifest with signing keys/fingerprints. <br>3. <code>jobDescriptor.json</code> persisted via AppendChainedAuditRow with jobId and paramsHash. <br>4. Evidence blobs referenced by evidenceRefs (serialized RNG state, SafeRound inputs, canonical decimals). <br>5. <code>forensic_manifest.json</code> enumerating artifact URIs and checksums. <br>6. Trace logs from CORE_Bootstrap and add-in loader. <br><strong>Evidence storage & retention patterns:</strong><br>1. Hot evidence store: <code>\\evidence\hot\&lt;module&gt;\&lt;correlationId&gt;\</code> for 30 days; access controlled and audited. <br>2. Warm archive: secure archive for 7 years with restricted access and rotation signing. <br>3. Cold archive: per regulatory requirement (e.g., 10+ years) with chain-of-custody metadata. <br><strong>Retention cadence:</strong> monthly verification job emits <code>audit.housekeeping</code> and rotates evidence per retention rules; proofs-of-delete stored in secure archive with signed manifests. </td></tr><tr><td data-label="REG_Audit — Per-function Expert Technical Breakdown"> <strong>Acceptance checklist before REG_Audit release (detailed)</strong><br>1. OWNERS.md lists owners and approvers. <br>2. API stability documented and versioned; backward compatibility tests passing. <br>3. Audit emit hooks in all modules validated in integration tests. <br>4. Cross-language golden vectors for rowHash and rotation signatures passing. <br>5. EvidenceStore encryption, key rotation, and access controls tested. <br>6. VerifyAuditChain acceptance tests and negative tamper detection tests. <br>7. CI static checks enforce forbidden patterns (no blocking IO on UI). <br><strong>Blocking conditions:</strong> missing audit emits on persistence flows, failed golden parity, missing signing key in release manifest, or legal-hold liveness failures. </td></tr><tr><td data-label="REG_Audit — Per-function Expert Technical Breakdown"> <strong>Extremely detailed test plan highlights & scripts (explicit, conceptual)</strong><br><strong>Unit tests:</strong><br>1. RowHash parity and canonical serialization tests across languages. <br>2. Append concurrency tests with advisory locks and O_APPEND modes. <br>3. EvidenceStore encryption/decryption and key rotation tests. <br>4. Rotation signing and signature verification tests (positive & negative). <br><strong>Integration tests:</strong><br>1. End-to-end UI -> EmitAuditEvent -> background flush -> rotation -> sign -> VerifyAuditChain. <br>2. Export/Import bundle roundtrip within CI environment. <br><strong>Property tests:</strong><br>1. Chain immutability under random insert/delete fault injections. <br>2. Deterministic replay property for sampled flows. <br><strong>Performance tests:</strong><br>1. EmitAuditEvent throughput under 10k rps synthetic load. <br>2. Rotation signing throughput with RSA/ECDSA keys. <br><strong>CI gating:</strong> all unit, integration, golden, and property tests must pass; any signature or hash mismatches block merges. </td></tr><tr><td data-label="REG_Audit — Per-function Expert Technical Breakdown"> <strong>Operator runbook quick commands & examples (concise and prescriptive)</strong><br>1. <code>diagnostics collect --correlation r-YYYYMMDD-abc</code> — collects <code>audit_tail.csv</code>, rotations, serialized RNG states, and <code>forensic_manifest.json</code>. <br>2. <code>audit rotate --now --signer &lt;keyRef&gt;</code> — perform on-demand rotation and signing. <br>3. <code>audit verify --range &lt;startRotation&gt;-&lt;endRotation&gt;</code> — run VerifyAuditChain for rotation range and produce verification report. <br>4. <code>audit export --rotations r1,r2 --dest /tmp/export.tar.gz</code> — atomic export of specified rotations. <br>5. <code>audit replay --correlation r-... --dry-run</code> — deterministic replay for forensics. <br><strong>When to call SRE:</strong> after two <code>AUDIT_EXPORT_ENOSPC</code> retries for critical exports, or when VerifyAuditChain reports signature verification failures for regulated rotations. Provide <code>forensic_manifest.json</code> and <code>audit_tail</code> in SRE ticket. </td></tr><tr><td data-label="REG_Audit — Per-function Expert Technical Breakdown"> <strong>Final governance & mandatory constraints (firm):</strong><br>1. No artifact consumed by other processes may be produced without an audit append (EmitAuditEvent/AppendChainedAuditRow). <br>2. Always persist paramsHash and evidenceRef for large/sensitive data; raw PII must never appear in top-level audit rows. <br>3. Use AtomicWrite for all audit-tier artifacts to avoid partial-read exposure. <br>4. Seed deterministic RNGs from correlationId for operator-visible sampling; persist RNG state when exact replayability is required. <br>5. Rotation signatures must use release manifest keys in regulated contexts; key rotation requires two-person approval. <br>6. CI must run VerifyAuditChain on smoke rotations before canary rollout. <br><strong>Checked:</strong> cross-cutting invariants, audit coverage, deterministic chain from UI → jobDescriptor → worker → audit rotations and signature; verified conceptual compliance and internal consistency. </td></tr><tr><td data-label="REG_Audit — Per-function Expert Technical Breakdown"> <strong>Appendix A — Example audit row schema (descriptive)</strong><br><strong>Fields required for REG_Audit rows:</strong> timestamp, correlationId, module, procedure, operatorId (optional), paramsHash, evidenceRef (optional), prevHash, rowHash, rotationId (optional), configHash, ribbonMapHash (where relevant), metadata object (duration_ms, attempts, artifactChecksum, tempPathList). <br><strong>Policy note:</strong> top-level audit rows must not include PII; sanitized full params stored encrypted in EvidenceStore and referenced by evidenceRef. </td></tr><tr><td data-label="REG_Audit — Per-function Expert Technical Breakdown"> <strong>Appendix B — Common failure modes & mitigations (expanded)</strong><br><strong>Failure mode: partial tail write visible to readers</strong><br>1. Likely cause: naive append without fsync or direct in-place writes. <br>2. Mitigation: use AtomicWrite semantics or O_APPEND + fsync patterns; implement tail verification on startup and rotate suspect tails into quarantine. <br><strong>Failure mode: missing signatures on rotations</strong><br>1. Likely cause: signerKeyRef unavailable or HSM offline. <br>2. Mitigation: have emergency offline signing procedure with multi-person approvals; mark rotation as <code>unsigned</code> and quarantine until signature restored. <br><strong>Failure mode: non-deterministic replay outcomes</strong><br>1. Likely cause: RNG seed not persisted or evidence missing. <br>2. Mitigation: require DeterministicRNG seed persisted via EvidenceStore when sampling is operator-visible. </td></tr><tr><td data-label="REG_Audit — Per-function Expert Technical Breakdown"> <strong>Appendix C — Governance checklists & PR requirements (explicit)</strong><br>1. PR must include unit & integration tests for changes. <br>2. Any change to rotation format or hash algorithm requires migration manifest and owner approvals. <br>3. Key management changes require SRE & security sign-off and CI golden re-signing. <br>4. Audit emission must be present for all persistence changes. </td></tr><tr><td data-label="REG_Audit — Per-function Expert Technical Breakdown"> <strong>Appendix D — Incident reconstruction example (ordered forensic steps)</strong><br><strong>Scenario:</strong> operator reports "Mismatch between exported artifact checksum and audit record for run <code>r-20260112-455</code>".<br><strong>Forensic reconstruction steps:</strong><br>1. Retrieve <code>audit.append</code> and <code>audit.export.completed</code> rows for correlationId; extract artifactChecksum and rotationId. <br>2. Pull rotation file containing the <code>allocation.complete</code> row and compute rowHash; compare with recorded rowHash. <br>3. Fetch EvidenceStore blobs referenced by evidenceRef (operator must authorize). <br>4. Run AuditReplayRun with evidence payloads and serialized RNG states; compute artifact checksum and compare with original export artifactChecksum. <br>5. If mismatch persists, inspect <code>audit.atomic_write.verification_failed</code> rows and temp artifact files for corruption evidence. <br>6. Produce <code>forensic_manifest.json</code> containing artifact paths and checksums and escalate to compliance if regulated. </td></tr><tr><td data-label="REG_Audit — Per-function Expert Technical Breakdown"> <strong>Appendix E — PQ & DAX quick checklists for template authors and report builders</strong><br><strong>PQ Template author checklist:</strong><br>1. Include <code>mChecksum</code> in template metadata and record preview seed in preview audit. <br>2. Mark <code>requiresHighPrecision</code> when template depends on canonical rounding; offload final numeric aggregation to worker SafeRound. <br>3. Parameterize seed for preview and persist seed in preview audit. <br>4. Use EvidenceStore for large payloads referenced in audit rows. <br><strong>DAX/report builder checklist:</strong><br>1. Consume <code>RunMetadata</code> table for artifact provenance and <code>artifactChecksum</code>. <br>2. Avoid allocation logic in DAX; use ETL persisted artifacts for allocation and rounding. <br>3. Use hashed stable keys for deterministic sampling filters surfaced by ETL. </td></tr><tr><td data-label="REG_Audit — Per-function Expert Technical Breakdown"> <strong>Final operational constraint (must not be bypassed):</strong><br>All processes that produce artifacts consumed by other processes must: persist job descriptors, seed deterministic RNGs from correlationId when sampling, use AtomicWrite for final artifacts, and emit necessary audit rows. Two-person approval required for rotation key changes and retention/PRUNE operations affecting regulated PII. </td></tr></tbody></table></div><div class="row-count">Rows: 38</div></div><div class="table-caption" id="Table5" data-table="Docu_0178_05" style="margin-top:2mm;margin-left:3mm;"><strong>Table 5</strong></div>
<div class="table-wrapper" data-table-id="table-5"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by REG_Error — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">REG_Error — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="REG_Error — Per-function Expert Technical Breakdown"> <strong>Module-level metadata (contract & overview):</strong><br><strong>Owner:</strong> TEAM_REG_ERROR (documented in OWNERS.md, required approvers listed in release manifest). <br><strong>Public API:</strong> <code>ErrorCodes</code>, <code>ClassifyError</code>, <code>WrapError</code>, <code>AnnotateError</code>, <code>AttachForensicEvidence</code>, <code>SerializeErrorForAudit</code>, <code>DeserializeErrorFromAudit</code>, <code>EmitErrorAudit</code>, <code>SafeErrorToUser</code>, <code>MapToOperatorMessage</code>, <code>IsTransientError</code>, <code>IsRetryableError</code>, <code>RetryPolicyFromError</code>, <code>EscalationPolicyDecision</code>, <code>ErrorChainVerify</code>, <code>RotateErrorSigningKeys</code>, <code>ValidateErrorSchema</code>, <code>ParseThirdPartyError</code>, <code>MapHTTPError</code>, <code>MapErrnoToErrorCode</code>, <code>CompactErrorChain</code>, <code>RehydrateErrorChain</code>, <code>ErrorMigrationTooling</code>, <code>ErrorTelemetryMetrics</code>, <code>ErrorHealthChecks</code>, <code>OperatorGuidanceForError</code>, <code>ErrorSimulator</code>. <br><strong>Audits emitted:</strong> <code>error.classified</code>, <code>error.wrapped</code>, <code>error.annotated</code>, <code>error.forensics.attached</code>, <code>error.audit.emit</code>, <code>error.safe_message.emitted</code>, <code>error.escalation.triggered</code>, <code>error.chain.verified</code>, <code>error.keyrotation</code>, <code>error.migration.applied</code>, <code>error.audit.fallback</code>, <code>error.attach.failed</code>. Each audit row must include <code>correlationId</code>, <code>module=REG_Error</code>, <code>procedure</code>, <code>errorCode</code>, <code>paramsHash</code>, <code>evidenceRef</code> when applicable, and timestamp. <br><strong>Purpose and intended use:</strong> centralize deterministic error taxonomy and handling across add-ins and worker processes; produce reproducible <code>errorId</code> and canonical serialization for forensic replay; supply safe UI messages and role-aware operator runbooks; standardize retry and escalation policies; persist and index sanitized forensic artifacts for compliance. <br><strong>Non-goals / constraints:</strong> avoid network I/O on UI-thread; do not persist raw PII in top-level audit rows; do not perform key management directly (use CORE_KeyManager); avoid automatic infrastructure remediation without explicit escalation policy; keep fast-path classification small and pure to allow embedding inside XLAM/XLL and managed workers. </td></tr><tr><td data-label="REG_Error — Per-function Expert Technical Breakdown"> <strong>Operational guarantees (module-level invariants & SLOs):</strong><br>1. Deterministic outputs: identical inputs produce identical <code>errorCode</code>, <code>errorId</code>, and classification flags. <br>2. Audit anchored: any error that affects durable state or operator decisions must emit at least one audit row referencing <code>correlationId</code> and <code>paramsHash</code>. <br>3. PII policy: top-level audit rows contain only hashes of sensitive parameters; full sanitized evidence stored encrypted and referenced by <code>evidenceRef</code>. <br>4. Crash-safety: evidence persistence uses <code>AtomicWrite</code> guaranteeing final-on-success semantics. <br>5. UI-thread safety: UI path functions (<code>SafeErrorToUser</code>, <code>IsTransientError</code>, minimal classification) must be synchronous non-blocking and never perform IO. <br>6. Observability: emit start/complete audits and local buffered metrics for telemetry; CORE_Telemetry handles audited batched uploads. <br><strong>Performance SLOs:</strong> median <code>ClassifyError</code> latency < 5ms; <code>SafeErrorToUser</code> < 15ms; audit append flush target < 2s under normal load. <br><strong>CI / acceptance gates:</strong> deterministic golden vectors for classification & serialization; cross-language parity for <code>errorId</code> generation; static analyzer enforces no IO on UI-thread. </td></tr><tr><td data-label="REG_Error — Per-function Expert Technical Breakdown"> <strong>Canonical <code>Error</code> object (schema & contract):</strong><br><strong>Required fields:</strong> <code>schemaVersion</code>, <code>errorId</code> (UUIDv4), <code>errorCode</code> (stable enum), <code>severity</code> (INFO/WARN/ERROR/CRITICAL), <code>message</code> (developer-readable), <code>safeMessage</code> (sanitized user-facing string), <code>timestamp</code> (ISO8601 UTC), <code>correlationId</code>, <code>module</code>, <code>procedure</code>, <code>paramsHash</code> (SHA256 hex), <code>evidenceRef</code> (optional pointer to encrypted artifact), <code>prevErrorId</code> (optional), <code>stackTraceFingerprint</code> (optional), <code>retryable</code> (bool), <code>transient</code> (bool), <code>escalation</code> (enum), <code>metadata</code> (sanitized map). <br><strong>Optional fields:</strong> <code>artifactChecksum</code>, <code>jobIdHash</code>, <code>operatorHintKey</code>, <code>forensicsChecklistId</code>, <code>prevChainHash</code> for compacted chains. <br><strong>Serialization guarantees:</strong> canonical JSON serialization with deterministic key ordering for cross-platform parity; all PII replaced or hashed before top-level inclusion; full sanitized payload persisted in evidence store referenced by <code>evidenceRef</code>. <br><strong>Schema & migrations:</strong> <code>schemaVersion</code> required; <code>DeserializeErrorFromAudit</code> must implement sequential migrations from older versions and emit <code>error.migration.applied</code> when migration steps run. </td></tr><tr><td data-label="REG_Error — Per-function Expert Technical Breakdown"> <strong><code>ErrorCodes</code> taxonomy & governance:</strong><br><strong>Contract:</strong> single source of truth enumerating canonical error identifiers used across modules and audit rows. Codes are immutable within a release; adding/removing requires OWNER approval and a migration manifest. <br><strong>Representative codes and default mappings (illustrative):</strong><br>1. <code>ERR_JOB_PERSISTENCE_FAILED</code> — job descriptor <code>AtomicWrite</code> failed; <code>severity=ERROR</code>; <code>retryable=true</code> if idempotent; <code>escalation=notify</code>. <br>2. <code>ERR_ATOMIC_WRITE_ENOSPC</code> — ENOSPC on <code>AtomicWrite</code>; <code>severity=ERROR</code>; <code>retryable=false</code> by default; operator runbook: staging fallback or infra escalation. <br>3. <code>ERR_PQ_INJECT_MISMATCH</code> — PQ injection checksum mismatch; <code>severity=CRITICAL</code> for regulated templates; <code>escalation=freezeExports</code> with <code>requiredApprovals=2</code>. <br>4. <code>ERR_RULES_ENGINE_FAILURE</code> — rules engine unhandled exception; <code>severity=ERROR</code>. <br>5. <code>ERR_VALIDATION_SCHEMA_MISMATCH</code> — config JSON Schema v7 mismatch; <code>severity=ERROR</code>. <br>6. <code>ERR_EXTERNAL_CONNECTOR_TIMEOUT</code> — external connector timeout; <code>severity=ERROR</code>; <code>retryable=true</code> with backoff. <br>7. <code>ERR_PERMISSION_DENIED</code> — ACL/EPERM; <code>severity=ERROR</code>; <code>retryable=false</code>. <br>8. <code>ERR_RETRY_EXHAUSTED</code> — retry wrapper exhausted; <code>severity=ERROR</code>; <code>escalation=notify</code>. <br>9. <code>ERR_FORGERY_DETECTED</code> — audit chain or artifact signature mismatch; <code>severity=CRITICAL</code>; <code>escalation=incident</code>. <br>10. <code>ERR_INTERNAL_UNKNOWN</code> — unmapped raw error; <code>severity=ERROR</code>. <br><strong>Governance metadata per code:</strong> owner, description, default severity, default retry policy, escalation level, operator message template id, forensics checklist id, regulatoryImpact flag. </td></tr><tr><td data-label="REG_Error — Per-function Expert Technical Breakdown"> <strong><code>ClassifyError(rawError, context)</code> — exhaustive technical breakdown</strong><br><strong>Purpose & contract:</strong> deterministic mapper from raw runtime or provider errors + context to canonical <code>Error</code>. Pure function; no IO; does not throw. <br><strong>Parameters:</strong> <code>rawError</code> (exception object, HTTP response, provider SDK error), <code>context</code> (<code>module</code>, <code>procedure</code>, <code>correlationId</code>, <code>paramsHash</code>, optional <code>idempotencyKey</code>). <br><strong>Return:</strong> canonical <code>Error</code> object with <code>errorCode</code>, <code>severity</code>, <code>transient</code>, <code>retryable</code>, <code>message</code>, <code>stackTraceFingerprint</code>. <br><strong>Primary invariants (must/shall):</strong><br>1. Determinism: same normalized <code>rawFingerprint</code> + <code>context</code> => identical classification and <code>errorId</code>. <br>2. Side-effect free: classification should not write disk or network. <br>3. Minimal extraction: prefer structured numeric codes over free-form messages; compute sanitized <code>messageHash</code> when message included. <br><strong>Algorithm & implementation notes:</strong><br>1. <strong>Canonicalize</strong> raw input to intermediate: <code>{type, errno, httpStatus, providerCode, messageSanitized, messageHash, stackFingerprint, provider, bodyFingerprint}</code>. Normalization replaces variable tokens (timestamps, UUIDs, file paths) with placeholders before hashing. <br>2. <strong>Lookup cascade:</strong> provider numeric code mapping → structured JSON <code>code</code>/<code>subcode</code> mapping → regex patterns on sanitized messages with anchored patterns → <code>MapErrnoToErrorCode</code> / <code>MapHTTPError</code> fallback → <code>ERR_INTERNAL_UNKNOWN</code>. <br>3. <strong>Compute <code>errorId</code>:</strong> HMAC_SHA256(<code>correlationId | canonicalFingerprint | classifierVersion</code>) truncated to deterministic UUID format for readability and dedup. <br>4. <strong>Derive flags:</strong> <code>transient</code> and <code>retryable</code> inferred using mapping metadata, <code>context.idempotencyKey</code>, and known provider semantics (e.g., 5xx transient, 4xx not transient unless documented). <br><strong>Edge cases & invalid inputs:</strong><br>1. Null <code>rawError</code>: return <code>ERR_INTERNAL_UNKNOWN</code> with metadata flag <code>null_raw=true</code>. <br>2. Oversized stack traces: compute <code>stackTraceFingerprint</code> and persist the full stack only via <code>AttachForensicEvidence</code> on escalation. <br>3. Opaque HTML body: compute <code>bodyHash</code> and persist body in evidence store if needed. <br><strong>Observability & audit fields:</strong> callers should emit <code>error.classified(correlationId, errorId, errorCode, classifierVersion, inputFingerprint)</code> after classification. <br><strong>Examples:</strong><br>1. POSIX ENOSPC classified to <code>ERR_ATOMIC_WRITE_ENOSPC</code>, <code>transient=true</code>, <code>retryable=false</code>. <br>2. HTTP 504 with <code>Retry-After</code> → <code>ERR_EXTERNAL_CONNECTOR_TIMEOUT</code>, <code>retryable=true</code> for idempotent operations. <br><strong>Tests & CI vectors:</strong> mapping table golden vectors, regex fuzz tests, cross-language parity for <code>errorId</code> generation. </td></tr><tr><td data-label="REG_Error — Per-function Expert Technical Breakdown"> <strong><code>WrapError(baseErrorOrRaw, context, wrapMetadata=null)</code> — wrapper & chain builder</strong><br><strong>Purpose & contract:</strong> create a wrapper <code>Error</code> that links to an existing <code>Error</code> (or a newly classified raw error) adding new <code>module</code>/<code>procedure</code> context while preserving immutability of base error. Synchronous; no IO. <br><strong>Parameters & return:</strong> <code>baseErrorOrRaw</code> (canonical <code>Error</code> or raw error), <code>context</code> (<code>module</code>, <code>procedure</code>, <code>correlationId</code>), optional <code>wrapMetadata</code>. Returns new <code>Error</code> object where <code>prevErrorId</code> points to the base <code>errorId</code>. <br><strong>Primary invariants:</strong><br>1. Bounded chain length: if <code>prevErrorChainLength &gt;= maxErrorChain</code> (config default 25), compact older portion into <code>prevChainHash</code> and persist the full chain via <code>AttachForensicEvidence</code> and set <code>prevErrorId=null</code> while adding <code>prevChainHash</code> and <code>evidenceRef</code>. <br>2. Deterministic metadata merge: lexicographic key ordering; in conflicts preserve base-level keys unless explicit override provided in <code>wrapMetadata</code>. <br>3. <code>safeMessage</code> preservation: do not overwrite downstream <code>safeMessage</code> unless wrapper severity is higher and requires an explicit replacement policy. <br><strong>Algorithm:</strong> if base is raw → call <code>ClassifyError</code> to create canonical base; else copy base fields; merge metadata; compute wrapper <code>errorId = HMAC_SHA256(prevErrorId | timestamp | context)</code> and return wrapper <code>Error</code>. <br><strong>Audit:</strong> emit <code>error.wrapped(correlationId, newErrorId, prevErrorId, wrapperModule)</code> after wrapping. <br><strong>Example narrative:</strong> worker catches storage write exception → <code>ClassifyError</code> → <code>WrapError(classifiedError, {module:&quot;REG_Export&quot;, procedure:&quot;AtomicWrite&quot;})</code> → <code>AnnotateError</code> → <code>AttachForensicEvidence</code> (if needed) → <code>EmitErrorAudit</code>. </td></tr><tr><td data-label="REG_Error — Per-function Expert Technical Breakdown"> <strong><code>AnnotateError(error, enrichment)</code> — lightweight enrichment</strong><br><strong>Purpose & contract:</strong> attach sanitized contextual metadata (jobDescriptorHash, tempPaths, artifactChecksum) to <code>Error</code>. Small sanitizable metadata permitted synchronously; large or PII-containing payloads require <code>AttachForensicEvidence</code> and must run in worker. <br><strong>Rules:</strong><br>1. No raw PII in <code>metadata</code>; compute <code>piiHash</code> for replaced PII and persist full sanitized payload with <code>AttachForensicEvidence</code>. <br>2. Idempotent and deterministic merging. <br><strong>Return:</strong> enriched <code>Error</code> object with merged <code>metadata</code> and, if <code>AttachForensicEvidence</code> used, a populated <code>evidenceRef</code>. <br><strong>Audit:</strong> caller may emit <code>error.annotated(correlationId, errorId, paramsHash)</code> to record enrichment events. </td></tr><tr><td data-label="REG_Error — Per-function Expert Technical Breakdown"> <strong><code>AttachForensicEvidence(error, evidenceBlob, evidenceMeta)</code> — evidence ingestion</strong><br><strong>Purpose & contract:</strong> sanitize, encrypt, and persist large forensic artifacts (logs, stacks, RNG serialized state, SafeRound inputs) and return <code>evidenceRef</code> to attach to <code>Error</code>. IO-bound; must run in worker or background process; never on UI-thread. <br><strong>Must/shall steps:</strong><br>1. <strong>Sanitize</strong>: deterministic PII redaction; record <code>piiHash</code> entries for removed elements. <br>2. <strong>Encrypt</strong> using <code>CORE_KeyManager</code> or approved project encryption. <br>3. <strong>Persist atomically</strong> via <code>AtomicWrite(evidencePath, encryptedBlob)</code>; compute SHA256 checksum and size. <br>4. <strong>Index</strong> evidence metadata: uploader, retentionPolicy, ACLs, checksum, size, provenance tags. <br>5. <strong>Return</strong> <code>evidenceRef = {path, checksum, size, retentionPolicy}</code> for attachment to <code>Error</code>. <br><strong>Edge cases & runbook:</strong><br>1. Encryption failure: store sealed local staging, emit <code>error.forensics.failed</code>, escalate to infra. <br>2. ENOSPC on evidence store: apply configured staging fallback (same-volume staging preferred) and emit <code>error.forensics.degraded</code>. <br><strong>Audit:</strong> <code>error.forensics.attached(correlationId, errorId, evidenceRef)</code> on success; <code>error.forensics.failed</code> on failure. </td></tr><tr><td data-label="REG_Error — Per-function Expert Technical Breakdown"> <strong><code>SerializeErrorForAudit(error)</code> & <code>DeserializeErrorFromAudit(blob)</code> — canonical serialization</strong><br><strong>Purpose & contract:</strong> canonical deterministic JSON used for audit rows and archival; support robust migrations across <code>schemaVersion</code>s. <br><strong>Canonical ordering:</strong> <code>schemaVersion</code>, <code>errorId</code>, <code>errorCode</code>, <code>severity</code>, <code>timestamp</code>, <code>correlationId</code>, <code>module</code>, <code>procedure</code>, <code>message</code>, <code>safeMessage</code>, <code>paramsHash</code>, <code>evidenceRef</code>, <code>prevErrorId</code>, <code>metadata</code>. <br><strong>Migrations:</strong> <code>Deserialize</code> applies ordered migrations from <code>error_migrations</code> sequentially; each applied migration emits <code>error.migration.applied(correlationId, errorId, fromVersion, toVersion)</code>. Migrations must be idempotent; tests in CI must exercise migration sequences with golden vectors. <br><strong>Validation:</strong> call <code>ValidateErrorSchema</code> during <code>Deserialize</code> and on ingest endpoints; failures produce <code>ERR_VALIDATION_SCHEMA_MISMATCH</code>. </td></tr><tr><td data-label="REG_Error — Per-function Expert Technical Breakdown"> <strong><code>EmitErrorAudit(error, auditClient=CORE_Audit)</code> — append-only emission</strong><br><strong>Purpose & contract:</strong> reliably append an audit row representing the <code>Error</code> into append-only audit buffer. Must be idempotent (dedupe by <code>errorId</code>) and robust to transient failures. <br><strong>Behavior & rules:</strong><br>1. Compute <code>paramsHash</code> over sanitized params and <code>auditRowChecksum</code>. <br>2. Append using <code>auditClient.append</code> with dedup key <code>errorId</code>. <br>3. On transient failures use <code>Retry</code> (idempotent_assert=true). <br>4. If append ultimately fails, persist fallback to <code>audit_tail_failed/&lt;correlationId&gt;-&lt;errorId&gt;.json</code> and emit <code>error.audit.fallback</code>. <br>5. When chain signing enabled, sign the appended row with <code>CORE_Signing</code> in worker context; signing operations are auditable. <br><strong>Metrics & observability:</strong> emit <code>error.audit.emit_latency_ms</code> and <code>error.audit.append_count</code> metrics. </td></tr><tr><td data-label="REG_Error — Per-function Expert Technical Breakdown"> <strong><code>SafeErrorToUser(error, locale=&quot;en-US&quot;, verbosity=&quot;concise&quot;)</code> — UI-safe message renderer</strong><br><strong>Purpose & contract:</strong> produce a sanitized user-facing message and a structured operator hint for display in ribbons and dialogs. Synchronous, no IO. <br><strong>Template rules:</strong><br>1. Templates packaged with release artifact at <code>REG_Error/templates/&lt;locale&gt;/&lt;errorCode&gt;.tmpl</code>. <br>2. Templates must only accept hashed or non-PII placeholders (<code>{artifactChecksum}</code>, <code>{jobIdHash}</code>, <code>{supportLink}</code>, <code>{correlationId}</code>). <br>3. For <code>CRITICAL</code> errors template must instruct to contact SRE and include <code>correlationId</code>. <br><strong>Return:</strong> <code>{userMessage, operatorHint, templateId}</code>. <code>operatorHint</code> may include <code>paramsHash</code> and <code>evidenceRef</code> pointers. <br><strong>Audit:</strong> <code>error.safe_message.emitted(correlationId, errorId, templateId)</code>. <br><strong>Examples:</strong><br>1. <code>ERR_ATOMIC_WRITE_ENOSPC</code> → <code>userMessage: &quot;Export failed: destination disk is full. Contact infrastructure with correlation r-2026-...&quot;.</code> <code>operatorHint</code> includes <code>mountPathHash</code>, <code>freeBytesSnapshotRef</code>. <br>2. <code>ERR_PQ_INJECT_MISMATCH</code> (regulated) → <code>userMessage: &quot;Template verification failed. Do not proceed with regulated exports; contact Compliance.&quot;</code> </td></tr><tr><td data-label="REG_Error — Per-function Expert Technical Breakdown"> <strong><code>MapToOperatorMessage(error, operatorRole)</code> — role-aware remediation checklists</strong><br><strong>Purpose & contract:</strong> return a role-specific remediation checklist for roles such as <code>operator</code>, <code>SRE</code>, <code>compliance</code>, or <code>release-owner</code>. Deterministic; no IO. <br><strong>Return:</strong> <code>{steps:[], approvalRequired:int, escalationLevel, runbookId}</code>. <br><strong>Runbook template rules:</strong> stored in release artifact; placeholders limited to non-PII tokens. <br><strong>Governance:</strong> two-person approval enforced for regulated outputs; approvals recorded as <code>approval.requested</code> / <code>approval.granted</code> audit rows. <br><strong>Example mapping (ERR_FORGERY_DETECTED):</strong><br>1. Freeze exports and set audit store read-only. <br>2. Collect <code>audit_tail</code> and evidence via <code>diagnostics collect --correlation</code>. <br>3. Run <code>ErrorChainVerify</code> and prepare regulatory package if forgery confirmed. <br>4. Notify legal/compliance and SRE; rotate signing keys if required. <br><strong>Audit:</strong> <code>error.escalation.triggered(correlationId, errorId, escalationLevel)</code>. </td></tr><tr><td data-label="REG_Error — Per-function Expert Technical Breakdown"> <strong><code>IsTransientError(error)</code> & <code>IsRetryableError(error, context)</code> — decision predicates</strong><br><strong>Purpose & contract:</strong> boolean predicates for orchestrators to choose retry/backoff or immediate failover. Synchronous and deterministic. <br><strong>Rules:</strong><br>1. <code>IsTransientError</code> true for ephemeral conditions like network timeouts, provider 5xx or rate-limits. <br>2. <code>IsRetryableError</code> true only if operation is idempotent by design or protected by an idempotency token (<code>context.idempotencyKey</code>). <br>3. Predicates conservative to avoid unsafe retries for non-idempotent operations. <br><strong>Examples:</strong><br>1. HTTP 429 with <code>Retry-After</code> → <code>IsTransientError=true</code>, <code>IsRetryableError=true</code> if client operation idempotent. <br>2. <code>ERR_PERMISSION_DENIED</code> → <code>IsTransientError=false</code>, <code>IsRetryableError=false</code>. </td></tr><tr><td data-label="REG_Error — Per-function Expert Technical Breakdown"> <strong><code>RetryPolicyFromError(error, defaultContext)</code> — derive retry policy</strong><br><strong>Purpose & contract:</strong> given an <code>Error</code> and service context, produce <code>{retries, baseMs, factor, maxBackoffMs, jitter, deterministicJitter, idempotent_assert}</code> deterministic for <code>errorCode</code> and <code>module</code>. <br><strong>Heuristics & rules:</strong><br>1. For regulated-critical flows: prefer fail-fast with staged-local fallback instead of extended retries. <br>2. External connector timeout default: <code>{retries:3, baseMs:200, factor:2, jitter:true}</code>. <br>3. Job persistence default: <code>{retries:5, baseMs:300, factor:2, idempotent_assert:true}</code> (requires idempotency token). <br>4. For CI/golden runs set <code>deterministicJitter=true</code> using <code>DeterministicRNG(correlationId)</code> so retry timing reproducible in tests. <br><strong>Audit:</strong> <code>error.retry.policy.applied(correlationId, errorId, policyHash)</code>. </td></tr><tr><td data-label="REG_Error — Per-function Expert Technical Breakdown"> <strong><code>EscalationPolicyDecision(error, serviceContext, failureMetrics)</code> — automated gating</strong><br><strong>Purpose & contract:</strong> deterministically choose escalation action (<code>none</code>, <code>notify</code>, <code>incident</code>, <code>freezeExports</code>, <code>rollback</code>) based on <code>errorCode</code>, <code>severity</code>, <code>serviceContext</code> (release stage, canary flag), and operational metrics (error rate, spike detection). <br><strong>Decision invariants:</strong><br>1. Two-person approval required for regulated outputs and releases. <br>2. Canary auto-rollback when error rate exceeds <code>canaryThreshold</code> for a configured window. <br>3. Decisions computed deterministically to produce reproducible rationale fingerprint (<code>rationaleHash</code>). <br><strong>Return & audit:</strong> <code>{action, requiredApprovals, rationaleHash}</code> and emit <code>error.escalation.decision(correlationId, errorId, action, rationaleHash)</code>. </td></tr><tr><td data-label="REG_Error — Per-function Expert Technical Breakdown"> <strong><code>ErrorChainVerify(auditSegment, releaseManifest, keys)</code> — audit chain verification</strong><br><strong>Purpose & contract:</strong> verify append-only audit chain integrity and signatures for a given <code>correlationId</code> or audit segment; intended for CI and forensic analysis. CPU & IO bound. <br><strong>Verification steps:</strong><br>1. Verify hash chaining across rows and monotonic timestamps per correlation. <br>2. Verify row signatures against public keys in <code>releaseManifest</code>. <br>3. Confirm artifact checksums referenced in audit rows equal computed checksums for artifacts when accessible. <br>4. Produce signed verification report persisted via <code>AtomicWrite</code> and return <code>{verified:bool, mismatches:[], signatureProofs:[]}</code>. <br><strong>Failure runbook:</strong> if mismatches found, lock audit store read-only, assemble <code>forensic_manifest</code>, and escalate as <code>ERR_FORGERY_DETECTED</code>. <br><strong>Audit:</strong> <code>error.chain.verified(correlationId, verified, mismatchCount)</code>. </td></tr><tr><td data-label="REG_Error — Per-function Expert Technical Breakdown"> <strong><code>RotateErrorSigningKeys(keyDescriptor)</code> — key rotation orchestration</strong><br><strong>Purpose & contract:</strong> rotate signing keys used for audit chain signing with canary verification and full promotion. Requires OWNER & SRE sign-offs and signed rotation manifest. <br><strong>Steps (must/shall):</strong><br>1. Generate new key via <code>CORE_KeyManager</code>. <br>2. Produce and sign <code>rotationManifest.json</code> using old key; persist via <code>AtomicWrite</code>. <br>3. Canary: sign sample audit segment with new key and run <code>ErrorChainVerify</code>. <br>4. Promote new key across systems on successful verification and archive old keys per retention. <br><strong>Rollback:</strong> if verification fails revert to old key and open incident; emit <code>error.keyrotation.failed</code>. <br><strong>Audit:</strong> <code>error.keyrotation(correlationId, previousKeyId, newKeyId, manifestChecksum)</code>. </td></tr><tr><td data-label="REG_Error — Per-function Expert Technical Breakdown"> <strong><code>ValidateErrorSchema(errorJson)</code> — schema validation</strong><br><strong>Purpose & contract:</strong> validate serialized error artifact against current <code>schemaVersion</code> and produce <code>(valid, diagnostics[])</code>. Synchronously used in ingestion and CI. <br><strong>Checks:</strong> presence and types for required fields, <code>paramsHash</code> correctness vs. supplied params, <code>evidenceRef</code> format validity, <code>timestamp</code> within allowable skew, and <code>prevErrorId</code> referential integrity when present. <br><strong>Failure mapping:</strong> validation errors mapped to <code>ERR_VALIDATION_SCHEMA_MISMATCH</code> with diagnostics for operator runbooks. </td></tr><tr><td data-label="REG_Error — Per-function Expert Technical Breakdown"> <strong><code>ParseThirdPartyError(payload, providerHint)</code> & <code>MapHTTPError(httpResponse)</code> — normalization</strong><br><strong>Purpose & contract:</strong> convert diverse vendor/provider error responses into canonical intermediate for <code>ClassifyError</code>. Mapping rules are provider-specific, versioned, and stored in release artifacts. <br><strong>Preferred parse order:</strong> numeric status codes → structured JSON code/subcode fields → provider-specific headers (e.g., <code>Retry-After</code>, <code>X-RateLimit-*</code>) → anchored regex on sanitized bodies → fallback to generic mapping (<code>ERR_INTERNAL_UNKNOWN</code>). <br><strong>HTTP specifics:</strong> parse <code>Retry-After</code>, <code>X-RateLimit-Limit/Remaining/Reset</code>, <code>X-Request-ID</code>; compute <code>bodyHash</code> for HTML/text fallback; persist full body in evidence only on escalation or on <code>AttachForensicEvidence</code> invocation. </td></tr><tr><td data-label="REG_Error — Per-function Expert Technical Breakdown"> <strong><code>MapErrnoToErrorCode(errno, syscallContext)</code> — OS errno normalization</strong><br><strong>Purpose & contract:</strong> map OS-level errno (POSIX) and Windows equivalents to canonical <code>ErrorCodes</code>. Table maintained with owner and cross-OS parity tests. <br><strong>Representative mappings:</strong> <code>ENOSPC</code> → <code>ERR_ATOMIC_WRITE_ENOSPC</code>; <code>EACCES</code> → <code>ERR_PERMISSION_DENIED</code>; <code>ENOMEM</code> → <code>ERR_OUT_OF_MEMORY</code>. <br><strong>CI tests:</strong> ensure parity via virtualization and Windows-shims in CI. </td></tr><tr><td data-label="REG_Error — Per-function Expert Technical Breakdown"> <strong><code>CompactErrorChain(errorChain)</code> & <code>RehydrateErrorChain(compactedRef)</code> — chain size management</strong><br><strong>Purpose:</strong> when error chains exceed configured limits, compact older portion into <code>prevChainHash</code> and persist full chain in evidence store; rehydrate for forensic replay. <br><strong>Rules:</strong><br>1. Compaction triggers when <code>chainLength &gt; maxErrorChain</code>. <br>2. Prior chain serialized canonical → <code>prevChainHash = SHA256(serializedPriorChain)</code>; prior chain persisted via <code>AttachForensicEvidence</code>. <br>3. Compacted <code>Error</code> stores <code>prevChainHash</code> and <code>evidenceRef</code>. <br><strong>Rehydration:</strong> uses <code>evidenceRef</code> to fetch and rehydrate full chain; rehydration step audited. </td></tr><tr><td data-label="REG_Error — Per-function Expert Technical Breakdown"> <strong>Cross-cutting Observability, Telemetry & Error catalog (concepts & mapping)</strong><br><strong>Audit schema:</strong> every <code>REG_Error</code> audit row must include <code>timestamp</code>, <code>correlationId</code>, <code>module=REG_Error</code>, <code>procedure</code>, <code>operatorId</code> (optional), <code>errorId</code>, <code>errorCode</code>, <code>severity</code>, <code>paramsHash</code>, <code>evidenceRef</code> (optional), <code>prevErrorId</code> (optional), <code>configHash</code>, <code>metadata</code> like <code>duration_ms</code>, <code>attempts</code>, <code>artifactChecksum</code>, <code>tempPathList</code>. Top-level audit rows must never contain raw PII. <br><strong>Key audit events:</strong> <code>error.classified</code>, <code>error.wrapped</code>, <code>error.forensics.attached</code>, <code>error.audit.emit</code>, <code>error.safe_message.emitted</code>, <code>error.escalation.triggered</code>, <code>error.chain.verified</code>. <br><strong>Metrics (buffered):</strong> <code>error.count</code>, <code>error.rate</code>, <code>error.retry.attempts</code>, <code>error.escalations</code>, <code>error.audit.emit_latency_ms</code>. CORE_Telemetry uploads metrics in audited batches; utilities avoid remote exports on UI-thread. <br><strong>Evidence policy:</strong> sanitized params stored encrypted in evidence store; top-level audit stores parameter hashes only; access controls enforced by evidence index and audited account actions. </td></tr><tr><td data-label="REG_Error — Per-function Expert Technical Breakdown"> <strong>Testing matrix, property tests, and cross-language golden governance</strong><br><strong>Unit tests required:</strong><br>1. Classification goldens: raw errno, HTTP responses, provider SDK errors → expected <code>ErrorCodes</code> with deterministic fingerprints. <br>2. <code>SerializeErrorForAudit</code> canonicalization and <code>DeserializeErrorFromAudit</code> migrations across versions. <br>3. <code>SafeErrorToUser</code> template rendering across locales and severity levels. <br>4. <code>AttachForensicEvidence</code> with FS and crypto mocks; simulate ENOSPC and EPERM. <br><strong>Integration tests:</strong><br>1. End-to-end error lifecycle: exception → classify → annotate → attach evidence → emit audit → chain verify. <br>2. Retry + escalation: inject transient connector failures → validate retry policy application and escalation triggers. <br>3. Key rotation canary verification runs. <br><strong>Property tests:</strong> deterministic <code>errorId</code> parity across languages; chain compaction/re-hydration integrity; PII redaction invariants under fuzzing. <br><strong>Cross-language golden gating:</strong> classification outputs and canonical serialized blobs for representative inputs must match across VBA/JS/Python/C# implementations and are enforced in CI. <br><strong>Performance tests:</strong> classification throughput microbenchmarks, audit append flush under load, evidence attach concurrency behavior. </td></tr><tr><td data-label="REG_Error — Per-function Expert Technical Breakdown"> <strong>Developer guidance, allowed & forbidden patterns (explicit)</strong><br><strong>Required patterns:</strong><br>1. Use <code>ClassifyError</code> immediately for caught exceptions within worker flow and call <code>EmitErrorAudit</code> to record the error. <br>2. UI handlers must use <code>SafeErrorToUser</code> and <code>EmitErrorAudit</code> with minimal sanitized metadata. <br>3. Use <code>RetryPolicyFromError</code> for consistent retry behavior and ensure idempotency tokens for retryable operations. <br>4. Persist large forensic artifacts via <code>AttachForensicEvidence</code> and reference <code>evidenceRef</code>. <br><strong>Forbidden patterns:</strong><br>1. Do not write raw PII to top-level audits or UI. Static analyzer must reject such changes. <br>2. Do not call <code>AttachForensicEvidence</code> on UI thread. <br>3. Do not mutate canonical <code>Error</code> objects after emission; wrap instead. <br>4. Do not bypass <code>EmitErrorAudit</code> for regulated flows. <br><strong>Code-review checklist:</strong> verify classification usage, audit emission, <code>SafeErrorToUser</code> usage on UI, idempotency for retryable flows, and evidence persistence for large artifacts. </td></tr><tr><td data-label="REG_Error — Per-function Expert Technical Breakdown"> <strong>Operational runbook & incident playbooks (detailed)</strong><br><strong>High-severity incident: <code>ERR_FORGERY_DETECTED</code> playbook:</strong><br>1. Immediately set audit store read-only and emit <code>error.containment</code> audit row. <br>2. Run <code>diagnostics collect --correlation &lt;id&gt;</code> to gather <code>audit_tail.csv</code>, serialized <code>Error</code> JSONs, evidence artifacts, and <code>forensic_manifest.json</code>. <br>3. Run <code>ErrorChainVerify</code> with release manifest keys to confirm chain mismatch. <br>4. If forgery confirmed: freeze exports, notify compliance/legal, prepare regulatory package including release manifest and audit rotations, and rotate signing keys via <code>RotateErrorSigningKeys</code> with canary flow. <br>5. Produce post-mortem and remediation artifacts: <code>forensic_manifest</code>, <code>audit_tail</code> snapshots, and golden fixtures required for regulator packages. <br><strong>AtomicWrite ENOSPC runbook:</strong><br>1. Inspect <code>error.forensics.attached</code> for mount path, <code>freeBytesSnapshotRef</code>, and tempPaths. <br>2. SSH to host and collect <code>df -h</code>, <code>vmstat</code>, <code>iostat</code> and <code>lsof</code> to identify lock holders. <br>3. Move non-critical artifacts to local staging (prefer same-volume staging to preserve atomic rename semantics) or expand volume. <br>4. Re-run export with <code>--stage-local</code> and validate checksums; if persistently failing escalate to infra with <code>forensic_manifest</code>. <br><strong>Retry exhaustion triage:</strong><br>1. Query <code>util.retry.attempt</code> metrics and <code>error.retry.policy.applied</code> audit rows to identify failing target and rate of retries. <br>2. Throttle concurrency and enable circuit-breaker for the failing service. <br>3. Investigate idempotency token presence; pause production calls if missing and implement idempotency persistence. <br>4. If infrastructure-related escalate to SRE with <code>forensic_manifest</code> artifacts. </td></tr><tr><td data-label="REG_Error — Per-function Expert Technical Breakdown"> <strong>Extremely detailed long-form narratives, examples, and forensic walkthroughs</strong><br><strong>Narrative 1 — Regulated end-of-period journal allocation (complete forensic trace):</strong><br>1. Operator triggers <code>AllocateJournalTotals</code> via <code>REG_Ribbon</code>; the ribbon handler performs input validation and emits <code>UserAction</code> audit containing <code>correlationId=r-20260117-xyz</code> and <code>paramsHash</code>. <br>2. The add-in constructs the canonical <code>jobDescriptor</code> with <code>jobId</code>, <code>paramsHash</code>, <code>configHash</code>, and <code>correlationId</code> and persists it using <code>AtomicWrite(jobDescriptorPath, jobJson)</code>. <br>3. <code>AtomicWrite</code> fails with OS <code>ENOSPC</code> (errno 28). The worker catches I/O exception and calls <code>ClassifyError(rawErr, context)</code>. <code>ClassifyError</code> normalizes errno → returns <code>Error</code> with <code>errorCode=ERR_ATOMIC_WRITE_ENOSPC</code>, <code>transient=true</code>, <code>retryable=false</code> (no staging configured). <br>4. Worker calls <code>AnnotateError</code> to attach <code>tempPathList</code>, <code>fsStats</code>, and <code>jobDescriptorHash</code> in metadata. Because metadata contains substantial content, worker calls <code>AttachForensicEvidence(error, sanitizedPayload, meta)</code> to persist encrypted evidence at <code>evidence/hot/r-20260117-xyz/evidence-&lt;errorId&gt;.enc</code>. <code>AttachForensicEvidence</code> returns <code>evidenceRef</code> with checksum and size. <br>5. Worker calls <code>WrapError(classifiedError, {module:&quot;REG_Export&quot;, procedure:&quot;AtomicWrite&quot;})</code> to create a contextual wrapper <code>Error</code> and then <code>EmitErrorAudit</code> to append the audit row. Audit append uses dedupe key <code>errorId</code> and signs the row if chain-signing enabled. <code>error.forensics.attached</code> and <code>error.audit.emit</code> audits recorded. <br>6. <code>EscalationPolicyDecision</code> evaluates recent job persistence failure rate and triggers <code>notify</code> to infra; <code>error.escalation.triggered</code> emitted. <br>7. Operator runs <code>diagnostics collect --correlation r-20260117-xyz</code> which packages <code>audit_tail.csv</code>, evidence artifacts, and <code>forensic_manifest.json</code> and sends them to infra for remediation. <br>8. Infra increases volume or clears space; operator re-runs job; <code>AtomicWrite</code> now succeeds; <code>util.atomic_write.completed</code> and <code>job.persisted</code> audit rows recorded. <br><strong>Key forensic artifacts produced:</strong> <code>jobDescriptor.json</code> persisted artifact, <code>evidence/evidence-&lt;errorId&gt;.enc</code>, <code>atomic_write.tempPaths</code> listing, <code>audit_tail.csv</code>, signed audit rows with <code>errorId</code>. These are compiled into the <code>forensic_manifest</code> for regulator submission if needed. <br><strong>Narrative 2 — PQ Template injection with numeric fidelity requirement (compliance-critical):</strong><br>1. Operator previews <code>ifrs15-highprecision.m</code>; PQ_Ribbon computes <code>seed = SeedFromCorrelation(correlationId, templateId)</code> and stores seed and <code>mChecksum</code> in <code>pq_preview</code> audit. <br>2. Operator injects template; <code>PQ_Injector</code> worker persists canonical M artifact via <code>AtomicWrite</code> to <code>artifacts/pq/m/&lt;artifactId&gt;.m</code> and returns <code>artifactChecksum</code>. <br>3. After injection, the workbook's query hash differs from <code>artifactChecksum</code> due to host-specific normalization. Host detects mismatch and calls <code>ClassifyError</code> mapping to <code>ERR_PQ_INJECT_MISMATCH</code> with <code>severity=CRITICAL</code> and <code>regulatoryImpact=IFRS</code>. <br>4. <code>AnnotateError</code> attaches <code>mChecksum</code>, <code>artifactChecksum</code>, <code>workbookQueryHash</code>, and <code>operatorIdHash</code>. Worker calls <code>AttachForensicEvidence</code> to persist diffs and sanitized M sources; evidence persisted and <code>evidenceRef</code> returned. <br>5. <code>EscalationPolicyDecision</code> returns <code>freezeExports</code> with <code>requiredApprovals=2</code>; system blocks regulated exports and notifies Compliance and Release Owners. <br>6. Two-person approval workflow required: approvals recorded as <code>approval.requested</code> and <code>approval.granted</code> audit rows; following approvals the injector re-injects signed artifact or reverts the workbook to signed query. All steps auditable. <br><strong>Narrative 3 — MatchMerge tie-break determinism and dispute replay:</strong><br>1. <code>MatchMerge</code> pipeline computes match scores for candidate pairs. Ties exist requiring deterministic tie-breaking. The pipeline seeds <code>DeterministicRNG(seedSource=SeedFromCorrelation(correlationId, &quot;matchmerge-v1&quot;))</code> and uses <code>rng.shuffle(candidates)</code> combined with <code>tieBreakerKeys</code> to reorder deterministically. <br>2. Merge proposals persisted using <code>AtomicWrite</code> as <code>proposal-&lt;id&gt;.json</code> and <code>artifactChecksum</code> stored. <code>util.rng.seeded</code> and <code>util.atomic_write.completed</code> audits emitted. <br>3. Operator disputes ordering; forensics team retrieves serialized RNG state and input snapshots from <code>evidenceRef</code> and runs <code>replay.run --evidenceRef</code> to reproduce exact ordering. <br>4. Forensics shows algorithm version updated between runs; <code>error.migration.applied</code> audit recorded for the run that used new tie-breaker policy. For resolution, teams either revert to previous tie-break policy via migration manifest or accept migration with owner-signed manifest and golden test updates. <br><strong>Narrative 4 — Retry exhaustion cascading failure leading to SRE incident:</strong><br>1. A worker repeatedly attempts to persist large report artifacts to remote NFS; calls guarded by <code>Retry(AtomicWrite, retries=5, idempotent_assert=true)</code>. Persistent NFS lag causes timeouts; <code>ClassifyError</code> returns <code>ERR_EXTERNAL_CONNECTOR_TIMEOUT</code> for each attempt. <br>2. After retries exhausted <code>WrapError</code> creates <code>ERR_RETRY_EXHAUSTED</code> wrapper; <code>AnnotateError</code> includes <code>attempts</code>, <code>backoffPolicy</code>, <code>tempPaths</code>. <code>EmitErrorAudit</code> records <code>ERR_RETRY_EXHAUSTED</code>. <br>3. <code>EscalationPolicyDecision</code> sees elevated retry exhaustion rate crossing <code>SRENotifyThreshold</code> and triggers <code>incident</code> with <code>requiredApprovals=1</code>. <code>ReportToSRE</code> creates incident artifacts and pushes evidence bundle for on-call triage. <br>4. SRE enables circuit-breaker for NFS writes, increases retry budgets temporarily for critical job descriptors, and works with infra to remediate NFS performance. All actions recorded via audit rows and post-mortem produced. </td></tr><tr><td data-label="REG_Error — Per-function Expert Technical Breakdown"> <strong>Conceptual Power Query (M) mapping — REG_Error best practices for PQ workflows</strong><br><strong>Context & constraints:</strong> Power Query (M) runs in host environments with variable runtimes and limited control over file I/O semantics. REG_Error cannot execute inside M; host-level helpers implement error handling and evidence persistence. <br><strong>Patterns & recommended practices:</strong><br>1. <strong>Host marshalling of M runtime errors:</strong> host captures PQ engine exceptions and constructs structured payload <code>{provider, httpStatus, messageSanitized, mChecksum, queryName, workbookId}</code> and passes to <code>ParseThirdPartyError</code> → <code>ClassifyError</code>. Audit produced: <code>pq_refresh.error</code> referencing <code>errorId</code>. <br>2. <strong>Atomic persistence before injection:</strong> <code>PQ_Injector</code> writes canonical M artifact using worker <code>AtomicWrite</code>; only inject into workbook after verifying workbook query hash matches persisted artifact checksum. Mismatches classified as <code>ERR_PQ_INJECT_MISMATCH</code> with diffs persisted to evidence. <br>3. <strong>Preview determinism:</strong> PQ_Ribbon computes preview seed <code>SeedFromCorrelation(correlationId, templateId)</code> and injects it as preview parameter into M template; preview audit logs <code>seed</code> and <code>mChecksum</code>. For heavyweight sampling or numerical fidelity tests, run in worker using <code>DeterministicRNG</code> and persist sample set as evidence. <br>4. <strong>Retry orchestration for PQ refresh:</strong> orchestrator wrapper persists refresh job descriptor (<code>AtomicWrite</code>) before attempting refresh; uses <code>RetryPolicyFromError</code> on connector failures and validates idempotency tokens for safe retries. <br>5. <strong>High-precision numeric transforms:</strong> flag templates <code>requiresHighPrecision</code> and execute final numeric aggregation in worker <code>SafeRound</code> to ensure cross-host parity; persist final authoritative artifact via <code>AtomicWrite</code> and attach to <code>pq_inject</code> audit. <br><strong>Example PQ operator flow (injection):</strong> preview → compute seed and <code>pq_preview</code> audit → operator selects inject → <code>PQ_Injector</code> worker <code>AtomicWrite</code> artifact → inject helper verifies workbook query hash → on mismatch classify and attach evidence / escalate. </td></tr><tr><td data-label="REG_Error — Per-function Expert Technical Breakdown"> <strong>Conceptual DAX mapping — REG_Error for semantic models and DAX patterns</strong><br><strong>Context:</strong> DAX is read-time expression language; cannot perform side-effects or persistent logging. All side-effecting error handling and audit must happen in ETL and worker layers. <br><strong>Patterns & recommendations:</strong><br>1. <strong>Persist RunMetadata in ETL:</strong> ETL writes <code>RunMetadata</code> atomically with fields <code>correlationId</code>, <code>artifactChecksum</code>, <code>errorIds[]</code>, <code>runTs</code>, <code>configHash</code>. DAX queries read <code>RunMetadata</code> to present model health flags. <br>2. <strong>Model health indicator:</strong> create DAX measure <code>ReconciledFlag := IF(RunMetadata.expectedChecksum = Model.artifactChecksum, 1, 0)</code>. If 0, dashboard shows generic "Data validation failed" and a pointer to <code>correlationId</code> for operator diagnostics, never raw error details. <br>3. <strong>Deterministic sampling in ETL:</strong> compute <code>HashKey = HMAC_SHA256(PrimaryKey | correlationSalt)</code> and persist selected samples; store <code>correlationSalt</code> in <code>RunMetadata</code> for reproducible sampling and forensics. <br>4. <strong>Rounding & allocations done in ETL:</strong> perform <code>SafeRoundResiduals</code> in ETL to produce resolved integer cent allocations; store both raw canonical decimals and rounded outputs in artifact; DAX consumes the final integer values. <br>5. <strong>Checksum reconciliation for models:</strong> ETL persists <code>artifact.manifest.json</code> with dataset-level checksums; DAX surfaces reconciliation status by comparing model-level computed checksums to <code>RunMetadata.expectedChecksum</code>. <br><strong>DAX operator narrative (model refresh failure):</strong> ETL fails <code>AtomicWrite</code> → classify error + attach evidence → persist <code>RunMetadata</code> with <code>errorIds</code> and <code>artifactChecksum=null</code> → DAX <code>ReconciledFlag=0</code> → UI shows correlation pointer for operator diagnostics. </td></tr><tr><td data-label="REG_Error — Per-function Expert Technical Breakdown"> <strong>Appendices — forensic artifacts, evidence paths, retention & access controls</strong><br><strong>Minimum forensic artifacts for any critical error:</strong><br>1. <code>audit_tail.csv</code> for correlation window including <code>UserAction</code>, <code>REG_Error</code> rows. <br>2. Serialized <code>Error</code> JSON files (canonical) and <code>evidenceRef</code> artifacts (encrypted). <br>3. <code>jobDescriptor.json</code> and persisted artifact files with SHA256 checksums. <br>4. Serialized RNG state blobs when deterministic sampling/tie-breakers used. <br>5. Canonical <code>SafeRound</code> input snapshots for numeric disputes. <br>6. <code>atomic_write.tempPaths</code> and <code>tempArtifact</code> listings from failed writes. <br>7. <code>forensic_manifest.json</code> capturing artifact URIs, checksums, <code>evidenceRef</code>s, and access control metadata. <br><strong>Evidence storage & retention:</strong><br>1. Hot evidence store: <code>\\evidence\hot\&lt;module&gt;\&lt;correlationId&gt;\</code> retained 30 days and strictly ACL-limited. <br>2. Warm archive: secure long-term archive for regulated retention (7 years common); chain-of-custody metadata required. <br>3. Cold archive: jurisdiction-specific retention per regulation. <br><strong>Access controls & verification:</strong> evidence encrypted at rest; access to evidence indexed by <code>evidenceRef</code> controlled via ACLs and audited on access; monthly retention verification jobs emit <code>housekeeping.audit</code> and <code>forensic_manifest.verify</code>. </td></tr><tr><td data-label="REG_Error — Per-function Expert Technical Breakdown"> <strong>Acceptance checklist before module release (comprehensive):</strong><br>1. OWNER entries present for <code>TEAM_REG_ERROR</code> and code owners listed in OWNERS.md. <br>2. <code>ErrorCodes</code> catalog finalized and included in release manifest; any new codes have owner approval. <br>3. Deterministic classification golden vectors present and passing for supported languages. <br>4. Canonical serialization and migration tests green across all <code>schemaVersion</code> transitions. <br>5. <code>SafeErrorToUser</code> templates present in required locales and translations certified by L10N. <br>6. Evidence attach flow tested end-to-end with encryption, <code>AtomicWrite</code>, indexing, and ACL checks. <br>7. Audit hooks validated with signed append tests and <code>ErrorChainVerify</code> executed successfully on sample runs. <br>8. Static analysis ensures no forbidden IO on UI-thread functions and no top-level PII leakage. <br><strong>Blocking conditions:</strong> missing audit emits, PII leakage detected, failing serialization/migration tests, or absent OWNER sign-offs. </td></tr><tr><td data-label="REG_Error — Per-function Expert Technical Breakdown"> <strong>Extremely detailed test plan highlights & scripts (explicit):</strong><br><strong>Unit tests:</strong><br>1. Classification mapping suites: errno cases, HTTP statuses, provider SDK errors, HTML bodies, and multi-lingual messages. <br>2. <code>SerializeErrorForAudit</code> canonicalization and <code>DeserializeErrorFromAudit</code> migration tests with edge-case fields. <br>3. <code>SafeErrorToUser</code> template rendering tests across locales and verbosity levels. <br>4. <code>AttachForensicEvidence</code> with simulated encryption failures and ENOSPC/EPERM scenarios. <br><strong>Integration tests:</strong><br>1. End-to-end error flow including evidence persistence and chain verify. <br>2. Retry + escalation tests: inject transient faults and assert correct <code>RetryPolicy</code> and escalation triggers. <br>3. Key rotation & canary verification integration. <br><strong>Property & fuzz tests:</strong><br>1. Deterministic <code>errorId</code> generation parity tests across language bindings. <br>2. PII redaction property tests with fuzzed inputs ensuring no PII in top-level audit. <br>3. Chain compaction/re-hydration integrity tests. <br><strong>Performance tests:</strong> classification throughput benchmarks, audit append flush under high event rates, evidence attachment concurrency stress. <br><strong>CI gating:</strong> golden parity, forbidden-API static checks, localization validations, and performance budgets. </td></tr><tr><td data-label="REG_Error — Per-function Expert Technical Breakdown"> <strong>Operator runbook quick commands & examples (concise & prescriptive):</strong><br>1. <code>diagnostics collect --correlation r-YYYYMMDD-abc</code> — packages <code>audit_tail.csv</code>, serialized <code>Error</code> JSON, evidence artifacts, and <code>forensic_manifest.json</code>. <br>2. <code>error.replay --correlation r-... --evidenceRef &lt;ref&gt;</code> — deterministic replay using persisted evidence <code>--dry-run</code> supported. <br>3. <code>error.chain.verify --correlation r-... --manifest release-manifest.json</code> — verify audit chain integrity. <br>4. <code>error.attach --error-id &lt;id&gt; --file &lt;path&gt;</code> — attach local artifact under maintenance window (ACL enforced). <br><strong>When to call SRE:</strong> after <code>ERR_ATOMIC_WRITE_ENOSPC</code> persists on critical job descriptors despite staging fallback, or <code>ERR_FORGERY_DETECTED</code> chain verification failures; include <code>forensic_manifest</code> and <code>audit_tail</code>. </td></tr><tr><td data-label="REG_Error — Per-function Expert Technical Breakdown"> <strong>Appendix A — example audit row schema (descriptive):</strong><br><strong>Fields required for error audits:</strong> <code>timestamp</code>, <code>correlationId</code>, <code>module</code>, <code>procedure</code>, <code>operatorId</code> (optional), <code>errorId</code>, <code>errorCode</code>, <code>severity</code>, <code>paramsHash</code>, <code>resultHash</code> (optional), <code>evidenceRef</code> (optional), <code>prevErrorId</code> (optional), <code>configHash</code>, <code>metadata</code> object including <code>duration_ms</code>, <code>attempts</code>, <code>artifactChecksum</code>, <code>tempPathList</code>. <br><strong>Policy:</strong> top-level audit rows must not contain raw PII; full sanitized parameters persisted in evidence store referenced by <code>evidenceRef</code>. </td></tr><tr><td data-label="REG_Error — Per-function Expert Technical Breakdown"> <strong>Appendix B — common failure modes & mitigations (expanded):</strong><br><strong>Failure mode: partial write observed by reader</strong><br>1. Cause: caller wrote directly to final path instead of using <code>AtomicWrite</code>, or rename semantics unreliable on network FS. <br>2. Mitigation: enforce <code>AtomicWrite</code> usage via static analyzer and test harness; run <code>InspectTempArtifacts</code> and <code>AtomicWriteRepair</code>; prefer same-volume staging for fallback. <br><strong>Failure mode: nondeterministic sampling complaint</strong><br>1. Cause: global RNG used or seed not persisted. <br>2. Mitigation: enforce <code>DeterministicRNG</code> seeded from <code>correlationId</code>; persist serialized RNG state for replay; add parity tests. <br><strong>Failure mode: rounding drift over repeated runs</strong><br>1. Cause: applying naive rounding repeatedly instead of <code>SafeRoundResiduals</code>. <br>2. Mitigation: adopt <code>bankers</code> or <code>residual_distribute</code> strategies, persist SafeRound audit events, and run property tests confirming sum-preservation and bias absence. </td></tr><tr><td data-label="REG_Error — Per-function Expert Technical Breakdown"> <strong>Appendix C — governance checklists & PR requirements (explicit):</strong><br>1. PR must include unit/integration tests for changed behavior and golden updates where classification changes. <br>2. New <code>ErrorCodes</code> require migration manifest, OWNER approval, and release-manifest entry. <br>3. Serialization changes require cross-language parity tests (VBA/JS/Python/C#). <br>4. Template/localization changes require L10N review and translation tests. <br><strong>Blocking conditions:</strong> missing audit emits for critical flows, PII leakage detection, or failing migration tests. </td></tr><tr><td data-label="REG_Error — Per-function Expert Technical Breakdown"> <strong>Appendix D — long-form incident reconstruction example (ordered):</strong><br><strong>Incident:</strong> Allocation mismatch reported for run <code>r-20260112-455</code> <br><strong>Forensic reconstruction steps:</strong><br>1. Retrieve <code>UserAction</code> and <code>REG_Error</code> audit rows for <code>correlationId=r-20260112-455</code>. <br>2. Download <code>evidenceRef</code> artifacts (serialized RNG state, SafeRound inputs) from evidence store and decrypt using <code>CORE_KeyManager</code> with audited access. <br>3. Restore RNG via <code>DeterministicRNG.restore_state()</code> and canonical decimal snapshots to rerun allocation deterministically. <br>4. Re-run allocation pipeline in replay mode using persisted RNG and <code>SafeRoundResiduals</code>; compute artifact checksum. <br>5. Compare replay checksum to original artifact checksum recorded in <code>util.atomic_write.completed</code>. <br>6. If mismatch found, collect <code>atomic_write.verification_failed</code> rows, temp artifacts, and include them in <code>forensic_manifest.json</code>. Escalate to Compliance if regulated. <br><strong>Outcome:</strong> reproducible replay confirms pipeline correctness or reveals root-cause and remediation path. </td></tr><tr><td data-label="REG_Error — Per-function Expert Technical Breakdown"> <strong>Appendix E — PQ & DAX short checklists for template authors and report builders (actionable):</strong><br><strong>PQ Template author checklist:</strong><br>1. Include <code>mChecksum</code> in template metadata and publish manifest. <br>2. Flag <code>requiresHighPrecision</code> and document numeric expectations. <br>3. Parameterize preview seed and persist seed in <code>pq_preview</code> audit for reproducibility. <br>4. Offload final numeric aggregation to worker <code>SafeRound</code> for regulated templates; persist authoritative artifact via <code>AtomicWrite</code>. <br>5. For regulated templates require owner-approved signed manifest before publishing. <br><strong>DAX/report builder checklist:</strong><br>1. Read <code>RunMetadata</code> for provenance and <code>artifactChecksum</code>. <br>2. Avoid rounding residuals and allocation logic in DAX; perform in ETL and persist integer cents. <br>3. Use hashed stable keys created in ETL for deterministic sampling within DAX filters. <br>4. Display only generic health flags and <code>correlationId</code> pointer for operator diagnostics. </td></tr><tr><td data-label="REG_Error — Per-function Expert Technical Breakdown"> <strong>Final mandatory constraints (non-negotiable):</strong><br>1. All artifacts consumed by other processes must be persisted via <code>AtomicWrite</code>. <br>2. Deterministic RNGs must be seeded from <code>correlationId</code> for operator-visible sampling and tie-breakers; persist RNG state when replay required. <br>3. All critical operations affecting persisted artifacts or regulated outputs must emit <code>error.audit.emit</code> and include <code>evidenceRef</code> when necessary. <br>4. UI-thread functions must not perform IO or heavy evidence persistence; schedule to worker. <br>5. PII must never appear in top-level audit rows; store sanitized evidence encrypted and reference by <code>evidenceRef</code>. <br><strong>Verification:</strong> CI static analyzers enforce forbidden patterns, golden parity tests, and audit emission validation as blocking checks. <br><strong>Checked:</strong> taxonomy coverage, audit chain completeness, deterministic chain (UI → job → worker → artifact → audit → replay), PQ & DAX conceptual mappings, operator runbooks, and forensic evidence handling — internal consistency audited and cross-validated. </td></tr></tbody></table></div><div class="row-count">Rows: 38</div></div><script src="assets/xlsx.full.min.js?v=1758605028" defer></script>
<script src="assets/script.js?v=1759748863" defer></script>
<script src="assets/worker.js?v=1758331710" defer></script>
<script>
(function(){
  const template = "{table}_{date}";
  const userVal = "";
  const hrefPrefix = "assets";
  function formatName(tableName) {
    const date = (new Date()).toISOString().slice(0,10);
    return template.replace('{table}', tableName).replace('{date}', date).replace('{user}', userVal);
  }
  const btn = document.getElementById('exportBtn');
  if(btn) {
    btn.addEventListener('click', async function() {
      try {
        const html = document.documentElement.outerHTML;
        if(html.length > 2000000) { alert('Export refused: html too large'); return; }
        if(window.Worker) {
          const workerUrl = (function(){ try{ return hrefPrefix + '/worker.js'; }catch(e){ return null; } })();
          if(workerUrl) {
            try {
              const worker = new Worker(workerUrl);
              worker.postMessage({html: html, format: 'pdf'});
              worker.onmessage = function(e) { console.log('worker:', e.data); alert('Worker replied: '+(e.data.msg||e.data.status)); };
            } catch(err) { console.warn('Worker create failed', err); alert('Worker not available'); }
          } else { alert('Worker not available'); }
        } else { alert('Export worker not supported in this environment.'); }
      } catch(err) { console.warn('Export failed', err); alert('Export worker not available. See console for details.'); }
    });
  }
})();
window.addEventListener('load', function(){ try{ document.querySelectorAll('.table-wrapper').forEach(function(e){ e.style.opacity='1'; }); }catch(e){} });
</script>
</div>
</body>
</html>