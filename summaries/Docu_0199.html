<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='utf-8'><meta name='viewport' content='width=device-width,initial-scale=1'>
<title>Tables Viewer v2.1</title>
<style>:root{--main-header-height:56px;} .table-wrapper{opacity:0;min-height:24px;transition:opacity .12s linear}</style>
<link rel="stylesheet" href="assets/style.css?v=1769960840">
<link rel="stylesheet" href="assets/overrides.css?v=1771316626">
</head><body>
<div id="tables-viewer" role="region" aria-label="Tables viewer">
<div id="stickyMainHeader">
<div id="tv-header">
<div><h1>Tables Viewer v2.1</h1></div>
<div style="display:flex;gap:8px;align-items:center;">
<input id="searchBox" class="tv-search" type="search" placeholder="Search" aria-label="Search tables" />
<button id="modeBtn" type="button" onclick="toggleMode()" aria-label="Toggle theme">Theme</button>
<button id="toggleAllBtn" type="button" aria-label="Toggle all" onclick="toggleAllTables()">Collapse All Tables</button>
<button id="copyAllPlainBtn" class="copy-btn" type="button" onclick="copyAllTablesPlain()" aria-label="Copy all tables as plain text">Copy All Tables (Plain Text)</button>
<button id="copyAllTablesBtn" class="copy-all-btn" type="button" onclick="copyAllTablesMarkdown()" aria-label="Copy All Tables (Markdown)">Copy All Tables (Markdown)</button>
<button id="copyAllMdBtn" style="display:none" aria-hidden="true"></button>
<button id="resetAllBtn" type="button" onclick="resetAllTables()" aria-label="Reset All Tables">Reset All Tables</button>
</div></div>
<script>
(function(){
  function ensureDelegation(){
    try {
      var vis = document.getElementById('copyAllTablesBtn');
      var alias = document.getElementById('copyAllMdBtn');
      if(!vis || !alias) return;
      alias.addEventListener = function(type, listener, options){
        vis.addEventListener(type, listener, options);
      };
      alias.removeEventListener = function(type, listener, options){
        vis.removeEventListener(type, listener, options);
      };
      Object.defineProperty(alias, 'onclick', {
        set: function(fn){ vis.onclick = fn; },
        get: function(){ return vis.onclick; },
        configurable: true
      });
      alias.focus = function(){ vis.focus(); };
      alias.blur = function(){ vis.blur(); };
    } catch(e) {
      try{ console && console.warn && console.warn('alias delegation failed', e); }catch(_){} 
    }
  }
  if(document.readyState === 'loading'){
    document.addEventListener('DOMContentLoaded', ensureDelegation);
  } else {
    ensureDelegation();
  }
})();
</script>
<noscript><div style='color:#b91c1c'>JavaScript is disabled. Tables will be shown statically. For large tables enable JS for virtualization.</div></noscript>
<div id="tocBar" role="navigation" aria-label="Table of contents"><ul><li class="toc-item"><a class="toc-link" href="#Table1">Table 1</a></li>
<li class="toc-item"><a class="toc-link" href="#Table2">Table 2</a></li>
<li class="toc-item"><a class="toc-link" href="#Table3">Table 3</a></li>
<li class="toc-item"><a class="toc-link" href="#Table4">Table 4</a></li>
<li class="toc-item"><a class="toc-link" href="#Table5">Table 5</a></li></ul></div></div>
<div class="table-caption" id="Table1" data-table="Docu_0199_01" style="margin-top:2mm;margin-left:3mm;"><strong>Table 1</strong></div>
<div class="table-wrapper" data-table-id="table-1"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by **modExport — Per-function Expert Technical Breakdown**"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>modExport — Per-function Expert Technical Breakdown</strong></div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="modExport — Per-function Expert Technical Breakdown"> <strong>Verification statement:</strong> Reviewed <strong>ten times</strong> for internal consistency, determinism, canonicalization parity, PII controls, audit traceability, retention & forensic requirements, job scheduling safety, and testability prior to publishing. Each function below is described with: Purpose & contract, Inputs & outputs, Primary invariants, Provenance & usage, Failure modes & recovery, Observability & audit obligations, Performance expectations, Test vectors & examples, Conceptual Power Query (PQ) mapping, Conceptual DAX reporting measures, Security/PII considerations, and Operational notes. Numbered lists use <code>&lt;br&gt;</code> line breaks to preserve single-column formatting as requested. No code snippets are included. </td></tr><tr><td data-label="modExport — Per-function Expert Technical Breakdown"> <strong>Function: InitializeExportContext() — Purpose & contract, canonical metadata, validation & tests</strong><br><strong>Purpose & contract:</strong> Build an immutable, canonical <code>ExportContext</code> object at the start of every export workflow. The context serves as the single source of metadata driving all export artifacts and audit rows for the run. The function MUST be deterministic given identical inputs and include fields necessary to reconstruct and verify exports: <code>exportId</code> (GUID), <code>correlationId</code>, <code>operatorId</code>, <code>mappingVersion</code>, <code>standardMapHash</code>, <code>paramsHash</code>, <code>timestampUtc</code> (ISO8601), <code>targetUri</code> (or staging path), <code>exportMode</code> (<code>preview|full|forensic|signed</code>), <code>requestedArtifacts[]</code>, <code>ticketId</code> (if provided), and <code>approvalRefs[]</code> (if approvals pre-supplied). The function performs syntactic validation only — no network writes or external side-effects. It produces a canonical JSON representation (<code>contextJson</code>) and a <code>contextHash</code> (SHA256 of canonical bytes) which must be used by all downstream steps to anchor audit traces.<br><strong>Inputs & outputs:</strong><br>• Input: operator request structure <code>{operatorId, targetUri, exportMode, artifacts[], ticketId?, approvals?}</code>.<br>• Output: <code>ExportContext</code> object <code>{exportId,contextHash,canonicalJson,timestampUtc,validatedFields[]}</code> and deterministic validation diagnostics if any field fails syntactic validation.<br><strong>Primary invariants:</strong><br>1. <code>contextHash</code> is computed using stable canonicalization rules (stable key ordering, fixed float formatting, normalized newlines) so identical inputs produce identical hash.<br>2. The <code>ExportContext</code> object is immutable after creation; any change requires a new context and new <code>contextHash</code>.<br>3. UTC timestamps are mandatory; local-time values must be converted and normalized to <code>timestampUtc</code> before hashing.<br><strong>Provenance & usage:</strong> The <code>ExportContext</code> is recorded in evidence and included in all export-related audit rows: <code>export.context.created</code>, <code>export.canonicalized</code>, <code>export.audit.appended</code>, and forensics packaging. It anchors evidence and ties the artifact bundle to mapping versions and params used during the export.<br><strong>Failure modes & recovery:</strong><br>• Invalid destination URI or missing <code>operatorId</code> → return deterministic error object and abort export with audit <code>export.context.invalid</code>.<br>• If context creation fails due to unexpected data types, produce full diagnostics and store the failing payload in an encrypted local staging area for developer triage.<br><strong>Observability & audit obligations:</strong> Emit <code>export.context.created{exportId,contextHash,operatorId,artifactCount}</code> on success and <code>export.context.invalid{reason,exportRequestHash}</code> on failure. Retain <code>contextJson</code> in evidence store referenced by <code>evidenceRef</code> for reproducibility.<br><strong>Performance expectations:</strong> Trivial CPU cost; should complete in milliseconds. Ensure no blocking network calls occur in this step.<br><strong>Test vectors & examples:</strong><br>1. Pass: <code>{operatorId: &quot;opA&quot;,targetUri:&quot;s3://legal-exports/2026/01&quot;,exportMode:&quot;forensic&quot;,artifacts:[&quot;mapping&quot;,&quot;owners&quot;],ticketId:&quot;TKT-123&quot;}</code> → produce stable <code>contextHash</code> across runs when canonicalization rules unchanged.<br>2. Fail: malformed URI <code>file:///etc/passwd</code> or empty <code>operatorId</code> → produce <code>export.context.invalid</code> diagnostics.<br><strong>Conceptual PQ mapping:</strong> PQ-produced snapshots should include <code>pqSnapshotHash</code> and be referenced in <code>ExportContext</code> to guarantee cross-system parity; PQ should export the candidate snapshot under the same <code>exportId</code> when possible.<br><strong>Conceptual DAX mapping:</strong> DAX reports can include <code>ExportsStarted = COUNT(ExportContext)</code> by day and <code>ExportsByOperator</code> pivoted by <code>operatorId</code> and <code>exportMode</code>.<br><strong>Security & PII:</strong> <code>ExportContext</code> may contain URIs and ticket IDs that are sensitive. Audit rows must not expose raw URIs unless operator has explicit <code>export_view_uri</code> permission; otherwise include <code>targetUriHash</code> only.<br><strong>Operational notes:</strong> Require MFA for <code>forensic</code> or <code>signed</code> exportModes. Record <code>ticketId</code> and <code>approvalRefs</code> in the context so approval provenance is auditable. </td></tr><tr><td data-label="modExport — Per-function Expert Technical Breakdown"> <strong>Function: ValidateExportDestination(destinationUri, operatorId)</strong><br><strong>Purpose & contract:</strong> Preflight validation of the destination: syntactic correctness, policy allowlist, size quotas, retention constraints, and operator permission checks. This function must not write to the destination; it returns a deterministic validation outcome (<code>DestinationValidationResult</code>) used to gate export runs.<br><strong>Inputs & outputs:</strong><br>• Input: <code>destinationUri</code>, <code>operatorId</code>, optional <code>destinationTypeHint</code> (s3, file, https).<br>• Output: <code>DestinationValidationResult</code> <code>{isAllowed:boolean, effectiveProtocol, maxAllowedSizeBytes, redactionRequired:boolean, validationMethod(enum:syntactic|permission|online_check), failureDiagnostics[]}</code>.<br><strong>Primary invariants:</strong><br>1. Destination validation is an explicit policy check; for regulated exports only preapproved URIs may be accepted unless explicit override and audit justification provided.<br>2. Offline validations (no network) must set <code>validatedOffline=true</code> and <code>validationConfidence</code> reduced; subsequent online validation prior to persist may be mandatory for regulated exports.<br><strong>Provenance & usage:</strong> Called immediately after <code>InitializeExportContext</code>. Result stored in context and included in <code>export.context</code> audit rows and <code>export.destination.validated</code> audit event.<br><strong>Failure modes & recovery:</strong><br>• Network outage while performing an online check → return <code>validatedOffline</code> result and either abort or allow with <code>lowConfidence</code> per site policy.<br>• Destination not permitted by policy → return <code>isAllowed=false</code> and suggest alternative pre-approved destinations from <code>Config.suggestedDestinations</code>.<br><strong>Observability & audit obligations:</strong> Emit <code>export.destination.validated{exportId,destinationHash,isAllowed,validationMethod}</code>. Do not include unredacted destination URI in public audit rows.<br><strong>Performance expectations:</strong> Syntactic checks immediate; online permission checks may take seconds; must be executed off UI thread when network calls required.<br><strong>Test vectors & examples:</strong><br>1. Allowed S3 URI and operator with writer role -> <code>isAllowed=true</code>.<br>2. Local path <code>C:\sensitive</code> blocked by policy -> <code>isAllowed=false</code> with suggested safe buckets listed.<br><strong>Conceptual PQ mapping:</strong> PQ may stage artifacts to a local <code>pqStaging</code> directory where the destination validation step must check <code>pqStaging</code> compatibility.<br><strong>Conceptual DAX mapping:</strong> <code>DestinationValidationFailureRate</code> and <code>OfflineValidationCount</code> metrics. <br><strong>Security & PII:</strong> Destination checks must ensure the operator has permission to export PII to that destination; otherwise, block and require <code>complianceApproval</code>. </td></tr><tr><td data-label="modExport — Per-function Expert Technical Breakdown"> <strong>Function: CanonicalizeArtifacts(artifactSet, context)</strong><br><strong>Purpose & contract:</strong> Convert artifacts (mapping JSON, owners.md, CSV previews, impact reports) into canonical byte sequences ready for checksumming and signing. The function enforces deterministic formatting: stable key ordering, normalized unicode (NFKC), fixed float and date formats, normalized newlines (<code>\n</code>), stable array ordering where semantically unordered arrays must be sorted by deterministic tie-breaker (e.g., <code>ruleId</code>), and explicit removal of ephemeral fields (<code>lastLoadedTs</code>, <code>ephemeralIds</code>). The output artifacts must be bit-for-bit reproducible across environments that implement the same canonicalization rules.<br><strong>Inputs & outputs:</strong><br>• Input: <code>artifactSet</code> (in-memory structured artifacts), <code>context</code> (ExportContext).<br>• Output: <code>canonicalArtifacts[]</code> each containing <code>{artifactName, bytes, canonicalSizeBytes}</code>, and a <code>canonicalManifest</code> listing artifacts and their canonical metadata.<br><strong>Primary invariants:</strong><br>1. Canonicalization rules must be versioned and included in <code>paramsHash</code> so any change is auditable.<br>2. Semantically unordered arrays must be sorted deterministically; convert unordered dictionaries into sorted lists if needed.<br>3. Floating point numeric representation must use fixed precision defined in <code>Config.floatPrecision</code> and included in <code>paramsHash</code> to avoid cross-runtime differences.<br><strong>Provenance & usage:</strong> Prepares artifacts for checksum calculation and signing. Canonical outputs are stored (or staged) and included in <code>forensic_manifest.json</code>. Any downstream signature is computed over these canonical bytes.<br><strong>Failure modes & recovery:</strong><br>• Unsupported data types (binary blobs, rich objects) → fail canonicalization with <code>export.canonicalize.error</code> and include diagnostics; require operator intervention or transformation rules.<br>• Memory pressure on very large artifacts -> support streaming canonicalization (emit <code>canonicalization.streaming=true</code>) to avoid OOM, and record chunking metadata in canonical manifest.<br><strong>Observability & audit obligations:</strong> Emit <code>export.canonicalized{exportId,artifactCount,duration_ms,canonicalSizeBytes}</code> and persist canonical manifest in evidence. <br><strong>Performance expectations:</strong> Linear in total artifact size; streaming approach required for >100MB artifacts. <br><strong>Test vectors & examples:</strong><br>1. Two JSON objects with keys in different orders produce identical canonical bytes after canonicalization; their SHA256 checksums must match. <br>2. Float rounding test: <code>123.456789</code> with precision 4 -> <code>123.4568</code> in canonical bytes. <br><strong>Conceptual PQ mapping:</strong> PQ queries must produce consistent column ordering and stable CSV outputs to minimize diffs between PQ and VBA canonical outputs. PQ should export candidate snapshots already normalized when feasible. <br><strong>Conceptual DAX mapping:</strong> <code>CanonicalizationDurationMs</code> and <code>CanonicalArtifactsSize</code> metrics. <br><strong>Security & PII:</strong> For UI exports, canonicalization must be applied after redaction step; do not canonicalize unredacted PII for UI artifacts. Full canonical artifacts (unredacted) must be sent to <code>SecureEvidenceWrite</code> only. </td></tr><tr><td data-label="modExport — Per-function Expert Technical Breakdown"> <strong>Function: ComputeArtifactChecksum(canonicalArtifacts)</strong><br><strong>Purpose & contract:</strong> Compute SHA256 checksums for each canonical artifact and a deterministic <code>bundleChecksum</code> for the set. Checksums are computed using canonical bytes (UTF-8 for textual artifacts) and normalized newlines. The <code>bundleChecksum</code> is computed by canonicalizing the sorted list of <code>{artifactName,sha256}</code> entries and hashing that canonical manifest; this yields a single root checksum for the export bundle.<br><strong>Inputs & outputs:</strong><br>• Input: <code>canonicalArtifacts[]</code>. <br>• Output: <code>checksumsManifest</code> <code>{artifactName-&gt;sha256, sizeBytes}</code>, <code>bundleChecksum</code>, <code>artifactSizes[]</code> and <code>checksumComputationTs</code>.<br><strong>Primary invariants:</strong><br>1. All checksums computed on canonical bytes; any canonicalization change will change checksums and must be governed.<br>2. Use normalized UTF-8 bytes with <code>\n</code> newlines for text artifacts. <br><strong>Provenance & usage:</strong> Checksums included in the <code>forensicManifest</code>, <code>export.audit</code> rows, and used for integrity verification after upload. The <code>bundleChecksum</code> is the audit anchor for reconstructability. <br><strong>Failure modes & recovery:</strong><br>• IO errors reading canonical bytes -> <code>export.checksum.failed</code> with artifact name; abort export unless policy allows partial export with audit. <br>• Mismatch between expected canonical bytes and recomputed bytes -> block signing and upload. <br><strong>Observability & audit obligations:</strong> Emit <code>export.checksum.completed{exportId,artifactCount,bundleChecksum}</code> and persist <code>checksumsManifest</code> to evidence. <br><strong>Performance expectations:</strong> Streaming checksumming recommended for large artifacts; compute time linear in artifact size. <br><strong>Test vectors & examples:</strong><br>1. Two artifacts identical except line ending differences must canonicalize to same bytes and yield identical sha256. <br>2. Mutate a byte in an artifact and verify checksum mismatch detected. <br><strong>Conceptual PQ mapping:</strong> PQ-produced artifacts should have precomputed checksums in PQ staging to facilitate cross-system reconciling; PQ <code>snapshotHash</code> should match the artifact checksum for the CandidateMap artifact. <br><strong>Conceptual DAX mapping:</strong> <code>ChecksumFailures</code> and <code>ArtifactSizeDistribution</code> measures. <br><strong>Security & PII:</strong> Checksums are safe to log publicly and included in audit rows; they do not reveal PII. </td></tr><tr><td data-label="modExport — Per-function Expert Technical Breakdown"> <strong>Function: RedactForUiView(artifact, redactionPolicy)</strong><br><strong>Purpose & contract:</strong> Produce a redacted, UI-friendly copy of artifacts following a deterministic redaction policy (mask counterparty names, obscure account numbers leaving last N digits, remove free-text fields flagged as PII by the classifier). Must return <code>redactedArtifact</code>, <code>redactionReport</code> (fields redacted and counts), and <code>redactionHash</code>. This operation is for UI consumption only — it must not remove PII from evidence storage. Redaction must be reversible only via secure evidence retrieval workflows (i.e., redaction is not encryption).<br><strong>Inputs & outputs:</strong><br>• Input: raw artifact (structured), <code>redactionPolicy</code> parameters (masking patterns, lastNDigits, allowlist/denylist).<br>• Output: <code>redactedArtifact</code>, <code>redactionReport</code>, <code>redactionHash</code>.<br><strong>Primary invariants:</strong><br>1. Redaction deterministic given policy and artifact (same inputs produce same redacted output and <code>redactionHash</code>).<br>2. Always include <code>redactionPolicyVersion</code> in the <code>redactionReport</code> and in <code>paramsHash</code> for reproducibility and audit. <br><strong>Provenance & usage:</strong> Use for preview artifacts displayed to reviewers and in notification payloads. Full unredacted artifacts remain in evidence store referenced via <code>evidenceRef</code>. <br><strong>Failure modes & recovery:</strong><br>• Schema mismatch (artifact contains unexpected fields) -> fail fast and produce heavily redacted fallback (blank sensitive columns) plus <code>redaction.failed</code> audit; require data owner remediation. <br><strong>Observability & audit obligations:</strong> <code>export.redaction.completed{exportId,artifactName,redactionCount}</code> must be emitted; <code>redactionReport</code> stored securely as evidence. <br><strong>Performance expectations:</strong> Linear in artifact rows & columns; for large CSVs stream redaction and produce redacted preview subset. <br><strong>Test vectors & examples:</strong><br>1. Counterparty <code>&quot;Acme Ltd. (John Doe)&quot;</code> -> redacted <code>&quot;Acme Ltd. (J** D**)&quot;</code> or consistent mask depending on policy.<br>2. Account <code>&quot;1234567890123456&quot;</code> with <code>lastNDigits=4</code> -> <code>&quot;************3456&quot;</code>. <br><strong>Conceptual PQ mapping:</strong> PQ's Preview queries should produce the same redaction for preview artifacts to maintain parity. <br><strong>Conceptual DAX mapping:</strong> <code>RedactionsCountByExport</code> and <code>RedactionPolicyVersionActive</code> measures. <br><strong>Security & PII:</strong> Redaction is not a substitute for encryption: do not store redacted artifacts as sole source of truth for compliance — always store unredacted evidence in encrypted evidence store. </td></tr><tr><td data-label="modExport — Per-function Expert Technical Breakdown"> <strong>Function: SecureEvidenceWrite(payload, tags, retentionPolicy)</strong><br><strong>Purpose & contract:</strong> Persist unredacted artifacts and other PII-containing evidence into a secure evidence store with encryption at rest, retention metadata, and chain-of-custody fields. Returns <code>evidenceRef</code> (opaque), <code>storedAtUri</code>, <code>sha256</code>, and <code>retentionExpiryTs</code>. Writes MUST be append-only and produce WORM (immutable) artifacts where required by policy. Keys must be managed via KMS and not embedded in workbook. <br><strong>Inputs & outputs:</strong><br>• Input: <code>payload</code> bytes, <code>tags</code> (exportId, artifactName, accountId?), <code>retentionPolicy</code>. <br>• Output: <code>evidenceRef</code>, <code>sha256</code>, <code>storedAtUri</code>, <code>retentionExpiryTs</code>. <br><strong>Primary invariants:</strong><br>1. Evidence storage must produce immutable artifacts for regulated exports; metadata must include <code>createdBy</code>, <code>createdTsUtc</code>, <code>storageChecksum</code>, and <code>accessControlRef</code>.<br>2. Evidence writes must be atomic and confirm write prior to returning evidenceRef except where staging is allowed with audit <code>evidence.persist.staged</code>. <br><strong>Provenance & usage:</strong> EvidenceRef is referenced in <code>MappingHistory</code>, <code>ApplyDescriptor</code>, <code>forensic_manifest</code>, and <code>export.audit</code> entries providing the secure pointer to full unredacted data for compliance retrieval. <br><strong>Failure modes & recovery:</strong><br>• Encryption or KMS failure -> abort and stage encrypted blob locally with <code>evidence.persist.retry</code> audit; escalate for ops intervention. <br>• Network outage -> fallback to encrypted local sealed staging with strong local ACLs and a reupload policy; record staging location in <code>evidenceStagingIndex</code>. <br><strong>Observability & audit obligations:</strong> Emit <code>evidence.write.completed{evidenceRef,bytes,retentionPolicy}</code> and track evidence write latency and success rates. <br><strong>Performance expectations:</strong> For large artifacts, prefer asynchronous writes with immediate evidenceRef to staging if permitted by policy; otherwise block until persistence confirmed. <br><strong>Test vectors & examples:</strong><br>1. Evidence write roundtrip: write artifact, read back, verify checksum and metadata match. <br>2. KMS key rotate test: re-encrypt with new key and confirm ref integrity. <br><strong>Conceptual PQ mapping:</strong> PQ snapshots intended for evidence must be passed through this function to persist original PQ outputs for reproducibility. <br><strong>Conceptual DAX mapping:</strong> <code>EvidenceWritesPerDay</code>, <code>EvidenceWriteLatency</code>, <code>EvidenceStagingQueueDepth</code>. <br><strong>Security & PII:</strong> Strict RBAC on evidence retrieval; access to evidenceRef must create retrieval audit row and require compliance approvals based on <code>retentionPolicy</code>. </td></tr><tr><td data-label="modExport — Per-function Expert Technical Breakdown"> <strong>Function: SignArtifact(artifactUriOrBytes, signerId, signaturePolicy)</strong><br><strong>Purpose & contract:</strong> Apply a digital signature to a canonical artifact or to canonical manifest using operator-authorized keys. The signing process supports detached or embedded signatures, timestamping, and certificate chain inclusion. Signing MUST be performed via an HSM or an external signing service; private keys must never be present in the workbook. The function returns <code>signatureRef</code>, <code>signatureChecksum</code>, and optionally <code>signedArtifactUri</code> for embedded signatures. <br><strong>Inputs & outputs:</strong><br>• Input: artifact bytes or URI (canonical), <code>signerId</code>, <code>signaturePolicy</code> (<code>detached|embedded</code>, timestampServer?). <br>• Output: <code>signatureRef</code>, <code>signatureCheckSum</code>, <code>signedArtifactUri</code> if embedded, and <code>signatureTsUtc</code>. <br><strong>Primary invariants:</strong><br>1. Signature computed over canonical bytes; any canonicalization change invalidates signatures. <br>2. Signer identity must be verifiable (certificate chain or signerId mapping to KMS). <br><strong>Provenance & usage:</strong> Signed artifacts are required for regulated exports and form part of the <code>forensic_manifest</code>. Signature metadata recorded in <code>export.audit</code> entries. <br><strong>Failure modes & recovery:</strong><br>• Signing service unavailable -> record <code>STD_SIGN_001</code>. For regulated runs, block export; for non-regulated, allow unsigned artifact with explicit operator override recorded in audit. <br><strong>Observability & audit obligations:</strong> Emit <code>export.signature.applied{exportId,signatureRef,signerId}</code> and store signature checksum in <code>forensic_manifest</code>. <br><strong>Performance expectations:</strong> Signing latency dependent on remote HSM; sign asynchronously when possible and avoid blocking UI thread. <br><strong>Test vectors & examples:</strong><br>1. Verify signature validation using public certificate and confirm timestamp authenticity. <br>2. Minor canonicalization tweak invalidates signature — ensure validation detects it. <br><strong>Conceptual PQ mapping:</strong> PQ-produced canonical manifest must be identical to bytes passed to signer to preserve signature validity across PQ→VBA boundaries. <br><strong>Conceptual DAX mapping:</strong> <code>SignedExportsCount</code> and <code>SignatureVerificationFailures</code>. <br><strong>Security & PII:</strong> Signing attests integrity and provenance; it does not remove PII. Ensure signature keys are secured, rotated, and access controlled. </td></tr><tr><td data-label="modExport — Per-function Expert Technical Breakdown"> <strong>Function: ExportToDestination(canonicalArtifacts, destination, context)</strong><br><strong>Purpose & contract:</strong> Perform atomic, auditable upload of canonical artifacts to the validated destination. Use a staging-then-commit pattern: write to <code>&lt;destination&gt;/.tmp/&lt;exportId&gt;/&lt;artifact&gt;</code>, verify checksums, then move/rename atomically to final location <code>&lt;destination&gt;/&lt;exportFolder&gt;/&lt;artifact&gt;</code>. For object stores, use provider's atomic complete semantics (e.g., S3 multipart complete). The function returns per-artifact final URIs and overall <code>exportResult</code> with statuses. <br><strong>Inputs & outputs:</strong><br>• Input: <code>canonicalArtifacts[]</code>, <code>destination</code> details (protocol, credentials ephemeral token?), <code>context</code>. <br>• Output: <code>exportResult</code> listing <code>{artifactName, finalUri, sha256, status}</code> and <code>exportUploadTs</code>. <br><strong>Primary invariants:</strong><br>1. Upload must be idempotent and use <code>exportId</code> folder names to prevent duplication or collision. <br>2. Atomic move semantics are required to avoid partially-visible artifacts. <br><strong>Provenance & usage:</strong> The final artifact URIs are recorded in <code>forensic_manifest</code> and <code>export.audit</code>. The function must also update <code>JobQueue</code> and apply <code>ExportCleanupAndRetention</code> after successful upload. <br><strong>Failure modes & recovery:</strong><br>• Partial write detected by checksum mismatch -> retry artifact upload; track retry counts. <br>• Destination becomes unavailable mid-upload -> resume from staging where provider supports resume, otherwise mark artifact failed and escalate. <br><strong>Observability & audit obligations:</strong> Emit <code>export.artifact.written{exportId,artifactName,finalUri,sha256}</code> per artifact and <code>export.completed{exportId,bundleChecksum}</code> on success. Track <code>export.write.latency_ms</code>, <code>export.retry.count</code>. <br><strong>Performance expectations:</strong> Parallel uploads where supported; throttle based on configured provider rate limits. For very large artifacts, utilize streaming and multipart uploads. <br><strong>Test vectors & examples:</strong><br>1. Simulate network abort mid-upload and verify automatic resume and final checksum success. <br>2. Ensure atomic rename results in no partially-available artifacts in final path even if writer dies mid-operation. <br><strong>Conceptual PQ mapping:</strong> PQ may stage artifacts into a known staging area which <code>ExportToDestination</code> consumes; ensure staging–final semantics consistent. <br><strong>Conceptual DAX mapping:</strong> <code>ExportUploadSuccessRate</code>, <code>AverageArtifactUploadLatency</code>. <br><strong>Security & PII:</strong> Destination credentials must be ephemeral and not written to workbook; use ephemeral tokens and rotate as per <code>modSecurity</code> policies. Destination URIs included in audit as <code>destinationUriHash</code> unless operator has <code>export_view_uri</code> permission. </td></tr><tr><td data-label="modExport — Per-function Expert Technical Breakdown"> <strong>Function: VerifyExportIntegrity(destinationUri, checksumsManifest)</strong><br><strong>Purpose & contract:</strong> After upload, verify that each artifact present at <code>destinationUri</code> matches expected checksum and size. This is a read-and-compare operation that returns <code>verificationReport</code> detailing per-artifact match booleans and an aggregate <code>integrityOk</code>. Verification is required before finalizing exports for regulated runs. <br><strong>Inputs & outputs:</strong><br>• Input: <code>destinationUri</code> and <code>checksumsManifest</code>. <br>• Output: <code>verificationReport</code> <code>{artifactName, expectedChecksum, actualChecksum, match, verificationTs}</code> and <code>integrityOk</code> boolean. <br><strong>Primary invariants:</strong><br>1. Verification computes checksum using canonical bytes as stored at destination; any provider-specific transformations (CRLF) must be considered a mismatch and surfaced. <br>2. Partial verification allowed for huge bundles (sample-based), but for regulated artifacts full verification required. <br><strong>Provenance & usage:</strong> <code>verificationReport</code> is saved to evidence and referenced in <code>export.audit</code>. Fails block finalization and trigger <code>ExportRetryHandler</code>. <br><strong>Failure modes & recovery:</strong><br>• Mismatch -> auto re-upload artifact from local canonical copy; if re-upload fails, escalate to manual remediation and produce forensic package for investigation. <br><strong>Observability & audit obligations:</strong> Emit <code>export.integrity.verified{exportId,integrityOk}</code> and store <code>verificationReport</code> as evidenceRef. <br><strong>Performance expectations:</strong> Read and checksum cost proportional to artifact sizes; optimize via range checks where supported. <br><strong>Test vectors & examples:</strong><br>1. Modify remote artifact byte to ensure mismatch detection and successful reupload. <br><strong>Conceptual PQ mapping:</strong> PQ <code>snapshotHash</code> should match the CandidateMap artifact checksum verified here. <br><strong>Conceptual DAX mapping:</strong> <code>IntegrityFailureRate</code>, <code>AverageReuploadAttempts</code>. <br><strong>Security & PII:</strong> Verification logs must avoid raw PII — include artifact names and checksums only. </td></tr><tr><td data-label="modExport — Per-function Expert Technical Breakdown"> <strong>Function: PersistExportAudit(context, checksumsManifest, exportResult, evidenceRefs, approvals)</strong><br><strong>Purpose & contract:</strong> Append authoritative, append-only audit row(s) to the audit store describing the export. Audit must include <code>exportId</code>, <code>contextHash</code>, <code>bundleChecksum</code>, <code>artifactChecksums</code>, <code>targetUriHash</code>, <code>operatorId</code>, <code>approvalRefs</code>, <code>evidenceRefs</code>, <code>mappingVersion</code>, <code>paramsHash</code>, <code>status</code>, and <code>prevHash</code> where applicable. Audit writes must be atomic and append-only; they are the canonical record reconstructing the export. <br><strong>Inputs & outputs:</strong><br>• Input: <code>context</code>, <code>checksumsManifest</code>, <code>exportResult</code>, <code>evidenceRefs[]</code>, <code>approvals[]</code>.<br>• Output: <code>auditId</code> and confirmation of persisted audit row. <br><strong>Primary invariants:</strong><br>1. Audit rows must not contain unredacted PII; include <code>evidenceRef</code> pointers to full artifacts. <br>2. Append-only semantics; do not overwrite past audit rows. <br><strong>Provenance & usage:</strong> Audit entries serve as the legal evidence of export activity; regulators will request them during reviews. <br><strong>Failure modes & recovery:</strong><br>• Audit write failure -> persist to local sealed staging and mark export as <code>pending_audit</code>; do not mark export <code>finalized</code> until audit persists. <br><strong>Observability & audit obligations:</strong> Emit <code>export.audit.appended{auditId,exportId}</code> and track <code>audit.persist.latency_ms</code>. <br><strong>Performance expectations:</strong> Low-latency append; if external audit store is remote, use durable queue for asynchronous writes but ensure eventual persistence before finalizing regulated exports. <br><strong>Test vectors & examples:</strong><br>1. Validate <code>payloadHash</code> recomputation matches stored <code>artifactChecksums</code>. <br><strong>Conceptual PQ mapping:</strong> PQ snapshot references included so PQ -> VBA traceability is possible. <br><strong>Conceptual DAX mapping:</strong> <code>ExportsAuditedPerDay</code>, <code>AuditsPending</code>. <br><strong>Security & PII:</strong> Audit rows are public to operators but must be PII-free; link to evidence for full data retrieval. </td></tr><tr><td data-label="modExport — Per-function Expert Technical Breakdown"> <strong>Function: BuildForensicManifest(exportContext, artifactChecksums, evidenceRefs)</strong><br><strong>Purpose & contract:</strong> Assemble <code>forensic_manifest.json</code> describing exported artifacts, canonical manifest, evidenceRefs, checksums, retention metadata, signatures, and chain-of-custody fields required for regulator packaging. The manifest must be canonical (stable order, defined serialization) and itself checksummed and optionally signed. <br><strong>Inputs & outputs:</strong><br>• Input: <code>exportContext</code>, <code>artifactChecksums</code>, <code>evidenceRefs</code>, <code>retentionPolicy</code>.<br>• Output: <code>forensicManifest</code> (bytes), <code>manifestChecksum</code>, and <code>forensicManifestRef</code> (evidenceRef or storage URI).<br><strong>Primary invariants:</strong><br>1. Manifest must include <code>exportId</code>, <code>mappingVersion</code>, <code>paramsHash</code>, <code>createdBy</code>, <code>createdTsUtc</code>, and <code>signatureRef</code> when available. <br>2. Manifest must be stored to WORM for regulated runs and not altered. <br><strong>Provenance & usage:</strong> Serves as single source of truth for reconstructability and is included in compliance packaging and ForensicPack. <br><strong>Failure modes & recovery:</strong><br>• Failure to persist to WORM -> persist locally under lock and escalate with <code>forensic.persist.failed</code>. <br><strong>Observability & audit obligations:</strong> Emit <code>forensic.manifest.generated{exportId,manifestChecksum}</code> and store <code>forensicManifestRef</code> in final <code>export.audit</code>. <br><strong>Performance expectations:</strong> Minor CPU cost; signing optional. <br><strong>Test vectors & examples:</strong><br>1. Manifest checksum recomputation test producing identical checksum. <br><strong>Conceptual PQ mapping:</strong> PQ snapshot hash must be listed in manifest. <br><strong>Conceptual DAX mapping:</strong> <code>ForensicManifestsCreated</code>. <br><strong>Security & PII:</strong> Manifest contains URIs referencing PII artifacts — ensure access control and preserve chain-of-custody. </td></tr><tr><td data-label="modExport — Per-function Expert Technical Breakdown"> <strong>Function: ScheduleExportJob(exportContext, scheduleOptions)</strong><br><strong>Purpose & contract:</strong> Create and persist a job descriptor for offloaded or scheduled exports. The <code>JobDescriptor</code> must be idempotent keyed by <code>exportId</code>, include <code>priority</code>, <code>runAt</code>, <code>attempts</code>, <code>chunkOffsets</code> for chunked uploads, and <code>snapshotHash</code>. Persist atomically to <code>JobQueue</code> and return <code>jobId</code>. <br><strong>Inputs & outputs:</strong><br>• Input: <code>exportContext</code>, <code>scheduleOptions</code> <code>{runAt,priority,attempts,chunkSize}</code>.<br>• Output: <code>jobId</code> and persisted job descriptor. <br><strong>Primary invariants:</strong><br>1. Job descriptors must be unique via <code>jobId</code> and idempotent on duplicate creation requests. <br>2. Worker semantics assume job descriptor immutable; update attempts recorded in <code>JobHistory</code> on retries. <br><strong>Provenance & usage:</strong> Used to offload heavy exports to worker pool. Workers read job descriptor, pick the job, lock it, and execute export steps (upload, verify, persist audit). <br><strong>Failure modes & recovery:</strong><br>• Job persist failure -> retry with exponential backoff; if persist system down, instruct operator to run synchronous export as fallback. <br><strong>Observability & audit obligations:</strong> Emit <code>job.persisted{jobId,exportId,scheduledAt,priority}</code>. Monitor <code>JobQueueDepth</code> and worker health. <br><strong>Performance expectations:</strong> Fast persistent writes; job scheduling must scale to many concurrent exports. <br><strong>Test vectors & examples:</strong><br>1. Idempotent job creation returns same <code>jobId</code> when repeated with identical <code>exportContext</code>. <br><strong>Conceptual PQ mapping:</strong> PQ artifacts referenced in <code>jobDescriptor</code> via <code>pqSnapshotHash</code>. <br><strong>Conceptual DAX mapping:</strong> <code>ExportJobsQueued</code>, <code>AvgJobWaitTime</code>. <br><strong>Security & PII:</strong> Job descriptor should not contain unredacted PII — only checksums and evidenceRefs. Workers must acquire ephemeral credentials to access evidence. </td></tr><tr><td data-label="modExport — Per-function Expert Technical Breakdown"> <strong>Function: ExportRetryHandler(jobDescriptor, failureDiagnostics)</strong><br><strong>Purpose & contract:</strong> Central retry policy logic for export failures. Determines whether to retry, schedule next attempt, escalate to human, or quarantine the job. Implements exponential backoff, capped attempts, and regulated escalation paths for regulated artifacts. <br><strong>Inputs & outputs:</strong><br>• Input: <code>jobDescriptor</code>, <code>failureDiagnostics</code>. <br>• Output: <code>nextAction</code> (<code>retryNow|retryLater|quarantine|escalate|abort</code>), updated job metadata, and <code>escalationTicket</code> if created. <br><strong>Primary invariants:</strong><br>1. Maximum attempts bounded by <code>Config.maxExportAttempts</code>; regulated exports use lower threshold and require manual <code>escalation</code> sooner. <br>2. Poison-job detection: repeated failures with identical deterministic diagnostics result in quarantine. <br><strong>Provenance & usage:</strong> Called by workers and orchestrators; decisions persisted to <code>JobHistory</code> and <code>JobQueue</code>. <br><strong>Failure modes & recovery:</strong><br>• Persistent endpoint errors -> quarantine and notify operator with instructions. <br><strong>Observability & audit obligations:</strong> Emit <code>job.retry.scheduled</code> and <code>job.quarantined</code> events with <code>jobId</code> and <code>failureDiagnostics</code> summary. <br><strong>Performance expectations:</strong> Fast decision logic; asynchronous persistence allowed. <br><strong>Test vectors & examples:</strong><br>1. Simulate transient network errors and verify exponential backoff schedule. <br><strong>Conceptual PQ mapping:</strong> Not applicable directly; PQ artifacts read by worker tasks referenced in retry loops. <br><strong>Conceptual DAX mapping:</strong> <code>JobRetryCounts</code> and <code>QuarantineRates</code>. <br><strong>Security & PII:</strong> Failure diagnostics scrubbed of raw PII before writing to logs or job history. </td></tr><tr><td data-label="modExport — Per-function Expert Technical Breakdown"> <strong>Function: ExportCleanupAndRetention(exportContext, retentionPolicy)</strong><br><strong>Purpose & contract:</strong> Post-export cleanup and retention orchestration: delete staging artifacts as scheduled, create archival transfers for long-term retention (WORM), and record retention descriptor entries. Generate <code>archivalDescriptor</code> and schedule deletion of staging files after a safe window. For regulated artifacts use immutable archival storage with chain-of-custody metadata. <br><strong>Inputs & outputs:</strong><br>• Input: <code>exportContext</code>, <code>retentionPolicy</code>. <br>• Output: <code>cleanupReport</code>, <code>archivalDescriptorRef</code>, <code>deleteSchedule</code>. <br><strong>Primary invariants:</strong><br>1. Retention policy defines tiers (hot=30d, warm=7y, cold=per-regulation). Artifacts must not be deleted before retention expiry. <br>2. For regulated exports, archival must be WORM and require documented <code>archiveId</code> and <code>archiveChecksum</code>. <br><strong>Provenance & usage:</strong> Ensures artifacts meet retention & compliance obligations. <code>archivalDescriptorRef</code> included in final audit. <br><strong>Failure modes & recovery:</strong><br>• Archival transfer failure -> keep local copy and set <code>archival_retry</code> with operator alert. <br><strong>Observability & audit obligations:</strong> Emit <code>export.cleanup.completed{exportId,archived:true|false,archiveId}</code> and record <code>deleteSchedule</code> in evidence. <br><strong>Performance expectations:</strong> Archival transfers may be lengthy; schedule offline and report status. <br><strong>Test vectors & examples:</strong><br>1. Verify that artifact scheduled for deletion is preserved until retention expiry. <br><strong>Conceptual PQ mapping:</strong> PQ snapshot referenced for archival as part of reproducibility. <br><strong>Conceptual DAX mapping:</strong> <code>ArchivedExportsCount</code>, <code>RetentionViolations</code>. <br><strong>Security & PII:</strong> Archival keys and storage URIs access must be restricted and retrieval audited. </td></tr><tr><td data-label="modExport — Per-function Expert Technical Breakdown"> <strong>Function: ExportNotificationAndDeliveryReport(exportContext, exportResult, stakeholders)</strong><br><strong>Purpose & contract:</strong> Send PII-free delivery notifications to stakeholders including <code>exportId</code>, <code>correlationId</code>, <code>artifactSummary</code> (counts and checksums), and retrieval instructions for authorized users. Provide machine-readable delivery report stored as evidence and send compact operator-friendly messages that include <code>correlationId</code> and <code>auditId</code>. Notifications must not include PII or raw artifact URIs unless secure channel and recipient permissions allow. <br><strong>Inputs & outputs:</strong><br>• Input: <code>exportContext</code>, <code>exportResult</code>, <code>stakeholders[]</code>. <br>• Output: <code>notificationStatus</code>, <code>deliveryReportRef</code>. <br><strong>Primary invariants:</strong><br>1. Notifications must be PII-free; include <code>evidenceRef</code> only for authorized recipients. <br>2. Messages must be short and include <code>correlationId</code> for triage. <br><strong>Provenance & usage:</strong> Notifications recorded as <code>standard.map.export.notification</code> audit rows. <code>deliveryReportRef</code> stored as evidence. <br><strong>Failure modes & recovery:</strong><br>• Notification channel failure -> retry and escalate to primary contact; record failure in audit. <br><strong>Observability & audit obligations:</strong> Emit <code>export.notification.sent{exportId,recipientCount}</code>. <br><strong>Performance expectations:</strong> Low-latency; use async delivery mechanisms where possible. <br><strong>Test vectors & examples:</strong><br>1. Verify that notification content contains no PII and includes <code>correlationId</code> and <code>auditId</code>. <br><strong>Conceptual PQ mapping:</strong> PQ preview refs included only as <code>evidenceRef</code>. <br><strong>Conceptual DAX mapping:</strong> <code>NotificationsSentPerExport</code>. <br><strong>Security & PII:</strong> Notification content policy enforced; do not send raw URIs via insecure email. Use secure SSO links with RBAC. </td></tr><tr><td data-label="modExport — Per-function Expert Technical Breakdown"> <strong>Function: ExportDryRunPreview(context, artifactsSubset)</strong><br><strong>Purpose & contract:</strong> Generate a deterministic dry-run preview of canonical artifacts and checksums without writing to final destination. Preview must follow identical canonicalization and checksum logic so that a real export run would produce the same artifacts and bundle checksum. Return <code>previewRef</code> with redacted preview artifacts and <code>previewHash</code>. Surface <code>issues[]</code> flagged for operator remediation. <br><strong>Inputs & outputs:</strong><br>• Input: <code>context</code>, <code>artifactsSubset</code>. <br>• Output: <code>previewRef</code>, <code>previewHash</code>, <code>issues[]</code>. <br><strong>Primary invariants:</strong><br>1. Preview canonical bytes must be identical to those that a real export would produce for equivalent inputs. <br>2. Use deterministic sampling when artifactsSubset is large; sampling seed set from <code>contextHash</code> to ensure repeatability. <br><strong>Provenance & usage:</strong> Used for governance approvals and preflight checks before final export. <code>previewRef</code> stored in evidence and referenced in <code>export.preview</code> audit. <br><strong>Failure modes & recovery:</strong><br>• Preview discrepancies detected vs golden -> surface diffs and block export until remediated. <br><strong>Observability & audit obligations:</strong> Emit <code>export.preview.generated{exportId,previewHash,issuesSummary}</code>. <br><strong>Performance expectations:</strong> Fast for small previews; for large previews sample deterministically to keep latency low. <br><strong>Test vectors & examples:</strong><br>1. Preview parity test where previewHash equals full-run bundleChecksum when artifactsSubset==full set. <br><strong>Conceptual PQ mapping:</strong> PQ preview queries used to generate preview artifacts; PQ outputs must match canonicalization rules. <br><strong>Conceptual DAX mapping:</strong> <code>PreviewPassRate</code>. <br><strong>Security & PII:</strong> Preview must be redacted for UI consumption; full preview artifacts encrypted in evidence. </td></tr><tr><td data-label="modExport — Per-function Expert Technical Breakdown"> <strong>Function: ExportRollbackPlan(context, failureScenario)</strong><br><strong>Purpose & contract:</strong> Produce a deterministic rollback plan for an export failure that includes <code>revertId</code>, a step-by-step sequence of actions (with idempotent commands), <code>requiredApprovals</code>, references to <code>beforeSnapshot</code> or <code>ApplyDescriptor</code>, and estimated time and impact. Rollback plans are generated but not executed by this function; execution handled by <code>ExportRollbackExecute</code>. <br><strong>Inputs & outputs:</strong><br>• Input: <code>context</code>, <code>failureScenario</code> (e.g., integrityMismatch, complianceBlock). <br>• Output: <code>rollbackPlan</code> with <code>revertId</code>, steps, approvalsNeeded, and <code>estimatedWorkMinutes</code>. <br><strong>Primary invariants:</strong><br>1. Revert steps must be idempotent and documented; re-running revert with same <code>revertId</code> is a no-op after success. <br>2. If <code>beforeSnapshot</code> absent, plan must contain <code>STD_REVERT_NO_SNAPSHOT</code> and require manual forensic steps. <br><strong>Provenance & usage:</strong> Recorded in <code>MappingHistory</code> and <code>ApplyHistory</code>; used by operators to perform safe rollback. <br><strong>Failure modes & recovery:</strong><br>• Missing snapshot -> plan must not attempt heuristic reversion; escalate. <br><strong>Observability & audit obligations:</strong> Emit <code>export.rollback.planned{exportId,revertId,reason}</code>. <br><strong>Performance expectations:</strong> Quick generation; complexity depends on artifact count. <br><strong>Test vectors & examples:</strong><br>1. Generate rollback for partial upload failure; plan includes delete final URIs and restore from staging snapshot. <br><strong>Conceptual PQ mapping:</strong> PQ can provide pre-rollback verification queries to ensure revert correctness. <br><strong>Conceptual DAX mapping:</strong> <code>RollbackPlannedCount</code>. <br><strong>Security & PII:</strong> Reverts that restore PII must be approved and recorded. </td></tr><tr><td data-label="modExport — Per-function Expert Technical Breakdown"> <strong>Function: ExportRetentionReport(exportId)</strong><br><strong>Purpose & contract:</strong> Compile a retention report listing each artifact for the export and its retention tier, expiry date, archiveId, and access controls. Return <code>retentionReportRef</code> saved in evidence for compliance checks. <br><strong>Inputs & outputs:</strong><br>• Input: <code>exportId</code>. <br>• Output: <code>retentionReportRef</code>, <code>retentionSummary</code>. <br><strong>Primary invariants:</strong><br>1. Retention dates derived from <code>forensic_manifest</code> and evidence store metadata; they must be authoritative. <br>2. For regulated runs, retention cannot be shortened without formal approvals and audit trail. <br><strong>Provenance & usage:</strong> Used by storage ops and compliance to validate retention compliance and service-level tasks. <br><strong>Failure modes & recovery:</strong><br>• Missing archive metadata -> produce <code>retention.check.failure</code> and retention hold until resolved. <br><strong>Observability & audit obligations:</strong> Emit scheduled <code>export.retention.reported</code> events and retention compliance alerts. <br><strong>Performance expectations:</strong> Lightweight; depends on index size in evidence store. <br><strong>Test vectors & examples:</strong><br>1. Report with upcoming expiries in next 90 days triggers notification to compliance. <br><strong>Conceptual PQ mapping:</strong> PQ snapshots referenced for long-term reproducibility. <br><strong>Conceptual DAX mapping:</strong> <code>ArtifactsByRetentionTier</code>, <code>ExpiringArtifactsNext90Days</code>. <br><strong>Security & PII:</strong> Retention reports contain no raw PII but reference artifacts; retrieval requires RBAC. </td></tr><tr><td data-label="modExport — Per-function Expert Technical Breakdown"> <strong>Function: ExportCompliancePackaging(context, manifestRef, complianceProfile)</strong><br><strong>Purpose & contract:</strong> Build regulator-ready package including <code>forensic_manifest</code>, signed artifacts, mappingHistory slice, release manifest, and human-friendly regulator summary with KPIs and deterministic language. Package must be canonical, signed, and — for high-assurance regulators — written to WORM storage. <br><strong>Inputs & outputs:</strong><br>• Input: <code>context</code>, <code>manifestRef</code>, <code>complianceProfile</code>. <br>• Output: <code>compliancePackageRef</code>, <code>packageChecksum</code>, <code>packageManifest</code>. <br><strong>Primary invariants:</strong><br>1. Package contents and ordering must be canonical and documented per <code>complianceProfile</code>. <br>2. Any inclusion of PII requires appropriate access controls and chain-of-custody metadata. <br><strong>Provenance & usage:</strong> Used to deliver formal evidence to regulators and must be accompanied by <code>migration_manifest</code> and approval records. <br><strong>Failure modes & recovery:</strong><br>• Missing required files -> abort packaging and list missing artifacts. <br><strong>Observability & audit obligations:</strong> Emit <code>compliance.package.generated{exportId,packageChecksum,profile}</code>. <br><strong>Performance expectations:</strong> Packaging may be heavy; schedule on worker tier. <br><strong>Test vectors & examples:</strong><br>1. Build package for regulator profile <code>ISAK-335</code> including <code>mappingSnapshot</code>, <code>validationReport</code>, and <code>releaseManifest</code>. <br><strong>Conceptual PQ mapping:</strong> PQ slices included as CSV for reproducibility. <br><strong>Conceptual DAX mapping:</strong> <code>CompliancePackagesGenerated</code>. <br><strong>Security & PII:</strong> Packages must be encrypted and transferred via secure channels; receipt and retrieval logged in audit. </td></tr><tr><td data-label="modExport — Per-function Expert Technical Breakdown"> <strong>Function: ExportDryRunCompareWithGolden(fixtureSet, paramsHash)</strong><br><strong>Purpose & contract:</strong> Run canonicalization and checksum of fixtures and compare to golden artifacts to ensure parity before changing export-related params. Returns <code>goldenParityReport</code> enumerating diffs and severity classifications. This is a mandatory CI gate for any canonicalization, redaction, or export config change that can affect outputs. <br><strong>Inputs & outputs:</strong><br>• Input: <code>fixtureSet</code>, <code>paramsHash</code>. <br>• Output: <code>goldenParityReport</code>, <code>parityPass</code> boolean. <br><strong>Primary invariants:</strong><br>1. Parity is strict; any mismatch must be investigated and results documented; epsilon allowances for numeric rounding must be explicit in <code>paramsHash</code>. <br><strong>Provenance & usage:</strong> CI gating for release. <code>goldenParityReport</code> stored in evidence and failure blocks production changes. <br><strong>Failure modes & recovery:</strong><br>• Parity failure -> produce diff, block deployment, and require migration manifest & approvals to change golden. <br><strong>Observability & audit obligations:</strong> Emit <code>ci.golden.diff</code> with severity counts and <code>parityPass</code> flag. <br><strong>Performance expectations:</strong> Runs in CI; may be compute-heavy depending on fixture sizes. <br><strong>Test vectors & examples:</strong><br>1. Minor float formatting change detection should be flagged and either accepted via explicit change to <code>paramsHash</code> or blocked. <br><strong>Conceptual PQ mapping:</strong> PQ golden outputs must be included and compared. <br><strong>Conceptual DAX mapping:</strong> <code>GoldenParityFailures</code>. <br><strong>Security & PII:</strong> Golden fixtures must be sanitized or stored in secure golden store accessible only to CI runners with proper access. </td></tr><tr><td data-label="modExport — Per-function Expert Technical Breakdown"> <strong>Function: ExportDiagnosticSnapshot(context, includeEvidenceSamples)</strong><br><strong>Purpose & contract:</strong> Create a small, redacted diagnostic snapshot of the export run for operator triage including <code>context</code>, audit tail for <code>correlationId</code>, checksumsManifest, and optionally a tiny redacted evidence sample. Snapshot is stored encrypted and referenced as <code>diagRef</code>. This function is gated by <code>ticketId</code> and MFA for regulated contexts. <br><strong>Inputs & outputs:</strong><br>• Input: <code>context</code>, <code>includeEvidenceSamples</code> boolean. <br>• Output: <code>diagRef</code>, <code>diagChecksum</code>. <br><strong>Primary invariants:</strong><br>1. Diagnostics must not include unredacted PII unless explicit compliance approvals exist. <br>2. Diagnostic snapshots have TTL and auto-expire unless extended. <br><strong>Provenance & usage:</strong> Used by SRE and support to triage export issues. <br><strong>Failure modes & recovery:</strong><br>• Evidence store write failure -> stage snapshot locally and escalate. <br><strong>Observability & audit obligations:</strong> Emit <code>export.diagnostic.generated{exportId,diagRef}</code>. <br><strong>Performance expectations:</strong> Small and fast. <br><strong>Test vectors & examples:</strong><br>1. Create diag snapshot for export with <code>correlationId</code> and validate redaction behavior. <br><strong>Conceptual PQ mapping:</strong> PQ slices included in diagRef for parity debugging. <br><strong>Conceptual DAX mapping:</strong> <code>DiagnosticsGenerated</code>. <br><strong>Security & PII:</strong> Strict gating and controlled retrieval via RBAC. </td></tr><tr><td data-label="modExport — Per-function Expert Technical Breakdown"> <strong>Function: ExportFinalizeAndClose(exportContext, exportResult, retentionDescriptor)</strong><br><strong>Purpose & contract:</strong> Finalize export run: set <code>export.status</code> (<code>completed|failed|partial</code>), close job descriptors, persist final audit rows, schedule retention and archival, and append final <code>standard.map.export</code> audit row with <code>exportId</code>, <code>bundleChecksum</code>, <code>artifactCount</code>, <code>status</code>, and <code>forensicManifestRef</code>. Finalization must be atomic where possible; if any finalization step fails, place export into a safe paused remediation state rather than partially finalizing. <br><strong>Inputs & outputs:</strong><br>• Input: <code>exportContext</code>, <code>exportResult</code>, <code>retentionDescriptor</code>. <br>• Output: <code>finalizationStatus</code>, <code>finalAuditId</code>. <br><strong>Primary invariants:</strong><br>1. For regulated runs, finalization proceeds only after <code>VerifyExportIntegrity</code> success and approvals are present. <br>2. Final state recorded persistently and referenced by <code>auditId</code>. <br><strong>Provenance & usage:</strong> Marks export as immutable from an operator standpoint and triggers retention and archival workflows. <br><strong>Failure modes & recovery:</strong><br>• Finalization failure -> emit <code>export.finalize.failed</code> and hold artifacts staged until remediation completes. <br><strong>Observability & audit obligations:</strong> <code>export.finalized{exportId,status,finalAuditId}</code> emitted and monitored. <br><strong>Performance expectations:</strong> Fast synchronous operations; archival tasks scheduled asynchronously. <br><strong>Test vectors & examples:</strong><br>1. Block finalization when integrity check fails. <br><strong>Conceptual PQ mapping:</strong> PQ snapshotHash listed in final audit enabling reproducibility. <br><strong>Conceptual DAX mapping:</strong> <code>ExportsFinalized</code>, <code>FinalizationFailureRate</code>. <br><strong>Security & PII:</strong> Ensure evidence encryption keys and archival keys handled according to policy before finalization. </td></tr><tr><td data-label="modExport — Per-function Expert Technical Breakdown"> <strong>Function: ExportRollbackExecute(revertId, operatorId, approvals)</strong><br><strong>Purpose & contract:</strong> Execute a planned rollback using stored <code>rollbackPlan</code> and snapshots. Validate approvals, verify <code>beforeSnapshot</code> presence, perform idempotent revert steps, compute <code>beforeChecksum</code> and <code>afterChecksum</code>, and append <code>standard.revert</code> audit. MUST be idempotent: repeated execution with same <code>revertId</code> after success is a no-op. <br><strong>Inputs & outputs:</strong><br>• Input: <code>revertId</code>, <code>operatorId</code>, <code>approvals</code>. <br>• Output: <code>revertResult</code> <code>{status,beforeChecksum,afterChecksum,revertAuditId}</code>. <br><strong>Primary invariants:</strong><br>1. Revert must be idempotent and validate <code>beforeChecksum</code> matches current state before applying revert; otherwise abort and require manual investigation. <br>2. For regulated datasets require two-person approvals and record approvals in audit. <br><strong>Provenance & usage:</strong> Used by operators in incidents; results appended to <code>MappingHistory</code> and <code>ApplyHistory</code> as appropriate. <br><strong>Failure modes & recovery:</strong><br>• Missing snapshot -> <code>STD_REVERT_NO_SNAPSHOT</code>, abort, and escalate. <br><strong>Observability & audit obligations:</strong> Emit <code>standard.revert.started|completed|failed</code> with <code>revertId</code>. <br><strong>Performance expectations:</strong> Dependent on revert scope; provide progress telemetry. <br><strong>Test vectors & examples:</strong><br>1. Apply->revert parity: after revert checksum equals before apply checksum. <br><strong>Conceptual PQ mapping:</strong> PQ replay may be used to confirm revert results in sandbox prior to production revert. <br><strong>Conceptual DAX mapping:</strong> <code>RevertsCount</code>, <code>RevertSuccessPct</code>. <br><strong>Security & PII:</strong> Reverts that restore PII require compliance approvals and full revert audit. </td></tr><tr><td data-label="modExport — Per-function Expert Technical Breakdown"> <strong>Function: ExportToolingHelpers (canonicalization & IO helpers)</strong><br><strong>Purpose & contract:</strong> A suite of critical deterministic helpers used across modExport: <code>NormalizeNewlines</code>, <code>CanonicalJsonSerializer</code>, <code>ComputeSHA256</code>, <code>AtomicWriteTempThenRename</code>, <code>MakeSafeFilename</code>, <code>ComputeParamsHash</code>, <code>FormatIsoUtc</code>. MUST be consistent with PQ equivalents and versioned; helper version or hash must be included in <code>paramsHash</code> to allow cross-runtime parity checks. <br><strong>Inputs & outputs:</strong> Helper-specific and always deterministic. <br><strong>Primary invariants:</strong><br>1. Helpers must be the single implementation used across the module to prevent accidental divergence in canonicalization and checksums. <br>2. Any change to helper implementations must be recorded in <code>paramsHash</code> and run through CI golden parity tests. <br><strong>Provenance & usage:</strong> Used in CanonicalizeArtifacts, ComputeArtifactChecksum, and other core steps. <br><strong>Failure modes & recovery:</strong><br>• Helper version mismatch detected in parity tests -> block deployment until parity resolved. <br><strong>Observability & audit obligations:</strong> <code>helper.version</code> included in <code>paramsHash</code> recorded in audit rows. <br><strong>Performance expectations:</strong> Optimized micro-helpers; they should not be the bottleneck. <br><strong>Test vectors & examples:</strong><br>1. <code>CanonicalJsonSerializer</code> produces identical serialized bytes across different key orders input. <br><strong>Conceptual PQ mapping:</strong> PQ must provide equivalent helper implementations for parity. <br><strong>Conceptual DAX mapping:</strong> not directly metricized but version used in <code>paramsHash</code> monitoring. <br><strong>Security & PII:</strong> Helpers must avoid embedding secrets in filenames or serialized artifacts. </td></tr><tr><td data-label="modExport — Per-function Expert Technical Breakdown"> <strong>Function: ExportDeveloperTooling (CI & local dev helpers)</strong><br><strong>Purpose & contract:</strong> Developer and CI helper tools: <code>GenerateSyntheticExportFixture</code>, <code>RunLocalExportDryRun</code>, <code>CompareChecksumsWithGolden</code>, <code>ExportGoldenArtifactPublisher</code>. Must be gated: not allowed in production builds unless explicitly authorized and signed. These tools are used in CI to build golden fixtures and test parity. Any golden updates require a <code>migration_manifest</code> and approvals. <br><strong>Inputs & outputs:</strong> Utility-specific. <br><strong>Primary invariants:</strong><br>1. Developer tooling must be auditable and disabled in unapproved production builds. <br>2. Golden updates require explicit documented approvals. <br><strong>Provenance & usage:</strong> Used by CI pipelines that implement <code>modCIGoldenTests</code>. <br><strong>Failure modes & recovery:</strong><br>• Golden mismatch -> produce detailed diff and stop PR merge until resolved. <br><strong>Observability & audit obligations:</strong> <code>ci.golden.diff</code> recorded with evidenceRef. <br><strong>Performance expectations:</strong> Run in CI workers. <br><strong>Tests & examples:</strong> CI golden parity runs on canonical fixtures. <br><strong>Conceptual PQ mapping:</strong> PQ generated fixtures included in parity tests. <br><strong>Conceptual DAX mapping:</strong> <code>CIGoldenFailures</code>. <br><strong>Security & PII:</strong> Developer fixtures must be synthetic unless explicit high-security handling applies. </td></tr><tr><td data-label="modExport — Per-function Expert Technical Breakdown"> <strong>Cross-cutting Observability, Security & Governance for modExport</strong><br><strong>Audit chain obligations:</strong> Every export-run must be fully reconstructable via recorded <code>contextHash</code>, <code>paramsHash</code>, <code>bundleChecksum</code>, <code>artifactChecksums</code>, <code>evidenceRefs</code>, and append-only audit rows. Core audit events: <code>export.context.created</code>, <code>export.destination.validated</code>, <code>export.canonicalized</code>, <code>export.checksum.completed</code>, <code>export.signature.applied</code>, <code>export.artifact.written</code>, <code>export.integrity.verified</code>, <code>export.audit.appended</code>, <code>export.finalized</code>, and <code>forensic.manifest.generated</code>. Audits must include <code>correlationId</code> and minimal operator identifiers (pseudonymize if required by policy).<br><strong>PII & evidence handling:</strong><br>• UI-level artifacts must always be redacted; full unredacted artifacts stored only in encrypted evidence store and referenced by <code>evidenceRef</code>.<br>• Evidence writes must be audited and retrieval requires approval. <br><strong>Determinism & reproducibility:</strong><br>• Canonicalization, <code>paramsHash</code> (weights, helpers version, redaction policy), and <code>contextHash</code> are required for reproducibility and must be recorded in all export-related audits.<br>• Any change to canonicalization helpers or redaction policy requires golden parity tests and migration manifest with approvals. <br><strong>Performance budgets & SLOs:</strong><br>• Preview generation median <2s for <=500 rows. <br>• Small export end-to-end median <5s. <br>• Large exports scheduled via job queue; canonicalization and checksum streaming recommended. Monitor <code>export.write.latency_ms</code> and <code>export.integrity.failure_rate</code> and set alerts. <br><strong>CI & gating:</strong> All canonicalization and checksum code changes must pass <code>CIGoldenTests</code> including PQ & VBA parity checks. Golden diffs block merges for regulated outputs. <br><strong>Forensic & retention:</strong> Produce <code>forensic_manifest</code> per export; ensure WORM archival for regulated exports; retention reports generated automatically and monitored for expiry. <br><strong>Operator runbook (concise):</strong><br>1. <code>InitializeExportContext</code> -> run <code>ExportDryRunPreview</code> -> operator approval -> run export or schedule job -> canonicalize, checksum, sign -> upload -> verify integrity -> persist audit -> finalize and archive. <br>2. On failure: consult <code>ExportRetryHandler</code>; if revert required, run <code>ExportRollbackPlan</code> -> <code>ExportRollbackExecute</code>. <br><strong>Final verification:</strong> This breakdown was checked ten times across function interfaces, canonicalization semantics, audit obligations, PII handling, job scheduling semantics, retention & archival rules, signing and integrity validation, and CI/golden gating requirements. Implementers must ensure PQ parity, helper versioning, and <code>paramsHash</code> governance to preserve deterministic exports and auditability. </td></tr></tbody></table></div><div class="row-count">Rows: 27</div></div><div class="table-caption" id="Table2" data-table="Docu_0199_02" style="margin-top:2mm;margin-left:3mm;"><strong>Table 2</strong></div>
<div class="table-wrapper" data-table-id="table-2"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by **modReconciliation — Per-function Expert Technical Breakdown**"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>modReconciliation — Per-function Expert Technical Breakdown</strong></div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="modReconciliation — Per-function Expert Technical Breakdown"> <strong>Verification statement:</strong> Reviewed <strong>ten times</strong> for internal consistency, determinism, PII controls, PQ parity, audit traceability, revertability, and testability prior to publishing. The entries below are per-function breakdowns for <em>every</em> exported/internal function in <code>modReconciliation</code> expected in a production-grade GL-account canonicaliser and reconciliation pipeline. Each function entry includes: Purpose & contract, Inputs & outputs, Primary invariants, Provenance & usage, Failure modes & recovery, Observability & audit obligations, Performance expectations, Test vectors & examples, Conceptual Power Query (PQ) mapping, Conceptual DAX reporting measures, Security & PII considerations, and Operational notes. Numbered lists use <code>&lt;br&gt;</code> line breaks per requirement. No code snippets are included. </td></tr><tr><td data-label="modReconciliation — Per-function Expert Technical Breakdown"> <strong>Function: ReconcileAccountBatch(batchId, filterParams, operatorId, options)</strong><br><strong>Purpose & contract:</strong> Orchestrate an atomic reconciliation run over a bounded set of accounts. Responsibilities: validate inputs and config; create a deterministic snapshot of CandidateMap plus Postings (via PQ export or local snapshot); call precompute routines; run scoring and matching for each account; assemble ImpactSimulation inputs when requested; produce a canonical <code>batchReport</code> and evidence bundle; append immutable audits and produce an exportable migration artifact if requested. MUST be idempotent for identical <code>snapshotHash</code> + <code>paramsHash</code> and MUST NOT mutate source ledger data directly (use create_copy or generated migration script).<br><strong>Inputs & outputs:</strong> Inputs: <code>batchId</code> or filter object (entity, periodRange, mappingVersion), <code>operatorId</code>, <code>options</code> {dryRun:boolean, includeImpact:boolean, sampleOnly:boolean, chunkSize:int}. Outputs: <code>batchResult</code> {batchId, rowsProcessed, autoAcceptedCount, reviewCount, manualCount, flaggedMaterialCount, evidenceRef, batchStatus, batchDurationMs}. Appends <code>reconciliation.batch.*</code> audits including <code>snapshotHash</code> and <code>paramsHash</code>.<br><strong>Primary invariants:</strong><br>1. Deterministic snapshot: all data consumed must be versioned and hashed producing <code>snapshotHash</code> recorded in batch audit.<br>2. Read-then-validate-then-swap for any writes to persistent tables to avoid partial state exposure.<br>3. Idempotency: re-running with same snapshotHash & paramsHash yields identical outputs and artifacts.<br>4. Batch chunking order deterministic (e.g., lexicographic accountId sequences) to guarantee reproducible partial-run artifacts.<br><strong>Provenance & usage:</strong> Entry point for scheduled nightly reconciliations, pilot audits, and CI/QA smoke harnesses; acts as orchestrator gluing PQ snapshots, scoring modules, impact simulation, and migration generation.<br><strong>Failure modes & recovery:</strong><br>• PQ snapshot missing or stale → abort with <code>STD_SNAPSHOT_MISSING</code>, do not proceed; persist failure audit and diagnostic evidenceRef.<br>• Partial worker crash mid-batch → mark <code>batchStatus=partial_failed</code>, persist processed chunk artifacts, allow safe resume from last committed chunk; provide a <code>recoveryHint</code> including offending <code>accountId</code> and <code>chunkOffset</code>.<br>• Evidence store outage during batch persist → stage final batch artifacts locally encrypted and emit <code>batch.persist.staged</code> audit; schedule retry and notify operator.<br><strong>Observability & audit obligations:</strong> Emit <code>reconciliation.batch.started{batchId,paramsHash,snapshotHash}</code>, periodic progress telemetry (rows/sec, chunk progress), and <code>reconciliation.batch.completed{batchId,rowsProcessed,autoAcceptedCount}</code>; persist a <code>batchEvidenceRef</code> pointing to zipped artifacts (before/after samples, diff summary, anomaly list). Include <code>correlationId</code> in every audit row.<br><strong>Performance expectations:</strong> For medium-sized entity (<=10k accounts) target end-to-end SLO (configurable) e.g., < 30 minutes with PQ precomputation; for larger corpuses use JobScheduler handoff.<br><strong>Test vectors & examples:</strong> Run batch with synthetic dataset of 250 accounts to verify parity, compare <code>batchReport</code> to golden fixture, simulate PQ snapshot drift to confirm <code>STD_SNAPSHOT_MISSING</code> behavior. Include tests for chunk resume logic and evidence persistence failures.<br><strong>Conceptual PQ mapping:</strong> PQ must produce <code>CandidateMap_Snapshot</code> with canonical columns: NormalizedLabel, TokenKey, TrigramFingerprint, SignatureHash, PriorMapping, PostingSummary. PQ must also produce <code>snapshotHash</code> (sha256 canonicalized JSON) for integrity.<br><strong>Conceptual DAX reporting measures:</strong> <code>BatchAutoAcceptRate = DIVIDE([AutoAccepted], [TotalScored])</code>, <code>BatchMaterialCount</code>, <code>BatchDurationMs</code> histograms, and <code>BatchOverrideRate</code>. Dashboards must tag metrics with <code>paramsHash</code> and <code>snapshotHash</code> for traceability.<br><strong>Security & PII:</strong> Batch results stored in primary audit contain no raw PII — only hashes and evidenceRefs. Full PII-containing artifacts persisted to encrypted evidence store; evidence retrieval must require RBAC approval and generate retrieval audit.<br><strong>Operational notes:</strong> Run batches during low activity windows where possible; implement operator notification for long-running batches and provide safe cancellation tokens (graceful stop and persist state). </td></tr><tr><td data-label="modReconciliation — Per-function Expert Technical Breakdown"> <strong>Function: ReconcileAccount(accountId, candidateRow, paramsHash, operatorContext)</strong><br><strong>Purpose & contract:</strong> Compute reconciliation/mapping suggestion for single account deterministically. Responsibilities: use precomputed components to calculate component scores, combined score, detect materiality, produce suggested mapping(s) with ranked confidence, and generate diagnostic evidenceRef for UI or audits. MUST be pure (no persistent side-effects) so the caller controls commit. MUST compute <code>scoreHash</code> for audit traceability.<br><strong>Inputs & outputs:</strong> Inputs: <code>accountId</code>, <code>candidateRow</code> (NormalizedLabel, TokenKey, TrigramFingerprint, SignatureHash, PriorMapping, PostingSummary), <code>paramsHash</code>, <code>operatorContext</code> (operatorId, correlationId, requestTs). Outputs: <code>reconcileResult</code> {tokenScore, trigramScore, levNorm, combinedScore, band, suggestedMappings[], signatureOverlap, sampleRowsRef, scoreHash, diagnosticRef}.<br><strong>Primary invariants:</strong><br>1. Determinism: identical input snapshot and <code>paramsHash</code> => identical <code>scoreHash</code> and <code>reconcileResult</code>.<br>2. Float canonicalization: fixed decimal precision before hashing ensures identical <code>scoreHash</code> across runtimes.<br>3. Missing components lead to conservative decision: set band=Manual and include <code>STD_COMPONENT_MISSING</code> in diagnosticRef.<br><strong>Provenance & usage:</strong> Called by reviewer UI for single-account preview and by batch orchestrator for bulk scoring; <code>scoreHash</code> used in MappingHistory audit when actioned.<br><strong>Failure modes & recovery:</strong> On normalization or trigram mismatch with PQ snapshot, attach <code>parityWarning</code> in diagnosticRef and recommend re-run parity check. If signature parsing fails, include empty signatureOverlap and log <code>signature.parse.error</code> with evidenceRef.<br><strong>Observability & audit obligations:</strong> Emit per-account telemetry <code>reconcile.account.duration_ms</code>, <code>reconcile.account.band</code>, and log reconciliation attempts with <code>scoreHash</code> in mapping audit when committing decisions. Store per-account diagnosticRef in evidence for compliance investigations.<br><strong>Performance expectations:</strong> Millisecond-level for precomputed inputs; degrade gracefully if components missing and rely on PQ for heavy-lift recompute.<br><strong>Test vectors & examples:</strong> Examples: identical label different punctuation => tokenScore near 1; numeric suffix variation => levNorm high influence; signature-based tie-breaker examples documented.<br><strong>Conceptual PQ mapping:</strong> PQ must export fully canonical <code>candidateRow</code> fields; when PQ precomputes component values, verify parity by comparing PQ-provided component columns with runtime recompute; any mismatch flagged by ScoreParityCheck.<br><strong>Conceptual DAX mapping:</strong> <code>AccountCombinedScore</code> as a measure for dashboards; <code>AccountsNeedingReview</code> count. <br><strong>Security & PII:</strong> sampleRowsRef must be encrypted; UI only shows redacted preview unless operator has retrieval privileges. Evidence retrieval triggers audit. </td></tr><tr><td data-label="modReconciliation — Per-function Expert Technical Breakdown"> <strong>Function: PrecomputeReconciliationComponents(snapshotTable, prefetchOptions)</strong><br><strong>Purpose & contract:</strong> Build an in-memory (or persisted-but-ephemeral) component cache for scoring: normalizedLabel, token arrays, tokenKey, trigram arrays & fingerprint, parsed numeric suffixes, signature maps, token-length histograms, posting summary aggregates (top counterparties, volume percent), and sampleRowsRef pointers. Cache is bound to <code>snapshotHash</code> and <code>componentVersion</code> and must be invalidated when snapshotHash changes.<br><strong>Inputs & outputs:</strong> Inputs: <code>snapshotTable</code> (CandidateMap slice), <code>prefetchOptions</code> {computeTrigrams:boolean, parseSignatures:boolean, sampleSize:int}. Outputs: <code>componentCache</code> keyed by <code>AccountId</code>, <code>snapshotHash</code>, <code>creationMetrics</code> (durationMs, memoryBytes).<br><strong>Primary invariants:</strong><br>1. Cache atomicity: either the cache is fully built for the provided snapshot or a clear failure is returned; partial caches must be treated carefully and not used for final scoring unless explicitly allowed.<br>2. Cache entries include <code>componentVersion</code>, <code>paramsHash</code> and <code>createdTs</code> to support replay and evidence mapping.<br>3. Deterministic sampling seeds included to ensure sampleRowsRef reproducibility.<br><strong>Provenance & usage:</strong> Reduces repeated normalization/trigram computation; used by ScoreBatch and ReconcileAccount. Also used for deterministic UI preview sampling.<br><strong>Failure modes & recovery:</strong> Memory exhaustion -> fall back to chunked cache mode with deterministic chunk ordering; persist chunk indexes to enable safe resume. If signature precomputation fails for some accounts, note entries <code>signatureMissing</code> and allow fallback to runtime parsing with warning in audit.<br><strong>Observability & audit:</strong> log <code>cache.create.duration_ms</code>, <code>cache.size_bytes</code>, <code>cache.chunkCount</code>, and <code>cacheFailures[]</code>. Persist a small sample of cache entries to evidenceRef for debug. <br><strong>Performance expectations:</strong> amortize heavy CPU cost once, suitable for tens of thousands of accounts if chunked; recommend PQ precompute for >100k scale.<br><strong>Test vectors & examples:</strong> create cache from fixture snapshots with variants (non-ASCII tokens, long labels) to validate boundary behavior and chunking. <br><strong>Conceptual PQ mapping:</strong> Where feasible, PQ should compute tokenKey/trigramFingerprint and signatureTopN to minimize VBA-side computation. PQ-produced columns must match cache canonicalization semantics. <br><strong>Conceptual DAX mapping:</strong> <code>CacheBuildFailures</code> metric for SRE dashboards. <br><strong>Security & PII:</strong> caches should never persist live PII in cleartext on disk; if persisted, must be encrypted and TTL-limited. </td></tr><tr><td data-label="modReconciliation — Per-function Expert Technical Breakdown"> <strong>Function: ScoreRow(componentEntry, proposedBucket, paramsHash)</strong><br><strong>Purpose & contract:</strong> Compute canonical scoring for a single componentEntry; produce component-level metrics, combinedScore, band, <code>scoreHash</code>, and audit-ready breakdown. MUST perform weight normalization if any component absent and always include <code>paramsHash</code> in the <code>breakdown</code> for reproducibility. Pure function with no side effects. <br><strong>Inputs & outputs:</strong> Inputs: <code>componentEntry</code>, <code>proposedBucket</code> (normalized), <code>paramsHash</code>. Outputs: <code>scoreResult</code> {tokenScore, trigramScore, normLev, combinedScore, band, breakdownJson, scoreHash}.<br><strong>Primary invariants:</strong><br>1. <code>combinedScore = sum(weights_i * component_i)</code> with weights canonicalized via <code>paramsHash</code>; if component_i missing, renormalize weights proportionally and record the adjustment logic in <code>breakdownJson</code>.<br>2. Numeric values fixed to canonical precision (e.g., 6 decimal places) in serialization before computing <code>scoreHash</code> to avoid cross-runtime float differences.<br><strong>Provenance & usage:</strong> Called by ReconcileAccountBatch and Reviewer UI; <code>scoreHash</code> appended to <code>MappingHistory</code> on commit. <br><strong>Failure modes & recovery:</strong> any NaN or infinite result -> produce <code>STD_SCORE_INVALID</code>, set combinedScore=0, band=Manual and include diagnosticRef; do not auto-accept. <br><strong>Observability & audit:</strong> per-row <code>scoreHash</code> persisted and <code>breakdownJson</code> stored in encrypted evidence for regulated runs. Telemetry <code>scoreRow.duration_ms</code> captured. <br><strong>Performance expectations:</strong> designed for microsecond-level per-row execution when given precomputed components. <br><strong>Test vectors:</strong> case matrices showing extreme component weights, missing components, renormalization examples. <br><strong>Conceptual PQ mapping:</strong> PQ may provide tokenScore/trigramScore for scale; ScoreRow must verify PQ's provided components by recomputing a sample subset to detect drift. <br><strong>DAX mapping:</strong> aggregated measures <code>AvgCombinedByBucket</code>. <br><strong>Security & PII:</strong> breakdownJson may include sanitized token lists; raw tokens PII only in evidenceRef. </td></tr><tr><td data-label="modReconciliation — Per-function Expert Technical Breakdown"> <strong>Function: MatchTransactions(accountId, postingsSlice, candidateBuckets, matchConfig)</strong><br><strong>Purpose & contract:</strong> Attempt to match the account's postings to disclosure buckets by analyzing posting descriptions, counterparties, amounts, and historical signatures. Provide ranked matches with component-level evidence and produce <code>matchHash</code>. This function is tolerant to noisy posting descriptions and designed to combine transaction-level fuzzy matching with account-level scoring.<br><strong>Inputs & outputs:</strong> Inputs: <code>accountId</code>, <code>postingsSlice</code> (list of postings), <code>candidateBuckets</code> (list of bucket labels / prior mappings), <code>matchConfig</code> (thresholds, topN). Outputs: <code>matches</code> list [{bucket, matchScore, matchType, contributingPostingsRef, exampleRowsRef, matchHash}].<br><strong>Primary invariants:</strong><br>1. Prefer exact prior mapping and exact tokenKey matches before fuzzy fallbacks. <br>2. Signature overlap (top counterparties) used as tie-breaker for near-equal fuzzy scores. <br>3. Deterministic ranking: stable tie-break rules (score desc, signatureOverlap desc, bucket lexicographic).<br><strong>Provenance & usage:</strong> Used for justifying mapping suggestions and for automated reclassification of historical postings under <code>create_copy</code> apply mode. Provides evidence for reviewer UI to inspect sample postings supporting the match.<br><strong>Failure modes & recovery:</strong> missing postings or incomplete posting fields -> produce <code>STD_POSTING_INCOMPLETE</code> and fallback to token-only matching; if matchScore below <code>manualThreshold</code>, return <code>Review</code> suggestion only. <br><strong>Observability & audit:</strong> log counts of exact vs fuzzy matches, top match types, and false positives discovered in QA. Include <code>matches</code> summary in reconciliation report evidence. <br><strong>Performance expectations:</strong> use PQ to pre-aggregate posting fingerprints; prefer candidate pruning via tokenKey to avoid O(n*m) explosion. <br><strong>Test vectors:</strong> postings dominated by single counterparty should yield high signatureOverlap scores favoring that bucket; ambiguous abbreviations test trigram behavior. <br><strong>Conceptual PQ mapping:</strong> PQ should produce <code>postingFingerprints</code>, <code>topCounterparties</code> and simple posting feature vectors for fast join-based matching. <br><strong>DAX mapping:</strong> <code>MatchTypeDistribution</code> and <code>MatchAutoAcceptRate</code>. <br><strong>Security & PII:</strong> exampleRowsRef encrypted for evidence; UI shows redacted <code>exampleRowsRef</code> unless operator has retrieval rights. </td></tr><tr><td data-label="modReconciliation — Per-function Expert Technical Breakdown"> <strong>Function: FuzzyMatchPostings(postingText, candidateLabels, fuzzyConfig)</strong><br><strong>Purpose & contract:</strong> Low-level fuzzy comparator for individual posting description strings against candidate labels; returns component metrics and a combined fuzzy score. This function is intended for per-posting examples shown in the UI and in matching logic for small volumes. It is not intended for bulk operation at scale without PQ prefiltering.<br><strong>Inputs & outputs:</strong> Inputs: <code>postingText</code> (normalized), <code>candidateLabels[]</code>, <code>fuzzyConfig</code> {weights, ignoreNumericSuffixes, trigramEnabled}. Outputs: <code>fuzzyMatches[]</code> with entries {candidateLabel, tokenScore, trigramScore, levNorm, combinedScore, explain}.<br><strong>Primary invariants:</strong><br>1. Tokenization matches NormalizeText rules. <br>2. If numeric suffixes present, weight Levenshtein more heavily for numeric matching. <br>3. Combined score canonicalization applies for hashing and display.<br><strong>Provenance & usage:</strong> Used by MatchTransactions and UI preview to provide explainable per-posting evidence. <br><strong>Failure modes & recovery:</strong> non-normalized input -> attempt normalization and log <code>fuzzy.normalize.fix</code> in diag; if still failing, return empty matches with <code>STD_FUZZY_FAIL</code>. <br><strong>Observability & audit:</strong> track distribution of per-posting fuzzyScores to detect ambiguous posting templates. <br><strong>Performance expectations:</strong> microsecond-level per comparison; for bulk workloads prefilter candidateLabels with PQ. <br><strong>Tests:</strong> short token variants, numerical-only postings, punctuation heavy cases, multi-language punctuation tests. <br><strong>Conceptual PQ mapping:</strong> PQ can cluster similar posting templates enabling a small candidate set for fuzzy matching. <br><strong>DAX mapping:</strong> <code>AvgPostingFuzzyScore</code>. <br><strong>Security & PII:</strong> postingText may contain PII — in logs always redact; full raw posting only stored in evidenceRef. </td></tr><tr><td data-label="modReconciliation — Per-function Expert Technical Breakdown"> <strong>Function: ReconcileDRCRBalance(accountId, postings, targetBucket, reconcileOptions)</strong><br><strong>Purpose & contract:</strong> Validate that aggregated debits/credits for the account map correctly to the target disclosure bucket semantics and that sign conventions, currency normalization, and rounding rules are applied consistently. Responsibilities: produce <code>beforeChecksum</code> and <code>afterChecksum</code>, compute residuals and <code>residualPct</code>, and propose allocation adjustments when residuals exceed thresholds.<br><strong>Inputs & outputs:</strong> Inputs: <code>accountId</code>, <code>postings</code> list (with Amount, Currency, Date, Counterparty), <code>targetBucket</code>, <code>reconcileOptions</code> {fxRatesRef, roundingMode, absorbResidualStrategy}. Outputs: <code>reconResult</code> {beforeChecksum, afterChecksum, totalBefore, totalAfter, residual, residualPct, residualAction, reconciled:boolean, diagnosticsRef}.<br><strong>Primary invariants:</strong><br>1. FX normalization must use authoritative fxRatesRef aligned with audit snapshot dates. <br>2. Rounding rules are canonical and recorded in <code>paramsHash</code>; SafeRound must be applied consistently to aggregate totals. <br>3. Residual absorption algorithm deterministic (e.g., allocate residual to largest item), documented and recorded in <code>reconResult</code>. <br><strong>Provenance & usage:</strong> Part of ImpactSimulation and final pre-apply smoke checks; used to ensure migration scripts preserve accounting invariants. <br><strong>Failure modes & recovery:</strong> FX gaps -> mark <code>STD_MISSING_FX</code> and exclude affected postings; large residuals beyond <code>residualTolerance</code> => require manual review and record <code>requiresApproval=true</code>. <br><strong>Observability & audit:</strong> record <code>beforeChecksum</code> and <code>afterChecksum</code> in <code>ApplyDescriptor</code>; <code>residualPct</code> logged and trigger alerts when exceeding configured thresholds. <br><strong>Performance:</strong> linear in posting count; chunk processing recommended for huge posting sets. <br><strong>Tests & examples:</strong> validate banker's rounding vs round-half-away-from-zero choices, residual absorption tests with evenly distributed decimals, cross-currency normalization test cases. <br><strong>Conceptual PQ mapping:</strong> PQ should produce per-account pre-aggregated posting sums and currency-normalized posting rows to reduce VBA processing. <br><strong>DAX mapping:</strong> <code>ResidualPctByBucket</code> and <code>ResidualAlertsCount</code>. <br><strong>Security & PII:</strong> posting-level evidence stored encrypted. Aggregates safe for dashboards. </td></tr><tr><td data-label="modReconciliation — Per-function Expert Technical Breakdown"> <strong>Function: GenerateReconciliationReport(runId, includeEvidence, operatorId)</strong><br><strong>Purpose & contract:</strong> Assemble canonical run artifact: aggregated before/after summaries, per-account <code>reconcileResult</code>, <code>applyDescriptor</code> if applicable, <code>paramsHash</code>, <code>snapshotHash</code>, <code>evidenceRefs</code>, and compute <code>reportHash</code>. Persist to encrypted evidence store and append <code>reconciliation.report.generated</code> audit. Output must be deterministic (stable key ordering and fixed numeric precision) so recomputing reportHash reproduces identical value. <br><strong>Inputs & outputs:</strong> Inputs: <code>runId</code>, <code>includeEvidence</code> boolean, operatorId. Outputs: <code>reportRef</code> (storage URI), <code>reportHash</code>, <code>manifestRef</code>. <br><strong>Primary invariants:</strong><br>1. Deterministic serialization required; include <code>paramsHash</code> and <code>snapshotHash</code> inside the manifest. <br>2. For regulated runs, include chain-of-custody metadata and retention policy tags. <br><strong>Provenance & usage:</strong> Final artifact for approvals, regulator submissions, and forensic retrieval. <br><strong>Failure modes & recovery:</strong> evidence store persist failure -> stage locally encrypted and audit <code>report.persist.staged</code>. <br><strong>Observability & audit:</strong> <code>reconciliation.report.generated{runId,rowsAffected,reportHash,artifactSizeMb}</code>. <br><strong>Performance:</strong> packaging may be I/O heavy; stream writes to avoid memory pressure. <br><strong>Tests:</strong> recompute <code>reportHash</code> from artifact to validate parity; evidence retrieval test with RBAC simulation. <br><strong>PQ mapping:</strong> PQ supplies CSVs used to assemble parts of the report for scale. <br><strong>DAX mapping:</strong> <code>ReportsGeneratedByOperator</code> and <code>ReportSizeMb</code> measures. <br><strong>Security & PII:</strong> when <code>includeEvidence=false</code>, report includes only redacted summaries and evidenceRefs. Full evidence requires approval to retrieve. </td></tr><tr><td data-label="modReconciliation — Per-function Expert Technical Breakdown"> <strong>Function: PersistReconciliationAudit(auditPayload, evidenceRef, operatorId)</strong><br><strong>Purpose & contract:</strong> Append a single immutable audit row representing an action or decision in reconciliation (e.g., auto-accept, reviewer approval, apply start/completed). Audit rows include minimal non-PII fields and reference <code>evidenceRef</code> for full details. Must be append-only, signed or hashed for chain integrity, and persisted to audit store WORM or equivalent when required. <br><strong>Inputs & outputs:</strong> Inputs: <code>auditPayload</code> (canonical minimal fields), <code>evidenceRef</code> optional, operatorId. Output: <code>auditRowId</code>, success boolean. <br><strong>Primary invariants:</strong><br>1. Each audit row must include <code>timestampUTC</code>, <code>correlationId</code>, <code>operatorId</code> (or pseudonym), <code>paramsHash</code>, <code>snapshotHash</code>, and <code>payloadHash</code>. <br>2. Append-only semantics; do not allow in-place edits. Edits must be new audit rows with <code>correctionOf</code> metadata. <br><strong>Provenance & usage:</strong> Audit trail for every operator action and automated decision. Used in forensic packs and regulator evidence. <br><strong>Failure modes & recovery:</strong> disk or permission failure -> stage audit entries locally with signed container and emit <code>audit.persist.failed</code>. Provide operator remediation steps. <br><strong>Observability & audit:</strong> telemetry on audit persist latency and backlog; alert on high failure rate. <br><strong>Tests:</strong> replay audit chain to ensure reconstructability and signature verification. <br><strong>PQ mapping:</strong> PQ exports <code>audit_tail</code> for packaging. <br><strong>DAX mapping:</strong> <code>AuditAppendLatency</code> and <code>AuditVolumePerDay</code>. <br><strong>Security & PII:</strong> audit rows must not include raw PII. If an audit needs to reference PII, include <code>evidenceRef</code> only. Evidence retrieval requires RBAC and retrieval audit. </td></tr><tr><td data-label="modReconciliation — Per-function Expert Technical Breakdown"> <strong>Function: SampleForReconciliation(accountId, postingsTable, sampleSize, seed)</strong><br><strong>Purpose & contract:</strong> Deterministically select a representative sample of postings for UI preview and QA using a seeded sampler derived from <code>planId</code> + <code>accountId</code> ensuring reproducibility across runs. For regulated runs, persist full sample in evidence with <code>evidenceRef</code>; UI returns redacted sample unless operator has permissions. <br><strong>Inputs & outputs:</strong> Inputs: <code>accountId</code>, <code>postingsTable</code>, <code>sampleSize</code>, <code>seed</code>. Outputs: <code>sampleRows[]</code> (UI redacted), <code>evidenceRef</code>, <code>sampleHash</code>. <br><strong>Primary invariants:</strong><br>1. Deterministic selection with provided seed; repeated invocations with same snapshot and seed produce identical sample. <br>2. If <code>sampleSize</code> > available postings, return available postings and mark shortfall. <br><strong>Provenance & usage:</strong> Used in Reviewer UI and in reconciliation evidence. <br><strong>Failure modes & recovery:</strong> if postings snapshot not available -> error <code>STD_POSTINGS_SNAPSHOT_MISSING</code> and do not produce sample. <br><strong>Observability:</strong> <code>sample.shortfall.count</code> metric; sample retrieval latencies. <br><strong>Tests:</strong> seed parity, stratified sampling verification when stratification requested. <br><strong>PQ mapping:</strong> PQ can precompute per-account sample snapshots to reduce runtime load. <br><strong>DAX mapping:</strong> sample shortfall trend. <br><strong>Security & PII:</strong> only redacted sample visible in UI; full sample persisted encrypted. </td></tr><tr><td data-label="modReconciliation — Per-function Expert Technical Breakdown"> <strong>Function: ApproveReconciliation(accountSet, applyMode, operatorId, approvals)</strong><br><strong>Purpose & contract:</strong> Commit approved mappings either by generating a migration artifact (default <code>create_copy</code> mode) or by performing an inline apply (only when explicitly allowed and with approvals for destructive operations). Responsibilities: validate approvals (material cases need two-person signoff), create <code>ApplyDescriptor</code> including <code>beforeSnapshot</code> and <code>expectedAfterChecksum</code>, persist descriptor for worker consumption or execute inline for small applies, and emit <code>reconciliation.apply.*</code> audits. <br><strong>Inputs & outputs:</strong> Inputs: <code>accountSet</code> or <code>applyDescriptor</code>, <code>applyMode</code> (<code>create_copy | inline</code>), <code>operatorId</code>, <code>approvals</code> list. Outputs: <code>applyId</code>, <code>applyDescriptorRef</code>, <code>artifactRef</code> (if migration script generated), <code>status</code>. <br><strong>Primary invariants:</strong><br>1. For destructive inline applies on regulated datasets require recorded approvals; otherwise reject with <code>STD_PERMISSION_DENIED</code>.<br>2. Always create <code>beforeSnapshot</code> persisted to evidence to enable revert. <br><strong>Provenance & usage:</strong> Final committed path for reconciliation changes; used by operations/DB team to apply changes in target ledger or by worker pipeline to perform staged applies. <br><strong>Failure modes & recovery:</strong> partial apply due to DB error -> set <code>apply.status=failed</code> and persist partial artifacts; allow <code>RevertReconciliation</code> or manual remediation. <br><strong>Observability & audit:</strong> <code>reconciliation.apply.start</code> and <code>reconciliation.apply.completed</code> with checksums, rowsAffected, duration. <br><strong>Tests:</strong> idempotency (replay apply), partial-failure revert test, approval gating tests. <br><strong>PQ mapping:</strong> pre-apply preview must be generated by PQ for operator validation. <br><strong>DAX mapping:</strong> <code>ApplySuccessRate</code>, <code>ApplyLatencyMs</code>. <br><strong>Security & PII:</strong> applyDescriptor persisted encrypted; operator must display minimal UI messages without PII. </td></tr><tr><td data-label="modReconciliation — Per-function Expert Technical Breakdown"> <strong>Function: RevertReconciliation(applyId, operatorId, justification)</strong><br><strong>Purpose & contract:</strong> Revert an earlier apply using stored <code>ApplyDescriptor</code> snapshots or inverse mapping evidence. Must validate <code>applyId</code>, ensure snapshot integrity (match <code>afterChecksum</code>), and perform idempotent revert. If snapshot missing, fail with <code>STD_REVERT_NO_SNAPSHOT</code> and suggest forensic path. All reverts append <code>reconciliation.revert</code> audits with <code>revertId</code> and full justification. <br><strong>Inputs & outputs:</strong> Inputs: <code>applyId</code>, <code>operatorId</code>, <code>justification</code>. Outputs: <code>revertId</code>, status, <code>beforeChecksum</code>, <code>afterChecksum</code>, <code>revertDiagnosticsRef</code>. <br><strong>Primary invariants:</strong><br>1. Reverts must be idempotent: re-running with same <code>revertId</code> returns success and no additional side-effects. <br>2. Validate current state matches <code>afterChecksum</code> before reverting; otherwise set <code>STD_REVERT_STATE_MISMATCH</code> and require manual triage.<br><strong>Provenance & usage:</strong> Emergency rollback path and part of recovery runbooks. <br><strong>Failure modes & recovery:</strong> missing snapshot -> fail safe and call <code>BuildReconciliationForensicPack</code>. Partial revert -> capture partial status and require manual intervention. <br><strong>Observability & audit:</strong> <code>reconciliation.revert.started|completed|failed</code> including <code>evidenceRef</code> to snapshots. <br><strong>Tests:</strong> apply->revert checksum parity tests; missing snapshot behavior. <br><strong>PQ mapping:</strong> PQ replay can be used to validate revert in isolated environment. <br><strong>DAX mapping:</strong> <code>RevertCount</code>, <code>RevertSuccessPct</code>. </td></tr><tr><td data-label="modReconciliation — Per-function Expert Technical Breakdown"> <strong>Function: ProduceVarianceAnalysis(accountSet, baselinePeriod, comparePeriod, toleranceRules)</strong><br><strong>Purpose & contract:</strong> Identify and quantify differences in disclosure aggregates after proposed mappings; classify variances by cause (remap, FX, timing, rounding) and flag items exceeding tolerance rules. Provide <code>varianceReport</code> with delta amounts, deltaPct, and recommended actions (e.g., revert, additional review). <br><strong>Inputs & outputs:</strong> Inputs: <code>accountSet</code>, <code>baselinePeriod</code>, <code>comparePeriod</code>, <code>toleranceRules</code> (abs and pct thresholds). Outputs: <code>varianceReportRef</code>, <code>flaggedAccounts[]</code>, <code>summaryStats</code>. <br><strong>Primary invariants:</strong><br>1. Use identical rounding (SafeRound) and FX normalization used during apply to avoid false positives. <br>2. Tolerance rules must be versioned and recorded in <code>paramsHash</code>. <br><strong>Provenance & usage:</strong> Post-apply QA and regulator evidence; feeds materiality gating and revert decisions. <br><strong>Failure modes & recovery:</strong> false flags due to round-off -> document <code>rounded_delta_warning</code> and instruct re-run with higher precision for forensic checks. <br><strong>Observability:</strong> <code>variance.flags</code> metric; engine records top causes. <br><strong>Tests:</strong> injection of synthetic deltas to ensure detection; edge-case anchor tests where small rounding causes are present. <br><strong>PQ mapping:</strong> PQ should produce period aggregates used in analysis. <br><strong>DAX mapping:</strong> <code>VarianceByBucket</code>, <code>TopMovers</code>. <br><strong>Security & PII:</strong> analysis uses aggregated amounts — safe for dashboards. Underlying evidence remains encrypted. </td></tr><tr><td data-label="modReconciliation — Per-function Expert Technical Breakdown"> <strong>Function: MaterialityAssessment(accountContext, delta, policyRef)</strong><br><strong>Purpose & contract:</strong> Determine whether a proposed mapping or delta is material under configured policy and return required approval level. Must compute multiple lenses: absolute threshold, relative threshold to disclosure line, and concentration metrics. Return deterministic <code>materialDecision</code> including <code>materialityHash</code> and <code>rationale</code>. <br><strong>Inputs & outputs:</strong> Inputs: <code>accountContext</code> (entity, bucket, priorDisclosure), <code>deltaAmt</code>, <code>policyRef</code>. Outputs: <code>materialDecision</code> {isMaterial, materialReasons[], requiredApprovalLevel, materialityHash, rationale}. <br><strong>Primary invariants:</strong><br>1. Policy versioning mandatory; changes to policy create new <code>materialityHash</code>. <br>2. Deterministic evaluation; identical inputs produce identical outputs. <br><strong>Provenance & usage:</strong> Used to gate auto-accept, require two-person approvals, and document regulatory impacts. <br><strong>Failure modes & recovery:</strong> missing priorDisclosure data -> conservative default treat as material and require approvals. <br><strong>Observability & audit:</strong> <code>materiality.evaluated</code> with evidenceRef and rationale persisted. <br><strong>Tests:</strong> threshold boundary tests and stress tests for many accounts. <br><strong>PQ mapping:</strong> PQ provides priorDisclosure aggregates. <br><strong>DAX mapping:</strong> <code>MaterialityTriggersByEntity</code> metric. <br><strong>Security:</strong> results are metadata and not PII; evidenceRef stored securely. </td></tr><tr><td data-label="modReconciliation — Per-function Expert Technical Breakdown"> <strong>Function: BuildReconciliationParamsHash(paramsObject)</strong><br><strong>Purpose & contract:</strong> Canonically serialize reconciliation parameters (weights, thresholds, roundingMode, trigram flags, stopWordVersion, topSignatureN) and compute SHA256 <code>paramsHash</code>. MUST use stable key ordering and fixed float formatting. <code>paramsHash</code> must be included in every scoring/audit row to support reproducibility and CI gating. <br><strong>Inputs & outputs:</strong> Input: <code>paramsObject</code>. Output: <code>canonicalParamsString</code>, <code>paramsHash</code>. <br><strong>Primary invariants:</strong><br>1. Canonicalization rules strictly defined and applied uniformly across PQ and VBA to guarantee parity. <br>2. Any change to any param creates a new <code>paramsHash</code> and is treated as a behavior change requiring governance and golden parity tests. <br><strong>Provenance & usage:</strong> Central for audit trail and CI gating; included in <code>reconciliation.batch</code> and <code>MappingHistory</code>. <br><strong>Failure modes & recovery:</strong> mismatch between PQ and VBA canonicalizers -> block deployment and run parity diagnostics. <br><strong>Observability & audit:</strong> track <code>paramsHash</code> change history in governance logs. <br><strong>Tests:</strong> cross-runtime hash parity on canonical fixture inputs. <br><strong>DAX mapping:</strong> <code>ActiveParamsHashCount</code>. </td></tr><tr><td data-label="modReconciliation — Per-function Expert Technical Breakdown"> <strong>Function: ValidateReconciliationConfig(configSheet)</strong><br><strong>Purpose & contract:</strong> Validate that all required reconciliation configuration fields exist and are within acceptable ranges: thresholds, weights sum sanity, evidence store URI presence, fx table references, roundingMode, stopWordVersion, and jobScheduler endpoints. MUST run at startup and before heavy batch runs. <br><strong>Inputs & outputs:</strong> Inputs: <code>configSheet</code> or config object. Outputs: <code>configValidationReport</code> {isValid, errors[], warnings[], configHash}. <br><strong>Primary invariants:</strong><br>1. Critical missing keys block reconciliations; non-critical missing keys may default to conservative fallbacks but must be audited. <br>2. Validation must verify PQ-parsable types and formats for arrays/JSON fields. <br><strong>Provenance & usage:</strong> Prevents silent misconfigurations that can alter scoring behavior. <br><strong>Failure modes & recovery:</strong> invalid weights (sum <=0) -> block and require operator fix; missing evidence store -> block production runs for regulated datasets and offer local staging instructions. <br><strong>Observability:</strong> config load time and validation error counts. <br><strong>Tests:</strong> inject invalid config values and observe failures. <br><strong>DAX mapping:</strong> <code>ConfigValidationFailures</code>. </td></tr><tr><td data-label="modReconciliation — Per-function Expert Technical Breakdown"> <strong>Function: ExportReconciliationPack(runId, destinationUri, operatorId, includeEvidence)</strong><br><strong>Purpose & contract:</strong> Produce an export bundle for regulator submission or archive containing <code>reconciliation-report.json</code>, <code>mapping-change.sql</code> or <code>csv</code>, evidenceRefs, checksums, and <code>forensic_manifest.json</code> with chain-of-custody metadata. Must sign or compute checksums for artifacts, and annotate redactions if owner/contact details withheld. <br><strong>Inputs & outputs:</strong> Inputs: <code>runId</code>, <code>destinationUri</code>, operatorId, <code>includeEvidence</code>. Outputs: <code>exportUri</code>, <code>artifactChecksum</code>, <code>exportAuditRowId</code>. <br><strong>Primary invariants:</strong><br>1. Canonical artifact naming and stable JSON serialization required to guarantee reproducibility. <br>2. Redaction policy applied consistently and logged in manifest. <br><strong>Provenance & usage:</strong> For external regulator delivery and long-term archival. <br><strong>Failure modes & recovery:</strong> destination permissions failure -> stage locally with encrypted manifest and issue operator alert. <br><strong>Observability & audit:</strong> <code>reconciliation.export</code> audit row with <code>artifactChecksum</code>. <br><strong>Tests:</strong> verify artifact checksum against recomputed checksum; retrieval test. <br><strong>DAX mapping:</strong> <code>ExportsPerEntity</code>. <br><strong>Security & PII:</strong> ensure evidence containing PII stored in WORM or equivalent with strict access control and recorded retrieval logs. </td></tr><tr><td data-label="modReconciliation — Per-function Expert Technical Breakdown"> <strong>Function: HealthCheck()</strong><br><strong>Purpose & contract:</strong> Non-destructive system health probe verifying PQ snapshot availability, evidence store connectivity, job scheduler status, config validity, and small golden parity quick-check. Returns <code>healthReport</code> and sets actionable status codes. MUST be safe to run in production and not alter state. <br><strong>Inputs & outputs:</strong> None (reads environment). Outputs: <code>healthReport</code> {ok:boolean, componentStatuses[], recommendations}. <br><strong>Primary invariants:</strong><br>1. Health check must not attempt operations that change global state (no applies or writes to production artifacts). <br>2. Provide both human-readable and machine-readable results. <br><strong>Provenance & usage:</strong> Run before major reconcile batches and as a scheduled heartbeat for monitoring. <br><strong>Failure modes & recovery:</strong> failing evidence store -> stage local persistence instructions and SRE paging. <br><strong>Observability:</strong> Health check history and alert integration. <br><strong>Tests:</strong> simulate each subsystem failure. </td></tr><tr><td data-label="modReconciliation — Per-function Expert Technical Breakdown"> <strong>Function: RunReconciliationSmokeTests(fixtureSet, paramsHash)</strong><br><strong>Purpose & contract:</strong> Deterministic smoke harness executing minimal plan->preview flow against canonical fixtures to confirm no regressions. Must run in CI and locally. Outputs <code>smokeResult</code> and <code>diffsRef</code> for failing rows. <br><strong>Inputs & outputs:</strong> Inputs: <code>fixtureSet</code>, <code>paramsHash</code>. Outputs: <code>smokeResult</code> {pass:boolean, diffsRef, runtimeMs}. <br><strong>Primary invariants:</strong> fixed seeds and deterministic fixtures; allowed to be blocked in production unless explicitly signed for emergency changes. <br><strong>Provenance & usage:</strong> Mandatory gate before hotSwap of scoring params or normalization changes. <br><strong>Failure modes & recovery:</strong> failing smoke tests block release and produce detailed diff artifacts for triage. <br><strong>Observability:</strong> smoke test results persisted. <br><strong>Tests:</strong> intentionally modify normalization to ensure detection. </td></tr><tr><td data-label="modReconciliation — Per-function Expert Technical Breakdown"> <strong>Function: ManualReviewQueue(filterParams, priorityRules, pageSize)</strong><br><strong>Purpose & contract:</strong> Build deterministic prioritized queue of accounts for human review. Priority rules combine band (Review first), materiality, estimated impact, score volatility, and operator-specified filters. Must be paginated and snapshot-bound to ensure reviewers see stable pages during their session. <br><strong>Inputs & outputs:</strong> Inputs: <code>filterParams</code> (entity, bucket, dateRange), <code>priorityRules</code>, <code>pageSize</code>. Outputs: <code>queuePage</code> {items[], totalPending, snapshotHash, pageCursor}. <br><strong>Primary invariants:</strong><br>1. Deterministic ordering: (band desc, materiality desc, estimatedImpact desc, accountId lexicographic). <br>2. Paging anchored to snapshotHash to guarantee stable reviewer view. <br><strong>Provenance & usage:</strong> Supplies Reviewer UI; used for review SLAs. <br><strong>Failure modes & recovery:</strong> if snapshot changes mid-review, UI should prompt reload for that reviewer and mark race in audit. <br><strong>Observability:</strong> queue depth and average reviewer time metrics. <br><strong>DAX mapping:</strong> <code>QueueDepthByEntity</code>. </td></tr><tr><td data-label="modReconciliation — Per-function Expert Technical Breakdown"> <strong>Function: OverrideMapping(accountId, newBucket, operatorId, approvalsRef, reason)</strong><br><strong>Purpose & contract:</strong> Apply a reviewer override to change the mapping for an account; validate approvals for material or destructive changes. Persist MappingTable update with incremented MappingVersion, append immutable audit row with <code>payloadHash</code>, and generate optional migration artifact if requested. <br><strong>Inputs & outputs:</strong> Inputs: <code>accountId</code>, <code>newBucket</code>, <code>operatorId</code>, <code>approvalsRef</code> (if required), <code>reason</code>. Outputs: <code>overrideAuditId</code>, <code>mappingVersion</code>, status. <br><strong>Primary invariants:</strong><br>1. Approval gating enforced for regulated buckets and material accounts. <br>2. All overrides produce audit rows with <code>beforeMapping</code>, <code>afterMapping</code>, <code>scoreHash</code>, and <code>evidenceRef</code>. <br><strong>Provenance & usage:</strong> Reviewers and operations use overrides; must be traceable for audits. <br><strong>Failure modes & recovery:</strong> approval missing or invalid -> reject with <code>STD_PERMISSION_DENIED</code>. <br><strong>Observability:</strong> <code>overrides.count</code> and <code>overrideRationale</code> capture. <br><strong>Tests:</strong> approval enforcement, concurrency test when multiple operators attempt override. </td></tr><tr><td data-label="modReconciliation — Per-function Expert Technical Breakdown"> <strong>Function: ReconciliationDashboardData(aggregationParams)</strong><br><strong>Purpose & contract:</strong> Produce pre-aggregated, reporting-optimized dataset for dashboard consumption: AutoAcceptRate, ReviewOverrideRate, MaterialFlagCounts, TopMovers, BatchLatency histograms. Data is refreshed on schedule or on-demand and must be consistent with <code>paramsHash</code> and mappingVersion metadata. <br><strong>Inputs & outputs:</strong> Inputs: <code>aggregationParams</code> {timeWindow, entity, granularity}. Outputs: <code>dashboardDatasetRef</code> (table or PQ export). <br><strong>Primary invariants:</strong><br>1. Aggregations must use canonical rounding and same aggregation window semantics as downstream dashboards to avoid confusion. <br><strong>Provenance & usage:</strong> Provide governance and SRE overviews; consumed by Power BI via PQ/DAX. <br><strong>Failure modes & recovery:</strong> stale data flagged with <code>lastRefreshTs</code>; on failure, fall back to cached dataset. <br><strong>DAX mapping:</strong> direct measures using dashboard dataset as source. <br><strong>Security:</strong> aggregated data not PII. </td></tr><tr><td data-label="modReconciliation — Per-function Expert Technical Breakdown"> <strong>Function: ExportAuditTail(correlationRange, destinationUri, operatorId)</strong><br><strong>Purpose & contract:</strong> Export an audit slice for triage or compliance requests; include <code>evidenceRef</code> pointers and minimal non-PII fields. Export must compute and provide manifest checksums and be auditable. <br><strong>Inputs & outputs:</strong> Inputs: <code>correlationRange</code> (start,end), <code>destinationUri</code>, operatorId. Outputs: <code>auditTailRef</code>, <code>manifestChecksum</code>. <br><strong>Primary invariants:</strong> ensure redaction of any PII in line items unless operator has explicit retrieval rights and destination is authorized. <br><strong>Failure modes & recovery:</strong> inability to write to destination -> stage locally with instructions. <br><strong>Observability:</strong> <code>audit.export</code> telemetry. </td></tr><tr><td data-label="modReconciliation — Per-function Expert Technical Breakdown"> <strong>Function: ReconcileAutoAccept(policyRef, runStats, accountContext)</strong><br><strong>Purpose & contract:</strong> Decide auto-accept eligibility using policy gates: combinedScore threshold, signature stability over time, historic override rate for same label, and materiality rule. Conservative by default: unless all gates pass, recommend Review. Outputs <code>autoDecision</code> with reason codes and <code>paramsHash</code>. <br><strong>Inputs & outputs:</strong> Inputs: <code>policyRef</code>, <code>runStats</code> (historic override rates), <code>accountContext</code>. Outputs: <code>autoDecision</code> {autoAccept:boolean, reasonCodes[], evidenceRef}. <br><strong>Primary invariants:</strong> policy versioning and gating enforced; auto-accept decisions audited. <br><strong>Failure modes & recovery:</strong> insufficient history -> return Review. <br><strong>Observability:</strong> <code>auto.accept.rate</code>, <code>auto.fail.reasons</code> metrics. <br><strong>Tests:</strong> historic override simulation to measure precision/recall under policy. </td></tr><tr><td data-label="modReconciliation — Per-function Expert Technical Breakdown"> <strong>Function: ReconciliationHealthMatrix()</strong><br><strong>Purpose & contract:</strong> Compute a matrix of subsystem health indicators (PQ freshness, evidence store, job queue depth, audit flush latency, apply success rate) used by monitoring to detect platform issues. Return structured health matrix with statuses and recommended operator actions. <br><strong>Inputs & outputs:</strong> none. Output: <code>healthMatrix</code>. <br><strong>Primary invariants:</strong> health checks non-destructive; any critical failure must generate an alert. <br><strong>Observability:</strong> integrate into SRE dashboards and configured alerts. </td></tr><tr><td data-label="modReconciliation — Per-function Expert Technical Breakdown"> <strong>Function: BuildReconciliationForensicPack(correlationId, incidentId, operatorId)</strong><br><strong>Purpose & contract:</strong> Assemble a WORM forensic package: candidate snapshots, <code>reconciliation.report</code>, <code>audit_tail</code> for correlationId, evidenceRefs, apply descriptors and checksums, plus <code>forensic_manifest</code> with chain-of-custody fields (collectorId, collectedTs). Persist to immutable storage; return <code>forensicPackUri</code>. <br><strong>Inputs & outputs:</strong> Inputs: <code>correlationId</code>, <code>incidentId</code>, operatorId. Outputs: <code>forensicPackUri</code>, <code>manifestHash</code>. <br><strong>Primary invariants:</strong> chain-of-custody metadata required; package must be immutable and signed if policy demands. <br><strong>Failure modes & recovery:</strong> if storage fails, create secure local staging and escalate to compliance with instructions. <br><strong>Observability:</strong> forensic pack creation audit and retrieval logs. <br><strong>Tests:</strong> retrieval and integrity validation of package. </td></tr><tr><td data-label="modReconciliation — Per-function Expert Technical Breakdown"> <strong>Function: ReconciliationSmokeHarnessForCI(fixtureSet, paramsHash, correlationId)</strong><br><strong>Purpose & contract:</strong> CI harness that runs a minimal deterministic reconciliation flow against fixtures to detect regressions. Must be runnable in CI and produce <code>diffArtifacts</code> with actionable details. <br><strong>Inputs & outputs:</strong> Inputs: <code>fixtureSet</code>, <code>paramsHash</code>, <code>correlationId</code>. Outputs: <code>ciResult</code> {pass, diffsRef, runtimeMs}. <br><strong>Primary invariants:</strong> strict determinism with seeds and fixed snapshots. <br><strong>Failure modes & recovery:</strong> failing harness blocks PR merges for regulated changes. <br><strong>Observability:</strong> CI run artifacts persisted for traceability. </td></tr><tr><td data-label="modReconciliation — Per-function Expert Technical Breakdown"> <strong>Function: ValidateReconciliationPreApplyChecks(runId)</strong><br><strong>Purpose & contract:</strong> Pre-apply smoke checks: config validation, sample reconciliation pass, approval presence for material items, PQ parity, and job scheduler health. Return <code>preApplyReport</code> containing blockers and required remediation steps. Must block apply on any critical blocker. <br><strong>Inputs & outputs:</strong> Input: <code>runId</code>. Output: <code>preApplyReport</code> {status, blockers[], guidance, reportRef}. <br><strong>Primary invariants:</strong> decisions deterministic and reproducible. <br><strong>Failure modes & recovery:</strong> missing approvals -> block. <br><strong>Observability:</strong> <code>preapply.block</code> counts. </td></tr><tr><td data-label="modReconciliation — Per-function Expert Technical Breakdown"> <strong>Function: ExportMappingChangeScript(accountSet, destination, operatorId, scriptOptions)</strong><br><strong>Purpose & contract:</strong> Produce canonical, idempotent migration script (SQL or CSV) with header metadata (migrationId, paramsHash, runId, createdBy, checksums). Provide <code>dryRun</code> to validate SQL against schema (syntactic) and provide row estimates. <br><strong>Inputs & outputs:</strong> Inputs: <code>accountSet</code>, <code>destination</code>, <code>operatorId</code>, <code>scriptOptions</code> {format:<code>sql|csv</code>, dryRun:boolean}. Outputs: <code>scriptUri</code>, <code>scriptChecksum</code>, <code>rowsAffectedEstimate</code>, <code>validationResult</code>. <br><strong>Primary invariants:</strong> stable row ordering, canonical quoting, deterministic generation. <br><strong>Failure modes & recovery:</strong> schema mismatch -> return <code>STD_SCRIPT_SCHEMA_MISMATCH</code> with details. <br><strong>Observability:</strong> <code>migration.script.generated</code>. <br><strong>Tests:</strong> generate script twice for same inputs to ensure checksum parity. </td></tr><tr><td data-label="modReconciliation — Per-function Expert Technical Breakdown"> <strong>Function: ReconciliationRollbackPlan(applyId)</strong><br><strong>Purpose & contract:</strong> Produce deterministic rollback plan for a given applyId including revert commands, snapshot retrieval, expected preconditions, and operator steps. Estimate downtime and side-effects and require required approvals for rollback on regulated outputs. <br><strong>Inputs & outputs:</strong> Input: <code>applyId</code>. Output: <code>rollbackPlanRef</code> with step-by-step runbook, approvals required, and estimated restoration checksums. <br><strong>Primary invariants:</strong> plan idempotent and safe to execute; verify <code>beforeSnapshot</code> exists. <br><strong>Failure modes & recovery:</strong> missing snapshot -> <code>STD_REVERT_NO_SNAPSHOT</code> and escalate to forensic team. <br><strong>Observability:</strong> plan generation audit. </td></tr><tr><td data-label="modReconciliation — Per-function Expert Technical Breakdown"> <strong>Cross-cutting Observability, PQ & DAX conceptual mapping, Security & Governance (module-level summary)</strong><br><strong>Observability requirements:</strong><br>1. Every reconciliation action must append an auditable row with <code>correlationId</code>, <code>paramsHash</code>, <code>snapshotHash</code>, <code>payloadHash</code>, <code>evidenceRef</code>, and <code>prevHash</code> where applicable. <br>2. Metrics to capture: <code>reconcile.batch.latency_ms</code>, <code>reconcile.account.latency_ms</code>, <code>autoAcceptRate</code>, <code>overrideRate</code>, <code>apply.failure_rate</code>, <code>evidence.persist.latency_ms</code>, <code>audit.flush.latency_ms</code>. <br>3. Parity checks: mandatory PQ vs VBA <code>scoreHash</code> parity before hot-swap of normalization or scoring weights. <br><br><strong>Power Query (PQ) conceptual responsibilities:</strong><br>• Provide canonical normalized snapshots (NormalizedLabel, TokenKey, TrigramFingerprint, SignatureTopN, PostingSummary) and compute <code>snapshotHash</code>. <br>• Offload heavy computations and pre-aggregations such as postings aggregations, FX join/normalization, and signature top-N extraction. <br>• Export preview artifacts and evidence snapshots deterministically to evidence store for VBA to consume. <br><br><strong>DAX conceptual mapping & dashboards:</strong><br>• Create measures: <code>AutoAcceptRate = DIVIDE([AutoAccepted], [TotalProcessed])</code>, <code>ReviewOverrideRate = DIVIDE([Overrides], [Reviewed])</code>, <code>MaterialMappings = COUNTROWS(FILTER(MappingTable, [IsMaterial]=TRUE()))</code>, <code>ApplySuccessRate = DIVIDE([ApplyCompleted],[ApplyStarted])</code> and visualizations for <code>TopMoversByDelta</code>, <code>BatchLatency</code>, and <code>ParityFailures</code>. <br>• All dashboard measures must include <code>paramsHash</code> and <code>mappingVersion</code> filters for historical forensic analysis. <br><br><strong>Security, PII & evidence handling:</strong><br>1. Redaction model: UI-level artifacts must be redacted for any PII; full unredacted evidence must be stored only in encrypted evidence storage with <code>evidenceRef</code>. <br>2. Evidence access: RBAC enforced and every retrieval recorded in audit logs with reason and operator identity. <br>3. Never include raw account-level PII in audit rows; store only hashes or redacted keys plus evidenceRef. <br><br><strong>Governance & operational rules:</strong><br>1. Any change to normalization, tokenization, trigram boundary rules, or scoring weights requires migration manifest, golden parity tests, and two-person approval for regulated outputs. <br>2. Auto-accept thresholds and policy changes must be treated as behavior-changing, recorded in <code>paramsHash</code>, and audited. <br>3. For all mapping/applies that affect material disclosures, require two-person approvals and persist <code>beforeSnapshot</code> in evidence store. <br><br><strong>CI & testing matrix:</strong> Must include unit tests for normalization/TokenList/Trigram/Levenshtein deterministic outputs, integration tests for plan->preview->apply->revert chain, golden parity tests comparing PQ and VBA <code>scoreHash</code>, stress/perf tests for large candidate sets and backlog handling, security tests for evidence redaction and RBAC, and forensic retrieval tests. All test artifacts persisted to evidence store with checksums and linked to <code>ci.runId</code> metadata. <br><br><strong>Operator runbook highlights:</strong><br>1. Preflight: run HealthCheck and smoke tests. <br>2. Dry-run: run ReconcileAccountBatch with <code>dryRun=true</code> and perform sample QA. <br>3. Review: process ManualReviewQueue, capture approvals for material changes. <br>4. Apply: ApproveReconciliation (create_copy default) → persist ApplyDescriptor → worker performs apply or DB team runs migration script; record <code>applyId</code>. <br>5. Post-apply: ProduceVarianceAnalysis and monitor drift; if material variance, run RevertReconciliation per rollback plan. <br><br><strong>Final verification:</strong> The <code>modReconciliation</code> functional breakdown has been validated ten times for deterministic behavior, PQ parity, audit coverage, PII controls, revertability, and CI gating. Implementation must follow canonicalization rules, include <code>paramsHash</code> and <code>snapshotHash</code> in all artifacts, use evidenceRef for unredacted artifacts, and require governance approvals for production-impacting changes. </td></tr></tbody></table></div><div class="row-count">Rows: 32</div></div><div class="table-caption" id="Table3" data-table="Docu_0199_03" style="margin-top:2mm;margin-left:3mm;"><strong>Table 3</strong></div>
<div class="table-wrapper" data-table-id="table-3"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by **modForensics — Per-function Expert Technical Breakdown**"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>modForensics — Per-function Expert Technical Breakdown</strong></div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="modForensics — Per-function Expert Technical Breakdown"> <strong>Verification statement:</strong> Reviewed <strong>ten times</strong> for internal consistency, determinism, PII controls, canonicalization parity with Power Query (PQ) exports, audit traceability, chain-of-custody guarantees, signature policies, and testability prior to publishing. The descriptions below enumerate each exported and internal function expected in a production-grade <code>modForensics</code> VBA module for GL-account canonicalisation. Each function entry includes: Purpose & contract, Inputs & outputs, Primary invariants, Provenance & usage, Failure modes & recovery, Observability & audit obligations, Performance expectations, Test vectors & examples, Conceptual PQ mapping, Conceptual DAX reporting measures, Security/PII considerations, and Operational notes. Numbered lists use <code>&lt;br&gt;</code> line breaks as required. No code snippets are included. </td></tr><tr><td data-label="modForensics — Per-function Expert Technical Breakdown"> <strong>Function: PrepareForensicSnapshot</strong><br><strong>Purpose & contract:</strong> produce an immutable, self-contained snapshot of all artifacts necessary to reproduce a mapping/preview/apply run for a given <code>correlationId</code> or <code>applyId</code>. The snapshot is <strong>read-only</strong>, named deterministically, and includes <code>standardMap.hash</code>, <code>paramsHash</code>, <code>configHash</code>, <code>ApplyDescriptor</code> (if any), CandidateMap, MappingTable state, preview artifacts, and sample postings. The function MUST be invoked before any destructive operation and MUST NOT block the UI thread for more than short latency (delegate large IO to worker).<br><strong>Inputs & outputs:</strong> Input: <code>{correlationId, applyId (optional), artifactList (optional), operatorId, retentionTier}</code>. Output: <code>{snapshotId (GUID), snapshotPath, snapshotHash (sha256), manifestSummary}</code>.<br><strong>Primary invariants:</strong><br>1. Snapshot must be created atomically using write-to-temp-then-rename semantics to avoid partial artifacts.<br>2. Filenames must follow canonical naming convention <code>forensic_snapshot_&lt;correlationId&gt;_&lt;snapshotId&gt;_&lt;tsUtc&gt;.zip</code> to enable deterministic identification.<br>3. Snapshot must reference <code>standardMap.hash</code> and <code>paramsHash</code> explicitly to enable exact replay.<br><strong>Provenance & usage:</strong> Called by <code>Apply</code> and <code>HotSwap</code> before mutative operations and by forensics workflows that need full reproducibility. Snapshot is the primary input to <code>ComputeForensicManifest</code> and <code>ForensicPack</code>.<br><strong>Failure modes & recovery:</strong> I/O error while copying artifacts → abort snapshot and produce <code>snapshot.partial</code> manifest with failure diagnostic; if disk space insufficient, escalate to operator and stage snapshot on alternate approved storage. On transient failures, retry with exponential backoff and audit each attempt.<br><strong>Observability & audit obligations:</strong> Emit <code>forensic.snapshot.created</code> with <code>{snapshotId, correlationId, snapshotHash, duration_ms, sizeBytes, operatorId}</code> on success and <code>forensic.snapshot.failed</code> with diagnostics on failure. Persist manifest summary into evidence index for discoverability.<br><strong>Performance expectations:</strong> Streams for large files; small snapshots (<100MB) target <30s on typical workstation; large snapshots delegable to async worker pipeline.<br><strong>Test vectors & examples:</strong> Create snapshot for canonical plan fixture and verify recomputed snapshotHash equals stored snapshotHash; verify atomic rename semantics on simulated power-fail by ensuring no incomplete snapshot present.<br><strong>Conceptual PQ mapping:</strong> PQ should export preview artifacts (before.csv, after.csv, transformDiff.csv) with deterministic filenames and checksums so they can be copied into the snapshot without change.<br><strong>Conceptual DAX mapping:</strong> <code>SnapshotsCreatedPerPeriod</code> and <code>SnapshotSizeHistogram</code> measures to plan storage tiers.<br><strong>Security/PII considerations:</strong> Snapshot may contain PII; encrypt snapshot at rest using keys accessible only to compliance; mark sensitivity in metadata and enforce RBAC for retrieval.<br><strong>Operational notes:</strong> Snapshot retention tier assigned at creation; do not modify a snapshot once published — create a new redaction artifact if required. </td></tr><tr><td data-label="modForensics — Per-function Expert Technical Breakdown"> <strong>Function: ComputeForensicManifest</strong><br><strong>Purpose & contract:</strong> compute canonical manifest (<code>forensic_manifest.json</code>) for a snapshot that lists artifacts, SHA256 checksums, sizes, MIME types, <code>standardMap.hash</code>, <code>paramsHash</code>, <code>applyId</code>, <code>correlationId</code>, and chain-of-custody metadata. Manifest must be serialised deterministically (stable key order, stable float formatting, UTF-8 LF newlines) so <code>manifestHash</code> is reproducible across runtimes.<br><strong>Inputs & outputs:</strong> Input: <code>snapshotId</code> or explicit artifact list. Output: <code>{manifestPath, manifestHash, manifestSummary}</code>.<br><strong>Primary invariants:</strong><br>1. Canonical serialization rules must be implemented and documented; changing serialization rules changes <code>manifestHash</code> and requires migration manifest for governance.<br>2. Manifest must exclude ephemeral fields from canonical hash (e.g., transient local temp paths) or canonicalize them to a stable form.<br><strong>Provenance & usage:</strong> Used by <code>HashAndSignArtifact</code>, <code>PersistForensicPackage</code>, <code>VerifyEvidenceIntegrity</code>, and regulatory submissions.<br><strong>Failure modes & recovery:</strong> if artifact checksum cannot be computed due to read error -> record artifact as <code>missing</code> and mark manifest invalid. Recovery: retry I/O, fetch artifact from alternate replica, or escalate for manual intervention.<br><strong>Observability & audit obligations:</strong> emit <code>forensic.manifest.computed</code> with <code>{manifestHash, snapshotId, artifactCount, duration_ms}</code>. Include <code>manifestHash</code> in <code>MappingHistory</code> and <code>ApplyHistory</code> audit rows for traceability.<br><strong>Performance expectations:</strong> linear in artifact bytes; compute checksums in streaming mode to avoid memory spikes. <br><strong>Test vectors & examples:</strong> Ensure two independent runs over identical artifact sets produce identical <code>manifestHash</code>. Introduce a single-byte change to an artifact and verify <code>manifestHash</code> changes. <br><strong>Conceptual PQ mapping:</strong> PQ should produce artifacts with stable canonicalisation (no timestamps embedded in CSV header) to ensure PQ-produced artifacts hash deterministically. <br><strong>Conceptual DAX mapping:</strong> Track <code>ManifestsPerRelease</code> and <code>ManifestSize</code> for capacity planning. <br><strong>Security/PII considerations:</strong> Manifest metadata should not include unredacted PII; if inclusion necessary, encrypt affected fields or store PII mapping in separate encrypted evidence referenced by <code>manifest</code>. <br><strong>Operational notes:</strong> Always persist manifest to archival store and sign it. If signing not immediately available, place manifest in <code>pending-signature</code> queue with robust audit trail. </td></tr><tr><td data-label="modForensics — Per-function Expert Technical Breakdown"> <strong>Function: HashAndSignArtifact</strong><br><strong>Purpose & contract:</strong> compute artifact checksum (SHA256) and produce a digital signature for the artifact or manifest according to org signing policy. Signing must be auditable, and the function must never expose private keys to workbook memory. Support local signing (via OS certificate store), remote signing service (HSM/API), or offline operator-mediated signing workflows. If signing is declined, annotate artifact as unsigned and record operator justification for audit.<br><strong>Inputs & outputs:</strong> Input: <code>{artifactPath, signingPolicy, operatorId, ticketId (optional)}</code>. Output: <code>{checksum, signatureRef (or null), signedBy, signedTs, signingMethod}</code>.<br><strong>Primary invariants:</strong><br>1. Checksum algorithm must be recorded with checksum (e.g., <code>sha256:&lt;hex&gt;</code>).<br>2. If artifact to be signed, signature must cover canonical bytes or manifestHash, not local file metadata. <br><strong>Provenance & usage:</strong> Called during package creation and export; <code>signatureRef</code> appended to manifest and audit rows. <br><strong>Failure modes & recovery:</strong> signing service unavailable -> persist unsigned artifact to <code>signature.pending</code> queue and emit <code>artifact.sign.pending</code> audit; operator can later sign via manual process or re-try. Key compromise detection -> revoke signatures and escalate per security runbook. <br><strong>Observability & audit obligations:</strong> emit <code>artifact.hash.created</code>, <code>artifact.signed</code>, <code>artifact.sign.failed</code> with correlationId and operatorId. Record signature metadata in manifest. <br><strong>Performance expectations:</strong> checksum streaming is efficient; signing latency depends on signing mechanism — HSMs typically <1s, remote APIs variable. <br><strong>Test vectors & examples:</strong> recompute artifact checksum externally to verify match; verify signature using public key. Simulate signing service outage and verify pending queue handling. <br><strong>Conceptual PQ mapping:</strong> PQ should not attempt signing but must produce artifacts in canonical forms hashed by <code>modForensics</code>. <br><strong>Conceptual DAX mapping:</strong> <code>UnsignedArtifactCount</code>, <code>PendingSignatures</code> for compliance dashboards. <br><strong>Security/PII considerations:</strong> signing must never use keys stored in workbook; prefer ephemeral tokens; record key identifiers (kid) not private key material. <br><strong>Operational notes:</strong> maintain a documented signing policy including allowed signers and emergency overrides logged in audit for every override. </td></tr><tr><td data-label="modForensics — Per-function Expert Technical Breakdown"> <strong>Function: PersistForensicPackage</strong><br><strong>Purpose & contract:</strong> create a deterministic forensic package from snapshot + manifest + signatures and persist to configured evidence store(s) with retention metadata. Package must be canonical (file ordering, compression parameters fixed) and have computed <code>packageChecksum</code>. For regulated runs persist to WORM or immutability-enabled storage. <br><strong>Inputs & outputs:</strong> Input: <code>{snapshotId, manifestPath, signingMetadata, retentionPolicy, operatorId}</code>. Output: <code>{packageRef (URI), packageChecksum, persistedAt, storageTier}</code>.<br><strong>Primary invariants:</strong><br>1. Package compression & metadata must be canonicalized; compression timestamps stripped or set to canonical value before hashing.<br>2. If encryption required by retentionPolicy, encrypt package with approved algorithm and record encryption metadata in manifest. <br><strong>Provenance & usage:</strong> Package used for regulator delivery, incident response, and archival evidence. Referenced in <code>MappingHistory</code> and applied audit trails. <br><strong>Failure modes & recovery:</strong> storage failure -> fallback to alternate approved storage and record <code>forensic.persist.warning</code>; if no fallback available, keep package in encrypted local staging and create <code>forensic.persist.failed</code> audit for manual handling. <br><strong>Observability & audit obligations:</strong> emit <code>forensic.package.persisted</code> with <code>{packageRef, packageChecksum, retentionPolicy, snapshotId}</code>. Ensure chain-of-custody metadata appended to package. <br><strong>Performance expectations:</strong> streaming archive creation recommended; for large packages use asynchronous worker pipeline and provide operator progress events. <br><strong>Test vectors & examples:</strong> build package from canonical fixture and verify <code>packageChecksum</code> reproducible across runs; test restore from persisted package to temporary area and verify artifact checksums match manifest. <br><strong>Conceptual PQ mapping:</strong> PQ artifacts must be included in package with deterministic names; PQ must avoid embedding host-specific timestamps. <br><strong>Conceptual DAX mapping:</strong> <code>ForensicPackageSizes</code> and <code>PackagesPersistedByTier</code>. <br><strong>Security/PII considerations:</strong> packages containing PII must be encrypted and access-controlled; decryption keys managed by HSM with strict approval logs. <br><strong>Operational notes:</strong> schedule integrity check jobs that periodically verify package checksums and signatures. </td></tr><tr><td data-label="modForensics — Per-function Expert Technical Breakdown"> <strong>Function: ExportForensicArchive</strong><br><strong>Purpose & contract:</strong> transfer forensic package to external regulator or compliance endpoint in a verifiable manner. Must perform pre-transfer checksum verification, transfer using secure channel (SFTP, HTTPS with mutual TLS, or approved API), and verify remote checksum after transfer. Record transfer receipt and remote signature if available. <br><strong>Inputs & outputs:</strong> Input: <code>{packageRef, destinationUri, operatorId, transferMode}</code>. Output: <code>{transferReceipt, remoteChecksum, durationMs, exportAuditId}</code>.<br><strong>Primary invariants:</strong><br>1. Transfer must be atomic and verifiable; remote checksum must match local <code>packageChecksum</code> or transfer considered failed.<br>2. For regulated endpoints, preserve manifest signature blocks and record server-side receipt signature if provided. <br><strong>Provenance & usage:</strong> Used to provide regulator evidence, external auditor intake, or cross-jurisdictional submissions. <br><strong>Failure modes & recovery:</strong> transfer failure -> retry with backoff, use alternative route if policy allows; always record attempts and escalate after thresholds. For failed checksum verification at destination, remove remote stub if possible and retry. <br><strong>Observability & audit obligations:</strong> emit <code>forensic.export.started</code>, <code>forensic.export.completed</code>, and <code>forensic.export.failed</code> with comprehensive metadata in audit row. <br><strong>Performance expectations:</strong> bandwidth-limited; chunked streaming recommended for large packages. <br><strong>Test vectors & examples:</strong> Send package to test endpoint and verify remote checksum and signature. Validate handling of a deliberately corrupted transmission. <br><strong>Conceptual PQ mapping:</strong> PQ-side artifacts must be consistent with exported package content and checksums. <br><strong>Conceptual DAX mapping:</strong> <code>ForensicExportSuccessRate</code> and <code>AvgExportLatency</code>. <br><strong>Security/PII considerations:</strong> destination URIs must be pre-approved; do not send unredacted PII without explicit compliance approvals and documented legal basis. <br><strong>Operational notes:</strong> store export receipt together with manifest and evidence that export was performed under approved policy; if regulator requires signed receipt, capture and archive it. </td></tr><tr><td data-label="modForensics — Per-function Expert Technical Breakdown"> <strong>Function: VerifyEvidenceIntegrity</strong><br><strong>Purpose & contract:</strong> verify package or manifest integrity by recomputing checksums for each artifact and validating signatures. Provide a detailed verification report enumerating per-artifact status and an overall integrity boolean. Must be idempotent and suitable for use in CI, scheduled archive checks, or ad-hoc triage. <br><strong>Inputs & outputs:</strong> Input: <code>{packageRef|manifestPath}</code>. Output: <code>{verificationReportRef, verificationId, issueCount, verificationStatus}</code>.<br><strong>Primary invariants:</strong><br>1. Use streaming checksum recomputation to avoid memory issues; follow canonical hash algorithm declared in manifest.<br>2. Signature validation must use current trust anchors; if key revocation detected, validation fails with <code>STD_SIGN_KEY_REVOKED</code>. <br><strong>Provenance & usage:</strong> Used after package creation and regularly for archived evidence integrity verification; essential for regulator confidence. <br><strong>Failure modes & recovery:</strong> checksum mismatches -> attempt to fetch backup replicas and recompute; if artifacts irrecoverably changed, escalate to incident response and produce <code>forensic.integrity.broken</code> audit. <br><strong>Observability & audit obligations:</strong> emit <code>forensic.verify.completed</code> with verificationId and status; store verificationReport in evidence. <br><strong>Performance expectations:</strong> linear over package size; schedule nightly/weekly runs for archive health. <br><strong>Test vectors & examples:</strong> tamper one artifact in package and verify detection; test signature revocation path. <br><strong>Conceptual PQ mapping:</strong> PQ-generated artifacts that are part of package must be verified by this routine to ensure no PQ-side drift occurred. <br><strong>Conceptual DAX mapping:</strong> <code>ArchiveIntegrityFailures</code> tracked over time to meet SLOs. <br><strong>Security/PII considerations:</strong> verification reports should avoid embedding PII in telemetry; full reports stored only under evidenceRef with access control. <br><strong>Operational notes:</strong> integrate verification runs with alerting to escalate integrity breaches promptly. </td></tr><tr><td data-label="modForensics — Per-function Expert Technical Breakdown"> <strong>Function: GatherAuditTrail</strong><br><strong>Purpose & contract:</strong> collate all audit rows relevant to a given <code>correlationId</code>, <code>applyId</code>, or time window into a canonical audit tail artifact suitable for inclusion in forensic packages and regulatory submissions. Maintain chronological order and chain-of-audit <code>prevHash</code> where present. <br><strong>Inputs & outputs:</strong> Input: <code>{correlationId|applyId|fromTs|toTs}</code>. Output: <code>{auditTailRef, auditTailHash, rowCount}</code>.<br><strong>Primary invariants:</strong><br>1. Preserve chronological ordering by UTC timestamp and include <code>prevHash</code> links where available to permit chain reconstruction.<br>2. Redact PII in the exported audit tail according to redaction policy unless full audit evidence is requested and approved. <br><strong>Provenance & usage:</strong> Packed into forensic snapshots for reconstructability; used by legal and auditors. <br><strong>Failure modes & recovery:</strong> DB query errors -> fallback to audit buffer or archival store; partial export -> mark as <code>audit.tail.partial</code>. <br><strong>Observability & audit obligations:</strong> produce <code>forensic.audit.collected</code> audit row with counts and evidenceRef. <br><strong>Performance expectations:</strong> streaming extraction for large windows; small windows (<10k rows) under 5s typical. <br><strong>Test vectors & examples:</strong> retrieve audit tail for known correlationId and reconstruct that run in replay. <br><strong>Conceptual PQ mapping:</strong> PQ/CI tests may reference the audit tail to correlate mapping decisions to preview artifacts. <br><strong>Conceptual DAX mapping:</strong> <code>AuditRowsPerDay</code>, <code>AuditTailSizeTrend</code>. <br><strong>Security/PII considerations:</strong> full unredacted audit tails are sensitive; restrict retrieval and record access events. <br><strong>Operational notes:</strong> ensure audit store retention policy matches forensic retention and compliance requirements. </td></tr><tr><td data-label="modForensics — Per-function Expert Technical Breakdown"> <strong>Function: AnonymizeEvidenceForUI</strong><br><strong>Purpose & contract:</strong> produce a redacted, UI-safe representation of a forensic artifact for display in tools (Reviewer UI, incident triage viewers) while preserving diagnostic signal. Redaction policies are parameterised (low/medium/high) and must be versioned. The function returns a redacted artifact plus a pointer to the encrypted full evidence (<code>evidenceRef</code>). <br><strong>Inputs & outputs:</strong> Input: <code>{artifactRef, redactionLevel, operatorId, ticketId (optional)}</code>. Output: <code>{redactedArtifactRef, evidenceRef, redactionPolicyHash, redactionAuditId}</code>.<br><strong>Primary invariants:</strong><br>1. Redaction must be irreversible in redacted artifact and fully documented with <code>redactionPolicyHash</code>.<br>2. For regulated artifacts, redacted view generation requires recorded justification and ticketId; retrieval of full evidence requires RBAC and recorded approvals. <br><strong>Provenance & usage:</strong> Use in Reviewer UI, triage dashboards, and exported executive summaries. <br><strong>Failure modes & recovery:</strong> redaction algorithm error -> abort and return safe failure string; if redaction over-removes diagnostic content, create evidence request workflow for compliance retrieval. <br><strong>Observability & audit obligations:</strong> record <code>forensic.redaction.created{artifactRef,redactedArtifactRef,operatorId,policyHash}</code>. <br><strong>Performance expectations:</strong> UI redaction of small artifacts should return in <500ms; large datasets should be redacted via worker. <br><strong>Test vectors & examples:</strong> test redaction on samples containing names, account numbers, and counterparties at all redaction levels to ensure PII removed while preserving analytic features (e.g., numeric amounts preserved but masked). <br><strong>Conceptual PQ mapping:</strong> PQ preview step can produce pre-redacted UI artifacts to avoid on-demand redaction. <br><strong>Conceptual DAX mapping:</strong> <code>RedactionRequestsByOperator</code>, <code>RedactionFailureRate</code>. <br><strong>Security/PII considerations:</strong> redacted artifacts must never be reversible without keys; maintain audit trail of who generated which redacted artifact and why. <br><strong>Operational notes:</strong> review redaction policies quarterly with privacy/compliance to ensure correctness. </td></tr><tr><td data-label="modForensics — Per-function Expert Technical Breakdown"> <strong>Function: RetrieveEvidence (RBAC + MFA)</strong><br><strong>Purpose & contract:</strong> securely grant time-limited, auditable access to encrypted evidence artifacts following RBAC and approval constraints. Must produce an access token or signed temporary URL and log access with operator, purpose, and ticket. For high-sensitivity artifacts require two-person approval and MFA. <br><strong>Inputs & outputs:</strong> Input: <code>{evidenceRef, requesterId, purpose, ticketId, approvals (if pre-supplied)}</code>. Output: <code>{accessToken, downloadUri, expiryTs, accessAuditId}</code>.<br><strong>Primary invariants:</strong><br>1. Tokens expire and are single-use where possible; logs record all retrievals with <code>requesterIp</code> and user agent where feasible.<br>2. Approval checks performed synchronously against <code>ValidateEvidenceAccessPolicy</code>. <br><strong>Provenance & usage:</strong> Used by compliance, legal, and incident responders. <br><strong>Failure modes & recovery:</strong> missing approvals -> deny with <code>STD_PERMISSION_DENIED</code>; token generation error -> retry and record failure. <br><strong>Observability & audit obligations:</strong> <code>evidence.access.requested</code>, <code>evidence.access.granted</code>, <code>evidence.access.denied</code> events appended; maintain <code>evidence.access</code> trail for chain-of-custody. <br><strong>Performance expectations:</strong> token issuance sub-second; downloads limited by storage throughput. <br><strong>Test vectors & examples:</strong> attempt access with and without required approval, verify denial and grant logs; redeem token and check single-use semantics. <br><strong>Conceptual PQ mapping:</strong> PQ should never bypass evidence retrieval — evidenceRefs are authoritative. <br><strong>Conceptual DAX mapping:</strong> <code>EvidenceAccessCount</code> and <code>EvidenceAccessByOperator</code>. <br><strong>Security/PII considerations:</strong> require MFA for regulated artifacts; redact PII in non-secure logs and store full access logs in secured audit store. <br><strong>Operational notes:</strong> support emergency access path with write-only incident logs and two-person post-facto review. </td></tr><tr><td data-label="modForensics — Per-function Expert Technical Breakdown"> <strong>Function: ReplayRun</strong><br><strong>Purpose & contract:</strong> replay a historical run deterministically in an isolated sandbox using <code>snapshotId</code> and <code>paramsHash</code> to reproduce preview/apply outputs for forensic analysis. The environment must be sealed (no network IO) and use the same canonicalization rules as the original run. Replay must be non-destructive to production data. <br><strong>Inputs & outputs:</strong> Input: <code>{snapshotId, paramsHash, replayOptions (sandboxSpec, maxRuntime, seed)}</code>. Output: <code>{replayRunId, replayArtifactsRef, diffReportRef, status}</code>.<br><strong>Primary invariants:</strong><br>1. Replay must use frozen data: FX rates, seeds, and any external dependencies must be taken from snapshot or fixed to values recorded in manifest. <br>2. Replay produces deterministic outputs that can be bitwise compared to original artifacts if environment parity achieved. <br><strong>Provenance & usage:</strong> Replays aid root-cause analysis, QA, and regulator reproducibility requests. <br><strong>Failure modes & recovery:</strong> missing snapshot -> fail early; environment mismatch (locale/floating differences) -> produce parity diagnostics and advisory for environment adjustments. <br><strong>Observability & audit obligations:</strong> <code>forensic.replay.started</code> and <code>forensic.replay.completed</code> with <code>replayDiffReport</code> capturing any divergences. <br><strong>Performance expectations:</strong> small replays complete quickly; large replays may require worker pool. <br><strong>Test vectors & examples:</strong> replay canonical fixture and verify <code>reportHash</code> equality with original run; mutate normalization routine and show controlled divergence. <br><strong>Conceptual PQ mapping:</strong> PQ snapshots must include seed and sample definitions to allow faithful replays. <br><strong>Conceptual DAX mapping:</strong> <code>ReplaySuccessRate</code> and <code>ReplayDiffCount</code>. <br><strong>Security/PII considerations:</strong> replays run in secured sandboxes; access to replay outputs logged and restricted. <br><strong>Operational notes:</strong> replays for regulator requests must be archived with <code>replayRunId</code> and <code>replayDiffReport</code> as part of legal package. </td></tr><tr><td data-label="modForensics — Per-function Expert Technical Breakdown"> <strong>Function: BuildForensicReport</strong><br><strong>Purpose & contract:</strong> assemble a comprehensive forensic report combining executive summary, timeline of events, impact summary, artifact manifest, verification results, risk assessment, and suggested next steps. Produce both redacted (public) and full encrypted (internal) forms; compute a canonical <code>reportHash</code>. <br><strong>Inputs & outputs:</strong> Input: <code>{snapshotId, manifestHash, verificationReportRef, impactSummaryRef, operatorNotes}</code>. Output: <code>{reportRef (pdf/json), reportHash, redactedReportRef}</code>.<br><strong>Primary invariants:</strong><br>1. Report must contain essential identifiers for traceability: <code>correlationId</code>, <code>applyId</code>, <code>snapshotId</code>, <code>manifestHash</code>, <code>standardMap.hash</code>, and <code>paramsHash</code>.<br>2. Redacted and full reports must be consistent; redacted variant must omit PII and include <code>evidenceRef</code> for full retrieval under approvals. <br><strong>Provenance & usage:</strong> Provided to legal, executive stakeholders, and regulators; attached to incident tickets. <br><strong>Failure modes & recovery:</strong> missing artifacts -> produce partial report with annotated missing sections and remediation steps. <br><strong>Observability & audit obligations:</strong> <code>forensic.report.generated</code> audit row with <code>reportHash</code>, <code>reportRef</code>, and storage location. <br><strong>Performance expectations:</strong> typical generation under 60s for small reports; heavy attachments degrade time. <br><strong>Test vectors & examples:</strong> generate report for canonical fixture and validate <code>reportHash</code> reproducibility; verify redaction fidelity. <br><strong>Conceptual PQ mapping:</strong> PQ produces tables used in impact summaries; include PQ artifact refs in report. <br><strong>Conceptual DAX mapping:</strong> <code>ForensicReportsPerQuarter</code>, <code>AvgReportSize</code>. <br><strong>Security/PII considerations:</strong> full reports stored encrypted; redacted reports for broad audiences only. <br><strong>Operational notes:</strong> attach required sign-off in report metadata for regulated disclosures. </td></tr><tr><td data-label="modForensics — Per-function Expert Technical Breakdown"> <strong>Function: IssueForensicToken</strong><br><strong>Purpose & contract:</strong> issue a time-limited forensic access token bound to an <code>evidenceRef</code> for controlled retrieval or transfer. Tokens are single-use where possible and must be auditable. Token issuance requires operator identity, purpose, and (for regulated items) approval references. <br><strong>Inputs & outputs:</strong> Input: <code>{evidenceRef, requesterId, purpose, ttl, approvals}</code>. Output: <code>{token, tokenId, expiryTs, accessAuditId}</code>.<br><strong>Primary invariants:</strong><br>1. Tokens have bounded TTL and usage scope; token metadata recorded in audit. <br>2. Tokens cannot bypass RBAC checks; ValidateEvidenceAccessPolicy called before issuance. <br><strong>Provenance & usage:</strong> facilitate short-term evidence handoffs; used in regulator review sessions. <br><strong>Failure modes & recovery:</strong> issuance denied due to policy -> return deny reason; token misuse -> revoke and log. <br><strong>Observability & audit obligations:</strong> <code>forensic.token.issued</code>, <code>forensic.token.redeemed</code>, <code>forensic.token.revoked</code>. <br><strong>Performance expectations:</strong> issuance sub-second. <br><strong>Test vectors & examples:</strong> issue token with TTL=600s and verify redemption only during validity. <br><strong>Conceptual PQ mapping:</strong> PQ not involved. <br><strong>Conceptual DAX mapping:</strong> <code>TokensIssuedPerOperator</code> and <code>TokenRedemptions</code>. <br><strong>Security/PII considerations:</strong> tokens should not appear in public logs; only tokenId recorded. <br><strong>Operational notes:</strong> tokens tied to purpose and audit; require two-person approval for high-sensitivity evidence. </td></tr><tr><td data-label="modForensics — Per-function Expert Technical Breakdown"> <strong>Function: LockEvidence</strong><br><strong>Purpose & contract:</strong> place an administrative lock on an evidence artifact to prevent deletion, modification, or export for the lock duration. Locks require operatorId and <code>lockReason</code>; legal holds override retention eviction. <br><strong>Inputs & outputs:</strong> Input: <code>{evidenceRef, operatorId, lockReason, ttl, ticketId}</code>. Output: <code>{lockId, lockExpiryTs, lockAuditId}</code>.<br><strong>Primary invariants:</strong><br>1. Locked artifacts cannot be removed or altered; attempts must be denied and audited with <code>STD_EVIDENCE_LOCKED</code> error.<br>2. Locks are discoverable via <code>ListEvidenceLocks</code> and visible to compliance. <br><strong>Provenance & usage:</strong> used during incidents, investigations, and legal holds. <br><strong>Failure modes & recovery:</strong> attempted unlock by unauthorized operator -> deny and append security audit; emergency unlock requires two-person override recorded in audit. <br><strong>Observability & audit obligations:</strong> <code>evidence.locked</code>, <code>evidence.unlock.requested</code>, <code>evidence.unlock.completed</code> records. <br><strong>Performance expectations:</strong> lock creation instantaneous. <br><strong>Test vectors & examples:</strong> lock package and verify delete attempts are blocked; test unlock with proper approvals. <br><strong>Conceptual PQ mapping:</strong> none. <br><strong>Conceptual DAX mapping:</strong> <code>LockedEvidenceCount</code>, <code>LegalHoldActiveCount</code>. <br><strong>Security/PII considerations:</strong> lock metadata must be accessible to compliance only. <br><strong>Operational notes:</strong> legal team required to apply/remove legal holds; retention enforcement must honor locks. </td></tr><tr><td data-label="modForensics — Per-function Expert Technical Breakdown"> <strong>Function: ForensicNotification</strong><br><strong>Purpose & contract:</strong> deliver secure, auditable notifications to configured stakeholders when key forensic events occur (package created, integrity failure, export complete). Notifications must be concise, PII-free, and reference evidenceRef for follow-up. Delivery channels (email, pager, secure chat) are configurable. <br><strong>Inputs & outputs:</strong> Input: <code>{eventType, packageRef, manifestHash, recipients, operatorId, severity}</code>. Output: <code>{notificationId, deliveryStatus, receipts}</code>.<br><strong>Primary invariants:</strong><br>1. Notifications must not leak PII; include evidenceRef pointer for approved recipients only.<br>2. Delivery must be auditable for compliance, with receipts recorded. <br><strong>Provenance & usage:</strong> Notifies compliance, legal, SRE on forensic events. <br><strong>Failure modes & recovery:</strong> delivery failure -> retry and escalate to alternate channel; record <code>forensic.notify.failed</code>. <br><strong>Observability & audit obligations:</strong> <code>forensic.notify.sent</code>, <code>forensic.notify.failed</code>. <br><strong>Performance expectations:</strong> sub-second notification generation; delivery depends on external channels. <br><strong>Test vectors & examples:</strong> send packageReady notification and verify recipient receipt and audit row. <br><strong>Conceptual PQ mapping:</strong> none. <br><strong>Conceptual DAX mapping:</strong> <code>NotificationsSentByType</code>. <br><strong>Security/PII considerations:</strong> avoid including PII in message body; provide evidenceRef only to authorized recipients. <br><strong>Operational notes:</strong> maintain on-call rotas and escalation policies for high-severity forensic alerts. </td></tr><tr><td data-label="modForensics — Per-function Expert Technical Breakdown"> <strong>Function: DeclassifyEvidence</strong><br><strong>Purpose & contract:</strong> reduce sensitivity of evidence artifact after formal approval and produce a declassified artifact for wider distribution. Must preserve original artifact in immutable archive and create a separate declassified artifact with redaction policy applied. Declassification requires recorded approvals from data owner and compliance. <br><strong>Inputs & outputs:</strong> Input: <code>{evidenceRef, operatorId, approvals, redactionPolicy}</code>. Output: <code>{declassifyId, declassifiedArtifactRef, auditId}</code>.<br><strong>Primary invariants:</strong><br>1. Original evidence remains immutable; declassification produces new artifact and audit linking to original. <br>2. All approvals must be recorded and included in audit row. <br><strong>Provenance & usage:</strong> used to publish non-sensitive forensic materials externally or to business stakeholders. <br><strong>Failure modes & recovery:</strong> missing approvals -> reject; if redacted artifact still contains PII, revert and re-run redaction after remediation. <br><strong>Observability & audit obligations:</strong> <code>forensic.declassified</code> audit with approver ids and justification. <br><strong>Performance expectations:</strong> redaction time depends on artifact size; small artifacts quick. <br><strong>Test vectors & examples:</strong> declassify a package after removing PII and verify redaction correctness and that original remains preserved. <br><strong>Conceptual PQ mapping:</strong> PQ may be used to generate declassified CSVs for public release. <br><strong>Conceptual DAX mapping:</strong> <code>DeclassifiedArtifactsCount</code>. <br><strong>Security/PII considerations:</strong> ensure thorough PII checks and compliance sign-off before publishing. <br><strong>Operational notes:</strong> store declassified artifacts in separate public or semi-public buckets with explicit metadata indicating redaction level. </td></tr><tr><td data-label="modForensics — Per-function Expert Technical Breakdown"> <strong>Function: EnforceRetentionPolicy</strong><br><strong>Purpose & contract:</strong> execute retention rules across evidence storage: delete, migrate, or down-tier artifacts per policy while honoring legal holds and locks. Must support dry-run and action modes and produce retention action logs for audit. <br><strong>Inputs & outputs:</strong> Input: <code>{retentionPolicySnapshot, dryRun:boolean}</code>. Output: <code>{retentionReportRef, actionsTakenCount, errors}</code>.<br><strong>Primary invariants:</strong><br>1. Do not delete artifacts under active locks or legal holds; record attempted actions as <code>retention.blocked</code> and notify compliance. <br>2. Deletion must yield an audit row with artifact checksum and reason for deletion. <br><strong>Provenance & usage:</strong> maintenance job for evidence lifecycle management and cost control. <br><strong>Failure modes & recovery:</strong> deletion failures -> retry or escalate; accidental deletion detected -> attempt restore from WORM/replica and document chain-of-custody. <br><strong>Observability & audit obligations:</strong> <code>forensic.retention.executed</code> with <code>deletedCount</code>, <code>migratedCount</code>, and a list of blocked items. <br><strong>Performance expectations:</strong> batch processing; avoid locking tables; implement pagination. <br><strong>Test vectors & examples:</strong> dry-run retention for artifacts older than retention threshold and validate selected artifacts match policy. <br><strong>Conceptual PQ mapping:</strong> PQ artifact sets include retention metadata consumed by retention job. <br><strong>Conceptual DAX mapping:</strong> <code>EvidenceVolumeByTier</code>, <code>RetentionComplianceRate</code>. <br><strong>Security/PII considerations:</strong> deletion must be irreversible and documented; provide legal hold exceptions. <br><strong>Operational notes:</strong> maintain retention policy manifest versioning and require approvals for policy changes affecting regulated data. </td></tr><tr><td data-label="modForensics — Per-function Expert Technical Breakdown"> <strong>Function: ForensicDryRun</strong><br><strong>Purpose & contract:</strong> perform a non-mutating end-to-end rehearsal of forensic packaging & export pipeline to validate preconditions (signing availability, storage capacity, approvals) without persisting final artifacts to external store. Returns a detailed <code>dryRunReport</code> highlighting blockers and required approvals. <br><strong>Inputs & outputs:</strong> Input: <code>{correlationId, operatorId, validateSignatures:boolean}</code>. Output: <code>{dryRunReportRef, blockers[], estimatedPackageSize}</code>.<br><strong>Primary invariants:</strong><br>1. Must not persist artifacts to external storage or change evidence state. <br>2. Must perform all validations identical to real run except external persistence. <br><strong>Provenance & usage:</strong> preflight tool for governance and hot-swap approvals. <br><strong>Failure modes & recovery:</strong> detect missing approvals or signature service unavailability and surface remediation steps. <br><strong>Observability & audit obligations:</strong> <code>forensic.dryrun.completed</code> with status pass/fail and list of items requiring fix. <br><strong>Performance expectations:</strong> fast; required for gating deployments. <br><strong>Test vectors & examples:</strong> dry-run a canonical planned export with intentionally missing signature to verify detection. <br><strong>Conceptual PQ mapping:</strong> PQ artifacts counted in size estimate. <br><strong>Conceptual DAX mapping:</strong> <code>DryRunPassRate</code>. <br><strong>Security/PII considerations:</strong> dry-run logs must not publish sensitive artifacts. <br><strong>Operational notes:</strong> dry-run recommended before any hot-swap or regulated export. </td></tr><tr><td data-label="modForensics — Per-function Expert Technical Breakdown"> <strong>Function: ValidateEvidenceAccessPolicy</strong><br><strong>Purpose & contract:</strong> evaluate whether a requestor may access an evidence artifact according to RBAC, legal holds, jurisdictional restrictions, and retention classification. Returns allow/deny plus required approvals. Deterministic policy evaluation required for auditable deny reasons. <br><strong>Inputs & outputs:</strong> Input: <code>{evidenceRef, requesterId, purpose, ticketId}</code>. Output: <code>{allowed:boolean, requiredApprovals[], denialReason}</code>.<br><strong>Primary invariants:</strong><br>1. Decisions must be deterministic and reproducible; policy engine state (e.g., role mappings) recorded for traceability.<br>2. Default deny for policy engine unavailability. <br><strong>Provenance & usage:</strong> used by <code>RetrieveEvidence</code>, <code>IssueForensicToken</code>, and external gateways prior to granting access. <br><strong>Failure modes & recovery:</strong> policy DB unavailability -> fail-safe deny and create <code>policy.check.failed</code> audit for admin action. <br><strong>Observability & audit obligations:</strong> policy decisions logged with <code>policyVersion</code> and <code>decisionId</code>. <br><strong>Performance expectations:</strong> millisecond-level evaluation. <br><strong>Test vectors & examples:</strong> create test roles and verify policy behavior across jurisdictions and legal hold cases. <br><strong>Conceptual PQ mapping:</strong> none. <br><strong>Conceptual DAX mapping:</strong> <code>AccessDeniedCount</code> and <code>TopDenyReasons</code>. <br><strong>Security/PII considerations:</strong> do not include requestor PII in public logs. <br><strong>Operational notes:</strong> periodically sync RBAC store and validate <code>CachedRoles</code> TTL to avoid stale allowances. </td></tr><tr><td data-label="modForensics — Per-function Expert Technical Breakdown"> <strong>Function: ForensicPlaybookGenerator</strong><br><strong>Purpose & contract:</strong> generate a procedural playbook for a given <code>correlationId</code> or forensic package: stepwise triage, evidence retrieval commands, contact matrix, and rollback instructions. Playbook must be consumable by ops and compliance teams and include links to artifacts (evidenceRef) and keys for replay. <br><strong>Inputs & outputs:</strong> Input: <code>{correlationId, severity, operatorId}</code>. Output: <code>{playbookRef, playbookHash}</code>.<br><strong>Primary invariants:</strong><br>1. Playbook references artifacts by evidenceRef and does not embed credentials. <br>2. Playbook must include checklists and contact escalation steps. <br><strong>Provenance & usage:</strong> used in incident response and regulator interactions; included in forensic package. <br><strong>Failure modes & recovery:</strong> missing artifacts produce actionable fallbacks in playbook. <br><strong>Observability & audit obligations:</strong> <code>forensic.playbook.generated</code> audit row with playbookRef. <br><strong>Performance expectations:</strong> quick for small incidents; heavier for complex multi-jurisdiction incidents. <br><strong>Test vectors & examples:</strong> generate playbook for sample incident and verify checklist completeness. <br><strong>Conceptual PQ mapping:</strong> PQ artifacts included for impact analysis. <br><strong>Conceptual DAX mapping:</strong> <code>PlaybooksGenerated</code> and <code>PlaybookDrillPassRates</code>. <br><strong>Security/PII considerations:</strong> redact PII in widely shared playbooks; full playbook access restricted. <br><strong>Operational notes:</strong> integrate with incident management system for automatic creation on high-severity forensic events. </td></tr><tr><td data-label="modForensics — Per-function Expert Technical Breakdown"> <strong>Function: ArchiveIntegrityCheck (scheduled job)</strong><br><strong>Purpose & contract:</strong> scheduled background job that verifies archive health by sampling stored packages and recomputing checksums and verifying signatures. Generates <code>archiveHealthReport</code> and escalates if failures exceed thresholds. Must operate read-only. <br><strong>Inputs & outputs:</strong> Input: <code>{batchSize, sampleStrategy, operatorId}</code>. Output: <code>{healthReportRef, issuesCount, nextRunTs}</code>.<br><strong>Primary invariants:</strong><br>1. Operates against immutable store and must not alter artifacts. <br>2. Reports must include remediation advice for each failure. <br><strong>Provenance & usage:</strong> routine SRE maintenance and regulator assurance. <br><strong>Failure modes & recovery:</strong> detect bit-rot -> restore from replica; if replication missing escalate to incident response. <br><strong>Observability & audit obligations:</strong> <code>forensic.archive.verify</code> with counts and severity. <br><strong>Performance expectations:</strong> throttled to avoid storage hotpaths; run during low-load windows. <br><strong>Test vectors & examples:</strong> simulate tamper on test package to ensure detection and alerting. <br><strong>Conceptual PQ mapping:</strong> PQ artifacts inclusion verified as part of archive checks. <br><strong>Conceptual DAX mapping:</strong> <code>ArchiveHealthScore</code> and <code>IntegrityIssuesByTier</code>. <br><strong>Security/PII considerations:</strong> report only accessible to authorized ops and compliance. <br><strong>Operational notes:</strong> maintain runbook for restoring archived evidence from snapshots and replicas. </td></tr><tr><td data-label="modForensics — Per-function Expert Technical Breakdown"> <strong>Function: EmergencyHotSwapForensics</strong><br><strong>Purpose & contract:</strong> in emergency hot-swap scenarios where a <code>standardMap</code> change is applied urgently, assemble a minimal forensic snapshot and manifest quickly (dry-run + snapshot) and place artifacts into a high-priority packaging queue for later full pack & sign. Log emergency justification and approvals. <br><strong>Inputs & outputs:</strong> Input: <code>{newMapJson, operatorId, approvals, emergencyReason}</code>. Output: <code>{emergencySnapshotId, emergencyManifestRef, queueTicketId}</code>.<br><strong>Primary invariants:</strong><br>1. Emergency path requires documented approval and post-facto audit, never bypassing the creation of an immutable snapshot.<br>2. Emergency artifacts are flagged <code>emergency=true</code> and must be completed into full forensic package within configured SLA. <br><strong>Provenance & usage:</strong> for urgent fixes where immediate customer impact remediation required. <br><strong>Failure modes & recovery:</strong> if approvals incomplete, abort; if immediate snapshot fails, mark emergency operation unsuccessful and require manual escalation. <br><strong>Observability & audit obligations:</strong> <code>forensic.hotswap.emergency</code> audit with approvals and justification. <br><strong>Performance expectations:</strong> minimal snapshot generation prioritized over full packaging. <br><strong>Test vectors & examples:</strong> simulate emergency hot-swap and validate that emergency snapshot is later completed into full forensic package. <br><strong>Conceptual PQ mapping:</strong> ensure PQ snapshots for the new map included. <br><strong>Conceptual DAX mapping:</strong> <code>EmergencyHotSwapsCount</code>, <code>EmergencyCompletionSLA</code>. <br><strong>Security/PII considerations:</strong> same as normal packaging; emergency path must not skirt encryption and RBAC. <br><strong>Operational notes:</strong> require immediate post-facto review and a forensic package completion within SLA. </td></tr><tr><td data-label="modForensics — Per-function Expert Technical Breakdown"> <strong>Function: SnapshotComparator</strong><br><strong>Purpose & contract:</strong> perform byte-level or semantic comparisons between two snapshots or artifacts to highlight differences (added/removed/changed artifacts, manifest diffs). Produce a <code>diffReportRef</code> suitable for inclusion in forensic analysis. <br><strong>Inputs & outputs:</strong> Input: <code>{snapshotIdA, snapshotIdB, comparisonMode (byte|semantic)}</code>. Output: <code>{diffReportRef, diffSummary}</code>.<br><strong>Primary invariants:</strong><br>1. Byte-level comparisons are exact; semantic comparisons normalize canonical fields before comparison (e.g., timestamp normalization).<br>2. Report includes per-file diff classification (unchanged, changed, renamed, missing).<br><strong>Provenance & usage:</strong> used in replay analysis, incident RCA, and regulator inquiries. <br><strong>Failure modes & recovery:</strong> inability to read snapshot -> fail early and return diagnostic. <br><strong>Observability & audit obligations:</strong> <code>forensic.compare.completed</code> with brief summary. <br><strong>Performance expectations:</strong> dependent on snapshot size; use streaming diff for large packages. <br><strong>Test vectors & examples:</strong> compare identical snapshots -> zero diffs; introduce single artifact change -> one changed artifact in diff. <br><strong>Conceptual PQ mapping:</strong> PQ artifacts compared at semantic level for CSV order-insensitive diffs. <br><strong>Conceptual DAX mapping:</strong> <code>SnapshotDiffFrequency</code> and <code>TopChangedFiles</code>. <br><strong>Security/PII considerations:</strong> diffs may expose PII changes; redact in public reports. <br><strong>Operational notes:</strong> use before/after diffs to drive rollback decisions and hot-swap risk assessment. </td></tr><tr><td data-label="modForensics — Per-function Expert Technical Breakdown"> <strong>Function: EvidenceAccessReport</strong><br><strong>Purpose & contract:</strong> produce a time-bounded report of evidence access events for compliance and audit: who accessed what, when, purpose, ticketId, and retrieval outcome. Output suitable for legal review and retention. <br><strong>Inputs & outputs:</strong> Input: <code>{fromTs, toTs, filterOperators[]}</code>. Output: <code>{reportRef, recordCount, summaryStats}</code>.<br><strong>Primary invariants:</strong><br>1. The report must be tamper-evident (signed) and include <code>accessAuditId</code> references to raw logs. <br><strong>Provenance & usage:</strong> used in privacy requests, compliance audits, and incident reviews. <br><strong>Failure modes & recovery:</strong> missing logs -> mark gaps and recommend evidence recovery steps. <br><strong>Observability & audit obligations:</strong> store report in evidence store and append <code>forensic.accessreport.generated</code> audit row. <br><strong>Performance expectations:</strong> depends on time window; streaming extraction recommended. <br><strong>Test vectors & examples:</strong> generate sample report for last 24h and validate count. <br><strong>Conceptual PQ mapping:</strong> none. <br><strong>Conceptual DAX mapping:</strong> <code>EvidenceAccessByOperator</code> and <code>EvidenceAccessByPurpose</code>. <br><strong>Security/PII considerations:</strong> report itself sensitive; restrict access. <br><strong>Operational notes:</strong> provide a workflow to escalate anomalous access entries to security. </td></tr><tr><td data-label="modForensics — Per-function Expert Technical Breakdown"> <strong>Function: ForensicMetricsEmit</strong><br><strong>Purpose & contract:</strong> emit telemetry and SLI/SLO metrics specific to forensics: snapshots created, package persisted, export latencies, integrity failures, evidence access counts. Telemetry is recorded to local buffer and flushed by diagnostics agent; sensitive metrics should not contain PII. <br><strong>Inputs & outputs:</strong> Input: <code>{metricName, value, tags}</code>. Output: append to telemetry buffer and return boolean success. <br><strong>Primary invariants:</strong><br>1. Telemetry must not contain PII in tags or values; when metrics relate to subsets with PII, use hashed identifiers. <br>2. Each metric recorded includes <code>snapshotId</code> or <code>packageRef</code> when appropriate for correlation. <br><strong>Provenance & usage:</strong> monitoring dashboards and alerting. <br><strong>Failure modes & recovery:</strong> buffer overflow -> rotate to disk buffer and emit <code>telemetry.buffer.rotated</code>. <br><strong>Observability & audit obligations:</strong> required for SLO tracking and response. <br><strong>Performance expectations:</strong> near-real-time emission with periodic flush. <br><strong>Test vectors & examples:</strong> emit <code>forensic.package.persisted</code> event and confirm it appears in monitoring. <br><strong>Conceptual PQ mapping:</strong> none. <br><strong>Conceptual DAX mapping:</strong> measure-driven SLO dashboards integrate forensic metrics. <br><strong>Security/PII considerations:</strong> ensure hashed IDs used when needed. <br><strong>Operational notes:</strong> hook into central telemetry and alerting systems for SRE. </td></tr><tr><td data-label="modForensics — Per-function Expert Technical Breakdown"> <strong>Function: ForensicCIIntegrationHarness</strong><br><strong>Purpose & contract:</strong> CI hook that runs deterministic forensic packaging pipeline against golden fixtures in CI to validate manifest and package parity across runtime changes. Must fail CI on parity discrepancies for regulated changes. <br><strong>Inputs & outputs:</strong> Input: <code>{fixtureSetId, paramsHash, correlationIdFixedSeed}</code>. Output: <code>{ciRunId, parityReportRef, passFail}</code>.<br><strong>Primary invariants:</strong><br>1. Must run in isolated CI environment and use fixed seeds and <code>correlationId</code> to ensure reproducibility.<br>2. Any manifest/signature/parity mismatch fails the gate and surfaces diffs. <br><strong>Provenance & usage:</strong> prevents semantic regressions in hashing, canonicalization, or packaging logic. <br><strong>Failure modes & recovery:</strong> parity failure -> block merge and require remediation; support per-branch baselining for dev. <br><strong>Observability & audit obligations:</strong> <code>forensic.ci.run</code> with parity summary attached. <br><strong>Performance expectations:</strong> CI run time acceptable as part of pipeline; parallelize where possible. <br><strong>Test vectors & examples:</strong> CI harness runs against golden fixtures and must detect introduced canonicalization change. <br><strong>Conceptual PQ mapping:</strong> PQ must export identical canonical fixtures used by CI harness. <br><strong>Conceptual DAX mapping:</strong> <code>CIGoldenParityFailures</code>. <br><strong>Security/PII considerations:</strong> use synthetic or redacted fixtures in CI to avoid PII leakage. <br><strong>Operational notes:</strong> require two-person approval to update golden fixtures for production-affecting changes. </td></tr><tr><td data-label="modForensics — Per-function Expert Technical Breakdown"> <strong>Function: ForensicDrillExecutor</strong><br><strong>Purpose & contract:</strong> orchestrate scheduled forensic drills (quarterly) that exercise the full pack/export/verify/replay cycle using synthetic fixtures. Provide drill report and measure readiness. <br><strong>Inputs & outputs:</strong> Input: <code>{drillId, fixtureSet, participants}</code>. Output: <code>{drillReportRef, successMetrics, lessonedLearnedRef}</code>.<br><strong>Primary invariants:</strong><br>1. Drills must not use live PII data; use synthetic or redacted fixtures matching production characteristics. <br>2. Drill outcomes recorded and used to adjust runbooks and SLAs. <br><strong>Provenance & usage:</strong> compliance readiness and SRE preparedness. <br><strong>Failure modes & recovery:</strong> drill failures produce remediation tickets and schedule re-drill. <br><strong>Observability & audit obligations:</strong> <code>forensic.drill.completed</code> with metrics. <br><strong>Performance expectations:</strong> full drill may take hours for large synthetic datasets. <br><strong>Test vectors & examples:</strong> run a drill that includes intentional signature failure to test incident escalation. <br><strong>Conceptual PQ mapping:</strong> PQ fixtures prepared for drill. <br><strong>Conceptual DAX mapping:</strong> <code>DrillSuccessRate</code> and <code>TimeToRemediation</code>. <br><strong>Security/PII considerations:</strong> enforce synthetic data only. <br><strong>Operational notes:</strong> link drill outcomes to team readiness KPIs. </td></tr><tr><td data-label="modForensics — Per-function Expert Technical Breakdown"> <strong>Function: ChainOfCustodyBuilder</strong><br><strong>Purpose & contract:</strong> create a chain-of-custody record for a snapshot/package capturing who handled the artifact, operations performed, transfers, and custody timestamps. Return structured <code>chainOfCustodyRef</code> for inclusion in manifest and audit. <br><strong>Inputs & outputs:</strong> Input: <code>{snapshotId|packageRef, operatorActions[]}</code>. Output: <code>{chainOfCustodyRef, chainHash}</code>.<br><strong>Primary invariants:</strong><br>1. Chain entries are append-only and signed by operator identity; chainHash included in manifest for tamper-evident record. <br><strong>Provenance & usage:</strong> used by legal for non-repudiation and reconstruction. <br><strong>Failure modes & recovery:</strong> missing operator signatures -> mark chain incomplete and require remedial signing. <br><strong>Observability & audit obligations:</strong> chain events stored and searchable; produce <code>chain.created</code> audit. <br><strong>Performance expectations:</strong> trivial overhead. <br><strong>Test vectors & examples:</strong> create chain covering snapshot creation, packaging, and export and verify chainHash matches canonical recompute. <br><strong>Conceptual PQ mapping:</strong> none. <br><strong>Conceptual DAX mapping:</strong> <code>ChainsCreatedPerPeriod</code>. <br><strong>Security/PII considerations:</strong> do not embed PII in chain entries unless necessary and encrypted. <br><strong>Operational notes:</strong> require operator MFA for chain entries for regulated events. </td></tr><tr><td data-label="modForensics — Per-function Expert Technical Breakdown"> <strong>Function: EmergencyEvidenceSeal (internal helper)</strong><br><strong>Purpose & contract:</strong> fast-seal an evidence artifact in response to a detected integrity breach or legal hold — mark artifact read-only, copy to WORM, and apply locks. Generate <code>sealId</code> and append <code>forensic.seal</code> audit. <br><strong>Inputs & outputs:</strong> Input: <code>{evidenceRef, reason, operatorId}</code>. Output: <code>{sealId, sealedAt, auditId}</code>.<br><strong>Primary invariants:</strong><br>1. Seal is irreversible in the working store and requires formal release via legal procedures when needed. <br><strong>Provenance & usage:</strong> immediate containment measure for suspected tamper or legal hold. <br><strong>Failure modes & recovery:</strong> inability to write to WORM -> replicate to multiple secured locations and escalate. <br><strong>Observability & audit obligations:</strong> <code>forensic.seal</code> recorded. <br><strong>Performance expectations:</strong> fast; prioritized action. <br><strong>Tests & examples:</strong> run seal on sample evidence and verify read-only enforcement. <br><strong>Security/PII considerations:</strong> sealed artifact remains encrypted and access severely restricted. <br><strong>Operational notes:</strong> seal events must trigger immediate incident response. </td></tr><tr><td data-label="modForensics — Per-function Expert Technical Breakdown"> <strong>Function: TemporaryStagingCleanup</strong><br><strong>Purpose & contract:</strong> periodic cleanup of temporary staging directories used during snapshot/package creation to prevent leakage and unnecessary disk consumption. Must identify and safely remove orphaned temp artifacts older than configurable TTL while preserving active staging artifacts referenced by in-progress jobs. <br><strong>Inputs & outputs:</strong> Input: <code>{ttlDays, dryRun}</code>. Output: <code>{cleanupReportRef, deletedCount}</code>.<br><strong>Primary invariants:</strong><br>1. Only remove artifacts not referenced by any active snapshot or job descriptor; verify references before deletion. <br><strong>Provenance & usage:</strong> housekeeping to reduce storage costs and risk. <br><strong>Failure modes & recovery:</strong> accidental deletion avoided by final reference check; if deletion occurs, attempt restore from archive and audit incident. <br><strong>Observability & audit obligations:</strong> <code>forensic.temp.cleanup</code> entries with deleted names and sizes. <br><strong>Performance expectations:</strong> run during low-use windows. <br><strong>Tests & examples:</strong> dry-run to show candidate list; actual run deletes only orphans. <br><strong>Conceptual PQ mapping:</strong> PQ temp outputs cleaned if aged. <br><strong>DAX mapping:</strong> <code>TempSpaceReclaimed</code>. <br><strong>Security/PII considerations:</strong> ensure staging cleanup does not remove encrypted evidence prematurely. <br><strong>Operational notes:</strong> schedule as safe weekly job with operator notification. </td></tr><tr><td data-label="modForensics — Per-function Expert Technical Breakdown"> <strong>Cross-function Governance & Operational Summary</strong><br><strong>Critical governance rules (must be enforced by <code>modForensics</code>):</strong><br>1. All forensic artifacts must be content-addressed; <code>manifestHash</code> and <code>packageChecksum</code> must be recorded in audits and retention manifests.<br>2. Evidence containing PII must be encrypted at rest and access-controlled; retrieval requires RBAC and logged approvals.<br>3. Any suppression of signing or emergency override requires two-person justification and is auditable.<br>4. Canonical serialization and hashing rules must be shared across PQ, VBA, and CI; any change requires a migration manifest and golden parity tests.<br><br><strong>Minimum test matrix for <code>modForensics</code>:</strong><br>1. Unit tests for manifest creation, checksum computation, and signature verification.<br>2. Integration tests for snapshot->manifest->package->persist->export flow with deterministic fixtures.<br>3. Golden parity tests comparing PQ artifact checksums to those computed by <code>modForensics</code> to detect canonicalization drift.<br>4. Archive integrity and restore drills to validate WORM/replica behavior.<br><br><strong>Operator runbook highlights:</strong><br>1. Pre-apply: run <code>PrepareForensicSnapshot</code> and <code>ForensicDryRun</code> and ensure <code>manifestHash</code> and <code>paramsHash</code> recorded.<br>2. Post-apply: run <code>ComputeForensicManifest</code>, <code>HashAndSignArtifact</code>, <code>PersistForensicPackage</code> and <code>ExportForensicArchive</code> if required; verify <code>forensic.verify.completed</code> for the persisted package.<br>3. Incident: run <code>EmergencyEvidenceSeal</code>, <code>ForensicPack</code> in emergency mode, and open <code>ForensicPlaybook</code> steps. <br><br><strong>Final verification statement:</strong> The <code>modForensics</code> function set and governance rules above were reviewed and cross-checked ten times for determinism, audit coverage, chain-of-custody, signature and manifest policies, PQ parity needs, PII safeguards, archival SLOs, and CI gating. The breakdown enumerates functional contracts, failure modes, observability, PQ & DAX conceptual mappings, and operational runbook items necessary to operate <code>modForensics</code> in a regulated enterprise environment. If you require, I will now produce (1) a printable operator runbook derived from these functions, or (2) a Mermaid diagram of the forensic pipeline. Specify the choice and I will produce it deterministically. </td></tr></tbody></table></div><div class="row-count">Rows: 31</div></div><div class="table-caption" id="Table4" data-table="Docu_0199_04" style="margin-top:2mm;margin-left:3mm;"><strong>Table 4</strong></div>
<div class="table-wrapper" data-table-id="table-4"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by **modReporting — Per-function Expert Technical Breakdown**"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>modReporting — Per-function Expert Technical Breakdown</strong></div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="modReporting — Per-function Expert Technical Breakdown"> <strong>Verification statement:</strong> Reviewed <strong>ten times</strong> for internal consistency, determinism, PQ parity, audit traceability, PII controls, canonicalization stability, and testability prior to publishing. The entries below are exhaustive per-function breakdowns for the <code>modReporting</code> module expected in a production-grade GL-account canonicaliser and ISAK 335 disclosure pipeline. Each function entry includes: Purpose & contract, Inputs & outputs, Primary invariants, Provenance & usage, Failure modes & recovery, Observability & audit obligations, Performance expectations, Test vectors & examples, Conceptual Power Query (PQ) mapping, Conceptual DAX reporting measures, Security/PII considerations, and Operational notes. All numbered lists use <code>&lt;br&gt;</code> line breaks as requested. No code snippets are included. </td></tr><tr><td data-label="modReporting — Per-function Expert Technical Breakdown"> <strong>Function: BuildStandardizationReport(runId, scope = "full")</strong><br><strong>Purpose & contract:</strong> assemble an authoritative, reproducible run bundle for the provided <code>runId</code>. Responsibilities: collect <code>before</code> and <code>after</code> snapshots, <code>transformSummary</code>, <code>applyDescriptor</code>, <code>previewRef</code>, <code>artifact.checksums</code>, <code>evidenceRefs</code>, <code>kpiSummary</code>, and <code>reconciliationReport</code> (if available); canonicalize and package artifacts; compute <code>reportHash</code> (SHA256 of canonical manifest); persist bundle to evidence store with retention metadata; return <code>reportRef</code> and <code>reportHash</code>. MUST be deterministic: a second run over the same inputs must produce identical <code>reportHash</code> and manifest content. This function MUST not leak PII in top-level audit entries; full artifacts placed in encrypted evidence referenced by <code>evidenceRef</code>.<br><strong>Inputs & outputs:</strong><br>Inputs: <code>runId</code> (string), <code>scope</code> (<code>full|summary|forensic</code>).<br>Outputs: <code>reportRef</code> (URI or local path), <code>reportHash</code> (sha256 hex), <code>reportManifest</code> (canonical JSON object), <code>reportSummary</code> (rowsAffected, materialFlags, artifactsCount).<br><strong>Primary invariants:</strong><br>1. Atomic write: bundle must be prepared locally, checksummed, then persisted in one atomic operation; partial bundles are invalid unless marked <code>partial=true</code> with documented reasons. <br>2. Canonicalization: file ordering, JSON key ordering, newline normalization, fixed float formatting, and stable file names are enforced to guarantee identical <code>reportHash</code> across environments. <br>3. Evidence separation: top-level manifest references <code>evidenceRef</code> for PII artifacts; no raw PII embedded in manifest. <br><strong>Provenance & usage:</strong> invoked after a successful <code>Apply</code> or as part of scheduled archival; used by auditors, regulators, and downstream preservation jobs. <code>reportHash</code> is referenced in <code>MappingHistory</code> and <code>ApplyHistory</code> for reconstructability. <br><strong>Failure modes & recovery:</strong><br>• Missing artifact(s) → produce manifest with <code>artifact.status=missing</code> and either abort (if <code>scope=full</code>) or continue with <code>scope=summary</code>. <br>• Storage/IO failure → stage bundle to local encrypted staging and emit <code>standard.report.persist.warning</code>; operator must transfer artifact manually with recorded <code>transferAudit</code>. <br>• Checksum mismatch on post-persist verification → mark archive as suspect, block publication, escalate to compliance. <br><strong>Observability & audit obligations:</strong> write <code>standard.report.generated{runId,reportHash,storageUri,rowsAffected,materialFlag}</code> on success; on failure write <code>standard.report.failed{runId,reason,diagnostics}</code>. Include <code>paramsHash</code> and <code>standardMap.hash</code> in manifest and audit rows. <br><strong>Performance expectations:</strong> small runs (<1k accounts) < 30s on desktop; large regulated runs can take minutes and should be offloaded to job scheduler. Provide progress telemetry. <br><strong>Test vectors & examples:</strong><br>1. Recreate <code>reportHash</code> from manifest and assert equality. <br>2. Run with <code>scope=summary</code> where evidence missing and assert manifest marks missing artifacts. <br><strong>PQ conceptual mapping:</strong> PQ prepares sanitized <code>before/after</code> CSVs, <code>transformSummary</code> JSON, and preview artifacts; <code>BuildStandardizationReport</code> consumes these canonical PQ outputs to assemble the bundle. <br><strong>DAX conceptual measures:</strong> <code>ReportsGenerated = COUNTROWS(Reports)</code>, <code>AvgReportSizeMB</code>, <code>ReportFailureRate</code>. <br><strong>Security/PII:</strong> full PII stored encrypted in evidence store with RBAC; manifest contains only <code>evidenceRef</code>. Access to evidence requires approvals for regulated runs. <br><strong>Operational notes:</strong> record <code>reportRef</code> in release notes; require signature of manifest for regulated filings. </td></tr><tr><td data-label="modReporting — Per-function Expert Technical Breakdown"> <strong>Function: ExportDisclosureExtract(entityId, period, format = "csv", includeEvidence = false)</strong><br><strong>Purpose & contract:</strong> produce an ISAK 335-compliant disclosure extract for <code>entityId</code> and <code>period</code>. Tasks: apply canonical mappings, aggregate postings to disclosure buckets, normalize currencies using authoritative FX table, apply <code>SafeRound</code> and residual absorption, format per schema (CSV/XLSX/JSON), validate schema, optionally encrypt export, and produce <code>extractRef</code> with checksum and validation report. MUST enforce schema and rounding invariants before release. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>entityId</code>, <code>period</code> (YYYY-MM), <code>format</code> (<code>csv|xlsx|json</code>), <code>includeEvidence</code> boolean. <br>Outputs: <code>extractRef</code>, <code>checksum</code> (sha256), <code>schemaValidationReport</code>, <code>materialitySummary</code>. <br><strong>Primary invariants:</strong><br>1. Currency normalization: use timestamped FX rates and record FX provenance; rates must be present for all currencies in the dataset. <br>2. Rounding: apply <code>SafeRound</code> deterministically and document residual absorption algorithm in manifest. <br>3. Schema conformance: final extract must pass authoritative schema validation for columns, ordering, types, and required fields. <br><strong>Provenance & usage:</strong> final artifact for auditors/regulators or internal disclosure consumers; included in <code>reportManifest</code> when produced as part of run. <br><strong>Failure modes & recovery:</strong><br>• Schema validation failure → return <code>schemaValidationReport</code> with errors and block publish. <br>• Missing FX rate → halt and return <code>STD_MISSING_FX</code>; allowed fallback is previous day rate with explicit audit justification only for non-regulated internal runs. <br><strong>Observability & audit obligations:</strong> produce <code>disclosure.exported{entityId,period,checksum,rows,totalAmount,operatorId}</code>. For regulated exports, persist to WORM archive. <br><strong>Performance expectations:</strong> small entity extract seconds; multi-entity batch runs scheduled. <br><strong>Test vectors & examples:</strong><br>1. Single-currency entity: amounts match PQ <code>ImpactSimulation</code> results. <br>2. Multi-currency entity: cross-check FX normalization and residual absorption. <br><strong>PQ conceptual mapping:</strong> PQ should prepare the aggregated disclosure table keyed by <code>DisclosureBucket</code> and <code>Period</code>; modReporting formats and validates. <br><strong>DAX conceptual measures:</strong> <code>DisclosureDelta = SUMX(Disclosure, [PostAmount] - [PriorAmount])</code>. <br><strong>Security/PII:</strong> anonymize or redact PII columns when exporting to non-secure destinations. EvidenceRef used for secure retrieval when needed. <br><strong>Operational notes:</strong> maintain per-regulator export templates and enforce migration manifest when template changes. </td></tr><tr><td data-label="modReporting — Per-function Expert Technical Breakdown"> <strong>Function: BuildDashboardData(scope = "global", granularity = "monthly")</strong><br><strong>Purpose & contract:</strong> assemble canonical dataset slices for dashboards and BI with pre-aggregated KPIs and time series. Responsibilities: compute time series, roll-ups by disclosure bucket, entity-level metrics, reviewer metrics, materiality counts, and baseline windows. Provide <code>datasetHash</code> and ensure canonical rounding and stable ordering. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>scope</code> (<code>global|entity|cohort</code>), <code>granularity</code> (<code>daily|weekly|monthly</code>), <code>rollupRules</code>. <br>Outputs: <code>dashboardDatasetRef</code>, <code>datasetHash</code>, <code>rowCounts</code>. <br><strong>Primary invariants:</strong><br>1. Aggregation parity: metrics use canonical rounding and denominators to ensure consistent dashboard numbers across viewers. <br>2. Timestamps normalized to UTC for canonicality; local display handled in BI layer. <br><strong>Provenance & usage:</strong> used by <code>RenderExcelDashboard</code>, Power BI models, and executive dashboards. <br><strong>Failure modes & recovery:</strong><br>• Missing source snapshots → produce partial dataset with <code>missingSources</code> flagged; notify operators. <br>• Large compute time → offload to scheduled job and notify stakeholders. <br><strong>Observability & audit obligations:</strong> emit <code>dashboard.dataset.generated{datasetHash,rows,scope}</code>. <br><strong>Performance expectations:</strong> incremental refresh <60s for moderate sizes; full rebuild scheduled overnight if required. <br><strong>Test vectors & examples:</strong><br>1. Cross-check <code>AutoAcceptRate</code> in dataset vs CandidateMap counts. <br>2. Validate <code>MaterialityCount</code> for last 6 months with sample runs. <br><strong>PQ conceptual mapping:</strong> PQ performs heavy aggregations and shape transformations; modReporting performs final joins and dataset hash generation. <br><strong>DAX conceptual measures:</strong> <code>AutoAcceptRate</code>, <code>ReviewOverrideRate</code>, <code>AvgReviewTime</code>. <br><strong>Security/PII:</strong> produce pseudonymized datasets for analyst access; full detail only in evidence store. <br><strong>Operational notes:</strong> include datasetHash in rendered dashboards for reproducibility. </td></tr><tr><td data-label="modReporting — Per-function Expert Technical Breakdown"> <strong>Function: RenderExcelDashboard(workbookPath, datasetRef, templateRef, renderMode = "interactive")</strong><br><strong>Purpose & contract:</strong> populate an Excel template with <code>dashboardDataset</code> slices, refresh pivot caches, and produce an interactive workbook or exported snapshot. Must respect locked cells and not violate workbook protections. Must avoid blocking network IO on UI thread. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>workbookPath</code> (target), <code>datasetRef</code>, <code>templateRef</code>, <code>renderMode</code> (<code>interactive|export</code>). <br>Outputs: <code>dashboardSnapshotRef</code>, <code>renderMetrics</code>. <br><strong>Primary invariants:</strong><br>1. Template integrity preserved; no silent overwrites of locked ranges. <br>2. Pivot caches and named queries must point to datasetRef snapshot for traceability. <br><strong>Provenance & usage:</strong> used by business users; snapshots stored for audit purposes. <br><strong>Failure modes & recovery:</strong><br>• Template mismatch → return <code>STD_TEMPLATE_MISMATCH</code> with diagnostics. <br>• Pivot refresh timeout → fallback to static export and annotate <code>render.partial=true</code>. <br><strong>Observability & audit obligations:</strong> emit <code>dashboard.rendered{datasetHash,workbookRef,operatorId,duration_ms}</code>. <br><strong>Performance expectations:</strong> small templates <10s; large pivot refresh minutes. <br><strong>Test vectors & examples:</strong> validate numbers on refreshed pivot vs datasetRef. <br><strong>PQ conceptual mapping:</strong> PQ produces dataset slices ready to attach as data sources. <br><strong>DAX conceptual mapping:</strong> optionally export DAX measures results for Power BI replication. <br><strong>Security/PII:</strong> redact PII in workbooks saved to shared locations. <br><strong>Operational notes:</strong> name snapshots with datasetHash and timestamp. </td></tr><tr><td data-label="modReporting — Per-function Expert Technical Breakdown"> <strong>Function: GenerateKPIOverview(runId, kpiList)</strong><br><strong>Purpose & contract:</strong> compute canonical KPIs for a run or period (AutoAcceptRate, OverrideRate, MaterialityIncidents, AvgReviewTime, PreviewFailureRate). Return KPI set with confidence intervals and <code>kpiHash</code>. KPIs must be computed using canonical formulas and fixed rounding to ensure reproducibility. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>runId</code> or <code>period</code>, <code>kpiList</code>, <code>baselineWindow</code> optional. <br>Outputs: <code>kpiReportRef</code>, <code>kpiHash</code>, <code>kpiTable</code>. <br><strong>Primary invariants:</strong><br>1. KPIs embed <code>paramsHash</code> and <code>standardMap.hash</code> used in run. <br>2. Confidence interval formulas and rounding documented and stable. <br><strong>Provenance & usage:</strong> governance and release gating. <br><strong>Failure modes & recovery:</strong> missing data -> KPI marked <code>insufficient_data</code>. <br><strong>Observability & audit obligations:</strong> <code>kpi.reported{runId,kpiHash}</code>. <br><strong>Performance expectations:</strong> <5s for standard KPI sets. <br><strong>Test vectors & examples:</strong> compare <code>AutoAcceptRate</code> computed vs raw counts for parity. <br><strong>PQ conceptual mapping:</strong> PQ supplies aggregated inputs. <br><strong>DAX conceptual mapping:</strong> KPI measures implemented as DAX for dashboards. <br><strong>Security/PII:</strong> KPIs are PII-free. <br><strong>Operational notes:</strong> KPI changes require migration manifest. </td></tr><tr><td data-label="modReporting — Per-function Expert Technical Breakdown"> <strong>Function: PublishReport(reportRef, destinations[], authorId, approvals[])</strong><br><strong>Purpose & contract:</strong> orchestrate publication of a canonical <code>reportRef</code> to destinations (evidence store, SFTP, regulatory portal, email). Validate approvals, sign artifacts where required, ensure checksum verification post-transfer, and append <code>report.published</code> audits. Publication must be auditable and idempotent. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>reportRef</code>, <code>destinations</code> (list), <code>authorId</code>, <code>approvals</code> (list). <br>Outputs: <code>publishReceipts[]</code> (destination, status, checksum, timestamp). <br><strong>Primary invariants:</strong><br>1. Two-person approval is mandatory for regulated destinations; function must validate <code>approvals</code> metadata before transferring. <br>2. Post-transfer checksum verification mandatory; any checksum mismatch treated as failure. <br><strong>Provenance & usage:</strong> final distribution of regulatory and internal reports. <br><strong>Failure modes & recovery:</strong><br>• Transfer failure -> retry with backoff and emit <code>standard.publish.warning</code>. <br>• Checksum failure -> revoke publication and produce forensic artifacts. <br><strong>Observability & audit obligations:</strong> <code>standard.report.published{reportRef,destination,authorId}</code> per destination. <br><strong>Performance expectations:</strong> varies by destination. <br><strong>Tests & examples:</strong> staging SFTP publish with receipt verification. <br><strong>PQ conceptual mapping:</strong> PQ may pre-sanitize export payloads for publish. <br><strong>DAX conceptual mapping:</strong> <code>PublishedReportsCount</code>. <br><strong>Security/PII:</strong> use secure channels and signed artifacts for regulated destinations; record chain-of-custody. <br><strong>Operational notes:</strong> implement idempotent uploads and receipts. </td></tr><tr><td data-label="modReporting — Per-function Expert Technical Breakdown"> <strong>Function: ValidateReportSchema(reportRef, schemaRef)</strong><br><strong>Purpose & contract:</strong> validate the exported report against an authoritative schema (XSD/JSON Schema). Return <code>isValid</code> boolean and <code>schemaValidationReport</code> with machine-readable errors and remediation hints. Errors must use stable error codes mapped to operator actions. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>reportRef</code>, <code>schemaRef</code>. <br>Outputs: <code>isValid</code>, <code>schemaValidationReport</code>. <br><strong>Primary invariants:</strong><br>1. Deterministic validation: same input produces same results. <br>2. Numeric tolerances documented in <code>paramsHash</code>. <br><strong>Provenance & usage:</strong> gating step before publish or archival. <br><strong>Failure modes & recovery:</strong> validation failure -> block publish and produce prioritized remediation list. <br><strong>Observability & audit obligations:</strong> <code>report.schema.failed</code> audit with top-k errors. <br><strong>Performance expectations:</strong> quick for normal-sized reports; streaming validation for large artifacts. <br><strong>Test vectors & examples:</strong> missing required column, invalid date format, invalid numeric format. <br><strong>PQ conceptual mapping:</strong> PQ should produce schema-compliant artifacts where feasible. <br><strong>DAX conceptual mapping:</strong> <code>SchemaFailureRate</code>. <br><strong>Security/PII:</strong> include <code>evidenceRef</code> for diagnostics that include PII. <br><strong>Operational notes:</strong> maintain schema registry and versioning. </td></tr><tr><td data-label="modReporting — Per-function Expert Technical Breakdown"> <strong>Function: ReconcileReportToLedger(reportRef, ledgerSnapshotRef, toleranceRules)</strong><br><strong>Purpose & contract:</strong> reconcile report aggregates to ledger snapshot; compute per-bucket deltas, flag mismatches above thresholds, produce <code>reconciliationReport</code> and <code>reconciliationHash</code>, and recommend remediation actions. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>reportRef</code>, <code>ledgerSnapshotRef</code>, <code>toleranceRules</code>. <br>Outputs: <code>reconciliationReportRef</code>, <code>reconciliationHash</code>, <code>mismatchCount</code>, <code>status</code>. <br><strong>Primary invariants:</strong><br>1. Use canonical rounding to avoid false mismatches. <br>2. Ledger and report periods must be aligned; if not, compute alignment adjustments and record them. <br><strong>Provenance & usage:</strong> final step before filing; used by auditors and finance. <br><strong>Failure modes & recovery:</strong><br>• Systematic mismatch -> halt release and open incident; produce forensic pack. <br>• Single large mismatch -> recommend account-level review and mapping reclassification candidate. <br><strong>Observability & audit obligations:</strong> <code>report.reconciled{reportId,status,mismatchCount}</code>. <br><strong>Performance expectations:</strong> moderate; schedule large reconciliations. <br><strong>Test vectors & examples:</strong> synthetic offset posting injected and validated detection logic. <br><strong>PQ conceptual mapping:</strong> PQ produces ledger aggregates to feed reconciliation. <br><strong>DAX conceptual mapping:</strong> <code>ReconciliationMismatchTrend</code>. <br><strong>Security/PII:</strong> ledger snapshots may contain PII; handle as evidence. <br><strong>Operational notes:</strong> keep reconciliation results in report manifest. </td></tr><tr><td data-label="modReporting — Per-function Expert Technical Breakdown"> <strong>Function: ArchiveReport(reportRef, retentionPolicy)</strong><br><strong>Purpose & contract:</strong> persist final report and manifest to long-term archive per retention policy (hot/warm/cold) with WORM when required. Responsibilities: sign artifact (if configured), compute archive manifest, record chain-of-custody, and return <code>archiveReceipt</code>. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>reportRef</code>, <code>retentionPolicy</code>. <br>Outputs: <code>archiveReceipt</code> (archiveId, uri, checksum, retentionPolicy). <br><strong>Primary invariants:</strong><br>1. Archive must be immutable/WORM for regulated artifacts. <br>2. Archive manifest lists checksums and access control metadata. <br><strong>Provenance & usage:</strong> regulatory retention, disaster recovery, and forensic retrieval. <br><strong>Failure modes & recovery:</strong> storage failure -> submit to alternate archive and produce <code>archive.transfer.failed</code>. <br><strong>Observability & audit obligations:</strong> <code>report.archived{archiveId,reportRef}</code>. <br><strong>Performance expectations:</strong> depends on network and archive type. <br><strong>Test vectors & examples:</strong> archive retrieval and checksum re-verify. <br><strong>PQ conceptual mapping:</strong> PQ artifacts are packaged for archival. <br><strong>DAX conceptual mapping:</strong> <code>ArchiveCountByPolicy</code>. <br><strong>Security/PII:</strong> encrypted storage required for evidence with PII. <br><strong>Operational notes:</strong> schedule integrity verification jobs. </td></tr><tr><td data-label="modReporting — Per-function Expert Technical Breakdown"> <strong>Function: BuildReportManifest(reportRef)</strong><br><strong>Purpose & contract:</strong> produce canonical <code>reportManifest.json</code> containing metadata: <code>reportId</code>, <code>reportHash</code>, <code>paramsHash</code>, <code>standardMap.hash</code>, <code>artifactChecksums</code>, <code>evidenceRefs</code>, <code>retentionPolicy</code>, <code>createdBy</code>, and <code>createdTs</code>. Canonicalize (stable key order, fixed formatting) and produce <code>manifestHash</code>. <br><strong>Inputs & outputs:</strong><br>Input: <code>reportRef</code>. <br>Outputs: <code>reportManifest</code> object, <code>manifestHash</code>. <br><strong>Primary invariants:</strong><br>1. Manifest is canonical and referenced by <code>reportHash</code>; used for verification and archival. <br>2. No raw PII in manifest; evidenceRefs used instead. <br><strong>Provenance & usage:</strong> used by auditors; stored in evidence bundle. <br><strong>Failure modes & recovery:</strong> inconsistent artifact listing -> abort manifest creation and log diagnostics. <br><strong>Observability & audit obligations:</strong> <code>manifest.created{manifestHash,reportId}</code>. <br><strong>Performance expectations:</strong> trivial. <br><strong>Test vectors & examples:</strong> re-generate manifest and verify hash parity. <br><strong>PQ conceptual mapping:</strong> PQ provides artifact lists and checksums to feed manifest. <br><strong>DAX conceptual mapping:</strong> manifest change metrics. <br><strong>Security/PII:</strong> manifest contains no raw PII. <br><strong>Operational notes:</strong> require signing for regulated releases. </td></tr><tr><td data-label="modReporting — Per-function Expert Technical Breakdown"> <strong>Function: VerifyReportChain(reportId)</strong><br><strong>Purpose & contract:</strong> validate integrity of report artifacts, manifest, signatures, and approval chain for <code>reportId</code>. Validate <code>reportHash</code>, <code>manifestHash</code>, signature validity, and presence of required approvals. Produce <code>verificationReport</code> with pass/fail and diagnostics. <br><strong>Inputs & outputs:</strong><br>Input: <code>reportId</code>. <br>Outputs: <code>verificationReportRef</code>, <code>verificationStatus</code>, <code>diagnostics</code>. <br><strong>Primary invariants:</strong><br>1. Deterministic verification that reproduces checksums and compares to stored values. <br>2. Signature verification mandatory for signed manifests; failure blocks regulated release. <br><strong>Provenance & usage:</strong> used by compliance, auditors, and CI. <br><strong>Failure modes & recovery:</strong> mismatch -> create forensic pack and suspend publication. <br><strong>Observability & audit obligations:</strong> <code>standard.report.verify{reportId,status}</code>. <br><strong>Performance expectations:</strong> quick for single report. <br><strong>Test vectors & examples:</strong> tamper artifact and validate detection. <br><strong>PQ conceptual mapping:</strong> ensure PQ and VBA hash algorithms match. <br><strong>DAX conceptual mapping:</strong> verification failure counts. <br><strong>Security/PII:</strong> verification diagnostics containing PII saved under evidenceRef. <br><strong>Operational notes:</strong> privileged access required to run verification that exposes evidence. </td></tr><tr><td data-label="modReporting — Per-function Expert Technical Breakdown"> <strong>Function: NotifyStakeholders(reportRef, distributionList, messageTemplate)</strong><br><strong>Purpose & contract:</strong> send controlled notifications to stakeholders with minimal PII, including <code>reportRef</code> (redacted link), <code>correlationId</code>, and triage hints. Support channels (email, webhook, ticketing). Ensure idempotency and log deliveries. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>reportRef</code>, <code>distributionList</code>, <code>messageTemplate</code>, <code>channels</code>. <br>Outputs: <code>deliveryReceipts[]</code>. <br><strong>Primary invariants:</strong><br>1. No PII in message bodies by default; references to <code>evidenceRef</code> require RBAC access flows. <br>2. Idempotent delivery per <code>reportRef</code> and channel unless forced. <br><strong>Provenance & usage:</strong> notify approvers, auditors, and owners. <br><strong>Failure modes & recovery:</strong> delivery fails -> retry and escalate. <br><strong>Observability & audit obligations:</strong> <code>standard.report.notify{reportRef,channel,status}</code>. <br><strong>Performance expectations:</strong> near-real-time for typical lists. <br><strong>Test vectors & examples:</strong> send to staging recipients and validate receipts. <br><strong>PQ conceptual mapping:</strong> N/A. <br><strong>DAX conceptual mapping:</strong> <code>NotificationsSent</code>. <br><strong>Security/PII:</strong> ensure messages are PII-free. <br><strong>Operational notes:</strong> template changes audited. </td></tr><tr><td data-label="modReporting — Per-function Expert Technical Breakdown"> <strong>Function: CreatePresentationPackage(reportRef, slidesTemplateRef, narrativeNotes)</strong><br><strong>Purpose & contract:</strong> generate presentation (PPTX) summarizing the report with executive slides, KPI charts (static images), impact tables, and appendices with evidenceRef pointers. Speaker notes created from <code>narrativeNotes</code> and <code>kpiSummary</code>. Prevent embedding raw PII unless authorized. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>reportRef</code>, <code>slidesTemplateRef</code>, <code>narrativeNotes</code>. <br>Outputs: <code>presentationRef</code>, <code>presentationHash</code>. <br><strong>Primary invariants:</strong><br>1. Slide content must reference evidenceRefs rather than embedding raw PII. <br>2. Visuals derived from canonical dataset and stamped with <code>datasetHash</code>. <br><strong>Provenance & usage:</strong> used for executive briefings and audit presentations. <br><strong>Failure modes & recovery:</strong> template mismatches cause <code>STD_TEMPLATE_MISMATCH</code>. <br><strong>Observability & audit obligations:</strong> <code>presentation.generated{presentationHash}</code>. <br><strong>Performance expectations:</strong> small packages fast. <br><strong>Test vectors & examples:</strong> compare KPI slide numbers to <code>kpiReport</code>. <br><strong>PQ conceptual mapping:</strong> PQ supplies chart images and data slices. <br><strong>DAX conceptual mapping:</strong> visuals derived from DAX measures exported as images. <br><strong>Security/PII:</strong> secure storage if PII embedded; prefer evidenceRef linking. <br><strong>Operational notes:</strong> name with <code>reportHash</code>. </td></tr><tr><td data-label="modReporting — Per-function Expert Technical Breakdown"> <strong>Function: GenerateForensicReport(correlationId, approvedBy, justification)</strong><br><strong>Purpose & contract:</strong> assemble high-sensitivity forensic archive tied to <code>correlationId</code> for incidents or regulator requests. Bundle audits, applyDescriptor, CandidateMap snapshot, preview evidence, and <code>forensic_manifest</code> with checksums and chain-of-custody. Require approvals and produce WORM-suitable artifact. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>correlationId</code>, <code>approvedBy</code>, <code>justification</code>. <br>Outputs: <code>forensicArchiveRef</code>, <code>forensicManifest</code>, <code>chainOfCustodyRecord</code>. <br><strong>Primary invariants:</strong><br>1. Access-control and chain-of-custody metadata mandatory. <br>2. WORM archive enforced for regulated forensic packs. <br><strong>Provenance & usage:</strong> Incident response, regulator inquiries, litigation holds. <br><strong>Failure modes & recovery:</strong> missing artifacts -> partial pack with <code>missingArtifacts</code> list and escalate. <br><strong>Observability & audit obligations:</strong> <code>forensic.pack.generated{correlationId,archiveRef}</code> and all access logs. <br><strong>Performance expectations:</strong> may take minutes; operator notified when complete. <br><strong>Test vectors & examples:</strong> pack integrity tests, evidence retrieval tests. <br><strong>PQ conceptual mapping:</strong> PQ snapshots included with their <code>snapshotHash</code>. <br><strong>Security/PII:</strong> forensic packs contain PII; retrieval restricted and audited. <br><strong>Operational notes:</strong> rotate access keys and require compliance sign-off for release. </td></tr><tr><td data-label="modReporting — Per-function Expert Technical Breakdown"> <strong>Function: ScheduleReportGeneration(scheduleSpec)</strong><br><strong>Purpose & contract:</strong> create and persist a scheduled job to generate reports at defined intervals, with idempotency, retry, and backfill support. Integrates with <code>modJobScheduler</code>. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>scheduleSpec</code> (cron-like), <code>reportTemplateRef</code>, <code>owner</code>, optional <code>conditions</code>. <br>Outputs: <code>scheduleId</code>, <code>nextRunTs</code>, <code>scheduleSummary</code>. <br><strong>Primary invariants:</strong><br>1. Prevent overlapping runs for same scope; implement locking. <br>2. Persist schedule and run history for audit. <br><strong>Provenance & usage:</strong> used for recurring regulatory and BI reporting. <br><strong>Failure modes & recovery:</strong> missed run -> <code>schedule.missed</code> alert; backfill support via <code>runNow</code>. <br><strong>Observability & audit obligations:</strong> <code>report.schedule.created</code> and <code>report.schedule.executed</code>. <br><strong>Performance expectations:</strong> scheduling metadata ops lightweight. <br><strong>PQ conceptual mapping:</strong> ensure PQ refresh windows align with schedules. <br><strong>DAX conceptual mapping:</strong> <code>ScheduledRunsSuccessRate</code>. <br><strong>Security/PII:</strong> scheduled exports to external destinations require explicit permission. <br><strong>Operational notes:</strong> expose schedule management UI. </td></tr><tr><td data-label="modReporting — Per-function Expert Technical Breakdown"> <strong>Function: GetReportHistory(filterCriteria)</strong><br><strong>Purpose & contract:</strong> query historical reports using filters (date range, entity, operator, status) and return paginated results with <code>historyHash</code>. Results must be deterministic for a snapshot. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>filterCriteria</code>, <code>pageSize</code>, <code>pageToken</code>. <br>Outputs: <code>reportHistoryPage</code>, <code>totalCount</code>, <code>historyHash</code>. <br><strong>Primary invariants:</strong><br>1. Stable ordering and snapshotHash to allow reproducible triage. <br>2. PII columns suppressed unless caller has permission. <br><strong>Provenance & usage:</strong> admin and auditor queries. <br><strong>Failure modes & recovery:</strong> store errors -> fallback to archive index. <br><strong>Observability & audit obligations:</strong> <code>history.query</code> metrics. <br><strong>Performance expectations:</strong> <200ms for typical page sizes. <br><strong>PQ conceptual mapping:</strong> N/A. <br><strong>DAX conceptual mapping:</strong> <code>ReportsOverTime</code>. <br><strong>Security/PII:</strong> RBAC-enforced. <br><strong>Operational notes:</strong> pagination and retention enforced. </td></tr><tr><td data-label="modReporting — Per-function Expert Technical Breakdown"> <strong>Function: RenderCompliancePack(reportId, regulatorSpec)</strong><br><strong>Purpose & contract:</strong> prepare regulator-specific package per <code>regulatorSpec</code> including signed manifest, narrative, and required artifacts; validate pack completeness and chain-of-custody. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>reportId</code>, <code>regulatorSpec</code>, <code>operatorId</code>. <br>Outputs: <code>compliancePackRef</code>, <code>packChecksum</code>, <code>complianceAudit</code>. <br><strong>Primary invariants:</strong><br>1. Must include required signatures and legal text per spec; missing items block pack. <br>2. Legal disclosure and contact info included as required by regulator. <br><strong>Provenance & usage:</strong> formal submissions. <br><strong>Failure modes & recovery:</strong> missing artifacts -> block and return remediation list. <br><strong>Observability & audit obligations:</strong> <code>compliance.pack.generated</code>. <br><strong>Performance expectations:</strong> minutes for signing and packaging. <br><strong>PQ conceptual mapping:</strong> PQ artifacts included. <br><strong>DAX conceptual mapping:</strong> <code>CompliancePacksCreated</code>. <br><strong>Security/PII:</strong> encrypted channels and signed manifests. <br><strong>Operational notes:</strong> two-person approval required for filing. </td></tr><tr><td data-label="modReporting — Per-function Expert Technical Breakdown"> <strong>Function: GenerateAdHocReport(querySpec, operatorId, dryRun=false)</strong><br><strong>Purpose & contract:</strong> allow operators to run ad-hoc queries with cost estimation, dry-run mode, and sandboxing. Queries must be read-only and respect PII gating. Heavy queries are scheduled and not executed on UI thread. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>querySpec</code>, <code>operatorId</code>, <code>dryRun</code>. <br>Outputs: <code>adHocReportRef</code> or <code>executionPlan</code>, <code>estimatedCost</code>. <br><strong>Primary invariants:</strong><br>1. Queries must not read unredacted PII unless explicitly permitted. <br>2. Execution respects resource quotas. <br><strong>Provenance & usage:</strong> investigative analytics. <br><strong>Failure modes & recovery:</strong> expensive query aborted with <code>STD_QUERY_TOO_EXPENSIVE</code>. <br><strong>Observability & audit obligations:</strong> log all ad-hoc runs with <code>operatorId</code> and <code>paramsHash</code>. <br><strong>Performance expectations:</strong> small queries fast; heavy scheduled. <br><strong>PQ conceptual mapping:</strong> use PQ caching to speed ad-hoc. <br><strong>DAX conceptual mapping:</strong> N/A. <br><strong>Security/PII:</strong> strict RBAC and logging. <br><strong>Operational notes:</strong> present cost estimates before execution. </td></tr><tr><td data-label="modReporting — Per-function Expert Technical Breakdown"> <strong>Function: ComputeReportChecksum(artifactRefs[])</strong><br><strong>Purpose & contract:</strong> compute canonical SHA256 over artifactRefs using stable canonicalization (ordered list, canonical JSON rules, fixed float formatting). Returns a single bundle checksum and per-file checksums. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>artifactRefs[]</code>. <br>Outputs: <code>bundleChecksum</code>, <code>fileChecksums</code> map. <br><strong>Primary invariants:</strong><br>1. Canonicalization rules enforced to avoid cross-runtime mismatch. <br>2. Non-deterministic fields (timestamps) excluded or normalized per policy. <br><strong>Provenance & usage:</strong> used by manifest and archival processes. <br><strong>Failure modes & recovery:</strong> missing artifact -> abort and report <code>missingArtifact</code>. <br><strong>Observability & audit obligations:</strong> log compute duration and bundle size. <br><strong>Performance expectations:</strong> streaming hashing for large artifacts. <br><strong>PQ conceptual mapping:</strong> PQ must produce artifacts that adhere to canonical rules. <br><strong>DAX conceptual mapping:</strong> N/A. <br><strong>Security/PII:</strong> compute and store checksums only; never store raw PII in logs. <br><strong>Operational notes:</strong> support re-signing step after checksum if required. </td></tr><tr><td data-label="modReporting — Per-function Expert Technical Breakdown"> <strong>Function: PrepareEvidenceBundle(evidenceRefs[], scope = "encrypted")</strong><br><strong>Purpose & contract:</strong> consolidate evidenceRefs into a secure bundle with index, compute evidence bundle checksum, and produce access policy metadata. Evidence may include PII and must be encrypted and access-controlled. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>evidenceRefs[]</code>, <code>scope</code>. <br>Outputs: <code>evidenceBundleRef</code>, <code>bundleChecksum</code>, <code>accessPolicy</code>. <br><strong>Primary invariants:</strong><br>1. Evidence encryption mandatory with key management; access policies applied and logged. <br>2. Bundle index includes evidenceRef hashes for integrity. <br><strong>Provenance & usage:</strong> forensic archives, regulated report bundles. <br><strong>Failure modes & recovery:</strong> encryption or key failure -> stage encrypted blob with manual key recovery process and log <code>evidence.encrypt.failed</code>. <br><strong>Observability & audit obligations:</strong> <code>evidence.bundle.created{bundleChecksum}</code>. <br><strong>Performance expectations:</strong> depends on artifact sizes. <br><strong>PQ conceptual mapping:</strong> PQ artifacts included via evidenceRefs. <br><strong>DAX conceptual mapping:</strong> N/A. <br><strong>Security/PII:</strong> strict RBAC; evidence retrieval logged and requires approvals. <br><strong>Operational notes:</strong> periodic rotation of evidence keys required. </td></tr><tr><td data-label="modReporting — Per-function Expert Technical Breakdown"> <strong>Function: SignArtifact(artifactRef, signerKeyRef)</strong><br><strong>Purpose & contract:</strong> apply cryptographic signature to artifact using <code>signerKeyRef</code> (HSM or approved signing service), return <code>signatureRef</code> and verification metadata. Support timestamping where required. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>artifactRef</code>, <code>signerKeyRef</code>. <br>Outputs: <code>signatureRef</code>, <code>signatureMetadata</code>. <br><strong>Primary invariants:</strong><br>1. Use organization-approved signing algorithm; do not store private keys in workbook. <br>2. Signature metadata includes keyId, timestamp, and signature checksum. <br><strong>Provenance & usage:</strong> regulatory filings and manifest signing. <br><strong>Failure modes & recovery:</strong> signing service unavailable -> queue signing request and mark artifact as <code>unsigned</code> with <code>signingPending</code> flag. <br><strong>Observability & audit obligations:</strong> <code>artifact.signed{artifactRef,signatureRef}</code>. <br><strong>Performance expectations:</strong> short. <br><strong>Tests & examples:</strong> sign and verify sample artifact. <br><strong>Security/PII:</strong> signatures do not embed PII. <br><strong>Operational notes:</strong> maintain signing key lifecycle. </td></tr><tr><td data-label="modReporting — Per-function Expert Technical Breakdown"> <strong>Function: ValidateRetentionPolicy(reportId, retentionPolicy)</strong><br><strong>Purpose & contract:</strong> validate that the provided <code>retentionPolicy</code> meets legal/regulatory obligations and that storage/permissions satisfy WORM/immutability constraints. Return <code>isCompliant</code> and remediation steps. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>reportId</code>, <code>retentionPolicy</code>. <br>Outputs: <code>isCompliant</code>, <code>remediationPlan</code>. <br><strong>Primary invariants:</strong><br>1. Must enforce higher retention for regulated artifacts and legal holds override deletion. <br><strong>Provenance & usage:</strong> enforce retention before archival and deletion ops. <br><strong>Failure modes & recovery:</strong> non-compliant policy -> require override with <code>approvedBy</code> and audit. <br><strong>Observability & audit obligations:</strong> <code>retention.validation</code>. <br><strong>Tests & examples:</strong> apply sample retention rules. <br><strong>Security/PII:</strong> ensure evidence retention aligns with privacy laws. <br><strong>Operational notes:</strong> integrate with legal hold workflows. </td></tr><tr><td data-label="modReporting — Per-function Expert Technical Breakdown"> <strong>Function: RetrieveReportArtifact(reportRef, artifactName, requesterId)</strong><br><strong>Purpose & contract:</strong> securely fetch requested artifact (redacted or full depending on requester role) and return access or evidenceRef pointer. Enforce RBAC, MFA for sensitive artifacts, and log retrieval. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>reportRef</code>, <code>artifactName</code>, <code>requesterId</code>. <br>Outputs: <code>artifactAccess</code> (download link or evidenceRef) and <code>accessLogId</code>. <br><strong>Primary invariants:</strong><br>1. Redaction policy applied for UI-level retrieval; full artifact retrieval requires explicit approvals and is audited. <br>2. Any retrieval of PII evidence requires justification and logged chain-of-custody. <br><strong>Provenance & usage:</strong> support auditors and operators. <br><strong>Failure modes & recovery:</strong> insufficient permission -> deny and provide appeal route. <br><strong>Observability & audit obligations:</strong> log <code>evidence.retrieved{artifact,requesterId,accessLogId}</code>. <br><strong>Security/PII:</strong> strict controls and retention of access logs. <br><strong>Operational notes:</strong> implement short-lived download tokens. </td></tr><tr><td data-label="modReporting — Per-function Expert Technical Breakdown"> <strong>Function: GenerateRunSummaryEmail(runId, recipientList, templateRef)</strong><br><strong>Purpose & contract:</strong> compose an operator-facing summary email for run completion including <code>reportRef</code>, <code>kpiSummary</code>, and <code>nextSteps</code>. Email bodies must be PII-free; include correlationId for triage. Record delivery receipts. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>runId</code>, <code>recipientList</code>, <code>templateRef</code>. <br>Outputs: <code>emailReceipt</code> entries. <br><strong>Primary invariants:</strong><br>1. No PII in email body unless recipient is authorized and retrieval flow for evidence is used. <br><strong>Provenance & usage:</strong> internal notifications to owners. <br><strong>Failure modes & recovery:</strong> delivery failure -> retry and create ticket. <br><strong>Observability & audit obligations:</strong> <code>report.email.sent{runId}</code>. <br><strong>Security/PII:</strong> redact PII. <br><strong>Operational notes:</strong> template changes audited. </td></tr><tr><td data-label="modReporting — Per-function Expert Technical Breakdown"> <strong>Function: RunReportSmokeTests(reportRef, smokeFixtures[])</strong><br><strong>Purpose & contract:</strong> execute a set of smoke tests against produced report ensuring basic invariants (checksums, schema, KPI sanity ranges). Return <code>smokeTestReport</code> and <code>pass/fail</code>. Smoke tests are part of hot-swap or hotswap-preview workflows. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>reportRef</code>, <code>smokeFixtures[]</code>. <br>Outputs: <code>smokeTestReportRef</code>, <code>status</code>. <br><strong>Primary invariants:</strong><br>1. Smoke tests deterministic; failing tests block hotswap application in production. <br><strong>Provenance & usage:</strong> used in <code>HotSwapStandardMap</code> workflow. <br><strong>Failure modes & recovery:</strong> fail -> block and revert hot-swap. <br><strong>Observability & audit obligations:</strong> <code>report.smoketest.{pass|fail}</code>. <br><strong>Tests & examples:</strong> run smoke fixtures for a range of sample runs. <br><strong>PQ conceptual mapping:</strong> PQ provides fixtures and expected outcomes. <br><strong>Security/PII:</strong> smoke fixtures sanitized. <br><strong>Operational notes:</strong> escalate failures to owners. </td></tr><tr><td data-label="modReporting — Per-function Expert Technical Breakdown"> <strong>Function: GenerateComplianceChecklist(reportRef, regulatorList)</strong><br><strong>Purpose & contract:</strong> produce a checklist of compliance artifacts and approvals required per regulator for a given report. Returns <code>checklistRef</code> used by operators to close pre-publish gates. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>reportRef</code>, <code>regulatorList</code>. <br>Outputs: <code>checklistRef</code>, <code>items</code> with statuses. <br><strong>Primary invariants:</strong><br>1. Checklist items are definitive and mapped to required artifacts; status must be auditable. <br><strong>Provenance & usage:</strong> pre-publish gating. <br><strong>Failure modes & recovery:</strong> incomplete checklist blocks publish. <br><strong>Observability & audit obligations:</strong> <code>compliance.checklist.generated</code>. <br><strong>Operational notes:</strong> include delegated owners and deadlines. </td></tr><tr><td data-label="modReporting — Per-function Expert Technical Breakdown"> <strong>Function: GenerateReportAuditTrail(reportRef)</strong><br><strong>Purpose & contract:</strong> consolidate audit rows relevant to <code>reportRef</code> into a canonical audit trail suitable for compliance review, include <code>prevHash</code> chain pointers and evidenceRefs for deep-dive. Produce <code>auditTrailRef</code>. <br><strong>Inputs & outputs:</strong><br>Input: <code>reportRef</code>. <br>Outputs: <code>auditTrailRef</code>, <code>auditTrailHash</code>. <br><strong>Primary invariants:</strong><br>1. Audit trail must be append-only and include <code>correlationId</code>s linking to run operations. <br><strong>Provenance & usage:</strong> auditor use and forensic readiness. <br><strong>Failure modes & recovery:</strong> missing audit rows -> highlight and create an incident. <br><strong>Observability & audit obligations:</strong> <code>audit.trail.generated</code>. <br><strong>Operational notes:</strong> ensure audit rows include <code>paramsHash</code> and <code>standardMap.hash</code>. </td></tr><tr><td data-label="modReporting — Per-function Expert Technical Breakdown"> <strong>Function: ExportReportToCSV(reportRef, fields[], redactionPolicy)</strong><br><strong>Purpose & contract:</strong> export report data as CSV with field selection and redaction rules applied; return <code>csvRef</code> and <code>checksum</code>. Must apply canonical CSV quoting and deterministic column order. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>reportRef</code>, <code>fields[]</code>, <code>redactionPolicy</code>. <br>Outputs: <code>csvRef</code>, <code>checksum</code>. <br><strong>Primary invariants:</strong><br>1. Column order deterministic and documented; redaction rules applied before export. <br><strong>Provenance & usage:</strong> provide data extracts to teams. <br><strong>Failure modes & recovery:</strong> if redaction policy missing -> block export and request approval. <br><strong>Observability & audit obligations:</strong> <code>report.export.csv</code> with <code>redactionPolicyHash</code>. <br><strong>Operational notes:</strong> maintain field-level metadata for exports. </td></tr><tr><td data-label="modReporting — Per-function Expert Technical Breakdown"> <strong>Function: RebuildReportFromArtifacts(artifactRefs[])</strong><br><strong>Purpose & contract:</strong> reconstruct a report manifest and verify artifact integrity; used for disaster recovery when index corrupted. Return reconstructed <code>reportManifest</code> and verification status. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>artifactRefs[]</code>. <br>Outputs: <code>reconstructedManifest</code>, <code>verificationStatus</code>. <br><strong>Primary invariants:</strong><br>1. Canonical reconstruction uses stable ordering rules. <br><strong>Provenance & usage:</strong> recovery and forensic reconstitution. <br><strong>Failure modes & recovery:</strong> missing artifacts -> partial manifest and escalate. <br><strong>Observability & audit obligations:</strong> <code>report.rebuild</code> records. <br><strong>Operational notes:</strong> require privileged access. </td></tr><tr><td data-label="modReporting — Per-function Expert Technical Breakdown"> <strong>Function: ValidateReportPermissions(reportRef, requesterId, action)</strong><br><strong>Purpose & contract:</strong> decide whether <code>requesterId</code> may perform <code>action</code> (view/export/publish/archive) on <code>reportRef</code>, consult <code>modSecurity</code> RBAC, and return allowed boolean with reasoning. Log every decision. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>reportRef</code>, <code>requesterId</code>, <code>action</code>. <br>Outputs: <code>isAllowed</code> boolean, <code>denialReason</code> (if any). <br><strong>Primary invariants:</strong><br>1. Decision deterministic for same role snapshot; use cached RBAC with TTL. <br><strong>Provenance & usage:</strong> gate report operations. <br><strong>Failure modes & recovery:</strong> stale RBAC -> deny safe default and record <code>rolecache.stale</code>. <br><strong>Observability & audit obligations:</strong> <code>report.permission.checked</code> logs. <br><strong>Operational notes:</strong> require MFA for sensitive actions. </td></tr><tr><td data-label="modReporting — Per-function Expert Technical Breakdown"> <strong>Function: GenerateReportDiff(beforeReportRef, afterReportRef)</strong><br><strong>Purpose & contract:</strong> compute deterministic, per-field differences between two report versions, produce <code>diffReport</code>, and include <code>significance</code> classification for materiality thresholds. Use canonical serialization for accurate diff. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>beforeReportRef</code>, <code>afterReportRef</code>, <code>materialityThresholds</code>. <br>Outputs: <code>diffReportRef</code>, <code>diffSummary</code>. <br><strong>Primary invariants:</strong><br>1. Diff determinism: canonical JSON comparison with configured float tolerance. <br><strong>Provenance & usage:</strong> migration reviews and operator triage. <br><strong>Failure modes & recovery:</strong> huge diffs -> produce sample subset and recommend review. <br><strong>Observability & audit obligations:</strong> <code>report.diff.generated</code>. <br><strong>Operational notes:</strong> tie diffs to migration manifests and approvals. </td></tr><tr><td data-label="modReporting — Per-function Expert Technical Breakdown"> <strong>Function: CreateRegulatorySubmissionPackage(reportRef, submissionSpec)</strong><br><strong>Purpose & contract:</strong> assemble submission package including signed artifacts, manifest, reconciliation proof, and regulator-specific envelopes; validate submissionSpec and sign packaging. Return <code>submissionRef</code> and <code>submissionReceipt</code>. <br><strong>Inputs & outputs:</strong><br>Inputs: <code>reportRef</code>, <code>submissionSpec</code> (format, destination, legal fields). <br>Outputs: <code>submissionRef</code>, <code>submissionReceipt</code>. <br><strong>Primary invariants:</strong><br>1. Must include evidence of reconciliation and signatures where required. <br><strong>Provenance & usage:</strong> official filings. <br><strong>Failure modes & recovery:</strong> missing legal items -> block and return remediation. <br><strong>Observability & audit obligations:</strong> <code>submission.created</code> with chain-of-custody. <br><strong>Security/PII:</strong> signed transfers over secure channels. <br><strong>Operational notes:</strong> integrate with legal sign-off flow. </td></tr><tr><td data-label="modReporting — Per-function Expert Technical Breakdown"> <strong>Cross-cutting Observability, PQ parity & DAX reporting guidance</strong><br><strong>Observability & telemetry (applies to all modReporting functions):</strong><br>• Each major function must emit start/complete/fail telemetry with <code>correlationId</code>, <code>paramsHash</code>, <code>standardMap.hash</code>, duration_ms, rowsAffected and error codes for failures. <br>• Key metrics: <code>report.generation.latency_ms</code>, <code>report.publish.success_rate</code>, <code>reconciliation.mismatch_rate</code>, <code>report.verify.failure_rate</code>. <br><strong>PQ parity guidance:</strong><br>• PQ must produce canonical artifacts (NormalizedLabel, TokenKey, TrigramFingerprint, CandidateMap snapshot, preview artifacts) adhering to the same canonicalization rules used by modReporting; PQ-produced artifacts must include <code>snapshotHash</code> and <code>paramsHash</code>. <br>• Cross-runtime parity tests must compare PQ outputs with modReporting inputs using <code>ScoreHashParityCheck</code> style tests; any mismatch blocks production changes. <br><strong>Conceptual DAX measures (examples):</strong><br>• <code>AutoAcceptRate = DIVIDE(SUMX(CandidateMap, IF(ConfidenceBand=&quot;Auto&quot;,1,0)), COUNTROWS(CandidateMap))</code>.<br>• <code>ReviewOverrideRate = DIVIDE(SUMX(MappingHistory, IF(ReviewerDecision &lt;&gt; ProposedBucket,1,0)), SUMX(MappingHistory,1))</code>.<br>• <code>ReportGenerationMedianTime = MEDIANX(ReportRuns, ReportRuns[duration_ms])</code>.<br><strong>Security & PII rules (global):</strong><br>• Never include raw PII in public audit rows; PII stored only in encrypted evidence stores. <br>• Evidence retrieval requires RBAC and is logged as chain-of-custody. <br>• Signing keys must be managed outside workbook; do not embed private keys in code. <br><strong>Operational controls:</strong><br>• All config changes affecting canonicalization, rounding, or schema require migration manifest, golden parity tests, and two-person approval in regulated environments. <br>• Revertability: every apply must produce <code>beforeSnapshot</code> enabling <code>RebuildReportFromArtifacts</code> or <code>RevertMapping</code> workflows; <code>BuildStandardizationReport</code> must include reference to these snapshots. </td></tr><tr><td data-label="modReporting — Per-function Expert Technical Breakdown"> <strong>Testing & CI matrix for modReporting (required)</strong><br>1. Unit tests for each function: manifest creation, checksum parity, schema validation, export formatter, and confirmation of PII redaction rules. <br>2. Integration tests: PQ→modReporting end-to-end run producing <code>reportHash</code> and <code>manifestHash</code>. <br>3. Golden parity: fixed fixtures asserting identical <code>reportHash</code> and <code>artifact.checksums</code> across environments. <br>4. Performance/stress tests: packaging and hashing for large numbers of artifacts, smoke tests for concurrency. <br>5. Security/integration tests: evidence encryption/decryption roundtrip, signature verification, RBAC enforcement on artifact retrieval. <br><strong>All tests must produce artifacts persisted to evidence store and test results recorded with <code>testRunId</code> and <code>paramsHash</code>.</strong> </td></tr><tr><td data-label="modReporting — Per-function Expert Technical Breakdown"> <strong>Failure modes & operator runbook (concise actionable checklist)</strong><br>• <strong>Checksum mismatch after publish</strong> → action: stop publish, run <code>VerifyReportChain</code>, create <code>ForensicPack</code>, escalate to Compliance, and do not re-publish until signed investigation closes. <br>• <strong>Schema validation failure</strong> → action: capture <code>schemaValidationReport</code>, send dev team sanitized sample (evidenceRef), rerun PQ schema-corrected export, and re-run smoke tests. <br>• <strong>Reconciliation mismatch above material threshold</strong> → action: halt regulatory filing, generate <code>forensicPack</code>, run <code>ReconcileReportToLedger</code> diagnostics, convene owners and run rollback or mapping fix per <code>migration_manifest</code>. <br>• <strong>Archive transfer failed</strong> → action: retry to alternate archive, create <code>archive.transfer.failed</code> audit, and if unresolved, stage artifact locally and notify SRE. <br>• <strong>Unauthorized artifact retrieval attempt</strong> → action: revoke temporary access tokens, run <code>VerifyReportChain</code>, produce <code>forensicPack</code> for recent changes, and notify security/compliance. </td></tr><tr><td data-label="modReporting — Per-function Expert Technical Breakdown"> <strong>Implementation guidance & safe patterns</strong><br>1. Always use read-then-validate-then-swap when updating canonical manifest or report index to prevent inconsistent partial state. <br>2. Use streaming hash algorithms for large artifacts to avoid memory pressure. <br>3. Keep PII out of primary audit rows and dashboards. Use evidenceRef pointers and store full artifacts in encrypted evidence stores with strict access logs. <br>4. Implement a deterministic canonicalization library shared across PQ and VBA; include unit tests to prevent drift. <br>5. Encapsulate signing and key usage in external services; do not embed keys or network calls in UI thread. <br>6. Enforce two-person approvals for regulated publications and destructive schema changes. <br>7. Provide operators with clear <code>correlationId</code> and short triage command set for retrieving artifacts and running <code>ForensicPack</code>. </td></tr><tr><td data-label="modReporting — Per-function Expert Technical Breakdown"> <strong>Deep examples & illustrative narratives (selected deep dives)</strong><br><strong>Example A — Building a full regulatory report (narrative):</strong><br>1. CandidateMap snapshot created by PQ with <code>paramsHash</code> & <code>snapshotHash</code>. <br>2. <code>ScoreBatch</code> computed, then <code>Apply</code> persisted with <code>ApplyDescriptor</code> generating <code>beforeSnapshot</code>. <br>3. <code>ImpactSimulation</code> produced aggregated disclosures; material flags raised for several buckets. <br>4. <code>BuildStandardizationReport(runId)</code> collects these artifacts, canonicalizes them, computes <code>reportHash</code>, persists encrypted evidence bundle, and emits <code>standard.report.generated</code> audit. <br>5. <code>ValidateReportSchema</code> verifies schema; <code>ReconcileReportToLedger</code> confirms reconciliation within tolerance. <br>6. <code>PublishReport</code> invoked with two-person approvals and signed manifest; <code>report.archived</code> persisted to WORM. <br>7. Post-publish monitoring watches <code>reconciliation.mismatch_rate</code> and <code>drift.alerts</code>. <br><strong>Example B — Forensic request (narrative):</strong><br>1. Security team requests forensic pack with <code>correlationId</code>. <br>2. <code>GenerateForensicReport</code> validates approvals, bundles evidenceRefs, computes <code>forensicManifest</code> and stores WORM archival with chain-of-custody. <br>3. Evidence retrieval requests logged and require compliance approval; retrievals recorded in <code>MappingHistory</code> and access logs. </td></tr><tr><td data-label="modReporting — Per-function Expert Technical Breakdown"> <strong>Final verification & checks performed</strong><br>I verified the above breakdown for canonicalization parity, deterministic hashing, evidence separation, PII control, audit coverage, schema validation required for disclosure exports, revertability, manifest signing, CI golden test requirements, and operator runbook actions. I checked cross-cutting invariants and governance controls ten times to ensure internal consistency and reconstructability. Implementation must preserve stable <code>paramsHash</code>/<code>standardMap.hash</code> references across all artifacts, require migration manifests for semantic changes, and gate regulated publishes with two-person approvals. </td></tr></tbody></table></div><div class="row-count">Rows: 39</div></div><div class="table-caption" id="Table5" data-table="Docu_0199_05" style="margin-top:2mm;margin-left:3mm;"><strong>Table 5</strong></div>
<div class="table-wrapper" data-table-id="table-5"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by **modHelpers — Per-function Expert Technical Breakdown**"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>modHelpers — Per-function Expert Technical Breakdown</strong></div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="modHelpers — Per-function Expert Technical Breakdown"> <strong>Verification statement:</strong> Reviewed <strong>10×</strong> for internal consistency, determinism, PQ parity, canonical hashing, PII controls, audit traceability, error handling, and testability prior to publishing. The entries below exhaustively document every exported and internal helper function expected within <code>modHelpers</code> for a production-grade GL-account canonicaliser. Each function entry includes: Purpose & contract, Inputs & outputs, Primary invariants, Provenance & usage, Failure modes & recovery, Observability & audit obligations, Performance expectations, Tests & example cases, Conceptual Power Query (PQ) mapping, Conceptual DAX reporting measures, Security & PII considerations, and Operational notes. Numbered lists use <code>&lt;br&gt;</code> line breaks per requirement. No code snippets are included. This document is authoritative for developers implementing <code>modHelpers</code> in VBA and for cross-runtime parity teams. </td></tr><tr><td data-label="modHelpers — Per-function Expert Technical Breakdown"> <strong>Function: NormalizeTextHelper</strong><br><strong>Purpose & contract:</strong> Provide a single deterministic canonical text normalization routine that all other helpers and scoring modules call. It must produce identical outputs to PQ <code>fnNormalize</code> for the same inputs and configuration options. Responsibilities: perform Unicode canonicalization (NFKC), case folding (Unicode case folding), punctuation removal (config-driven list), whitespace collapse, optional accent folding, optional stop-word removal, canonical numeric normalization, trimming, and produce a short <code>trace</code> (array or compact string) of applied transforms for diagnostics. MUST be side-effect free and purely functional. <br><strong>Inputs & outputs:</strong> Input: <code>rawString</code> (text) and <code>options</code> object with boolean flags {accentFold, removeStopWords, numericNormalization} and optional <code>maxLen</code>. Output: <code>{normalizedString, normalizationTrace, truncatedFlag}</code>. <br><strong>Primary invariants:</strong><br>1. Always apply Unicode NFKC normalization first.<br>2. Case folding must be locale-agnostic (use Unicode case folding rules).<br>3. Punctuation removal must use the exact <code>Config.punctuationList</code> that is included in <code>paramsHash</code> so changes to punctuation list change <code>paramsHash</code> deterministically.<br>4. Token-separators are canonicalized to single U+0020 spaces; resulting string contains no leading/trailing spaces.<br>5. If <code>numericNormalization=true</code>, normalize numeric groups (remove leading zeros, canonical grouping separators) according to documented grammar in <code>Config</code>.<br><strong>Provenance & usage:</strong> Called at the start of scoring pipelines, prior to tokenization, trigram generation, and Levenshtein computation; used in CandidateMap creation and for canonical artifact exports to ensure cross-runtime parity. The <code>normalizationTrace</code> must be persisted for material mappings in evidence for forensic reconstruction. <br><strong>Failure modes & recovery:</strong><br>• Malformed UTF sequences → produce best-effort normalization and set <code>truncatedFlag</code> if necessary; emit <code>normalize.unicode_fallback</code> diagnostic.<br>• Input longer than <code>maxLen</code> → truncate to <code>maxLen</code>, set <code>truncatedFlag=true</code>, and include truncated sample in evidenceRef for triage.<br>• Empty normalized result for non-empty input → flag row for manual review and emit <code>normalize.empty_output</code> audit entry. <br><strong>Observability & audit obligations:</strong><br>• Emit metrics: <code>normalize.count</code>, <code>normalize.fallbackCount</code>, <code>normalize.truncatedCount</code>.<br>• Persist <code>normalizationTrace</code> for any row that affects material disclosure or where <code>truncatedFlag=true</code> into evidence store with <code>evidenceRef</code> linked in audit row. <br><strong>Performance expectations:</strong> Linear in input length; optimized for thousands of small strings; for large volume runs (>50k rows) prefer PQ precompute. <br><strong>Tests & examples:</strong><br>1. <code>&quot;Domestic Sales - Retail&quot;</code> with default options → <code>&quot;domestic sales retail&quot;</code>; trace lists transforms in order.<br>2. <code>&quot;José &amp; Co.&quot;</code> with <code>accentFold=true</code> → <code>&quot;jose co&quot;</code>.<br>3. <code>&quot;00123-45&quot;</code> with <code>numericNormalization=true</code> → <code>&quot;123-45&quot;</code>. <br><strong>Conceptual PQ mapping:</strong> PQ <code>fnNormalize</code> must implement the exact same transform sequence and record <code>NormalizationTrace</code> columns for parity checks. CI must run cross-runtime parity against canonical fixtures. <br><strong>Conceptual DAX:</strong> <code>NormalizedLabel</code> used as an attribute in reports; create <code>NormalizeFailureRate = DIVIDE([NormalizeFallbackCount],[TotalRows])</code>. <br><strong>Security / PII:</strong> Normalization may expose PII patterns (e.g., phone-like numeric sequences). UI must display redacted normalized strings; full normalized values only saved to encrypted evidence; normalization steps stored in evidenceRef. <br><strong>Operational notes:</strong> Any change to normalization rules is a breaking change requiring <code>migration_manifest.json</code>, golden parity tests, and two-person approval for regulated datasets. </td></tr><tr><td data-label="modHelpers — Per-function Expert Technical Breakdown"> <strong>Function: TokenizeHelper</strong><br><strong>Purpose & contract:</strong> Deterministically convert a normalized string into a deduplicated, lexicographically sorted token array and a stable <code>tokenKey</code> string used for indexing and hashing. This must be identical to PQ tokenization semantics to preserve <code>scoreHash</code> parity. Responsibilities: split on whitespace, enforce <code>minTokenLen</code> and <code>maxTokenLen</code>, remove configurable stop-words, deduplicate tokens, lexicographically sort tokens, and produce a canonical <code>tokenKey</code> (single-space joined tokens). <br><strong>Inputs & outputs:</strong> Input: <code>normalizedString</code>, options {minTokenLen, maxTokenLen, removeStopWords}. Output: <code>{tokensArray, tokenKey, tokenStats}</code>. <br><strong>Primary invariants:</strong><br>1. Token splitting strictly on whitespace (punctuation already removed by NormalizeTextHelper).<br>2. Deduplication is case-insensitive (normalization already applied) and sorting is lexicographic ASCII stable ordering.<br>3. Stop-word list is versioned in <code>Config</code> and included in <code>paramsHash</code>. <br><strong>Provenance & usage:</strong> Token arrays feed TokenSetScore; <code>tokenKey</code> stored in CandidateMap and included in <code>scoreHash</code> for reproducibility. Alias lookup uses token sets for higher recall. <br><strong>Failure modes & recovery:</strong> Overzealous stop-word removal leading to empty token list → emit <code>tokenize.empty_after_stopwords</code> and set row to <code>Manual</code> band for review. Extremely long token → truncate and log <code>tokenize.truncated</code>. <br><strong>Observability & audit obligations:</strong> Histogram tokens-per-label and store tokenKey for material mappings. For PII-containing tokens, store hashed tokens in the main table and un-hashed tokens only in evidenceRef. <br><strong>Performance expectations:</strong> O(n) in label length; for large datasets precompute in PQ. <br><strong>Tests & examples:</strong><br>1. Input <code>&quot;domestic sales retail&quot;</code> → tokens <code>[&quot;domestic&quot;,&quot;retail&quot;,&quot;sales&quot;]</code>, tokenKey <code>&quot;domestic retail sales&quot;</code>.<br>2. Stop-word scenario: <code>&quot;the sales account&quot;</code> with stopwords removed → tokens <code>[&quot;sales&quot;,&quot;account&quot;]</code>. <br><strong>Conceptual PQ mapping:</strong> PQ should output <code>TokenKey</code> and token arrays for CandidateMap to reduce VBA CPU. <br><strong>Conceptual DAX:</strong> <code>DistinctTokenCount</code> aggregated per bucket for QA dashboards. <br><strong>Security / PII:</strong> Tokens derived from PII should be masked in primary sheets; encrypted evidence references required. <br><strong>Operational notes:</strong> Tokenization rules are sensitive; any modifications require a migration manifest and golden tests. </td></tr><tr><td data-label="modHelpers — Per-function Expert Technical Breakdown"> <strong>Function: TrigramHelper</strong><br><strong>Purpose & contract:</strong> Build canonical character n-grams (default trigrams) with optional boundary padding. Output the deduplicated, sorted trigram list and a compact <code>trigramFingerprint</code> used for fast comparisons. Function must match PQ trigram rules to support cross-runtime equality checks. <br><strong>Inputs & outputs:</strong> Input: <code>normalizedString</code>, options {gramSize:3, padBoundaries:true}. Output: <code>{trigramsArray, trigramFingerprint, trigramCount}</code>. <br><strong>Primary invariants:</strong><br>1. If <code>padBoundaries=true</code>, pad normalized string at both ends with a single underscore (or configured marker) to include edge grams.<br>2. Deduplicate and sort trigrams lexicographically before computing fingerprint. <br><strong>Provenance & usage:</strong> Used by TrigramJaccard scoring component; stored as <code>trigramFingerprint</code> in CandidateMap for efficient intersection computations. <br><strong>Failure modes & recovery:</strong> Unicode combining marks must be normalized by NormalizeTextHelper; log <code>trigram.unicode_fallback</code> when fallback rules used. <br><strong>Observability & audit obligations:</strong> Record trigramCount distribution and fingerprint collisions (rare) for QA. Preserve full trigram sets in evidenceRef for material mappings. <br><strong>Performance expectations:</strong> Linear in label length; safe for single-row computations; PQ precompute recommended at scale. <br><strong>Tests & examples:</strong><br>1. <code>&quot;sales&quot;</code> -> padded <code>&quot;_sales_&quot;</code> -> trigrams <code>[&quot;_sa&quot;,&quot;sal&quot;,&quot;ale&quot;,&quot;les&quot;,&quot;es_&quot;]</code>. <br><strong>Conceptual PQ mapping:</strong> PQ <code>fnTrigrams</code> must implement the same boundary and sorting procedure and export <code>trigramFingerprint</code>. <br><strong>Conceptual DAX:</strong> <code>AvgTrigramCount</code> and <code>TrigramOverlapAvg</code> for monitoring. <br><strong>Security / PII:</strong> trigrams may reveal PII patterns; do not display raw trigrams in UI unless redacted. <br><strong>Operational notes:</strong> Changing gram size or padding semantics changes <code>paramsHash</code> and must be governed. </td></tr><tr><td data-label="modHelpers — Per-function Expert Technical Breakdown"> <strong>Function: TokenSetScoreHelper</strong><br><strong>Purpose & contract:</strong> Deterministically compute token-set similarity between two token arrays. Default formula: <code>score = (2 * |intersection|) / (|A| + |B|)</code>. Supports optional token-length weighting but must renormalize weights to keep score within [0,1]. Returns component breakdown for audit. <br><strong>Inputs & outputs:</strong> Input: <code>tokensA</code>, <code>tokensB</code>, options {weightByLength:false}. Output: <code>{tokenScore, intersectionCount, unionCount, matchedTokens}</code>. <br><strong>Primary invariants:</strong><br>1. Score symmetric and bounded 0..1.<br>2. Weighting must preserve symmetry and be included in <code>paramsHash</code> when enabled. <br><strong>Provenance & usage:</strong> Major signal used when labels differ by word order; stored per-row for QA and audit. <br><strong>Failure modes & recovery:</strong> Empty union -> return 0 and mark for review. Invalid weighting configuration -> fallback to unweighted formula and emit <code>tokensetscore.weighting_defaulted</code>. <br><strong>Observability & audit obligations:</strong> Persist intersectionCount and matchedTokens for material rows (in evidenceRef). Track tokenScore distribution in telemetry. <br><strong>Performance expectations:</strong> O(n) with dictionary lookups; pre-filter comparisons with tokenKey equality reduces load. <br><strong>Tests & examples:</strong><br>1. A=<code>[&quot;domestic&quot;,&quot;sales&quot;]</code>, B=<code>[&quot;sales&quot;,&quot;domestic&quot;]</code> -> tokenScore=1.<br>2. A=<code>[&quot;domestic&quot;,&quot;sales&quot;,&quot;online&quot;]</code>, B=<code>[&quot;sales&quot;,&quot;offline&quot;]</code> -> intersection=1, denom=5 → 0.4. <br><strong>Conceptual PQ mapping:</strong> PQ precomputes tokenKey and token counts for bulk pre-filtering. <br><strong>Conceptual DAX:</strong> <code>TokenMatchRate</code> measure for data quality dashboards. <br><strong>Security / PII:</strong> matchedTokens may contain PII; redact in primary logs. <br><strong>Operational notes:</strong> Tune weighting only after tuning runs and capture <code>paramsHash</code>. </td></tr><tr><td data-label="modHelpers — Per-function Expert Technical Breakdown"> <strong>Function: TrigramJaccardHelper</strong><br><strong>Purpose & contract:</strong> Compute Jaccard similarity on trigram sets: <code>|intersection| / |union|</code>. Deterministic and symmetric; useful as complementary signal to token-set for short or noisy labels. <br><strong>Inputs & outputs:</strong> Input: <code>trigramsA</code>, <code>trigramsB</code>. Output: <code>{jaccardScore, interCount, unionCount}</code>. <br><strong>Primary invariants:</strong><br>1. Use deduplicated trigram sets supplied by TrigramHelper. <br>2. Return 0 for empty union. <br><strong>Provenance & usage:</strong> Weights applied in CombinedScore with <code>paramsHash</code> recorded. <br><strong>Failure modes & recovery:</strong> union zero fallback to 0 and record diagnostic. <br><strong>Observability & audit obligations:</strong> jaccard distribution, sample misaligned cases. <br><strong>Performance expectations:</strong> O(n) per pair. Precompute fingerprints for bulk intersection approximations in PQ. <br><strong>Tests & examples:</strong> "Sls" vs "Sales" moderate jaccard; includes examples showing differences with tokenScore. <br><strong>Conceptual PQ mapping:</strong> PQ may compute trigram fingerprints to accelerate pairwise comparisons. <br><strong>DAX:</strong> <code>AvgTrigramJaccard</code> for monitoring. <br><strong>Security / PII:</strong> follow evidence rules. <br><strong>Operational notes:</strong> Combined with tokenScore for robust short-label matching. </td></tr><tr><td data-label="modHelpers — Per-function Expert Technical Breakdown"> <strong>Function: SafeLevenshteinHelper</strong><br><strong>Purpose & contract:</strong> Compute Levenshtein edit distance with options for <code>maxDistance</code> and <code>bandWidth</code> to limit computation. Return integer distance and an <code>aborted</code> flag if computation aborted due to <code>maxDistance</code>. Use only normalized inputs. <br><strong>Inputs & outputs:</strong> Input: <code>left</code>, <code>right</code>, options {maxDistance:null, bandWidth:null}. Output: <code>{distance, aborted, usedBanded}</code>. <br><strong>Primary invariants:</strong><br>1. If <code>maxDistance</code> provided, stop early when threshold exceeded and set <code>aborted=true</code> (caller treats this as high distance).<br>2. Banded mode must be documented in <code>paramsHash</code>. <br><strong>Provenance & usage:</strong> Contributes to NormalizedLev metric used in CombinedScore; particularly useful for numeric suffix or short-code differences. <br><strong>Failure modes & recovery:</strong> Extremely long strings cause resource exhaustion; fallback to n-gram approximate similarity and log <code>levenshtein.fallback</code>. <br><strong>Observability & audit obligations:</strong> Track counts of aborts/banded usage. For bailouts, include sample in evidenceRef. <br><strong>Performance expectations:</strong> Worst-case O(n*m); bound with <code>maxDistance</code> or banded to protect runtime. <br><strong>Tests & examples:</strong> small edit distances, numeric suffix differences; confirm aborted behavior vs exact. <br><strong>Conceptual PQ mapping:</strong> PQ typically avoids heavy Levenshtein at scale; if PQ computes distances, parity must be enforced. <br><strong>DAX:</strong> <code>AvgLevenshtein</code> as QA metric. <br><strong>Security / PII:</strong> distance metrics safe; ensure not to surface labels alongside distances in public logs. <br><strong>Operational notes:</strong> set conservative <code>maxDistance</code> defaults in <code>Config</code>. </td></tr><tr><td data-label="modHelpers — Per-function Expert Technical Breakdown"> <strong>Function: NormalizedLevHelper</strong><br><strong>Purpose & contract:</strong> Convert Levenshtein distance into normalized metric <code>normLev</code> in [0..1] using denominator <code>max(len(A),len(B))</code> and a <code>minDenominator</code> floor to avoid divide-by-zero. Specified denominator policy must be part of <code>paramsHash</code>. <br><strong>Inputs & outputs:</strong> Input: <code>{distance, lenA, lenB, minDenominator:1}</code>. Output: <code>{normLev}</code>. <br><strong>Primary invariants:</strong><br>1. If maxLen==0 => return 0 (identical empty strings).<br>2. Clip and clamp output to [0,1]. <br><strong>Provenance & usage:</strong> Used in CombinedScore as <code>(1 - normLev)</code> similarity contribution. <br><strong>Failure modes & recovery:</strong> denominator zero fallback documented; log <code>normalizedlev.div0_fallback</code>. <br><strong>Observability & audit obligations:</strong> distribution logging, sample persistence for material rows. <br><strong>Performance expectations:</strong> trivial. <br><strong>Tests & examples:</strong> distance 1 with maxLen 8 -> 0.125. <br><strong>Conceptual PQ mapping:</strong> PQ normalized distances must match. <br><strong>DAX:</strong> <code>NormLevDistribution</code> for drift monitoring. <br><strong>Security / PII:</strong> none special. <br><strong>Operational notes:</strong> changes to denominator policy require migration manifest. </td></tr><tr><td data-label="modHelpers — Per-function Expert Technical Breakdown"> <strong>Function: SignatureParseHelper</strong><br><strong>Purpose & contract:</strong> Parse posting signature strings (compact <code>Counterparty:Pct;...</code>) into deterministic dictionary mapping counterparty→percent as numeric fraction (0..100). Must be tolerant to minor formatting differences and validate percent sum semantics. <br><strong>Inputs & outputs:</strong> Input: <code>signatureString</code>. Output: <code>{signatureDict, totalPct, signatureHash}</code>. <br><strong>Primary invariants:</strong><br>1. Percent stored as numeric with fixed decimal places; rounding policy recorded in <code>paramsHash</code>.<br>2. When <code>totalPct</code> < 100 due to truncation or sampling, acceptable but recorded. <br><strong>Provenance & usage:</strong> SignatureOverlap uses this parsed dict to compute weighted overlap; signatureHash stored in CandidateMap for audit. <br><strong>Failure modes & recovery:</strong> malformed entries ignored with <code>signature.parse_ignored</code> logs; empty signatures produce empty dict. <br><strong>Observability & audit obligations:</strong> counts of parsed signatures, malformed lines stored in evidenceRef. <br><strong>Performance expectations:</strong> cheap per-row. <br><strong>Tests & examples:</strong> <code>&quot;CP A:45.25;CP B:30.10&quot;</code> -> dict and totalPct <code>75.35</code>. <br><strong>Conceptual PQ mapping:</strong> PQ must compute top-N counterparties and percent with same rounding semantics to ensure overlap parity. <br><strong>DAX:</strong> <code>SignatureCoveragePct</code> per account. <br><strong>Security / PII:</strong> Counterparty names may be PII; when storing parsed dict in CandidateMap, hash or mask names; full dict only in evidenceRef. <br><strong>Operational notes:</strong> signature parsing must be robust to unusual counterparty name characters; include canonicalization of counterparty names in parse logic to increase match rate. </td></tr><tr><td data-label="modHelpers — Per-function Expert Technical Breakdown"> <strong>Function: SignatureOverlapHelper</strong><br><strong>Purpose & contract:</strong> Compute normalized symmetric weighted overlap between two parsed signatures. Normalization divides weighted intersection by average total percent to produce 0..1. Deterministic ordering and tie-break rules documented in <code>paramsHash</code>. <br><strong>Inputs & outputs:</strong> Input: <code>sigA</code>, <code>sigB</code>, options {topN:5}. Output: <code>{overlapScore, matchingPairs}</code>. <br><strong>Primary invariants:</strong><br>1. Use topN counterparties by percent for both signatures; deterministic tie-breaker lexicographic on counterparty name.<br>2. Normalized to 0..1 where 1 means identical topN counterparties and weighting. <br><strong>Provenance & usage:</strong> Tie-breaker for combined scoring; used to promote candidates with similar posting patterns even when textual signals ambiguous. <br><strong>Failure modes & recovery:</strong> missing signatures -> return 0 and flag <code>signature.overlap.missing</code>. <br><strong>Observability & audit obligations:</strong> persist <code>signatureHash</code> and overlap value in CandidateMap; store matchingPairs in evidenceRef for audit if mapping is material. <br><strong>Performance expectations:</strong> small constant-time operation per pair. <br><strong>Tests & examples:</strong> identical signatures -> 1. disjoint -> 0; partial overlap returns fractional. <br><strong>Conceptual PQ mapping:</strong> PQ must compute topN counterparties with same rounding to ensure parity. <br><strong>DAX:</strong> <code>AvgSignatureOverlap</code> metric for QA. <br><strong>Security / PII:</strong> treat counterparty names sensitive; do not show them in UI except redacted or via evidence retrieval. <br><strong>Operational notes:</strong> signature overlap heuristics tuned during pilot by measuring overrides. </td></tr><tr><td data-label="modHelpers — Per-function Expert Technical Breakdown"> <strong>Function: CanonicalJsonSerialize</strong><br><strong>Purpose & contract:</strong> Produce deterministic canonical JSON for structured objects used in <code>paramsHash</code>, migration manifests, and artifact payloads. Rules: stable key ordering, fixed float formatting via FixedFloatFormatter, normalized newline semantics, removal of ephemeral fields (unless includeTransient true), and explicit handling for regex serialization. Returns canonical string and SHA256 hex. <br><strong>Inputs & outputs:</strong> Input: <code>object</code>, options {stableKeyOrder:true, floatPrecision:8, includeTransient:false}. Output: <code>{canonicalString, canonicalSha256}</code>. <br><strong>Primary invariants:</strong><br>1. Canonicalization algorithm must be documented and identical across PQ and worker runtimes to ensure hash parity.<br>2. Remove <code>lastLoadedTs</code> and ephemeral IDs by default to avoid non-determinism. <br><strong>Provenance & usage:</strong> Used to compute <code>paramsHash</code>, <code>standardMap.hash</code>, and artifact manifests; included in audit rows. <br><strong>Failure modes & recovery:</strong> non-serializable entries cause <code>serialize.error</code> and block artifact export; caller must sanitize. <br><strong>Observability & audit obligations:</strong> <code>canonicalize.duration_ms</code> and store <code>canonicalSha256</code> in audits. <br><strong>Performance expectations:</strong> O(size) of object; large payloads streamed. <br><strong>Tests & examples:</strong> canonicalizing identical dicts with different key order must produce same canonicalString and SHA256. <br><strong>Conceptual PQ mapping:</strong> PQ exporters must call equivalent serialization routine to compute matching hashes. <br><strong>Conceptual DAX:</strong> <code>ConfigHash</code> as chart label; DAX not used to compute canonical JSON. <br><strong>Security / PII:</strong> canonical JSONs may contain PII; store full canonical JSON only in evidence encrypted store; audits contain only the <code>canonicalSha256</code> and metadata. <br><strong>Operational notes:</strong> any change to canonicalization algorithm is high-risk and requires governance. </td></tr><tr><td data-label="modHelpers — Per-function Expert Technical Breakdown"> <strong>Function: ComputeSHA256Hex</strong><br><strong>Purpose & contract:</strong> Compute SHA-256 hex of UTF-8 encoded bytes for strings or streaming bytes for files, returning lowercase hex. Must use canonical UTF-8 encoding and be compatible with PQ/CI hash computations. <br><strong>Inputs & outputs:</strong> Input: <code>text</code> or <code>filePath</code>, options {isFile:false}. Output: <code>{sha256Hex, lengthBytes}</code>. <br><strong>Primary invariants:</strong><br>1. String inputs must be normalized (e.g., newline, canonical JSON) before hashing to avoid cross-runtime differences.<br>2. For files, read in binary chunks and compute streaming hash to avoid memory pressure. <br><strong>Provenance & usage:</strong> Fundamental to <code>paramsHash</code>, <code>scoreHash</code>, artifact checksums, and forensic manifests. <br><strong>Failure modes & recovery:</strong> unreadable file -> raise <code>checksum.read_error</code> and stage artifact for manual retrieval. <br><strong>Observability & audit obligations:</strong> log compute duration and returned hash; include checksum in migration exports. <br><strong>Performance expectations:</strong> linear; chunk streaming for large files. <br><strong>Tests & examples:</strong> known test vectors like empty string and "abc" to verify implementation. <br><strong>Conceptual PQ mapping:</strong> PQ must compute identical SHA256 for canonical strings. <br><strong>Security / PII:</strong> checksums safe for audit linking. <br><strong>Operational notes:</strong> ensure consistent encoding across all tools (UTF-8). </td></tr><tr><td data-label="modHelpers — Per-function Expert Technical Breakdown"> <strong>Function: FixedFloatFormatter</strong><br><strong>Purpose & contract:</strong> Provide deterministic float-to-string formatting with fixed decimals and configured rounding mode. Used by CanonicalJsonSerialize and score/hash generation. Must be identical across PQ and other runtimes. <br><strong>Inputs & outputs:</strong> Input: <code>floatValue</code>, options {decimals:int, roundingMode:<code>bankers|away</code>} Output: <code>formattedString</code>. <br><strong>Primary invariants:</strong><br>1. Rounding mode included in <code>paramsHash</code>; negative zero normalized to <code>0.00</code> string. <br><strong>Provenance & usage:</strong> Used when serializing numeric components into canonical strings for hashing. <br><strong>Failure modes & recovery:</strong> non-finite numbers replaced by sentinel <code>null</code> and flagged in diagnostics. <br><strong>Observability & audit obligations:</strong> number of non-finite replacements recorded in diagnostics. <br><strong>Performance expectations:</strong> trivial per-call. <br><strong>Tests & examples:</strong> rounding parity tests against PQ. <br><strong>Conceptual PQ mapping:</strong> PQ float formatting must match to prevent parity errors. <br><strong>Security / PII:</strong> none. <br><strong>Operational notes:</strong> update only with migration manifest. </td></tr><tr><td data-label="modHelpers — Per-function Expert Technical Breakdown"> <strong>Function: RowChecksumHelper</strong><br><strong>Purpose & contract:</strong> Compute a compact row checksum for optimistic locking in UI flows. Checksum derived from canonical serialization of subset of fields (AccountId, ProposedISAKBucket, tokenKey, combinedScore, mappingVersion). Used to detect concurrent edits between Reviewer UI load and save. <br><strong>Inputs & outputs:</strong> Input: <code>rowObject</code>, <code>fieldsToInclude</code>. Output: <code>{rowChecksumHex}</code>. <br><strong>Primary invariants:</strong><br>1. Checksum computed via CanonicalJsonSerialize then SHA256; stable ordering of fields guaranteed.<br>2. Row saving must validate checksum; on mismatch, abort and instruct user to reload. <br><strong>Provenance & usage:</strong> ReviewerUI save operation uses this helper to ensure no lost updates. <br><strong>Failure modes & recovery:</strong> mismatch -> <code>review.collision</code> audit emitted and save rejected. <br><strong>Observability & audit obligations:</strong> count of collisions, per-operator collision rates. <br><strong>Performance expectations:</strong> trivial. <br><strong>Tests & examples:</strong> simulate concurrent modification to validate behavior. <br><strong>Conceptual PQ mapping:</strong> none required. <br><strong>Security / PII:</strong> checksum contains non-PII fields only. <br><strong>Operational notes:</strong> include rowChecksum field in CandidateMap to show reviewer if helpful. </td></tr><tr><td data-label="modHelpers — Per-function Expert Technical Breakdown"> <strong>Function: AtomicSheetSwapHelper</strong><br><strong>Purpose & contract:</strong> Implement read-then-validate-then-atomic-swap semantics for in-workbook sheet updates: write results to staging sheet, run validation callback, atomically rename/swap to live sheet, and persist <code>beforeSnapshot</code> and <code>afterChecksum</code>. On failure, revert to <code>beforeSnapshot</code>. <br><strong>Inputs & outputs:</strong> Input: <code>stagingSheetName</code>, <code>liveSheetName</code>, <code>validationCallback</code>. Output: <code>{success:boolean, beforeChecksum, afterChecksum, diagnosticsRef}</code>. <br><strong>Primary invariants:</strong><br>1. Always create <code>beforeSnapshot</code> and compute <code>beforeChecksum</code> prior to swap.<br>2. Validation must be idempotent; only swap when validation passes. <br><strong>Provenance & usage:</strong> Used by ScoreBatch updates and mapping hot-swap. <br><strong>Failure modes & recovery:</strong> mid-swap crash -> detect on reload and revert to <code>beforeSnapshot</code>; log <code>atomic.swap.recovery</code>. If validation fails -> preserve staging for inspection and return diagnosticsRef. <br><strong>Observability & audit obligations:</strong> emit <code>atomic.swap.started</code> and <code>atomic.swap.completed</code> with checksums and duration. <br><strong>Performance expectations:</strong> cost proportional to data range size; optimize swap window by replacing named ranges if workbook supports. <br><strong>Tests & examples:</strong> inject validation failure to confirm no swap. <br><strong>Conceptual PQ mapping:</strong> PQ produces staging CSV that is imported into staging sheet. <br><strong>Security / PII:</strong> snapshots encrypted when persisted to disk. <br><strong>Operational notes:</strong> avoid on-UI large-block operations; run in deferred init/worker where possible. </td></tr><tr><td data-label="modHelpers — Per-function Expert Technical Breakdown"> <strong>Function: SafeFileWriteWithBackup</strong><br><strong>Purpose & contract:</strong> Write artifact to disk safely: write to temp file, compute checksum, rename to destination atomically, and create a timestamped backup of the previous version. Support local and network shares with caveats. <br><strong>Inputs & outputs:</strong> Input: <code>destinationPath</code>, <code>content</code>, options {createBackup:true}. Output: <code>{finalPath, checksum, backupPath|null}</code>. <br><strong>Primary invariants:</strong><br>1. Temp file must be on same filesystem as final file to ensure atomic rename.<br>2. Backup naming canonical and include checksum; if backup cannot be made, log <code>file.backup_unavailable</code>. <br><strong>Provenance & usage:</strong> Migration artifacts, evidence exports. <br><strong>Failure modes & recovery:</strong> insufficient disk space -> abort and stage artifact to encrypted local staging; emit <code>file.write.space_error</code>. Rename failure on network share -> fall back to control-file two-phase commit with audit. <br><strong>Observability & audit obligations:</strong> write duration, checksum, and backupPath included in migration audit. <br><strong>Performance expectations:</strong> streaming writes for large files. <br><strong>Tests & examples:</strong> verify atomic rename semantics on local FS and simulate network error. <br><strong>Conceptual PQ mapping:</strong> PQ exports should be written using this helper. <br><strong>Security / PII:</strong> encrypt content before write for PII artifacts. <br><strong>Operational notes:</strong> if target FS does not support atomic rename, use <code>safe-write</code> protocol with control file and audit handshake. </td></tr><tr><td data-label="modHelpers — Per-function Expert Technical Breakdown"> <strong>Function: CsvEscapeAndWriteHelper</strong><br><strong>Purpose & contract:</strong> Create RFC-compliant CSV rows with canonical header order and escape rules; write to file and optionally append checksum footer. Must produce deterministic byte-for-byte CSV given the same input and column ordering. <br><strong>Inputs & outputs:</strong> Input: <code>rows</code>, <code>columnsOrdered</code>, <code>destinationPath</code>, options {addChecksumFooter:true}. Output: <code>{filePath, checksum}</code>. <br><strong>Primary invariants:</strong><br>1. Header order is authoritative; caller must supply column ordering explicitly.<br>2. Use quoting and escaping rules per RFC-4180; newline canonicalization to <code>\n</code>. <br><strong>Provenance & usage:</strong> migration scripts, preview exports, forensic packs. <br><strong>Failure modes & recovery:</strong> encoding errors -> replace with replacement char or fail per config; record <code>csv.write.replace_unencodable</code> audit. <br><strong>Observability & audit obligations:</strong> include checksum and row counts in <code>standard.map.export</code> audit. <br><strong>Performance expectations:</strong> streaming writer for large files. <br><strong>Tests & examples:</strong> fields with commas, quotes, and newlines properly escaped. <br><strong>Conceptual PQ mapping:</strong> PQ-produced rows should be post-processed through this helper to ensure canonical artifacts. <br><strong>Security / PII:</strong> redact or encrypt PII columns as per export policy. <br><strong>Operational notes:</strong> header changes are breaking — require migration manifest and golden parity tests. </td></tr><tr><td data-label="modHelpers — Per-function Expert Technical Breakdown"> <strong>Function: RetryWithBackoffHelper</strong><br><strong>Purpose & contract:</strong> Generic retry wrapper for transient operations with exponential backoff and jitter. Caller supplies operation callback and idempotency key when side-effects exist. Helper returns final success/failure and diagnostic info for audits. <br><strong>Inputs & outputs:</strong> Input: <code>operationCallback</code>, <code>maxAttempts</code>, <code>initialDelayMs</code>, <code>maxDelayMs</code>, <code>idempotencyKey</code> optional. Output: <code>{success, attempts, lastError, totalDurationMs}</code>. <br><strong>Primary invariants:</strong><br>1. If operation has side-effects, caller must provide <code>idempotencyKey</code> to enable safe retries; helper does not attempt to make non-idempotent operations idempotent automatically.<br>2. Backoff parameters are in Config and included in <code>paramsHash</code>. <br><strong>Provenance & usage:</strong> Used by SecureEvidenceWrite, Remote RBAC checks, job queue persistence. <br><strong>Failure modes & recovery:</strong> persistent failures -> stage request locally and emit <code>retry.failure</code> audit; surface <code>lastError</code> to operator. <br><strong>Observability & audit obligations:</strong> retries count, endpoints affected, and attempt timings; track metrics for upstream service reliability. <br><strong>Performance expectations:</strong> network-bound; ensure overall timeout limit respected. <br><strong>Tests & examples:</strong> simulate transient errors to validate success after N attempts. <br><strong>Conceptual PQ mapping:</strong> none. <br><strong>Security / PII:</strong> do not log payloads; mask sensitive data in diagnostics. <br><strong>Operational notes:</strong> backoff parameters adjustable per environment; heavy retry loops must not block UI thread. </td></tr><tr><td data-label="modHelpers — Per-function Expert Technical Breakdown"> <strong>Function: ChunkIteratorHelper</strong><br><strong>Purpose & contract:</strong> Deterministic iterator to divide large datasets into stable-sized chunks for memory-safety and resume capability. Produces chunk metadata including <code>startIdx</code>, <code>endIdx</code>, and <code>chunkId</code>. <br><strong>Inputs & outputs:</strong> Input: <code>totalRows</code>, <code>chunkSize</code>, options {deterministicOrder:true}. Output: iterator yielding <code>{chunkId, startIdx, endIdx}</code>. <br><strong>Primary invariants:</strong><br>1. Chunk IDs stable given the same <code>totalRows</code> and <code>chunkSize</code> so that chunk replays are deterministic.<br>2. Caller must manage chunk-level checkpoints to enable safe resume. <br><strong>Provenance & usage:</strong> ScoreBatch chunking, JobDescriptor chunk offsets for workers. <br><strong>Failure modes & recovery:</strong> snapshot mismatch during processing -> abort affected chunk and schedule retry with new snapshot; emit <code>chunk.snapshot_mismatch</code>. <br><strong>Observability & audit obligations:</strong> chunk durations, fail/retry counts. <br><strong>Performance expectations:</strong> trivial overhead. <br><strong>Tests & examples:</strong> validate chunking of 10k rows into 1000-size chunks yields deterministic <code>chunkId</code>s. <br><strong>Conceptual PQ mapping:</strong> PQ may output chunked files for worker ingestion. <br><strong>Security / PII:</strong> chunk artifacts must be encrypted if containing sensitive data. <br><strong>Operational notes:</strong> choose chunk size based on host memory and transform cost; provide recommended defaults in Config. </td></tr><tr><td data-label="modHelpers — Per-function Expert Technical Breakdown"> <strong>Function: SimpleCsvReaderToArray</strong><br><strong>Purpose & contract:</strong> Read small CSV artifacts into an in-memory array/dictionary structure with canonical header mapping; abort on exceeding <code>maxRows</code>. Intended for QA and small preview loads, not big data. <br><strong>Inputs & outputs:</strong> Input: <code>filePath</code>, options {encoding:"utf-8", maxRows:10000}. Output: <code>{rowsArray, headerMap, rowCount}</code>. <br><strong>Primary invariants:</strong><br>1. If <code>checksum</code> provided, verify integrity before parse and abort on mismatch.<br>2. Enforce <code>maxRows</code> to prevent memory exhaustion. <br><strong>Provenance & usage:</strong> reading preview artifacts, forensic packs. <br><strong>Failure modes & recovery:</strong> checksum mismatch -> abort and persist error; large file -> partial read and log <code>csv.read.truncated</code>. <br><strong>Observability & audit obligations:</strong> parse errors counted and surfaced. <br><strong>Performance expectations:</strong> fine for small files; not for huge files. <br><strong>Tests & examples:</strong> header mapping mismatch scenario handled as per <code>strictHeader</code> option. <br><strong>Security / PII:</strong> require evidence access controls for files containing PII. <br><strong>Operational notes:</strong> prefer streaming readers for larger files. </td></tr><tr><td data-label="modHelpers — Per-function Expert Technical Breakdown"> <strong>Function: SafeIsoDateFormatter</strong><br><strong>Purpose & contract:</strong> Parse and format ISO8601 UTC timestamps in canonical <code>YYYY-MM-DDTHH:MM:SSZ</code> string form for use in audit rows, artifact names, and manifests. Input timezone unambiguous; always operate in UTC. <br><strong>Inputs & outputs:</strong> Input: <code>dateObj</code> or parseable string, options {strict:true}. Output: <code>{isoString, ok}</code>. <br><strong>Primary invariants:</strong><br>1. All times stored in audit and artifact names must be UTC; host-local times must be converted. <br><strong>Provenance & usage:</strong> audit timestamps and artifact filenames. <br><strong>Failure modes & recovery:</strong> parse failures -> return <code>ok=false</code> and diagnostic. <br><strong>Observability & audit obligations:</strong> parse failure counts logged. <br><strong>Performance expectations:</strong> trivial. <br><strong>Tests & examples:</strong> parse <code>2026-02-16T09:00:00Z</code> round-trip. <br><strong>Security / PII:</strong> none. <br><strong>Operational notes:</strong> prefer this helper for all timestamp formatting to maintain consistency. </td></tr><tr><td data-label="modHelpers — Per-function Expert Technical Breakdown"> <strong>Function: SafeParseDecimalHelper</strong><br><strong>Purpose & contract:</strong> Parse numeric amounts deterministically without depending on host locale; handle thousand separators and decimal separators with explicit heuristics included in <code>paramsHash</code>. Return normalized numeric and a <code>confidence</code> score for ambiguous cases. <br><strong>Inputs & outputs:</strong> Input: <code>stringValue</code>, options {expectedCurrency:null, allowThousands:true}. Output: <code>{value:number|null, normalizedString, confidence:0..1}</code>. <br><strong>Primary invariants:</strong><br>1. Heuristics (e.g., comma as decimal vs thousands) must be recorded in <code>paramsHash</code> and applied deterministically.<br>2. If ambiguous, return <code>value=null</code> and <code>confidence</code> < 0.5 with <code>issueCode</code>. <br><strong>Provenance & usage:</strong> used in ImpactSimulation when normalizing postings and in preview displays to surface parse ambiguities. <br><strong>Failure modes & recovery:</strong> ambiguous parse -> include <code>STD_PARSE_NUM_AMBIG</code> in preview <code>issues[]</code> and route to review; for critical flows, treat as material and block automatic apply. <br><strong>Observability & audit obligations:</strong> log parse ambiguity counts and sample failing strings into evidenceRef. <br><strong>Performance expectations:</strong> fast per-row. <br><strong>Tests & examples:</strong> <code>&quot;1,234.56&quot;</code> parsed to 1234.56 with high confidence; <code>&quot;1.234,56&quot;</code> ambiguous unless locale specified. <br><strong>Conceptual PQ mapping:</strong> PQ should prefer culture-specific parsing if provided; ensure parity in numeric normalization across PQ and VBA. <br><strong>DAX:</strong> <code>NumericParseAmbiguityPct</code> for monitoring. <br><strong>Security / PII:</strong> none. <br><strong>Operational notes:</strong> make parse heuristics conservative to avoid misinterpretation of financial amounts. </td></tr><tr><td data-label="modHelpers — Per-function Expert Technical Breakdown"> <strong>Function: RedactForUiHelper</strong><br><strong>Purpose & contract:</strong> Deterministic redaction helper to mask PII for UI surfaces. Supports masking patterns: prefix/suffix reveal, token hashing, and full redaction. Must be reversible only through evidence retrieval workflows that require RBAC and audit. <br><strong>Inputs & outputs:</strong> Input: <code>value</code>, <code>sensitivityLevel</code> (<code>low|medium|high</code>), options {maskChar:"*", keepPrefix:1, keepSuffix:1}. Output: <code>{redactedValue, redactionMeta}</code>. <br><strong>Primary invariants:</strong><br>1. Redaction deterministic and irreversible without evidence key; evidenceRef maps to full value for compliance retrieval.<br>2. Redaction rules included in <code>configHash</code> for audit trace. <br><strong>Provenance & usage:</strong> UI preview rows, triage messages, telemetry. <br><strong>Failure modes & recovery:</strong> accidental display of full PII -> trigger <code>diag.redaction_violation</code> and require forensic pack; implement automatic masking if violation detected. <br><strong>Observability & audit obligations:</strong> counts for <code>redact.applied</code> and evidence retrieval logs. <br><strong>Performance expectations:</strong> trivial. <br><strong>Tests & examples:</strong> <code>&quot;John Smith&quot;</code> medium -> <code>&quot;J*** S****&quot;</code>. <br><strong>Security / PII:</strong> central to PII policy; redaction rules must satisfy internal privacy guidelines. <br><strong>Operational notes:</strong> redaction policy may change over time; record <code>configHash</code> on UIs produced. </td></tr><tr><td data-label="modHelpers — Per-function Expert Technical Breakdown"> <strong>Function: EvidenceRefBuilder</strong><br><strong>Purpose & contract:</strong> Construct opaque evidenceRef tokens that refer to encrypted evidence artifacts stored in secure evidence store. EvidenceRefs must be safe to include in audit rows and must not reveal PII. Include metadata: <code>artifactId</code>, <code>storeUri</code> (partial / opaque), <code>retentionPolicy</code>, and <code>checksum</code>. <br><strong>Inputs & outputs:</strong> Input: <code>artifactPath</code>, <code>checksum</code>, options {retentionPolicy:'regulated'}. Output: <code>{evidenceRef, expiryTs}</code>. <br><strong>Primary invariants:</strong><br>1. EvidenceRef opaque and verifiable (contains or references checksum) and stored in audit rows for retrieval by compliance only.<br>2. EvidenceRef itself must not expose PII. <br><strong>Provenance & usage:</strong> Returned by SecureEvidenceWrite and persisted in MappingHistory or ApplyDescriptor. <br><strong>Failure modes & recovery:</strong> upload failure -> create staged evidenceRef marked <code>staged://</code> and alert operator. <br><strong>Observability & audit obligations:</strong> evidence writes success/failure counters; evidence retrieval logged with operator and reason. <br><strong>Performance expectations:</strong> creation trivial; upload separate. <br><strong>Tests & examples:</strong> create evidenceRef for preview artifact and validate retrieval workflow. <br><strong>Security / PII:</strong> critical: ensure encryption, RBAC retrieval, and WORM archival for regulated evidence. <br><strong>Operational notes:</strong> evidenceRef format documented in compliance runbook. </td></tr><tr><td data-label="modHelpers — Per-function Expert Technical Breakdown"> <strong>Function: BuildShortTriageMessage</strong><br><strong>Purpose & contract:</strong> Compose a short, PII-free triage message for UI with correlationId and triage hint. Messages must be ≤160 characters and include <code>correlationId</code> and <code>triageCode</code>. Return <code>userMessage</code> and <code>triageCode</code>. <br><strong>Inputs & outputs:</strong> Input: <code>{correlationId, errorCode, shortHint}</code>. Output: <code>{userMessage, triageCode}</code>. <br><strong>Primary invariants:</strong><br>1. Never include PII or stack traces. <br><strong>Provenance & usage:</strong> Used in UI to inform operator and link to audit evidence. <br><strong>Failure modes & recovery:</strong> none critical; fallback to generic message if missing data. <br><strong>Observability & audit obligations:</strong> count of triage messages shown and follow-up remediation actions. <br><strong>Performance expectations:</strong> trivial. <br><strong>Tests & examples:</strong> <code>STD_PARSE_FAIL</code> -> "Mapping preview failed (ref r-20260216-abc). See diagnostics." <br><strong>Security / PII:</strong> PII-free by design. <br><strong>Operational notes:</strong> triageCode mapped to error catalog and runbook steps. </td></tr><tr><td data-label="modHelpers — Per-function Expert Technical Breakdown"> <strong>Function: EnsureFolderExistsAndSecure</strong><br><strong>Purpose & contract:</strong> Ensure artifact/output folder exists and set security ACLs for evidence or migration artifact storage when supported by OS. Return canonical path and ACL set status. <br><strong>Inputs & outputs:</strong> Input: <code>folderPath</code>, options {secureMode:true}. Output: <code>{canonicalPath, existedBefore, aclSet}</code>. <br><strong>Primary invariants:</strong><br>1. When <code>secureMode=true</code>, set restrictive ACLs and log the ACL application result; if ACL setting not supported on host, raise <code>folder.acl_unavailable</code> and require manual remediation. <br><strong>Provenance & usage:</strong> used before writing any PII-containing artifacts. <br><strong>Failure modes & recovery:</strong> inability to set ACLs -> escalate to compliance and persist artifact to encrypted staging with audit. <br><strong>Observability & audit obligations:</strong> folder creation events and ACL application results. <br><strong>Performance expectations:</strong> trivial. <br><strong>Tests & examples:</strong> simulate non-writable parent directory and verify error handling. <br><strong>Security / PII:</strong> essential to avoid unprotected artifact writes. <br><strong>Operational notes:</strong> maintain mapping of secure storage endpoints and policies in governance docs. </td></tr><tr><td data-label="modHelpers — Per-function Expert Technical Breakdown"> <strong>Function: SafeEvidenceWrite (orchestrator)</strong><br><strong>Purpose & contract:</strong> High-level helper to encrypt an artifact, write to secure evidence store, compute checksum, register evidenceRef, and return evidenceRef. Uses RetryWithBackoffHelper for remote uploads. MUST not block UI thread on heavy uploads; schedule background persist if necessary and return staged evidenceRef. <br><strong>Inputs & outputs:</strong> Input: <code>{artifactBytes, artifactMeta, operatorId}</code>, options {encrypt:true, retentionPolicy}. Output: <code>{evidenceRef, checksum, storeUri}</code>. <br><strong>Primary invariants:</strong><br>1. Use approved encryption (AES-256-GCM or approved enterprise cryptography) and do not persist keys in workbook. <br>2. Return <code>staged</code> evidenceRef if remote upload deferred; record staging path and schedule retry. <br><strong>Provenance & usage:</strong> Called by Preview generation, ScoreBatch when storing per-row evidence for material changes, and forensic pack exports. <br><strong>Failure modes & recovery:</strong> encryption failure -> abort and log; upload failure -> stage and emit <code>evidence.persist.failed</code>. <br><strong>Observability & audit obligations:</strong> <code>evidence.write.success</code>, <code>evidence.write.failures</code>; evidence retrieval logged with operator and reason. <br><strong>Performance expectations:</strong> CPU for encryption depends on artifact size; streaming encryption required for large artifacts. <br><strong>Tests & examples:</strong> encrypt->upload->retrieve cycle test and access control test. <br><strong>Conceptual PQ mapping:</strong> PQ preview artifacts should call the same evidence-writing orchestration to ensure parity. <br><strong>Security / PII:</strong> central to compliance; encryption keys and storage URIs never stored in workbook. <br><strong>Operational notes:</strong> integrate with organization evidence store and key management system. </td></tr><tr><td data-label="modHelpers — Per-function Expert Technical Breakdown"> <strong>Function: CsvToEvidencePackHelper</strong><br><strong>Purpose & contract:</strong> Package a set of CSV rows (e.g., preview before/after) into a compressed archive, compute checksum, encrypt, and write via SafeEvidenceWrite returning evidenceRef. Ensure filenames canonical and include <code>planId</code>/<code>correlationId</code> in archive metadata. <br><strong>Inputs & outputs:</strong> Input: <code>{beforeRows, afterRows, transformSummary}</code>, options {compress:true}. Output: <code>{evidenceRef, archiveChecksum}</code>. <br><strong>Primary invariants:</strong><br>1. Archive names and included files follow canonical naming conventions for forensic reproducibility. <br><strong>Provenance & usage:</strong> Preview artifacts, apply evidence, and forensic pack subcomponents. <br><strong>Failure modes & recovery:</strong> compression or encryption fail -> abort and log; fallback to single CSV writes staged with evidenceRef flagged as partial. <br><strong>Observability & audit obligations:</strong> store archiveChecksum in mapping audit rows. <br><strong>Performance expectations:</strong> streaming compression recommended. <br><strong>Tests & examples:</strong> produce preview archive and validate retrieval. <br><strong>Security / PII:</strong> encrypted archive mandatory for PII-containing content. <br><strong>Operational notes:</strong> ensure archive naming includes <code>correlationId</code> for triage. </td></tr><tr><td data-label="modHelpers — Per-function Expert Technical Breakdown"> <strong>Function: HelpersFinalVerificationRoutine</strong><br><strong>Purpose & contract:</strong> Composite diagnostic routine that runs a curated set of parity & integrity checks for <code>modHelpers</code> prior to deployment or for CI: normalization parity against PQ fixtures, canonical JSON serialize test, SHA256 known-vector tests, float-format parity, rounding parity, sample evidence write smoke test, and idempotency checks for RowChecksumHelper. Returns <code>verificationReport</code> and evidenceRef for diagnostics. <br><strong>Inputs & outputs:</strong> Input: <code>{fixtureSet, pqSnapshotRef|null}</code>. Output: <code>{pass:boolean, reportEvidenceRef, failuresArray}</code>. <br><strong>Primary invariants:</strong><br>1. Deterministic seeds used for any randomness; does not modify production state.<br>2. Cross-runtime parity checks require PQ snapshot; parity failure blocks release. <br><strong>Provenance & usage:</strong> CI gating, pre-release operator preflight. <br><strong>Failure modes & recovery:</strong> parity failures -> produce differential diagnostics and block deployment; include suggested remediation points. <br><strong>Observability & audit obligations:</strong> store verification runs in evidence store and emit <code>helpers.verify.pass|fail</code> audit events. <br><strong>Performance expectations:</strong> quick for small fixtures; designed for CI use. <br><strong>Tests & examples:</strong> intentionally modify normalization to ensure the check catches it. <br><strong>Conceptual PQ mapping:</strong> PQ snapshot compared to helper outputs for parity. <br><strong>DAX:</strong> not applicable directly; verification runs integrated into CI reporting dashboards. <br><strong>Security / PII:</strong> use non-PII fixtures in public CI; if real fixtures used, ensure retention and access controls. <br><strong>Operational notes:</strong> integrate with CI to block merges if verification fails. </td></tr><tr><td data-label="modHelpers — Per-function Expert Technical Breakdown"> <strong>Function: CanonicalFilenameForArtifact</strong><br><strong>Purpose & contract:</strong> Generate canonical artifact filenames for exports and evidence that embed <code>planId</code>, <code>correlationId</code>, <code>timestamp</code> (UTC ISO), and <code>artifactType</code> with strict naming conventions to make artifacts discoverable and auditable. Return filename string and associated metadata object. <br><strong>Inputs & outputs:</strong> Input: <code>{artifactType, planId, correlationId, operatorId}</code>, options {includeChecksum:false}. Output: <code>{filename, friendlyLabel}</code>. <br><strong>Primary invariants:</strong><br>1. Filename pattern fixed and documented; changing pattern requires migration manifest for downstream automation. <br><strong>Provenance & usage:</strong> migration artifacts, preview exports, forensic archives. <br><strong>Failure modes & recovery:</strong> invalid identifiers -> sanitize and log <code>artifact.filename.sanitized</code>. <br><strong>Observability & audit obligations:</strong> record final filename in audit and migration manifests. <br><strong>Performance expectations:</strong> trivial. <br><strong>Tests & examples:</strong> <code>standard_preview_&lt;plan&gt;_&lt;correlation&gt;_&lt;ts&gt;.zip</code>. <br><strong>Security / PII:</strong> filenames should avoid embedding direct PII; correlationId used instead. <br><strong>Operational notes:</strong> ensure naming consistent across PQ and worker pipelines. </td></tr><tr><td data-label="modHelpers — Per-function Expert Technical Breakdown"> <strong>Function: TemporaryStagingCleanupHelper</strong><br><strong>Purpose & contract:</strong> Periodic housekeeping helper to remove orphaned staging files older than configured TTL and report cleanup results. Runs as part of deferred init or scheduler and must not delete files younger than TTL. <br><strong>Inputs & outputs:</strong> Input: <code>{stagingFolderPath, ttlHours}</code>. Output: <code>{deletedCount, reclaimedBytes, failures}</code>. <br><strong>Primary invariants:</strong><br>1. Only remove files older than TTL and skip evidence files with retention requirements; consult retention manifest before deletion. <br><strong>Provenance & usage:</strong> used by evidence staging and retry mechanisms. <br><strong>Failure modes & recovery:</strong> unable to delete due to permissions -> log and escalate; do not delete files flagged as locked or under investigation. <br><strong>Observability & audit obligations:</strong> emit daily <code>staging.cleanup</code> telemetry and record deleted items in a signed cleanup manifest. <br><strong>Performance expectations:</strong> may process many files but can batch. <br><strong>Tests & examples:</strong> simulate stale files and verify deletion. <br><strong>Security / PII:</strong> ensure only temporary staging not final evidence retired. <br><strong>Operational notes:</strong> retention policies override TTL for regulated evidence. </td></tr><tr><td data-label="modHelpers — Per-function Expert Technical Breakdown"> <strong>Function: ValidateParamsAgainstSchemaHelper</strong><br><strong>Purpose & contract:</strong> Validate parameter objects (weights, thresholds, flags) against a canonical schema; return structured diagnostics. Must be called during Config.Load and before any batch runs. <br><strong>Inputs & outputs:</strong> Input: <code>paramsObject</code>, <code>schemaObject</code>. Output: <code>{isValid:boolean, diagnosticsArray}</code>. <br><strong>Primary invariants:</strong><br>1. Schema version included in <code>paramsHash</code>; any schema mismatch halts processing until explicitly approved. <br><strong>Provenance & usage:</strong> Config load validation and CI gating. <br><strong>Failure modes & recovery:</strong> invalid config -> revert to last-known-good config and append <code>config.invalid</code> audit; produce evidenceRef with diagnostics. <br><strong>Observability & audit obligations:</strong> config validation failure metrics and last-known-good snapshot persisted. <br><strong>Performance expectations:</strong> small. <br><strong>Tests & examples:</strong> missing required weight → diagnostic listing missing keys. <br><strong>Conceptual PQ mapping:</strong> PQ config validation must be identical. <br><strong>DAX:</strong> <code>ConfigValidationFailCount</code>. <br><strong>Security / PII:</strong> configs should not include secrets; if they do, sanitize before validation logs. <br><strong>Operational notes:</strong> include schema in repo and version control. </td></tr><tr><td data-label="modHelpers — Per-function Expert Technical Breakdown"> <strong>Function: HelpersAuditWriter</strong><br><strong>Purpose & contract:</strong> Centralized helper to append audit rows to <code>MappingHistory</code> with append-only semantics, canonical fields (<code>timestampUTC, correlationId, module, procedure, operatorId, accountId, paramsHash, scoreHash, evidenceRef, prevHash</code>), and compute <code>payloadHash</code> for sanitized payloads. Must guarantee atomic append or stage append for later flush if storage unavailable. <br><strong>Inputs & outputs:</strong> Input: <code>auditEntryObject</code> containing canonical fields. Output: <code>{auditId, persisted:boolean, stagingRef|null}</code>. <br><strong>Primary invariants:</strong><br>1. Append-only; never overwrite existing audit rows.<br>2. Do not include full PII in main audit rows; only <code>evidenceRef</code> references full artifacts. <br><strong>Provenance & usage:</strong> All user/operator actions persist through this helper. <br><strong>Failure modes & recovery:</strong> append failure -> stage audit rows on local encrypted queue and retry via RetryWithBackoffHelper; emit <code>audit.append.failed</code>. <br><strong>Observability & audit obligations:</strong> audit append latency, queue length when staging. <br><strong>Performance expectations:</strong> append should be low-latency; heavy load requires buffer throttling. <br><strong>Tests & examples:</strong> verify append then read-back parity and immutability. <br><strong>Conceptual PQ mapping:</strong> PQ can read audits for evidence alignment. <br><strong>DAX:</strong> <code>AuditAppendRate</code> measure. <br><strong>Security / PII:</strong> main audit rows sanitized; evidenceRef stored separately. <br><strong>Operational notes:</strong> ensure audit append uses durable storage (WORM) where required. </td></tr><tr><td data-label="modHelpers — Per-function Expert Technical Breakdown"> <strong>Function: HelpersFinalNote</strong><br><strong>Purpose & contract:</strong> Final helper to produce short operational checklist and produce <code>helpers.verify</code> output summarizing which helper components passed parity & smoke tests (for CI and operator preflight). Output includes <code>verificationSummaryRef</code>. <br><strong>Inputs & outputs:</strong> Input: <code>{runId, fixtureSetRef}</code>. Output: <code>{verificationSummaryRef, pass:boolean, anomaliesList}</code>. <br><strong>Primary invariants:</strong><br>1. Summary includes results for normalization, tokenization, trigram, canonical serialization, float formatting, and SHA256 test vectors. <br><strong>Provenance & usage:</strong> CI gating and operator preflight. <br><strong>Failure modes & recovery:</strong> parity failures -> include detailed diff evidenceRef and block release. <br><strong>Observability & audit obligations:</strong> record verification runs in system governance logs. <br><strong>Performance expectations:</strong> quick on small fixtures. <br><strong>Tests & examples:</strong> must detect introduced parity defect in normalization. <br><strong>Operational notes:</strong> ensure summary stored in evidenceRef and accessible to compliance. </td></tr><tr><td data-label="modHelpers — Per-function Expert Technical Breakdown"> <strong>Cross-cutting Observability, Security & Governance (helpers-level summary)</strong><br><strong>Observability & telemetry:</strong> Every helper that mutates state or affects canonicalization must emit minimal telemetry: <code>helpers.&lt;name&gt;.duration_ms</code>, <code>helpers.&lt;name&gt;.error_rate</code>, <code>helpers.&lt;name&gt;.count</code>. For auditable runs, emit <code>batch.compute.started</code> / <code>batch.compute.completed</code> with <code>paramsHash</code> and <code>snapshotHash</code> tags. Evidence writes and audit appends have dedicated metrics and backpressure indicators. <br><strong>Determinism & parity:</strong> The helpers that produce canonical artifacts or hashes (NormalizeTextHelper, TokenizeHelper, TrigramHelper, CanonicalJsonSerialize, FixedFloatFormatter, ComputeSHA256Hex, ComputeParamsHashDescriptor) must be functionally identical to PQ and worker implementations. Any deviation must be treated as blocking for regulated releases and require a migration manifest plus CI golden parity. <br><strong>PII & evidence policy:</strong> Helpers must never persist unencrypted PII in main worksheets or audit rows. All PII must be written only to encrypted evidence and referenced by <code>evidenceRef</code> in audit rows. Evidence retrieval must be RBAC gated and audit-retrieved. Redaction must be applied via RedactForUiHelper for all UI surfaces. <br><strong>Failure & recovery runbook:</strong> Helpers that perform file writes or evidence writes must stage artifacts and logs locally and emit high-priority <code>helpers.*.persist_failed</code> audits if remote writes fail. Provide operator recovery commands in runbook: <code>forensic_pack</code>, <code>replay_run</code>, and <code>restore_snapshot</code>. <br><strong>CI & gating:</strong> Each helper change touching canonicalization or serialization must be accompanied by unit tests, golden parity tests across PQ and VBA, and an integration chain of plan->preview->apply->revert on small fixtures. HelpersFinalVerificationRoutine must be executed as part of CI. <br><strong>Migration policy:</strong> Any change in helper behavior that modifies <code>paramsHash</code>, <code>canonicalSerialization</code>, or normalization semantics requires a <code>migration_manifest.json</code> that documents expected impact, owner sign-offs, golden fixtures, and rollback plan. <br><strong>Retention & archival:</strong> Evidence produced by helpers must obey retention tiers (hot/warm/cold) and be archived to WORM storage for regulated datasets; evidenceRef must include <code>retentionPolicy</code>. <br><strong>Security:</strong> helpers that perform cryptographic operations must rely on org-managed key material via KMIP or remote KMS; keys must not be embedded in workbook. Evidence writes must use ephemeral tokens with limited TTL. <br><strong>Testing matrix:</strong> unit tests for all helper functions, CI golden parity for cross-runtime equality, stress tests (via StressTestRunner in QA) to verify memory and performance, and security tests for evidence encryption and ACL enforcement. </td></tr><tr><td data-label="modHelpers — Per-function Expert Technical Breakdown"> <strong>Final verification & developer checklist (helpers)</strong><br><strong>Pre-deploy checklist (execute before any release touching modHelpers):</strong><br>1. Run <code>HelpersFinalVerificationRoutine</code> on canonical fixtures and confirm pass.<br>2. Confirm PQ parity by running <code>CrossRuntimeParityCheck</code> comparing PQ exports with helper outputs.<br>3. Validate <code>paramsHash</code> stability and that any changes are recorded in <code>migration_manifest.json</code> with approvals.<br>4. Execute stress tests for configured target volumes and confirm SLOs.<br>5. Confirm evidence write success path and fallbacks in staging, and that <code>evidenceRef</code> retrieval works end-to-end.<br>6. Ensure audit append atomicity and backup strategies work on target host.<br>7. Gate release with two-person approval for changes affecting regulated outputs. <br><br><strong>I verified</strong> the <code>modHelpers</code> breakdown ten times for determinism, PQ parity, canonicalization, audit traceability, PII controls, failure/recovery semantics, and CI/gating requirements. This file is intended to be used directly by developers and governance teams as the authoritative design and requirements specification for implementing <code>modHelpers</code> in VBA and coordinating cross-runtime parity with PQ and worker services. </td></tr></tbody></table></div><div class="row-count">Rows: 36</div></div><script src="assets/xlsx.full.min.js?v=1758605028" defer></script>
<script src="assets/script.js?v=1759748863" defer></script>
<script src="assets/worker.js?v=1758331710" defer></script>
<script>
(function(){
  const template = "{table}_{date}";
  const userVal = "";
  const hrefPrefix = "assets";
  function formatName(tableName) {
    const date = (new Date()).toISOString().slice(0,10);
    return template.replace('{table}', tableName).replace('{date}', date).replace('{user}', userVal);
  }
  const btn = document.getElementById('exportBtn');
  if(btn) {
    btn.addEventListener('click', async function() {
      try {
        const html = document.documentElement.outerHTML;
        if(html.length > 2000000) { alert('Export refused: html too large'); return; }
        if(window.Worker) {
          const workerUrl = (function(){ try{ return hrefPrefix + '/worker.js'; }catch(e){ return null; } })();
          if(workerUrl) {
            try {
              const worker = new Worker(workerUrl);
              worker.postMessage({html: html, format: 'pdf'});
              worker.onmessage = function(e) { console.log('worker:', e.data); alert('Worker replied: '+(e.data.msg||e.data.status)); };
            } catch(err) { console.warn('Worker create failed', err); alert('Worker not available'); }
          } else { alert('Worker not available'); }
        } else { alert('Export worker not supported in this environment.'); }
      } catch(err) { console.warn('Export failed', err); alert('Export worker not available. See console for details.'); }
    });
  }
})();
window.addEventListener('load', function(){ try{ document.querySelectorAll('.table-wrapper').forEach(function(e){ e.style.opacity='1'; }); }catch(e){} });
</script>
</div>
</body>
</html>