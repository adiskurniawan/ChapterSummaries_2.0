<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='utf-8'><meta name='viewport' content='width=device-width,initial-scale=1'>
<title>Tables Viewer v2.1</title>
<style>:root{--main-header-height:56px;} .table-wrapper{opacity:0;min-height:24px;transition:opacity .12s linear}</style>
<link rel="stylesheet" href="assets/style.css?v=1759925496">
<link rel="stylesheet" href="assets/overrides.css?v=1766911164">
</head><body>
<div id="tables-viewer" role="region" aria-label="Tables viewer">
<div id="stickyMainHeader">
<div id="tv-header">
<div><h1>Tables Viewer v2.1</h1></div>
<div style="display:flex;gap:8px;align-items:center;">
<input id="searchBox" class="tv-search" type="search" placeholder="Search" aria-label="Search tables" />
<button id="modeBtn" type="button" onclick="toggleMode()" aria-label="Toggle theme">Theme</button>
<button id="toggleAllBtn" type="button" aria-label="Toggle all" onclick="toggleAllTables()">Collapse All Tables</button>
<button id="copyAllPlainBtn" class="copy-btn" type="button" onclick="copyAllTablesPlain()" aria-label="Copy all tables as plain text">Copy All Tables (Plain Text)</button>
<button id="copyAllTablesBtn" class="copy-all-btn" type="button" onclick="copyAllTablesMarkdown()" aria-label="Copy All Tables (Markdown)">Copy All Tables (Markdown)</button>
<button id="copyAllMdBtn" style="display:none" aria-hidden="true"></button>
<button id="resetAllBtn" type="button" onclick="resetAllTables()" aria-label="Reset All Tables">Reset All Tables</button>
</div></div>
<script>
(function(){
  function ensureDelegation(){
    try {
      var vis = document.getElementById('copyAllTablesBtn');
      var alias = document.getElementById('copyAllMdBtn');
      if(!vis || !alias) return;
      alias.addEventListener = function(type, listener, options){
        vis.addEventListener(type, listener, options);
      };
      alias.removeEventListener = function(type, listener, options){
        vis.removeEventListener(type, listener, options);
      };
      Object.defineProperty(alias, 'onclick', {
        set: function(fn){ vis.onclick = fn; },
        get: function(){ return vis.onclick; },
        configurable: true
      });
      alias.focus = function(){ vis.focus(); };
      alias.blur = function(){ vis.blur(); };
    } catch(e) {
      try{ console && console.warn && console.warn('alias delegation failed', e); }catch(_){} 
    }
  }
  if(document.readyState === 'loading'){
    document.addEventListener('DOMContentLoaded', ensureDelegation);
  } else {
    ensureDelegation();
  }
})();
</script>
<noscript><div style='color:#b91c1c'>JavaScript is disabled. Tables will be shown statically. For large tables enable JS for virtualization.</div></noscript>
<div id="tocBar" role="navigation" aria-label="Table of contents"><ul><li class="toc-item"><a class="toc-link" href="#Table1">Table 1</a></li>
<li class="toc-item"><a class="toc-link" href="#Table2">Table 2</a></li>
<li class="toc-item"><a class="toc-link" href="#Table3">Table 3</a></li>
<li class="toc-item"><a class="toc-link" href="#Table4">Table 4</a></li>
<li class="toc-item"><a class="toc-link" href="#Table5">Table 5</a></li>
<li class="toc-item"><a class="toc-link" href="#Table6">Table 6</a></li>
<li class="toc-item"><a class="toc-link" href="#Table7">Table 7</a></li>
<li class="toc-item"><a class="toc-link" href="#Table8">Table 8</a></li>
<li class="toc-item"><a class="toc-link" href="#Table9">Table 9</a></li>
<li class="toc-item"><a class="toc-link" href="#Table10">Table 10</a></li>
<li class="toc-item"><a class="toc-link" href="#Table11">Table 11</a></li>
<li class="toc-item"><a class="toc-link" href="#Table12">Table 12</a></li></ul></div></div>
<div class="table-caption" id="Table1" data-table="Docu_0163_01" style="margin-top:2mm;margin-left:3mm;"><strong>Table 1</strong></div>
<div class="table-wrapper" data-table-id="table-1"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown — src/rules/progressive.py"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown — src/rules/progressive.py</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — src/rules/progressive.py"> <strong>File-level responsibilities</strong><br><br>This module encapsulates the "progressive" rules engine used to compute progressive tax-like calculations (PPh21 progressive step in your pipeline). It is the focused, pure-calculation surface of the ruleset: accept canonical, validated input (taxable income / income stream records and rule metadata), apply deterministic bracket logic, produce numeric results plus a machine-consumable decision trace. Keep this module pure (no DB, no network). All I/O — reading rule files, writing audit entries, queuing jobs — must happen outside of this module. Expose a small surface of well-documented, composable functions for upstream workers and the simulator endpoint. Ensure idempotence, deterministic rounding, and stable ordering for all aggregate and trace outputs. </td></tr><tr><td data-label="Technical Breakdown — src/rules/progressive.py"> <strong><code>compute_progressive_tax(taxable_amount: Decimal, brackets: Sequence[Bracket], *, rounding: RoundingOptions) -&gt; Decimal</code></strong><br><br><strong>Responsibility:</strong> The canonical public pure function that calculates the final tax liability for a single taxable base using a list of ascending brackets. It must be side-effect free, thread-safe, and deterministic across runs. <br><br><strong>Inputs:</strong> a Decimal <code>taxable_amount</code>, <code>brackets</code> where each <code>Bracket</code> is a frozen/immutable tuple/dict <code>{upper_bound: Decimal|None, rate: Decimal, label: str?, deductible: Decimal?}</code>, and <code>rounding</code> options specifying currency precision and rounding mode. <br><br><strong>Outputs:</strong> a Decimal representing the tax due. <br><br><strong>Behavioral notes & edge-cases:</strong> treat <code>upper_bound=None</code> as infinity; handle zero and negative taxable amounts (return zero for <=0); guard against malformed brackets (non-ascending upper_bound) by raising <code>RulesError</code> before any numeric ops. Use fixed-point arithmetic (<code>Decimal</code>) only; never float. Implement rounding only once at the final stage unless business rules require per-bracket rounding (configurable). <br><br><strong>Performance & scaling:</strong> O(n_brackets). Bracket list is small; fine to iterate. <br><br><strong>Tests:</strong> golden cases (single bracket, multi-bracket spanning multiple thresholds), boundary conditions at bracket edges, negative/zero inputs, very large inputs, malformed brackets causing a <code>RulesError</code>. Unit tests must assert exact Decimal equality when rounding options are fixed. </td></tr><tr><td data-label="Technical Breakdown — src/rules/progressive.py"> <strong><code>calculate_tax_breakdown(taxable_amount: Decimal, brackets: Sequence[Bracket], *, per_bracket_rounding: bool=False) -&gt; List[BracketResult]</code></strong><br><br><strong>Responsibility:</strong> Produce a breakdown suitable for CSV/manifest/trace that lists per-bracket taxable amount, marginal taxed amount, rate, and per-bracket tax. This is the structure used by <code>recorder</code>/<code>auditor</code> to write an explainable audit line. <br><br><strong>Inputs/Outputs:</strong> Inputs same as <code>compute_progressive_tax</code>. Output is an ordered list of <code>BracketResult</code> objects <code>{bracket_label, lower_bound, upper_bound, taxable_in_bracket, rate, tax_in_bracket}</code>. Also include cumulative fields (running_total_tax, running_taxable). <br><br><strong>Behavior:</strong> Must mirror <code>compute_progressive_tax</code> logic exactly. If <code>per_bracket_rounding</code> is true, round each <code>tax_in_bracket</code> according to rules; otherwise round only the final total. The function must guarantee that the sum of <code>tax_in_bracket</code> equals <code>compute_progressive_tax(...)</code> within the configured rounding tolerance. <br><br><strong>Tests:</strong> verify identity with <code>compute_progressive_tax</code> across many random inputs; boundary tests at bracket limits; cross-check sum-of-parts equals total. </td></tr><tr><td data-label="Technical Breakdown — src/rules/progressive.py"> <strong><code>apply_personal_allowances(taxable_income: Decimal, allowances: Allowances, *, rules_date: date|None) -&gt; Decimal</code></strong><br><br><strong>Responsibility:</strong> Apply per-person allowance logic (personal exemption, dependents, fixed reductions) before passing the net amount to bracket computation. This function centralizes allowance semantics so tests can vary policy without touching bracket code. <br><br><strong>Inputs:</strong> canonical <code>taxable_income</code> and an <code>Allowances</code> structure (immutable) containing named allowance amounts and rules effective dates. <br><br><strong>Outputs:</strong> net taxable amount (Decimal) not less than zero. <br><br><strong>Edge-cases & semantics:</strong> When allowances exceed gross income, return zero (never negative). Allowances may be conditional on <code>rules_date</code> (effective/expiry). If unknown allowance keys exist, ignore with a logged validation warning (no exception). <br><br><strong>Tests:</strong> combination tests with multiple allowances, time-gated allowance activation, overflow to zero, float-vs-decimal comparison. </td></tr><tr><td data-label="Technical Breakdown — src/rules/progressive.py"> <strong><code>get_effective_brackets(rules: Mapping, *, as_of: date|None) -&gt; Sequence[Bracket]</code></strong><br><br><strong>Responsibility:</strong> Deterministically extract and normalize bracket definitions from the raw <code>rules</code> structure (loaded by an external loader). Convert different storage shapes (legacy arrays, versioned dicts) into canonical <code>Bracket</code> objects sorted by <code>lower_bound</code> ascending. Must not read files. <br><br><strong>Behavior:</strong> Validate monotonicity (no overlap, no gap constraints only when required), normalize percent values expressed as <code>5</code> vs <code>0.05</code>, and normalize currency units (thousands vs units) if rule metadata indicates a scale. Throw <code>RulesValidationError</code> for irrecoverable formatting problems. Return annotated brackets (include <code>source_rule_id</code>, <code>effective_date</code>). <br><br><strong>Tests:</strong> malformed rules detection, percent normalization, scale normalization, stable sorting under shuffled inputs. </td></tr><tr><td data-label="Technical Breakdown — src/rules/progressive.py"> <strong><code>simulate_progressive_run(record: CanonicalRecord, rules: RulesBundle, *, debug: bool=False) -&gt; SimulationResult</code></strong><br><br><strong>Responsibility:</strong> High-level function used by the <code>/simulate</code> endpoint and unit tests. Orchestrates: validate input with <code>validate_progressive_inputs</code>, compute allowances, derive effective brackets, compute breakdown and total tax, and return a <code>SimulationResult</code> structure that includes inputs, outputs, and a full decision trace. Must remain CPU-bound and pure (no external side effects). <br><br><strong>Outputs:</strong> <code>SimulationResult</code> containing <code>net_taxable</code>, <code>total_tax</code>, <code>breakdown</code>, <code>applied_rules_meta</code>, <code>warnings</code>, and a <code>trace</code> (see <code>build_decision_trace</code>). <br><br><strong>Observability:</strong> When <code>debug</code> is true include intermediate numeric values with high precision (unrounded) to aid debugging; avoid leaking secrets in trace. <br><br><strong>Tests:</strong> golden fixtures from real cases, snapshots of <code>SimulationResult</code>, diff tests to assert trace stability across code changes. </td></tr><tr><td data-label="Technical Breakdown — src/rules/progressive.py"> <strong><code>validate_progressive_inputs(record: Mapping) -&gt; ValidationResult</code></strong><br><br><strong>Responsibility:</strong> Lightweight validator for a single canonical record: ensure required numeric fields exist and are <code>Decimal</code>-coercible, detect suspicious values (extremely large incomes), ensure required metadata (tax year, tax residency flag, applied_rule_version) exist. Return an object with <code>is_valid</code>, <code>errors[]</code>, <code>warnings[]</code>, and canonicalized typed values if valid. Do not raise for non-fatal issues; raise only for structural failures that make computation impossible. <br><br><strong>Tests:</strong> missing-field behavior, coercion of strings and ints to Decimal, detection of outliers, error messages stable and machine-parseable. </td></tr><tr><td data-label="Technical Breakdown — src/rules/progressive.py"> <strong><code>explain_progressive_decision(result: SimulationResult, *, format: Literal[&#x27;text&#x27;,&#x27;structured&#x27;]=&#x27;structured&#x27;) -&gt; Union[str, Dict]</code></strong><br><br><strong>Responsibility:</strong> Create human- and machine-friendly explanations from a <code>SimulationResult</code>. When <code>format==&#x27;text&#x27;</code> produce a concise, reviewer-ready plain-text summary (no PII). When <code>&#x27;structured&#x27;</code> produce a machine-readable map with fields: <code>applied_rules</code>, <code>inputs_summary</code>, <code>breakdown_summary</code>, <code>rationale</code>, and <code>audit_hint</code>. Must not contain raw identifiers or secrets; redact identifiers per the project's redaction rules. <br><br><strong>Use-cases:</strong> used by CLI <code>--explain</code>, admin UI, and audit records. <br><br><strong>Tests:</strong> snapshot tests for both formats and redaction tests that assert PII is absent. </td></tr><tr><td data-label="Technical Breakdown — src/rules/progressive.py"> <strong><code>build_decision_trace(steps: Sequence[Step], meta: Mapping) -&gt; DecisionTrace</code></strong><br><br><strong>Responsibility:</strong> Assemble a stable, compact trace structure representing each logical step of the progressive calculation (validation → allowances → bracketization → per-bracket tax → totals). Each <code>Step</code> must include <code>name</code>, <code>inputs</code> (small), <code>outputs</code> (small), <code>duration_ms</code> (optional), and <code>status</code>. The trace <strong>must</strong> be JSON-serializable and size-bounded (configurable, default 16KB) to avoid blowing up audit storage. If trace size exceeds limit, collapse low-value steps with a summary and set <code>trace_truncated=true</code>. <br><br><strong>Operational notes:</strong> include <code>applied_rule_version</code> and <code>runtime_fingerprint</code> in meta. This function is the single source of truth for trace shape; other modules (recorder) depend on it. <br><br><strong>Tests:</strong> size limit enforcement, deterministic ordering, truncation path correctness. </td></tr><tr><td data-label="Technical Breakdown — src/rules/progressive.py"> <strong><code>serialize_trace_for_audit(trace: DecisionTrace, *, redact: Callable[[str],str]) -&gt; Dict</code></strong><br><br><strong>Responsibility:</strong> Produce the JSON-compatible dict that will be persisted to append-only audit logs. Enforce redaction policy (use caller-supplied <code>redact</code> function), remove binary blobs, convert Decimal → string with fixed precision, and annotate with <code>schema_version</code>. Do not perform I/O here. <br><br><strong>Tests:</strong> redaction correctness, Decimal serialization tests, schema conformance checks. </td></tr><tr><td data-label="Technical Breakdown — src/rules/progressive.py"> <strong><code>hydrate_brackets_from_config(raw_brackets: Sequence[Mapping], *, defaults: Mapping = None) -&gt; Sequence[Bracket]</code></strong><br><br><strong>Responsibility:</strong> Turn heterogeneous config fragments into validated <code>Bracket</code> objects. Accepts multiple shape variants (tiny DSL: <code>{upto: &#x27;50m&#x27;, rate: &#x27;5%&#x27;}</code> or <code>{upper: 50000000, rate: 0.05}</code>). Normalize numeric units, percent strings, and optional <code>deductible</code> fields. Supply defaults from <code>defaults</code> where fields missing. Ensure the returned sequence uses <code>lower_bound</code> + <code>upper_bound</code> consistent semantics. <br><br><strong>Edge-cases:</strong> preserve original <code>source_line</code> for diagnostics in returned brackets. Fail fast on ambiguous units (e.g., both <code>k</code> and <code>m</code> present) unless <code>defaults</code> resolve. <br><br><strong>Tests:</strong> string percent parsing, unit suffix parsing, roundtrip sanity. </td></tr><tr><td data-label="Technical Breakdown — src/rules/progressive.py"> <strong><code>_round_currency(value: Decimal, precision: int, mode: str=&#x27;HALF_UP&#x27;) -&gt; Decimal</code></strong><br><br><strong>Responsibility:</strong> Private deterministic rounding helper used across the module. Always use <code>Decimal.quantize</code> with an explicit context. Centralizing rounding prevents subtle differences between functions. <br><br><strong>Tests:</strong> assert behavior for .5 boundaries, negatives, and large scales; cross-check with configured rounding modes. </td></tr><tr><td data-label="Technical Breakdown — src/rules/progressive.py"> <strong><code>_safe_divide(numerator: Decimal, denominator: Decimal, *, default: Decimal=Decimal(&#x27;0&#x27;)) -&gt; Decimal</code></strong><br><br><strong>Responsibility:</strong> Private helper to avoid <code>DivisionByZero</code> in marginal computations; return <code>default</code> when denominator is zero and emit a <code>DivisionWarning</code> in logs (caller handles logging). <br><br><strong>Tests:</strong> division by zero, normal division, large/very small denominators. </td></tr><tr><td data-label="Technical Breakdown — src/rules/progressive.py"> <strong><code>load_progressive_rules_from_file(path: str) -&gt; RulesBundle</code></strong><br><br><strong>Responsibility:</strong> Convenience loader used by integration tests and demo CLI — parse YAML/JSON rule bundles and return <code>RulesBundle</code> but note: <strong>this function is allowed only to parse local files</strong> and should be small; file reading side-effects are limited to this function alone. Keep it deterministic: validate file signatures/hashes if provided, and annotate <code>RulesBundle</code> with <code>source_path</code> and <code>file_hash</code>. <br><br><strong>Security:</strong> Never reveal file contents accidentally in traces; only expose <code>applied_rule_version</code> and <code>source_path</code> (redacted in audit). <br><br><strong>Tests:</strong> file parsing success, invalid schema handling, hash calculation. </td></tr><tr><td data-label="Technical Breakdown — src/rules/progressive.py"> <strong><code>public_api_run(record: Mapping, rules_loader: Callable[...,RulesBundle], audit_serializer: Callable, *, settings: Settings) -&gt; RunResult</code></strong><br><br><strong>Responsibility:</strong> High-level adapter used by workers: orchestrates validation, rule-loading (via injected <code>rules_loader</code>), the simulation flow, and finally returns <code>RunResult</code>. This function may call <code>serialize_trace_for_audit</code> and pass the serialized trace onward to <code>audit_serializer</code> (but must not perform direct I/O itself; <code>audit_serializer</code> is injected to do the side-effect). Keep this function thin and well-instrumented. <br><br><strong>Idempotency & retries:</strong> ensure the function is idempotent for the same <code>record</code> and <code>applied_rule_version</code> by computing a content-hash of the core inputs and returning a <code>RunResult</code> with an <code>idempotency_key</code>. Worker-layer must use this key. <br><br><strong>Tests:</strong> integration-style tests with fake <code>rules_loader</code> and <code>audit_serializer</code> to assert correct ordering and side-effect calls. </td></tr><tr><td data-label="Technical Breakdown — src/rules/progressive.py"> <strong><code>validate_bracket_consistency(brackets: Sequence[Bracket]) -&gt; None</code></strong><br><br><strong>Responsibility:</strong> Pure validator that asserts brackets are non-overlapping, contiguous if required by policy, ascending, and that rates are in <code>[0,1]</code>. Raise <code>RulesValidationError</code> with structured <code>error_code</code> and <code>detail</code> on failure. Keep message machine-parseable for CI gating. <br><br><strong>Tests:</strong> all permutations that cause overlap, rate out-of-range, gaps when gaps are disallowed. </td></tr><tr><td data-label="Technical Breakdown — src/rules/progressive.py"> <strong><code>_format_decimal_for_display(d: Decimal, precision: int) -&gt; str</code></strong><br><br><strong>Responsibility:</strong> Formatting helper used by human-facing explanation functions. Must not be used for serializing audit decimals — audit uses <code>serialize_trace_for_audit</code>. Ensure locale-neutral formatting (no thousands separator unless explicitly requested). <br><br><strong>Tests:</strong> formatting with negative values and extreme magnitudes. </td></tr><tr><td data-label="Technical Breakdown — src/rules/progressive.py"> <strong>Implementation patterns & guardrails</strong><br><br>— <strong>Purity:</strong> Functions that compute numbers must be pure. Any I/O (file, DB, queues, telemetry) must be injected via caller-provided functions. <br>— <strong>Decimal everywhere:</strong> Use <code>Decimal</code> for all monetary arithmetic; do not mix with floats. Provide a single import alias for <code>Decimal</code> at top to make searches easy. <br>— <strong>Stable serialization:</strong> Decision traces and bracket results must be stable (field order and names) to keep audit diffs meaningful. Use <code>OrderedDict</code> or explicit serialization order where necessary. <br>— <strong>Size-bounding traces:</strong> Always enforce the trace size limit to avoid denial-of-service via huge input records. Truncate low-value fields and keep at least top-level summaries. <br>— <strong>Error types:</strong> Use domain-specific exceptions <code>RulesError</code>, <code>RulesValidationError</code>, <code>CalculationError</code> with structured payload <code>{code, message, meta}</code> to aid callers in retry-vs-fatal decisions. <br>— <strong>Logging & observability:</strong> Emit structured diagnostic events around slow inputs (threshold configurable) and rule-loading latencies. Do not log full PII; logs must reference <code>idempotency_key</code> and <code>safe_record_hash</code>. <br>— <strong>Rounding policy:</strong> Centralize rounding policy in module-level constants or <code>Settings</code> passed in. Prefer final-stage rounding unless regulation requires per-bracket rounding. Document this policy clearly at the top of the file. </td></tr><tr><td data-label="Technical Breakdown — src/rules/progressive.py"> <strong>Recommended tests & CI checks</strong><br><br>1. <strong>Unit tests (fast):</strong> each pure function including rounding, safe divide, hydration, and validators with exhaustive boundary cases. <br>2. <strong>Property tests:</strong> fuzz taxable amounts across bracket permutations and assert invariants (non-negative tax, monotonicity with income increase). <br>3. <strong>Integration tests (simulator):</strong> sample canonical fixtures that include expected <code>SimulationResult</code> JSON snapshots; assert trace stability. <br>4. <strong>Contract tests:</strong> <code>calculate_tax_breakdown</code> and <code>compute_progressive_tax</code> must be consistent; run in CI as a contract check. <br>5. <strong>Performance test:</strong> measure a hot path calculation count (e.g., 10k calculations) to ensure no pathological allocations; anchor in CI if heavy. <br><br>Automate these checks and fail builds on changes that alter serialized trace shapes or <code>applied_rule_version</code> mismatches. </td></tr><tr><td data-label="Technical Breakdown — src/rules/progressive.py"> <strong>Operational & security checklist</strong><br><br>— Ensure <code>applied_rule_version</code> is part of every <code>DecisionTrace</code> and exported in manifests. <br>— Confirm traces contain no raw PII (IDs should be hashed or redacted by <code>serialize_trace_for_audit</code>). <br>— Validate that bracket definitions loaded at runtime are signed or cryptographically hashed when used in production (verify via loader outside this module). <br>— Add metric hooks: <code>progressive.calc.latency_ms</code>, <code>progressive.trace.size_bytes</code>, and <code>progressive.calc.error_rate</code>. </td></tr><tr><td data-label="Technical Breakdown — src/rules/progressive.py"> <strong>Maintenance & developer notes</strong><br><br>— Keep the module small: if a helper grows complex (e.g., percent/unit parsing), move to <code>parsers.py</code>/<code>validators.py</code>. <br>— When adding new fields to the <code>DecisionTrace</code>, bump <code>trace_schema_version</code> and record migration notes. <br>— If regulation changes (new bracket types, temporary surtax), prefer adding a tiny, versioned transformation step in <code>get_effective_brackets</code> rather than branching core computation. <br>— Provide a <code>build_test_rules_bundle()</code> test helper in tests to supply consistent <code>RulesBundle</code> fixtures. </td></tr><tr><td data-label="Technical Breakdown — src/rules/progressive.py"> <strong>Final verification</strong><br><br>Cross-checked coverage for expected functions and edge-cases; ensured documentation of inputs/outputs, error behavior, observability hooks, and test requirements. If your existing <code>progressive.py</code> contains names different from those above, map them to the closest described function and align tests to the specified contracts. </td></tr></tbody></table></div><div class="row-count">Rows: 22</div></div><div class="table-caption" id="Table2" data-table="Docu_0163_02" style="margin-top:2mm;margin-left:3mm;"><strong>Table 2</strong></div>
<div class="table-wrapper" data-table-id="table-2"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown — src/rules/ter.py"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown — src/rules/ter.py</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — src/rules/ter.py"> <strong>File-level responsibilities</strong><br><br>This module contains the pure, deterministic tax-calculation primitives used by the pipeline's rules engine to compute TER-related outputs (tax amounts, withheld amounts, breakdown by component, and decision traces). Keep this file <em>pure</em>: no network, file I/O, database access, or global mutable state. All external data (rate tables, thresholds, policy flags) must be passed in as arguments (or provided via small factory wrappers in a separate integration module). Functions must be numeric-robust (use <code>Decimal</code>), stable across Python versions, and fully testable with property and example-based tests. Focus on clarity of contracts, explicit rounding policies, and comprehensive decision tracing for auditability. Unit tests must exercise boundary cases (zero, negative, very large incomes), bracket edges, and mixed benefit types. </td></tr><tr><td data-label="Technical Breakdown — src/rules/ter.py"> <strong><code>calculate_ter(record: Mapping[str,Any], rates: RateConfig, opts: CalculationOptions) -&gt; CalculationResult</code></strong><br><br><strong>Responsibility</strong>: Canonical, public entrypoint used by workers to compute TER results for a single canonical record. It orchestrates component-level normalization, validation, taxable income computation, bracket application, rounding, and assembling a structured audit trace and summary result.<br><br><strong>Contract & types</strong>: Input <code>record</code> is a read-only mapping (no mutation). <code>rates</code> is a small typed object (<code>RateConfig</code>) containing bracket definitions, non-cash valuation rules, thresholds, and rounding policy. <code>opts</code> contains flags (verbose tracing, simulation_mode, period pro-ration info). Returns a <code>CalculationResult</code> dataclass/dict containing: <code>taxable_income</code>, <code>tax_before_rounding</code>, <code>tax</code>, <code>detail</code> (per-component taxes), <code>decision_trace</code>, <code>applied_rate_version</code>, and <code>meta</code> (timings, fingerprints).<br><br><strong>Purity & side-effects</strong>: Pure. No logging or IO inside; observability hooks should be provided by caller if needed. Deterministic: given the same inputs returns same output. Idempotent.<br><br><strong>Error handling</strong>: Should raise well-typed <code>ValidationError</code> or <code>CalculationError</code> with machine-friendly codes. Never swallow arithmetic errors silently. Use assertions only for internal invariants; use typed exceptions for user-facing validations.<br><br><strong>Performance</strong>: Optimized for single-record low-latency execution (microsecond-to-millisecond range). Avoid large allocations; reuse ephemeral tuples/lists. Keep algorithmic complexity linear in the number of income components and brackets.<br><br><strong>Testing</strong>: Golden fixtures (small, medium, edge), property-based tests (bracket monotonicity), and round-trip tests with <code>simulate_run_for_one</code>. Include tests that assert idempotence and trace contents. </td></tr><tr><td data-label="Technical Breakdown — src/rules/ter.py"> <strong><code>compute_taxable_income(components: Sequence[IncomeComponent], deductions: Deductions, opts: CalculationOptions) -&gt; Decimal</code></strong><br><br><strong>Responsibility</strong>: Reduce individual income components (salary, allowances, bonuses, non-cash benefits) into a single taxable_income value, applying eligible deductions and exemptions in the precise order defined by tax policy. Return a <code>Decimal</code> expressed in local currency units.<br><br><strong>Contract & types</strong>: Inputs are typed sequences/dicts — no implicit side-effecting mutation. Use <code>Decimal</code> for all amounts. <code>opts</code> contains per-period flags (annualize, pro-rate days, effective_date).<br><br><strong>Business rules & ordering</strong>: Explicitly document order-of-operations: gross → pre-tax deductions → benefits valuation → statutory exemptions → progressive/flat adjustments. This function <em>encodes</em> the operator precedence (order matters for rounding and thresholds).<br><br><strong>Edge cases</strong>: Negative incomes (loss-of-year, refunds) must be handled predictably (either floor at zero or preserve sign depending on policy flag). If <code>deductions</code> exceed gross, policy must be explicit (zero taxable vs negative carry-forward).<br><br><strong>Testing</strong>: Unit tests around boundary conditions (exactly-at-threshold deductions, fractional cents), random fuzz tests, and comparative tests vs reference spreadsheets. </td></tr><tr><td data-label="Technical Breakdown — src/rules/ter.py"> <strong><code>apply_rate_brackets(taxable_income: Decimal, brackets: Sequence[Bracket]) -&gt; Tuple[Decimal, List[BracketApplication]]</code></strong><br><br><strong>Responsibility</strong>: Apply a bracketed (progressive or tiered) rate schedule to <code>taxable_income</code> and return the unrounded tax plus a per-bracket breakdown used in the decision trace.<br><br><strong>Contract</strong>: <code>brackets</code> is an ordered sequence where each <code>Bracket</code> defines <code>{lower_inclusive, upper_inclusive_or_None, rate, fixed_component_if_any}</code>. Function must <em>NOT</em> mutate <code>brackets</code> and must accept open-ended last bracket (<code>upper = None</code>).<br><br><strong>Precision & rounding</strong>: Compute intermediate amounts in <code>Decimal</code> with a high-precision local context. <strong>Do not</strong> round per-bracket unless policy requires – prefer summing unrounded bracket subtotals and apply rounding once at the final stage (document policy). If the jurisdiction requires per-bracket rounding or withholding withholding step rounding, make that behavior configurable via <code>CalculationOptions</code>.<br><br><strong>Performance</strong>: O(N) in bracket count. Avoid repeated <code>Decimal</code> context changes. Pre-validate monotonicity of bracket bounds and raise errors on overlapping or gaps if <code>opts.strict_brackets</code> is enabled.<br><br><strong>Testing</strong>: test exact boundary transitions, very large incomes, zero income, and fractional-currency tests. </td></tr><tr><td data-label="Technical Breakdown — src/rules/ter.py"> <strong><code>prorate_for_period(amount: Decimal, source_period: PeriodSpec, target_period: PeriodSpec, method: str=&#x27;days&#x27;) -&gt; Decimal</code></strong><br><br><strong>Responsibility</strong>: Convert annualized rules or amounts to the record's effective period (e.g., monthly payroll pro-ration). This util is used for annual thresholds, PTKP, and similar items that are defined on a different time base.<br><br><strong>Contract</strong>: Inputs must be explicit about <code>source_period</code> (e.g., year) and <code>target_period</code> (e.g., payroll days in month). <code>method</code> supports 'days', 'business-days', 'simple-fraction'. Document leap-year handling and timezone-agnostic day counts. Return value is <code>Decimal</code> in same currency units.<br><br><strong>Edge cases & guards</strong>: If <code>target_period</code> length is zero, raise <code>InvalidPeriodError</code>. Allow <code>cap=True</code> option to avoid prorating beyond original amount. Ensure behavior is stable for partial-month and cross-month pro-rations.<br><br><strong>Testing</strong>: tests covering leap-years, short-months, zero-day guards, and standard 30/31/28 cases. </td></tr><tr><td data-label="Technical Breakdown — src/rules/ter.py"> <strong><code>compute_non_cash_benefits_taxable_value(benefits: Sequence[Benefit], valuation_rules: BenefitRules) -&gt; Decimal</code></strong><br><br><strong>Responsibility</strong>: Convert non-cash benefits into a taxable monetary amount per policy: valuation caps, employer-declared values, statutory proxies, and imputed income rules.<br><br><strong>Contract</strong>: Must accept a list of typed <code>Benefit</code> objects (type, declared_value, measured_metric) and <code>valuation_rules</code> dict. Must be deterministic and pure. Return <code>Decimal</code> taxable value and a per-benefit breakdown suitable for auditing.<br><br><strong>Design notes</strong>: Keep valuation strategies pluggable (strategy by benefit type). Avoid performing I/O for external valuations — callers must resolve market rates and pass them in. Provide a <code>policy_fallback</code> to ensure a default valuation path when incomplete data is passed.<br><br><strong>Testing</strong>: include benefit-specific tests (car benefit with fixed cap, meal vouchers with per-day limits), and degeneracy tests when declared_value is missing. </td></tr><tr><td data-label="Technical Breakdown — src/rules/ter.py"> <strong><code>apply_withholding_rules(gross_components: Sequence[IncomeComponent], pre_tax_deductions: Deductions, withholding_policy: WithholdingPolicy, opts: CalculationOptions) -&gt; WithholdingResult</code></strong><br><br><strong>Responsibility</strong>: Implement withholding-specific logic (e.g., separate treatment for wages vs pensions, special exemptions for certain employers). Output a <code>WithholdingResult</code> containing adjusted gross, withheld_amount, and explanatory trace entries.<br><br><strong>Contract & separation of concerns</strong>: This function is <em>withholding-focused</em> and should not re-implement bracket logic; instead, it prepares adjusted taxable income for <code>apply_rate_brackets</code>. Avoid duplicating rules: if progressive rules overlap, orchestrate via <code>calculate_ter</code> only.<br><br><strong>Edge cases</strong>: Multiple simultaneous exemptions (apply most-favourable or cumulative depending on policy flag). Document precedence and provide tests simulating conflicting flags. </td></tr><tr><td data-label="Technical Breakdown — src/rules/ter.py"> <strong><code>round_tax_amount(amount: Decimal, rounding_policy: RoundingPolicy) -&gt; Decimal</code></strong><br><br><strong>Responsibility</strong>: Encapsulate rounding semantics: round-half-to-even, round-up-to-nearest, floor (truncate), or currency-specific rules. This single small function centralizes rounding to avoid divergent behaviors across the codebase.<br><br><strong>Contract</strong>: Pure function returning <code>Decimal</code>. Must be deterministic irrespective of the global <code>Decimal</code> context; pass precise quantize pattern (e.g., cents) or use explicit quantize in function.<br><br><strong>Testing</strong>: Exhaustive tests around .005 thresholds, negative values, extremely large amounts, and policy permutations. </td></tr><tr><td data-label="Technical Breakdown — src/rules/ter.py"> <strong><code>validate_inputs(record: Mapping[str,Any], schema: Optional[Schema]=None) -&gt; None</code></strong><br><br><strong>Responsibility</strong>: Lightweight, strict validation of shape and basic invariants required by the calculation functions: presence and numeric-ness of key fields, non-negative day counts, acceptable currency codes, and consistent date ranges. This function should <em>not</em> attempt deep business validation (leave that to validators.py), but enforce the low-level contract so calculation functions can assume sane inputs.<br><br><strong>Contract</strong>: Pure. Raise <code>InputValidationError</code> with a machine-friendly error code and a human-friendly message. Include <code>path</code> pointers to offending fields. Keep messages short and localizable.<br><br><strong>Testing</strong>: cover missing keys, wrong types, and boundary numeric values. </td></tr><tr><td data-label="Technical Breakdown — src/rules/ter.py"> <strong><code>build_decision_trace(steps: Sequence[TraceStep], metadata: TraceMeta) -&gt; DecisionTrace</code></strong><br><br><strong>Responsibility</strong>: Assemble a compact, structured decision trace from intermediate values produced during calculation. Each <code>TraceStep</code> should include <code>step_id</code>, <code>inputs</code>, <code>outputs</code>, <code>applied_rule_id</code> (if any), <code>rationale</code> (short), and optionally <code>evidence</code> (reference keys). The resulting <code>DecisionTrace</code> must be serializable to JSON (avoid <code>Decimal</code> -> convert to strings or provide serializer hooks).\<br><br><strong>Audit requirements</strong>: Decision trace fields must be stable and concise. Do not embed large blobs (entire record bodies) — store references (hashes) instead. Include <code>trace_id</code> and <code>runtime_fingerprint</code> in <code>metadata</code> for correlation.<br><br><strong>Privacy & security</strong>: Ensure traces redact PII by default (email, SSN, national id) unless caller explicitly requests full trace under an approval flag. Document the redaction policy and provide tests verifying no PII leaks. </td></tr><tr><td data-label="Technical Breakdown — src/rules/ter.py"> <strong><code>simulate_run_for_one(record: Mapping[str,Any], rates: RateConfig, opts: CalculationOptions) -&gt; SimulationResult</code></strong><br><br><strong>Responsibility</strong>: A testing convenience wrapper that runs <code>calculate_ter</code> with <code>opts.simulate=True</code> and returns both result and a human-friendly narrative summary designed for the <code>simulate</code> HTTP endpoint. This function should be pure and safe for interactive use in dev/test: no I/O, but allow <code>opts.verbose_trace=True</code> to return expanded trace nodes.<br><br><strong>Testing</strong>: Used heavily by integration tests and UI debug endpoints; include golden scenarios and regression artifacts. </td></tr><tr><td data-label="Technical Breakdown — src/rules/ter.py"> <strong>(Integration-only) <code>load_rate_brackets_from_store(version: str, loader: Callable) -&gt; RateConfig</code></strong><br><br><strong>Responsibility</strong>: <em>If present in module, must be very small and lazy.</em> Prefer keeping this out of the pure module; if included, it must be a tiny adapter that <em>only</em> calls the provided <code>loader</code> callable and validates the returned structure. It must not perform network retries or heavy transformations. Return type must be the same <code>RateConfig</code> consumed by calculation functions.<br><br><strong>Guideline</strong>: Prefer to implement such loaders in <code>imports.py</code> or <code>paths.py</code> and inject into <code>calculate_ter</code>. Tests should stub the <code>loader</code> callable. </td></tr><tr><td data-label="Technical Breakdown — src/rules/ter.py"> <strong>Design & implementation patterns / guardrails</strong><br><br>— <strong>Decimal-only arithmetic</strong>: Use <code>Decimal</code> everywhere for money; never use float. Set a module-level <code>DEFAULT_CONTEXT</code> for precision and use local contexts sparingly. Quantize only at the final rounding step unless explicit policy requires per-step rounding.<br>— <strong>Type annotations & dataclasses</strong>: Define compact typed objects (<code>RateConfig</code>, <code>Bracket</code>, <code>CalculationOptions</code>, <code>CalculationResult</code>) to make contracts explicit. Keep them immutable (frozen dataclasses or namedtuples).<br>— <strong>No hidden IO</strong>: All external data must be injected. If a helper must access external configuration, require a small factory function to be passed in by the caller. Avoid <code>import</code> time side-effects.<br>— <strong>Decision tracing</strong>: Every non-trivial transforming function returns both a primary value and an optional <code>TraceStep</code> that <code>calculate_ter</code> can assemble. This pattern keeps unit tests focused and makes traces composable.<br>— <strong>Configurable rounding & policy flags</strong>: Explicit <code>CalculationOptions</code> must carry rounding rules, strictness, and trace verbosity. Defaults must be safe for production (e.g., strict validation enabled).<br>— <strong>Validation boundary</strong>: <code>validate_inputs</code> enforces shape; business rule validation (e.g., duplicate exemptions) belongs in <code>validators.py</code>. Keep <code>ter.py</code> focused on numerics and traceability.<br>— <strong>Testing hygiene</strong>: Provide a <code>build_test_rates()</code> helper in tests (not in production code) that constructs representative <code>RateConfig</code>s. Include a matrix of tests that cross-check: <code>compute_taxable_income</code> + <code>apply_rate_brackets</code> == <code>calculate_ter</code> result. Add fuzz tests and property-based tests for bracket monotonicity and rounding invariants.<br>— <strong>Observability</strong>: The module itself should not log. Provide small, well-defined hook points (callable args) where callers can receive <code>TelemetryEvent</code> objects (timings, bracket counts, intermediate values). Keep telemetry payloads small.<br>— <strong>Error semantics</strong>: Distinguish <code>InputValidationError</code> (bad caller data), <code>PolicyError</code> (misconfigured rates), and <code>CalculationError</code> (unexpected arithmetic failure). Use structured error codes to allow workers to decide whether to dead-letter, retry, or escalate. Provide small helper <code>is_retriable(err)</code> for orchestration code. </td></tr><tr><td data-label="Technical Breakdown — src/rules/ter.py"> <strong>Recommended tests & CI checks</strong><br><br>1. Unit tests for each function with deterministic inputs (including negative tests).<br>2. Property tests for bracket application (monotonicity, additivity).<br>3. Integration tests that run <code>calculate_ter</code> against golden fixtures (expected amounts and traces).<br>4. Round-trip serialization tests for <code>DecisionTrace</code> (redaction).<br>5. Performance microbenchmarks to assert single-run latency budgets under typical worker hardware. </td></tr><tr><td data-label="Technical Breakdown — src/rules/ter.py"> <strong>Operational notes & developer guidance</strong><br><br>- Keep the file < 400 lines; if it grows, split helpers into <code>ter_brackets.py</code> and <code>ter_valuation.py</code>. <br>- When adding new product features (e.g., new benefit valuation), add a small <code>strategy</code> object and register via <code>CalculationOptions</code> rather than branching deeply inside core functions. <br>- Avoid expanding the public API surface: export only <code>calculate_ter</code>, <code>simulate_run_for_one</code>, and <code>round_tax_amount</code>; keep other helpers module-private unless needed elsewhere. <br>- When upgrading tax schedules, treat <code>RateConfig</code> as immutable and include a <code>version</code> field; calculation outputs must record <code>applied_rate_version</code>. </td></tr></tbody></table></div><div class="row-count">Rows: 15</div></div><div class="table-caption" id="Table3" data-table="Docu_0163_03" style="margin-top:2mm;margin-left:3mm;"><strong>Table 3</strong></div>
<div class="table-wrapper" data-table-id="table-3"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown — ptkp.py"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown — ptkp.py</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — ptkp.py"> <strong>File-level responsibilities</strong><br><br>This module encapsulates the domain logic for computing PTKP (Penghasilan Tidak Kena Pajak) used by the calculation pipeline. Its single responsibility is: given a canonical person/employment context and the current rule set, compute the non-taxable income amount and a deterministic, auditable breakdown. Keep the module pure and deterministic: functions should be side-effect free where possible, rely on injected rule tables or explicit loader helpers, and never perform I/O at import time. The file must expose a small, well-documented public surface (one or two entry points) intended for use by workers, the <code>simulate</code> endpoint, and unit tests. Cross-cutting concerns (caching, loading, validation) are delegated to collaborators (<code>constants.py</code>, <code>validators.py</code>, <code>recorder.py</code>, <code>settings.py</code>) or implemented as tiny, testable helpers here. Log only high-level decision points (not PII); emit a decision trace object suitable for <code>recorder.append()</code> so audits and replay are possible. </td></tr><tr><td data-label="Technical Breakdown — ptkp.py"> <strong><code>calculate_ptkp(person: Mapping, income_context: Mapping, rules: Optional[Mapping]=None) -&gt; Mapping</code></strong><br><br><strong>Purpose:</strong> Primary public entrypoint called by the rules engine/worker to compute PTKP for a canonical record. Returns a structured result including <code>ptkp_amount</code>, <code>components</code>, <code>applied_rule_version</code>, <code>trace</code>, and <code>errors</code> (if any non-fatal warnings).<br><br><strong>Inputs:</strong> <code>person</code> (required canonical fields: <code>marital_status</code>, <code>dependents_count</code>, <code>residency_status</code>, <code>date_of_birth</code>, optional flags), <code>income_context</code> (income period, gross/annualized amounts, payroll frequency), optional <code>rules</code> map (lookup table for PTKP values).<br><br><strong>Outputs:</strong> Serializable dict (not raw Decimal objects if system prefers JSON) containing numeric <code>ptkp_amount</code>, detailed <code>components</code> (per-subcomponent amounts), <code>metadata</code> (rule_version, effective_date), and deterministic <code>trace</code> list describing each decision step.<br><br><strong>Invariants / Guarantees:</strong> deterministic given identical inputs and <code>rules</code>; pure (no disk/network I/O). Idempotent: repeated calls with same inputs produce same result. Does not mutate inputs.<br><br><strong>Error handling & validation:</strong> calls <code>validate_person_input</code> and returns a structured error (400-equivalent domain error) for missing required fields. Non-fatal mismatches (unknown marital status mapped to <code>single</code> with a warning) are returned as warnings in <code>errors</code> and included in the <code>trace</code>.<br><br><strong>Observability:</strong> emit one high-level event (via optional logger) with <code>applied_rule_version</code>, <code>ptkp_amount</code>, and <code>trace_id</code>. Traces must be redacted for PII before logging. Unit tests should assert schema of return value and that <code>trace</code> includes entries for each rule lookup. </td></tr><tr><td data-label="Technical Breakdown — ptkp.py"> <strong><code>ptkp_amount_for_status(marital_status: str, dependents: int, rules: Mapping) -&gt; Decimal</code></strong><br><br><strong>Purpose:</strong> Pure, small helper that maps a normalized marital status and number of dependents to a PTKP numeric value using the provided <code>rules</code> table. This is the core deterministic mapping function used by <code>calculate_ptkp</code>.<br><br><strong>Inputs:</strong> <code>marital_status</code> (normalized enum: <code>single</code>, <code>married</code>, <code>married_separate</code>, <code>head_of_household</code>, etc.), <code>dependents</code> (int >= 0), <code>rules</code> (map containing base PTKP for statuses and per-dependent allowances).<br><br><strong>Outputs:</strong> numeric value (Decimal) representing PTKP. Should never return negative values — floor at zero.<br><br><strong>Edge cases:</strong> if <code>dependents</code> exceeds maximum defined in <code>rules</code>, apply capped rule or fallback to a configurable <code>rules[&#x27;max_dependent_count&#x27;]</code> and emit a trace entry. If <code>rules</code> missing keys, raise <code>RulesLookupError</code> (domain error) to be handled by the caller.<br><br><strong>Test</strong>: pure unit tests covering all statuses, 0..N dependents, and out-of-range dependents. </td></tr><tr><td data-label="Technical Breakdown — ptkp.py"> <strong><code>load_ptkp_rules(source: Optional[str]=None, settings: Optional[Settings]=None) -&gt; Mapping</code></strong><br><br><strong>Purpose:</strong> Lightweight loader/adapter that returns the canonical PTKP table (map of statuses → amounts, dependent allowance, effective_date, version). This function should prefer an injected <code>rules</code> in memory (fast path) and only perform I/O when explicitly asked (e.g., <code>source</code> points to a file). Keep top-level import-time I/O prohibited — call sites must provide settings or call this explicitly at startup.<br><br><strong>Behavioral notes:</strong> support multiple sources: in-memory constants, JSON/YAML rule file, DB-backed rule registry. Always validate schema (required keys: <code>version</code>, <code>effective_date</code>, <code>statuses</code>, <code>dependent_allowance</code>) and normalize numeric types (Decimal). Return errors as typed exceptions (<code>RulesValidationError</code>) with human-friendly messages for operator use.<br><br><strong>Caching:</strong> consumer-facing: callers may cache the returned map keyed by <code>version</code>. If loader does I/O, provide optional <code>ttl</code> parameter to avoid hot-path reads. Unit tests should mock each supported source and test schema validation. </td></tr><tr><td data-label="Technical Breakdown — ptkp.py"> <strong><code>resolve_rule_version(requested_version: Optional[str], rules_index: Mapping) -&gt; str</code></strong><br><br><strong>Purpose:</strong> Decision helper that selects an applied rule version based on requested version, effective dates, or latest stable. Encapsulates policy: if <code>requested_version</code> present and valid → use it; if absent → use rules_index.latest_stable (or the rule whose effective_date ≤ calculation_date). Document behavior for backdating (calculations for past payroll periods must use the rule valid at that period).<br><br><strong>Guarantees:</strong> returns a stable version identifier present in <code>rules_index</code> or raises <code>VersionResolutionError</code>. Must not mutate <code>rules_index</code>.<br><br><strong>Test cases:</strong> explicit version, null (pick latest stable), date-based backfill, unknown version → error. </td></tr><tr><td data-label="Technical Breakdown — ptkp.py"> <strong><code>validate_person_input(person: Mapping) -&gt; None</code></strong><br><br><strong>Purpose:</strong> Small validator to enforce required fields and canonical shapes for the <code>person</code> object used by <code>calculate_ptkp</code>. This is a thin wrapper delegating deep validation to <code>validators.py</code> when available; keep file-local quick checks (types, presence) to fail fast.<br><br><strong>Behavior:</strong> raises domain-specific exceptions for missing/invalid fields (e.g., <code>MissingFieldError(&#x27;marital_status&#x27;)</code>, <code>InvalidTypeError(&#x27;dependents_count&#x27;)</code>). Non-critical normalization (string case, numeric coercion) can be returned as warnings in <code>calculate_ptkp</code> instead of hard failure.<br><br><strong>Security & privacy:</strong> never log whole <code>person</code> object on failure — log only non-sensitive keys (marital_status, dependents_count) and a unique input hash. Unit tests should include malformed inputs to verify error messages are actionable. </td></tr><tr><td data-label="Technical Breakdown — ptkp.py"> <strong><code>annualize_income(raw_income: Decimal, income_period: str, target_period: str = &#x27;annual&#x27;) -&gt; Decimal</code></strong><br><br><strong>Purpose:</strong> Helper to convert income values across periods (monthly → annual, weekly → annual, pro-rated partial months). Not every caller needs this; <code>calculate_ptkp</code> may call it when input incomes are period-specific and PTKP is defined annually.<br><br><strong>Behavior & invariants:</strong> idempotent for identical <code>raw_income</code> and <code>income_period</code>. Support common <code>income_period</code> enums: <code>annual</code>, <code>monthly</code>, <code>biweekly</code>, <code>weekly</code>, <code>daily</code>. Guard against division/rounding surprises by using Decimal and explicit rounding mode documented in file-level header. Tests should validate conversion factors and boundary cases (leap years, 0 income). Keep function pure and fast (O(1)). </td></tr><tr><td data-label="Technical Breakdown — ptkp.py"> <strong><code>breakdown_ptkp_components(rule_map: Mapping, marital_status: str, dependents: int) -&gt; Mapping</code></strong><br><br><strong>Purpose:</strong> Produce a human-readable componentized explanation of the PTKP calculation suitable for inclusion in audit records and export manifests (<code>components</code>: list of <code>{name, amount, rule_key}</code>). For example: <code>[{name: &#x27;PTKP Base (single)&#x27;, amount: X, rule_key: &#x27;base.single&#x27;}, {name: &#x27;Dependent allowance (1)&#x27;, amount: Y, rule_key: &#x27;dependent.per_person&#x27;}]</code>.<br><br><strong>Requirements:</strong> Include <code>rule_key</code> (stable path into the rule table) and <code>applied_rule_version</code> in every component. The function must remain side-effect free and only format data; actual persistence is done by <code>recorder</code>. Unit tests should assert the structure and that sum(components) == computed PTKP. </td></tr><tr><td data-label="Technical Breakdown — ptkp.py"> <strong><code>build_decision_trace(inputs: Mapping, rules_used: Mapping, decisions: Mapping) -&gt; List[Mapping]</code></strong><br><br><strong>Purpose:</strong> Create a deterministic, minimal decision trace that describes the reasoning steps used to compute PTKP. Each trace entry should be <code>{step_id, name, input_snapshot_hash, rule_key, looked_up_value, reason, ts}</code>. Keep trace small (avoid embedding full PII) and suitable for appending to JSONL audit logs. The trace is the canonical artifact used by SRE/compliance to replay and explain calculations.<br><br><strong>Privacy:</strong> redact or hash any PII fields in <code>input_snapshot_hash</code>. Do not store raw SSN/IDs in trace. Tests: trace length and content determinism. </td></tr><tr><td data-label="Technical Breakdown — ptkp.py"> <strong><code>cache_key_for_inputs(person: Mapping, income_context: Mapping, rule_version: str) -&gt; str</code></strong><br><br><strong>Purpose:</strong> Deterministic cache key generator for optional memoization of PTKP results. Use canonical serialization (sorted keys) and a stable hashing algorithm (SHA-256) applied to non-sensitive fields; the function should exclude PII fields or map them to tokens. This helper is tiny but security-sensitive — document which fields are excluded and why.<br><br><strong>Usage:</strong> callers may use result as idempotency key for job dedupe; never use raw cache key as an audit key without redaction. Unit tests should assert identical keys for semantically identical inputs (ordering of keys shouldn't matter). </td></tr><tr><td data-label="Technical Breakdown — ptkp.py"> <strong><code>ptkp_for_residency_override(residency_status: str, base_ptkp: Decimal, rules: Mapping) -&gt; Decimal</code></strong><br><br><strong>Purpose:</strong> Optional policy hook to adjust PTKP based on residency rules (non-residents, special treaties, foreigners). Keep logic minimal and driven by <code>rules</code> table flags: <code>rules[&#x27;residency_overrides&#x27;]</code> mapping. If no override configured, return <code>base_ptkp</code> unchanged. This function should be pluggable and easy to mock in tests.<br><br><strong>Edge cases:</strong> treaties or temporary statuses must be represented as explicit rule entries; unknown statuses produce a non-destructive warning in the calculation trace. </td></tr><tr><td data-label="Technical Breakdown — ptkp.py"> <strong><code>exportable_summary(result: Mapping) -&gt; Mapping</code></strong><br><br><strong>Purpose:</strong> Create a compact, stable summary suitable for exports/manifest and for the HTTP <code>simulate</code> response. Include <code>ptkp_amount</code>, <code>components</code> (compact), <code>applied_rule_version</code>, <code>calculation_time_ms</code>, and <code>trace_id</code>. Avoid adding raw inputs or PII. This transforms internal rich result into an outward-facing record. Unit tests should verify summary contains exactly the allowed keys. </td></tr><tr><td data-label="Technical Breakdown — ptkp.py"> <strong>Implementation patterns & guardrails</strong><br><br>— <strong>Purity & determinism:</strong> Keep mapping and numeric functions pure. No global mutable state. <br>— <strong>Rule versioning:</strong> All returned results must include <code>applied_rule_version</code> and <code>applied_rule_digest</code>. <br>— <strong>Precision:</strong> Use <code>Decimal</code> with a declared context and rounding policy; document the rounding mode at the top of the file. <br>— <strong>Observability:</strong> Emit only non-sensitive high-level metrics: <code>ptkp_calculations_total</code>, <code>ptkp_calc_latency_ms</code> (histogram), and <code>ptkp_rule_version</code> as a tag. <br>— <strong>Privacy:</strong> Never log or persist raw PII from <code>person</code>; traces/hashes must be deterministic but redacted. <br>— <strong>Testing hygiene:</strong> Provide unit tests for every public function and property-based tests for <code>ptkp_amount_for_status</code> (random marital_status/dependents within ranges). Include golden fixtures keyed by <code>rules.version</code>. <br>— <strong>Failure modes:</strong> For missing/invalid rules, raise explicit domain errors so startup or the task runner can mark job as failed or degraded; do not silently continue with guessed defaults unless explicitly allowed by <code>settings.strict=false</code>. </td></tr><tr><td data-label="Technical Breakdown — ptkp.py"> <strong>Recommended tests & CI checks</strong><br><br>1. Unit tests for mapping function across all statuses and dependencies. <br>2. Integration test: <code>calculate_ptkp</code> wired with <code>load_ptkp_rules</code> using a known fixture and asserting full <code>trace</code> content. <br>3. Property-based tests for numeric stability & idempotency (same input → same output & same trace). <br>4. Audit test: assert <code>trace</code> contains <code>rule_key</code> and <code>applied_rule_version</code>. <br>5. Security test: ensure no PII appears in logs or traces by scanning outputs. <br>6. Backfill/regression test: calculation for historical payroll date uses historical <code>effective_date</code> rule. </td></tr><tr><td data-label="Technical Breakdown — ptkp.py"> <strong>Operational & maintenance notes</strong><br><br>— When updating rules, increment <code>rules.version</code> and add migration notes; include new version in manifest and in audit records. <br>— If adding new marital statuses or allowance types, add them to <code>rules</code> and unit tests immediately. <br>— Limit public API surface: prefer a single <code>calculate_ptkp</code> entrypoint plus small helpers for testing. <br>— Keep heavy I/O (DB, S3) out of this module; inject dependencies. </td></tr></tbody></table></div><div class="row-count">Rows: 15</div></div><div class="table-caption" id="Table4" data-table="Docu_0163_04" style="margin-top:2mm;margin-left:3mm;"><strong>Table 4</strong></div>
<div class="table-wrapper" data-table-id="table-4"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown — dtp_engine.py"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown — dtp_engine.py</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — dtp_engine.py"> <strong>File-level responsibilities</strong><br><br><code>dtp_engine.py</code> is the deterministic calculation core for DTP-related rules (the “DTP engine”). Its job is to take normalized canonical input (snapshots or row batches), apply a specific, versioned rule-set, and produce canonical results plus a decision trace for auditing and reconciliation. Keep this module pure where possible: heavy I/O (DB, object store, network) should be behind thin adapters passed in via <code>engine</code> factory args. The file must explicitly record the <code>applied_rule_version</code>, be deterministic (same inputs + rule version → same outputs), and produce append-only auditable traces. Prefer clear separation between (1) rule compilation, (2) pure arithmetic/logic evaluation per record, (3) batch orchestration and aggregation, and (4) side-effecting persistence/telemetry. Document expected inputs (canonical schema) at the head of the file and assert them at start of public entrypoints. </td></tr><tr><td data-label="Technical Breakdown — dtp_engine.py"> <strong><code>create_engine(config: EngineConfig, clients: Clients) -&gt; DtpEngine</code></strong><br><br>Factory that returns a configured <code>DtpEngine</code> object. Responsibilities: validate <code>config</code> (timeouts, parallelism), attach clients (object store, recorder, metrics), set caches (compiled rule cache, expression cache), and wire a safe <code>shutdown()</code> method. Must not perform long-running I/O at creation — only validate configuration and lazy-load heavy artifacts later. Attach idempotency guards so <code>create_engine</code> may be called repeatedly in tests. Unit tests should instantiate with minimal <code>FakeClients</code> and assert the returned engine exposes documented methods. </td></tr><tr><td data-label="Technical Breakdown — dtp_engine.py"> <strong><code>load_rule_set(rule_version: str, allow_cached: bool = True) -&gt; CompiledRuleSet</code></strong><br><br>Load and compile rules for a given <code>rule_version</code>. Behavior: (1) check local in-memory LRU cache, (2) fetch rule source from configured <code>rule_store</code> (file system, object store, or DB), (3) validate schema and semantics, (4) compile into an internal representation optimized for execution (bytecode/AST or precomputed function closures), and (5) attach metadata (version, effective_date, checksum). Must enforce strict validation (semantic checks, forbidden side-effects). When <code>allow_cached</code> is false, force recompile. Failures: raise <code>RuleLoadError</code> with clear cause and debugging metadata. Tests: provide malformed, conflicting, and large-rule sets to assert error messages, cache hits/misses, and compile performance. </td></tr><tr><td data-label="Technical Breakdown — dtp_engine.py"> <strong><code>compile_rule(rule_source: RuleSource) -&gt; CompiledRule</code></strong><br><br>Pure function that turns a single rule into an executable artifact. Responsibilities: parse the rule expression, type-check referenced inputs, precompute constant subexpressions, and produce a callable <code>execute(record, ctx) -&gt; (result_fragment, trace_fragment)</code>. Must forbid I/O and side-effects; sandboxing or safe-eval approaches recommended. Provide deterministic numbering for trace nodes. Unit tests: assert correctness on many edge-cases (nulls, extreme numerics, division by zero handled predictably). </td></tr><tr><td data-label="Technical Breakdown — dtp_engine.py"> <strong><code>validate_inputs(snapshot_meta: SnapshotMeta) -&gt; None</code></strong><br><br>Lightweight guards verifying the snapshot matches required shape: expected columns, column types, row counts (optional), and rule-compatible invariants (e.g., tax_year within rule effective dates). On mismatch raise <code>InputValidationError</code> with actionable hints. This function must be cheap and run early to fail fast. Tests: missing columns, additional unknown columns, malformed types. </td></tr><tr><td data-label="Technical Breakdown — dtp_engine.py"> <strong><code>run_job(job_id: str, snapshot_ref: SnapshotRef, rule_version: str, run_ctx: RunContext) -&gt; JobResult</code></strong><br><br>Top-level orchestration used by workers. Responsibilities: (1) idempotency check (job already completed? short-circuit), (2) load snapshot metadata & validate, (3) acquire compiled rules via <code>load_rule_set</code>, (4) choose execution strategy (single-threaded, process pool, or async tasks) based on <code>run_ctx</code> and <code>config.parallelism</code>, (5) stream rows through <code>execute_batch</code> or <code>execute_row</code>, (6) aggregate outputs and traces, (7) persist results and append audit record(s), and (8) emit metrics and job completion events. Must be timeboxed by <code>run_ctx.timeout</code>; on partial failure mark job as <code>degraded</code> and persist partial outputs with a strong <code>failure_reason</code>. Tests: idempotency, simulated worker crashes, partial timeouts producing <code>degraded</code> status. </td></tr><tr><td data-label="Technical Breakdown — dtp_engine.py"> <strong><code>execute_batch(rows: Iterable[Record], compiled_rules: CompiledRuleSet, ctx: ExecutionContext) -&gt; (List[Result], List[Trace])</code></strong><br><br>Process a batch of canonical records. Responsibilities: vectorize where possible (apply same rule across rows without Python-level loop if rules allow), maintain stable ordering, collect per-row results and decision traces. Must enforce per-row isolation: one row's exception cannot poison the entire batch. Exceptions for rows should be captured and returned as row-level diagnostics, not thrown out of the batch routine (unless batch-level invariants fail). Provide backpressure hooks and progress callbacks. Tests: large batches, rows with deterministic exceptions, ordering invariants. </td></tr><tr><td data-label="Technical Breakdown — dtp_engine.py"> <strong><code>execute_row(record: Record, compiled_rules: CompiledRuleSet, ctx: ExecutionContext) -&gt; (Result, Trace)</code></strong><br><br>Pure evaluator that applies compiled rules to a single record and emits a deterministic <code>Result</code> and <code>Trace</code>. Responsibilities: evaluate precompiled rule closures, apply short-circuiting where rules specify priorities, perform numeric normalization (decimal context, rounding rules), and return a structured trace with node-level evaluation times, inputs used, and any warnings (e.g., data coersion). Must be deterministic and not perform I/O. Error handling: convert arithmetic errors into traceable warnings and a stable fallback value if rule metadata permits, otherwise mark row <code>error</code>. Tests: precision edge-cases, nonstandard numerics, rule short-circuit semantics. </td></tr><tr><td data-label="Technical Breakdown — dtp_engine.py"> <strong><code>evaluate_rule(compiled_rule: CompiledRule, record: Record, ctx: ExecutionContext) -&gt; (value, trace_node)</code></strong><br><br>Evaluate a single compiled rule node. Responsibilities: small, pure, and extremely well-tested numerical/logic operations. Should include guards for safe evaluation (time-limited, iteration depth limit for recursive expressions), and capture per-rule timing. Return enriched trace node for later assembly. Tests: ensure trace node schema stability; test side-by-side with <code>compile_rule</code> to detect regressions. </td></tr><tr><td data-label="Technical Breakdown — dtp_engine.py"> <strong><code>apply_adjustments(result: Result, adjustments: List[AdjustmentSpec], ctx: ExecutionContext) -&gt; Result</code></strong><br><br>Post-processing step to apply business adjustments (e.g., rounding, withholding caps, progressive application of reliefs). This is a pure transformation of <code>Result</code> and must be idempotent for identical inputs. Document precedence and commutativity rules: if multiple adjustments conflict, engine must apply them in defined order and record which adjustments took precedence in the trace. Tests: overlapping adjustments, zero/negative values, max/min enforcement. </td></tr><tr><td data-label="Technical Breakdown — dtp_engine.py"> <strong><code>merge_batch_results(batch_results: Iterable[Result], strategy: str) -&gt; AggregateResult</code></strong><br><br>Aggregate per-row results into job-level outputs: totals, per-taxpayer summaries, and analytics used by exporters. Must support multiple merge strategies (sum, weighted-sum, latest-wins) parameterized by <code>strategy</code>. Emit consistency checks (sum of parts == reported total) and include them in the <code>JobResult</code> metadata. Tests: aggregation correctness, large numeric stability (use decimals), and checksum verification. </td></tr><tr><td data-label="Technical Breakdown — dtp_engine.py"> <strong><code>persist_results(output_ref: OutputDestination, results: Iterable[Result], traces: Iterable[Trace], metadata: JobMetadata) -&gt; PersistReceipt</code></strong><br><br>Side-effecting function that writes results and traces to object store and metadata DB. Responsibilities: write atomically where possible (stage+commit), include applied rule version metadata and checksums, and return receipt including storage paths and checksums. Must be robust: retry transient errors with backoff, but avoid double-writing (use idempotency keys). Always write the audit entry via <code>recorder.append()</code> with append-only semantics. Tests: simulate transient failures, idempotent re-run behavior, and consistency between stored results and produced checksum. </td></tr><tr><td data-label="Technical Breakdown — dtp_engine.py"> <strong><code>record_audit(job_id: str, job_result: JobResult, receipts: PersistReceipt) -&gt; None</code></strong><br><br>Append an audit record that includes: <code>job_id</code>, <code>applied_rule_version</code>, <code>start_ts</code>, <code>end_ts</code>, <code>status</code>, <code>checksums</code>, <code>output_paths</code>, <code>trace_path</code> (or inline sample), and <code>runtime_fingerprint</code>. Must be append-only and deterministic in serialization. Failures in audit append must be surfaced as high-severity alerts but should not block best-effort publish of outputs (record the audit failure in job metadata). Tests: idempotent append behavior and integrity of JSONL lines. </td></tr><tr><td data-label="Technical Breakdown — dtp_engine.py"> <strong><code>dry_run(record: Record, rule_version: Optional[str] = None, overrides: Optional[dict] = None) -&gt; (Result, Trace, Diagnostics)</code></strong><br><br>Developer helper used by <code>/simulate</code> and CLI. Runs the engine in a fully-local mode: no persistence, verbose trace, and optional <code>overrides</code> allowing runtime substitution of rule constants. Must be fast and return rich diagnostics (rule execution order, timings, intermediate values). Tests: replicate simulation outputs in integration tests; ensure <code>dry_run</code> never writes external state. </td></tr><tr><td data-label="Technical Breakdown — dtp_engine.py"> <strong><code>get_runtime_summary(job_result: JobResult) -&gt; EngineSummary</code></strong><br><br>Produce a compact runtime summary useful for telemetry: rule count, evaluated rows, error_count, median per-row latency, peak memory, and <code>applied_rule_version</code>. This is a pure transformation used by metrics collectors. Ensure summary computation is cheap and safe to call in-process. Tests: correctness over known job_result fixtures. </td></tr><tr><td data-label="Technical Breakdown — dtp_engine.py"> <strong><code>export_decision_trace(job_id: str, trace_path: str, format: str = &quot;jsonl&quot;) -&gt; ExportReceipt</code></strong><br><br>Utility to produce exportable trace bundles consumed by auditors or reconcilers. Responsibilities: validate trace size; allow slicing by taxpayer IDs or time ranges; sign or HMAC the exported bundle if config requires. Must enforce sensitive-data redaction according to configured policy. Tests: large-trace slicing, HMAC/manifest inclusion, redaction rules. </td></tr><tr><td data-label="Technical Breakdown — dtp_engine.py"> <strong><code>shutdown(timeout: float = None) -&gt; None</code></strong><br><br>Graceful shutdown: flush any in-memory trace buffers, persist pending metric counters, and stop background compilation/cache eviction threads. Should be idempotent and safe to call from worker lifecycle. Respect <code>timeout</code> and emit warnings if resources remain locked. Tests: verify flush, repeated shutdown calls, and preserved metrics after shutdown. </td></tr><tr><td data-label="Technical Breakdown — dtp_engine.py"> <strong>Error classes & invariants</strong><br><br>Define and document engine-specific exceptions (<code>RuleLoadError</code>, <code>InputValidationError</code>, <code>ExecutionError</code>, <code>PersistError</code>, <code>JobIdempotencyError</code>). Maintain invariants: <code>Result</code> schema stability, <code>Trace</code> node canonical fields (<code>node_id</code>, <code>parent_id</code>, <code>rule_id</code>, <code>value</code>, <code>status</code>, <code>duration_ns</code>), and <code>applied_rule_version</code> must appear in every persisted artifact. Unit tests should assert that exceptions carry structured <code>.details</code> for automated handling. </td></tr><tr><td data-label="Technical Breakdown — dtp_engine.py"> <strong>Observability & instrumentation</strong><br><br>Every public entrypoint must emit structured metrics and tracing spans: <code>dtp.run.duration</code>, <code>dtp.row.latency.hist</code>, <code>dtp.rule.compile.duration</code>, <code>dtp.job.status{ok,degraded,failed}</code>, and counters for <code>dtp.rows.processed</code>, <code>dtp.rows.errors</code>. Integrate with trace-id propagation: accept <code>run_ctx.trace_id</code> and ensure it is included in audit records. Provide a dry-run metric sink for tests. </td></tr><tr><td data-label="Technical Breakdown — dtp_engine.py"> <strong>Performance & scaling</strong><br><br>Document expected throughput and memory characteristics. Recommend default decimal contexts and concurrency model (per-worker process, per-job thread pool). For large snapshots prefer streaming (<code>execute_batch</code> with windowed reads) and incremental persist to avoid high memory usage. Provide guidance and benchmarks: e.g., <code>N</code> rows/sec per CPU for typical rule complexity and a formula for estimating memory = base + row_size * concurrency. </td></tr><tr><td data-label="Technical Breakdown — dtp_engine.py"> <strong>Security & data handling</strong><br><br>Never log PII in plaintext. Where traces include masked inputs, provide deterministic masking helpers (hash-with-salt) so auditability is preserved without leaking secrets. Use HMAC to sign manifests and ensure exported bundles include <code>applied_rule_version</code>, <code>generator_version</code>, and checksums. Tests: redaction tests and manifest signature verification. </td></tr><tr><td data-label="Technical Breakdown — dtp_engine.py"> <strong>Testing matrix & CI</strong><br><br>Required tests: unit tests for <code>compile_rule</code>, <code>evaluate_rule</code>, and numerical edge-cases; contract tests for <code>execute_row</code> determinism across Python versions; integration tests using <code>dry_run</code> against golden fixtures; lifecycle tests for <code>run_job</code> with mocked clients verifying persist + audit; chaos tests forcing partial failures and asserting <code>degraded</code> semantics. Include a small fuzz harness for random inputs to catch numeric & trace serialization bugs. Automate rule-version cross-compatibility checks in CI. </td></tr><tr><td data-label="Technical Breakdown — dtp_engine.py"> <strong>Operational guardrails & runbook</strong><br><br>Before production promotion ensure: default <code>strict=True</code>, rule-store retention policy in place, monitoring for <code>dtp.job.failed</code> and <code>dtp.rows.errors</code> alerts, a rollback plan for <code>applied_rule_version</code>, and a validated manifest signing key rotation schedule. Provide runbook steps for common failures: missing rule version, object store write failures, and high row-error rates (threshold-based automated quarantine). </td></tr><tr><td data-label="Technical Breakdown — dtp_engine.py"> <strong>Extensibility notes</strong><br><br>Keep pure-rule evaluation free of engine plumbing so <code>ptkp.py</code>, <code>ter.py</code>, <code>progressive.py</code> style modules can be reused in <code>simulate</code> and other tools. When adding new features (e.g., lineage tagging or ML-based anomaly flags), add them as post-processors (<code>apply_adjustments</code> style) to avoid touching core deterministic evaluation. Document how to add a new rule primitive and ensure backward compatibility via explicit <code>rules_migrations/</code> entries. </td></tr></tbody></table></div><div class="row-count">Rows: 24</div></div><div class="table-caption" id="Table5" data-table="Docu_0163_05" style="margin-top:2mm;margin-left:3mm;"><strong>Table 5</strong></div>
<div class="table-wrapper" data-table-id="table-5"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown — bukti_potong_csv.py"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown — bukti_potong_csv.py</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — bukti_potong_csv.py"> <strong>File-level responsibilities</strong><br><br>Export canonical, audited "bukti potong" (tax withholding receipts) as a CSV bundle that is safe for publication and downstream ingestion. This module is responsible for transforming canonical internal records into a stable CSV layout, applying necessary business rules and redaction, computing and persisting export metadata (checksums, row counts, totals, applied_rule_version, generator_version), and producing an atomic, signed bundle ready for publishing. Keep IO streaming-friendly (never buffer entire dataset in memory), deterministic (stable column ordering and row sort when required), idempotent (re-run exports should produce identical outputs given identical inputs and settings), and testable (pure mapping/formatting helpers separated from side-effectful writers). Document operational knobs at the top of the file (<code>CSV_DIALECT</code>, <code>ENCODING</code>, <code>CHUNK_ROWS</code>, <code>SIGNING_KEY_REF</code>, <code>REDACTION_RULES</code>, <code>STRICT_VALIDATION</code>) and ensure defaults are safe for production (UTF-8 with BOM only when explicitly required, conservative quoting rules).<br><br><strong>Operational guarantees:</strong> deterministic manifest fields (applied_rule_version, generator_version), redaction must be reversible-only for audit (store raw sensitive fields in vault/audit, not in export), export must fail loudly if <code>settings.strict</code> and critical invariants break, otherwise produce a degraded bundle with clear <code>manifest.degraded = true</code> and <code>manifest.hints</code> explaining missing pieces. Instrumentation: emit metrics (rows_written, export_duration_ms, bytes_written, export_errors), and structured audit log entries for every export operation including <code>trace_id</code> and <code>export_id</code>. Provide a small <code>render_export_for_test(records, columns)</code> pure helper used by unit tests. </td></tr><tr><td data-label="Technical Breakdown — bukti_potong_csv.py"> <strong><code>render_bukti_potong_csv(records, output_stream, settings, *, export_id=None) -&gt; ExportResult</code></strong><br><br><strong>Responsibility:</strong> canonical top-level orchestrator used by CLI, server, and pipeline to produce the CSV export into <code>output_stream</code> (file-like). It validates inputs, opens staging artifacts, registers telemetry, invokes streaming writer helpers, and finalizes the manifest and signature. Must not assume <code>records</code> fits in memory; must stream. Return value (<code>ExportResult</code>) is a small dataclass/dict with <code>{export_id, rows_written, bytes_written, checksum, manifest}</code>. <br><br><strong>Inputs:</strong> <code>records</code> (iterable/generator of canonical records), <code>output_stream</code> (writable binary/text stream opened by caller), <code>settings</code> (configuration), optional <code>export_id</code> (idempotency key).<br><br><strong>Behavior & ordering:</strong> 1) validate <code>settings</code> and required columns; 2) load column mapping and CSV dialect; 3) call <code>write_csv_header</code>; 4) iterate records: <code>validate_record_for_export</code> -> <code>map_record_to_columns</code> -> <code>format_row_values</code> -> <code>write_csv_row</code> (streamed) while accumulating totals and checksums; 5) after loop, call <code>finalize_manifest</code> and <code>attach_checksums</code> and optionally <code>sign_bundle</code> (if settings require signing); 6) flush and return <code>ExportResult</code>.<br><br><strong>Failure modes & semantics:</strong> if a record triggers a fatal validation error and <code>settings.strict</code> is true, abort early and raise a domain <code>ExportError</code> (with partial progress in exception payload). If <code>strict</code> is false, emit a structured warning audit entry, skip or mark row as invalid in a separate validation report, and continue. Always ensure the output stream is left in a deterministic state (truncate on fatal failure when writing to a temp/staging file; callers are responsible for atomic rename).<br><br><strong>Observability & tests:</strong> metric emissions, structured audit for start/end with duration and counts. Unit tests: simulate <code>records</code> generator raising mid-stream and assert partial results and rollback behavior when <code>strict=True</code>. Integration tests: run against a temp file and assert checksum and exact CSV byte-for-byte match for a golden fixture. </td></tr><tr><td data-label="Technical Breakdown — bukti_potong_csv.py"> <strong><code>map_record_to_columns(record, column_map, settings) -&gt; List[str]</code></strong><br><br><strong>Responsibility:</strong> Convert a canonical record into an ordered list of string cell values matching the CSV column schema. This is a pure function: no IO, no side effects, deterministic given inputs. It should perform column mapping, per-column formatting (dates, currency, boolean), redaction marks where required by settings (but not irreversible redaction for audit; leave any irreversible redaction to post-processing), and stable rounding rules. <br><br><strong>Inputs/Outputs:</strong> <code>record</code> (dict/obj), <code>column_map</code> (ordered column descriptors: name, type, nullable, formatter), <code>settings</code>. Output is a list of strings exactly matching column order.<br><br><strong>Edge cases & invariants:</strong> Missing optional fields yield empty strings. Missing non-optional fields produce a <code>ValidationError</code> (unless <code>settings.lenient</code> then produce placeholder and log). Numeric formatting must use <code>Decimal</code> and string formatting that avoids scientific notation. Dates must use ISO-8601 or specified <code>settings.date_format</code>. Tests should include locale-sensitive values (commas, decimal separators) to ensure consistent formatting. </td></tr><tr><td data-label="Technical Breakdown — bukti_potong_csv.py"> <strong><code>validate_record_for_export(record, schema, settings) -&gt; ValidationResult</code></strong><br><br><strong>Responsibility:</strong> Row-level validation prior to mapping: ensure types conform, required fields present, business-rule checks (e.g., <code>npwp</code> format, tax_period present, withholding calculation sanity checks). Must be fast and pure. Return a structured <code>ValidationResult</code> with <code>{ok: bool, errors: List[Error], warnings: List[Warning], normalized: Optional[record]}</code>. The orchestrator decides whether to fail/skip based on <code>settings.strict</code>. <br><br><strong>Implementation notes:</strong> Prefer pydantic-like validation but keep dependency light. Support "soft rules" (warnings) and "hard rules" (errors). Include canonicalization (trim whitespace, uppercase IDs) so mapping receives normalized input. Tests should cover each rule and ensure expected severity mapping. </td></tr><tr><td data-label="Technical Breakdown — bukti_potong_csv.py"> <strong><code>compute_aggregates_and_totals(record_stream, running_state) -&gt; None</code></strong><br><br><strong>Responsibility:</strong> Maintain export-level running aggregates while rows are written: <code>total_gross</code>, <code>total_tax_withheld</code>, <code>row_count</code>, <code>unique_npwp_count</code>, any rule-specific accumulators. This is stateful and called by the orchestrator per-row. Keep this logic separate from IO to enable unit tests. Ensure numeric accumulation uses <code>Decimal</code> with an enforced scale to avoid floating point drift. On export completion attach these aggregates to manifest. <br><br><strong>Failure/overflow:</strong> Protect against pathological counts by validating monotonic growth and provide guardrails (max_rows threshold from settings) to prevent runaway exports. </td></tr><tr><td data-label="Technical Breakdown — bukti_potong_csv.py"> <strong><code>write_csv_header(writer, column_map, dialect, encoding) -&gt; None</code></strong><br><br><strong>Responsibility:</strong> Emit CSV header row in the exact order specified by <code>column_map</code>. This function must be idempotent and check <code>writer.state._header_written</code> before writing. Ensure column names are stable and normalized to upstream expectations (no trailing whitespace). Support optional alternate header modes (human-friendly vs canonical) controlled by <code>settings.header_mode</code>. Tests: header-only export produces CSV with single header row and zero data rows. </td></tr><tr><td data-label="Technical Breakdown — bukti_potong_csv.py"> <strong><code>write_csv_row(writer, row_values, dialect) -&gt; None</code></strong><br><br><strong>Responsibility:</strong> Low-level streaming write of a single CSV row using <code>csv.writer</code> or equivalent safe CSV serializer. Must honor <code>dialect</code> (quoting, delimiter) and ensure proper newline handling across platforms. Must catch and convert <code>UnicodeEncodeError</code> to a well-described <code>ExportEncodingError</code> with guidance about offending characters and recommended <code>settings.encoding</code>. Must be extremely small and synchronous. Maintain an internal bytes-written counter and update <code>writer.state</code> atomically. Unit tests: multi-line field, embedded quotes, and delimiter-in-field scenarios. </td></tr><tr><td data-label="Technical Breakdown — bukti_potong_csv.py"> <strong><code>stream_records_in_chunks(records, chunk_size) -&gt; Iterator[List[record]]</code></strong><br><br><strong>Responsibility:</strong> Utility generator to batch input records into predictable chunk sizes to balance IO and memory. Used when computing aggregates or when downstream sink benefits from batching. Must preserve input order and not pre-consume the entire iterator. Tests: generator respects chunk_size and yields final partial chunk correctly. </td></tr><tr><td data-label="Technical Breakdown — bukti_potong_csv.py"> <strong><code>attach_checksums_and_metadata(file_path, manifest, hash_algo=&#x27;sha256&#x27;) -&gt; Manifest</code></strong><br><br><strong>Responsibility:</strong> After CSV file is fully written and flushed, compute its checksum(s), size, and optionally row_count if not already tracked. Update <code>manifest</code> with these fields (<code>checksums: {sha256: &quot;...&quot;, md5: &quot;...&quot;}</code>, <code>size_bytes</code>, <code>rows</code>, <code>generator_version</code>, <code>applied_rule_version</code>). Must stream file in chunks to compute checksum without loading file into memory. Ideally compute checksums during write to avoid a second full read; if doing post-write, document tradeoffs. Tests: deterministic checksum for golden fixture. </td></tr><tr><td data-label="Technical Breakdown — bukti_potong_csv.py"> <strong><code>sign_bundle(staging_path, key_ref, signature_algorithm=&#x27;hmac-sha256&#x27;|&#x27;rsa-pss&#x27;) -&gt; SignatureMetadata</code></strong><br><br><strong>Responsibility:</strong> Sign the bundle (or manifest) with an HMAC or private key and return a signature object to attach to manifest. Never keep private key material in process memory longer than needed; prefer signing via a key provider client (KMS) with a short-lived handle. If local signing is used, read key from a <code>SecretHandle</code> or <code>SecretRef</code> pattern and zero memory after use. For HSM/KMS signing, call the provider client and attach provider signature metadata (key_id, algorithm, signature). Fail export with a clear error if signing is required and fails (unless <code>settings.allow_unsigned_degraded</code> in which case produce manifest with <code>signed=false</code> and audit note). Tests: signature verification test using public key. </td></tr><tr><td data-label="Technical Breakdown — bukti_potong_csv.py"> <strong><code>atomic_publish(staging_path, final_path) -&gt; None</code></strong><br><br><strong>Responsibility:</strong> Make the written bundle available to consumers atomically. For local filesystem, use atomic <code>rename</code>; for object stores, write to staging prefix then call provider atomic move/put operation or utilize object-store metadata and conditional write. Ensure manifest and signature are published together. On partial failures, ensure staging is cleaned up or left in a clearly named <code>failed/{export_id}</code> prefix for forensic inspection. Document permissions required for publisher role (least privilege). Integration tests: simulate rename failure and confirm cleanup policy. </td></tr><tr><td data-label="Technical Breakdown — bukti_potong_csv.py"> <strong><code>redact_pii(value, rules) -&gt; str</code></strong><br><br><strong>Responsibility:</strong> Deterministic redaction according to configured rules: mask all but last N digits, replace certain keys with <code>&lt;REDACTED&gt;</code>, or apply tokenization when required. This function should be deterministic and idempotent: the same input and rule yields same output. Do not perform irreversible redaction for audit copies; instead, store redacted value in the export but retain raw value in a secure audit store referenced by <code>audit_ref</code>. Provide a <code>mode</code> parameter (<code>export</code> vs <code>audit</code>) to control behavior. Tests: regex-based redaction, idempotence, performance on long strings. </td></tr><tr><td data-label="Technical Breakdown — bukti_potong_csv.py"> <strong><code>finalize_manifest(manifest, aggregates, settings) -&gt; Manifest</code></strong><br><br><strong>Responsibility:</strong> Merge running aggregates, computed checksums, applied_rule_version, generator_version, generation timestamp, environment, and signature metadata into the final manifest structure used by <code>manifest.py</code> and the publishing layer. Validate manifest schema and, if <code>settings.strict_manifest</code> is true, run a manifest validator and fail if schema checks fail. Include <code>hints</code> and <code>degraded</code> flags when appropriate. Unit tests: manifest round-trip (serialize→deserialize) unchanged. </td></tr><tr><td data-label="Technical Breakdown — bukti_potong_csv.py"> <strong><code>build_column_map(settings, schema_registry=None) -&gt; ColumnMap</code></strong><br><br><strong>Responsibility:</strong> Load or compute the ordered list of output columns with metadata (name, type, optional, formatter, redaction rule). Support overrides via <code>settings.column_overrides</code> and allow loading from a centralized schema registry or local YAML/JSON. Validate that required canonical fields exist in column_map. This helper isolates schema evolution: new columns should be appended and documented with <code>effective_date</code>. Provide a compatibility check function that returns a migration plan if an older manifest is requested. Tests: column_map loaded from default and from override produce expected orders. </td></tr><tr><td data-label="Technical Breakdown — bukti_potong_csv.py"> <strong><code>validate_export_contracts(manifest, consumers) -&gt; List[ContractViolation]</code></strong><br><br><strong>Responsibility:</strong> Optional post-generation check to ensure produced CSV conforms to downstream consumer contracts (column presence, column types, cryptographic expectations). Return a list of violations; if any fatal and <code>settings.strict_contracts</code>, abort publish. Use for QA gates before making the bundle public. Tests: simulate consumer expecting an extra column and ensure violation is reported. </td></tr><tr><td data-label="Technical Breakdown — bukti_potong_csv.py"> <strong>Implementation patterns & guardrails</strong><br><br>— <strong>Streaming-first</strong>: avoid full in-memory materialization. Use incremental checksum and counting. <br>— <strong>Idempotence</strong>: exports should accept an <code>export_id</code> or idempotency key; repeated runs with same input+settings produce identical artifacts. <br>— <strong>Secrets</strong>: signing keys accessed via <code>SecretRef</code> or KMS clients; never hardcode private keys. <br>— <strong>Observability</strong>: emit metrics (rows, bytes, duration), structured logs with trace_id/export_id, and an audit line per export. <br>— <strong>Fail-fast vs degraded</strong>: <code>settings.strict</code> decides whether to abort on validation or signing failures. Document which invariants are critical. <br>— <strong>Testing hygiene</strong>: provide <code>render_export_for_test</code> with deterministic fake clocks and sample records for golden diff tests. Include fuzz tests for CSV edge cases (commas, newlines, unicode). <br>— <strong>Security & privacy</strong>: ensure PII redaction rules are configurable and applied consistently; avoid writing raw PII into exported CSVs. When exports include masked values, include <code>manifest.redaction_policy</code> describing scheme. <br>— <strong>Performance</strong>: benchmark writer path with real-world cardinalities; tune <code>CHUNK_ROWS</code> and <code>IO</code> buffer sizes; prefer buffered writes and chunked reads for checksum. <br>— <strong>Operational checklist</strong>: verify <code>generator_version</code> bump policy, ensure signing key rotation plan, include smoke test producing one small CSV and verifying checksum/signature in CI. </td></tr></tbody></table></div><div class="row-count">Rows: 16</div></div><div class="table-caption" id="Table6" data-table="Docu_0163_06" style="margin-top:2mm;margin-left:3mm;"><strong>Table 6</strong></div>
<div class="table-wrapper" data-table-id="table-6"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown — src/exporters/spt_masa_prefill.py"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown — src/exporters/spt_masa_prefill.py</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — src/exporters/spt_masa_prefill.py"> <strong>File-level responsibilities</strong><br><br>This module produces the SPT Masa <em>prefill</em> artifact used by the export pipeline. It is the canonical place where canonical calculation results (audit-records / recorder outputs) are transformed into the tax-authority prefill payload, signed, checksummed, and staged for publication. Keep the file focused on pure transformations and deterministic serialization; side effects (storage, signing key material access, network I/O) must be delegated to well-defined adapter interfaces injected at call sites. Design for auditability: every operation that changes data must return (or be able to derive) a small, stable provenance object that includes <code>source_snapshot_id</code>, <code>applied_rule_version</code>, <code>generator_version</code>, <code>trace_id</code>, and a content-hash. Use <code>decimal.Decimal</code> for all money math, and prefer streaming generation when producing XML/large payloads. Validate inputs aggressively and fail fast during export preparation (the pipeline will decide whether a failure is fatal or degradaed).<br><br><strong>Top-level contract</strong>: export orchestrator calls a single entrypoint (<code>make_spt_masa_prefill</code>) with an <code>ExportContext</code> describing snapshot(s), applied rule version, requested format, signer reference, and destination adapter. The function either returns an <code>ExportResult</code> (with artifact locations and manifest) or raises a well-typed <code>PrefillError</code> describing recoverability semantics. Unit tests must be deterministic and rely on small, human-readable fixtures for canonical rows and expected XML fragments. </td></tr><tr><td data-label="Technical Breakdown — src/exporters/spt_masa_prefill.py"><strong><code>make_spt_masa_prefill(ctx: ExportContext) -&gt; ExportResult</code></strong><br><br><strong>Responsibility</strong>: canonical orchestration: validate inputs, load data, transform to prefill rows, render, validate, sign, compute checksums, assemble manifest, persist artifacts, and return final <code>ExportResult</code>. This is <em>orchestrator-only</em> — delegate every external interaction (object store, signer, logger, time) to injected adapters in <code>ctx</code> to remain testable. <br><br><strong>Inputs</strong>: <code>ExportContext</code> (snapshot id(s), target period, settings, signer_id, trace_id, destination adapter).<br><strong>Outputs</strong>: <code>ExportResult</code> (artifact URIs, manifest object, applied metadata).<br><br><strong>Contracts & invariants</strong>: idempotent for identical <code>ctx</code> (use content-hash + idempotency key when persisting). Deterministic: same inputs → same bytes/signature. Must not mutate <code>ctx</code> in place. Attach <code>runtime_fingerprint</code> to returned <code>ExportResult</code>. <br><br><strong>Side effects</strong>: calls <code>persist_prefill_artifact</code> and <code>persist_manifest</code> via destination adapter; may call <code>sign_prefill</code>. All side effects must be visible in returned <code>ExportResult</code> (locations + checksums). <br><br><strong>Failure modes & retries</strong>: transient storage/signer errors should surface as <code>TransientPrefillError</code> (retryable); validation errors should raise <code>ValidationPrefillError</code> (non-retryable). Orchestrator should not swallow fatal exceptions. <br><br><strong>Testing</strong>: end-to-end integration test with a fake object-store and fake signer; golden fixture asserting manifest fields, checksums and sample XML fragments. </td></tr><tr><td data-label="Technical Breakdown — src/exporters/spt_masa_prefill.py"><strong><code>validate_inputs(ctx: ExportContext) -&gt; None</code></strong><br><br><strong>Responsibility</strong>: early validation of the context: presence and format of <code>snapshot_id</code>, <code>period</code> in YYYY-MM, signer existence (logical, not credential check), allowed output formats, and <code>settings.strict</code> guardrails. Do not perform network I/O here; only local/structural checks. <br><br><strong>Idempotency</strong>: pure function → safe to call multiple times. <br><br><strong>Failure</strong>: raise <code>InvalidExportContext</code> with structured field-level details suitable for CLI/HTTP responses. Provide small helper <code>explain()</code> for user-readable messages. </td></tr><tr><td data-label="Technical Breakdown — src/exporters/spt_masa_prefill.py"><strong><code>prefill_from_snapshot(snapshot_id: str, loader: SnapshotLoader, ctx_meta: dict) -&gt; Iterator[CanonicalRecord]</code></strong><br><br><strong>Responsibility</strong>: stream canonical records for the requested snapshot from storage without loading all records into memory. Yield <code>CanonicalRecord</code> items ready for mapping. Use the <code>loader</code> adapter for I/O. Attach provenance (snapshot id, retrieved range) to each <code>CanonicalRecord</code>. <br><br><strong>Performance</strong>: must be streaming-friendly and use backpressure (iterators/generators). For very large snapshots, fallback to chunked iteration. <br><br><strong>Failure modes</strong>: missing snapshot → <code>SnapshotNotFound</code>; corrupt snapshot → <code>SnapshotCorrupt</code> with example <code>record_id</code> and byte offset. Provide retry hints in exceptions. </td></tr><tr><td data-label="Technical Breakdown — src/exporters/spt_masa_prefill.py"><strong><code>map_records_to_prefill_rows(records: Iterable[CanonicalRecord], rules_meta: RulesMeta, mapping_config: MappingConfig) -&gt; Iterator[SPTRow]</code></strong><br><br><strong>Responsibility</strong>: deterministic mapping from canonical records to SPT prefill rows. Single responsibility: mapping + lightweight enrichment (e.g., currency normalization). All domain logic that is purely deterministic should be here (field mapping, defaulting rules, identity transformations). <br><br><strong>Contracts</strong>: Do not sign or round money values here; use <code>Decimal</code> and preserve precision. Return rich <code>SPTRow</code> objects with both raw fields and <code>diagnostics</code> array for per-row warnings. Keep mapping idempotent - repeated mapping of same record must produce identical <code>SPTRow</code>. <br><br><strong>Validation</strong>: produce non-fatal <code>RowWarning</code>s in <code>diagnostics</code> for recoverable issues and <code>RowError</code> for absolute rejects; the orchestrator may choose to include or drop rows based on <code>settings.strict</code>. <br><br><strong>Testing</strong>: parametric unit tests covering boundary values (missing NPWP, zero incomes, multi-currency, negative adjustments). Mock <code>rules_meta</code> to validate mapping branches. </td></tr><tr><td data-label="Technical Breakdown — src/exporters/spt_masa_prefill.py"><strong><code>aggregate_monthly_summary(rows: Iterable[SPTRow]) -&gt; MonthlySummary</code></strong><br><br><strong>Responsibility</strong>: compute rollups required by the prefill manifest: totals per taxpayer, totals per withholding type, counts, and integrity checks (row count vs. declared). Use <code>Decimal</code> accumulation and explicit rounding at final step only. Return a <code>MonthlySummary</code> that becomes part of the manifest and is embedded in the XML header. <br><br><strong>Determinism</strong>: preserve stable ordering when converting aggregates to lists (sort keys lexicographically). <br><br><strong>Edge cases</strong>: empty row set → zeroed summary object. Include <code>computed_checksum</code> over the canonical row sequence when possible. </td></tr><tr><td data-label="Technical Breakdown — src/exporters/spt_masa_prefill.py"><strong><code>render_prefill_xml(prefill_bundle: PrefillBundle, template_loader: TemplateLoader, xml_options: XmlOptions) -&gt; bytes</code></strong><br><br><strong>Responsibility</strong>: render the structured <code>PrefillBundle</code> into the required XML format for SPT Masa. Prefer streaming templating (e.g., generator that writes pieces) to avoid holding large strings. Keep rendering deterministic: stable element ordering, explicit namespace declarations, and canonical whitespace rules. <br><br><strong>Inputs</strong>: <code>PrefillBundle</code> (header/meta, summary, rows), template reference. <br><br><strong>Outputs</strong>: <code>bytes</code> encoded in UTF-8. Use <code>utf-8</code> and normalize newline to <code>\n</code>. <br><br><strong>Performance</strong>: for large row sets, generate XML incrementally and write directly to a signed/hashed writer when possible (see <code>compute_checksums</code>). <br><br><strong>Validation</strong>: minimal structural checks; rely on <code>validate_prefill_xml</code> for schema constraints. <br><br><strong>Testing</strong>: golden fixtures asserting canonical XML for representative <code>PrefillBundle</code> inputs (small and large). Check stable output across invocations. </td></tr><tr><td data-label="Technical Breakdown — src/exporters/spt_masa_prefill.py"><strong><code>validate_prefill_xml(xml_bytes: bytes, schema: Schema, validator: XmlValidator) -&gt; ValidationResult</code></strong><br><br><strong>Responsibility</strong>: run XSD/schema validation and business-level assertions that are easier to express against the produced XML (e.g., header period matches rows, totals match aggregated sums). Return a <code>ValidationResult</code> object with categorized <code>errors</code> and <code>warnings</code>. Do not mutate the bytes. <br><br><strong>Failure semantics</strong>: schema errors must be returned as fatal in <code>ValidationResult</code> (or raise <code>SchemaValidationError</code>) according to <code>ctx.settings.strict</code>. Business-level mismatches can be <code>Warning</code>s by default. Provide <code>explain()</code> for human-readable diagnostics. <br><br><strong>Testing</strong>: include tests that feed invalid XML and assert precise error messages and line/column info where possible. </td></tr><tr><td data-label="Technical Breakdown — src/exporters/spt_masa_prefill.py"><strong><code>sign_prefill(xml_bytes: bytes, signer: SignerAdapter, signer_ref: str, signing_policy: SigningPolicy) -&gt; SignedPackage</code></strong><br><br><strong>Responsibility</strong>: produce a signed package: attached signature (detached/embedded per policy), signer metadata (key id, algorithm), and optionally a signature manifest compatible with downstream verifier. This function must NOT load private key material directly; it calls the <code>SignerAdapter</code> which performs signing within a secure boundary (HSM, KMS, or local key with strict ACL). <br><br><strong>Contracts</strong>: signing is side-effecting but controlled: <code>SignerAdapter.sign(bytes, algorithm)</code> returns signature bytes and signer metadata. The function returns <code>(signed_bytes, signature_meta)</code>; the original <code>xml_bytes</code> must remain intact. <br><br><strong>Failure & audit</strong>: if signer fails, raise <code>SigningError</code> with clear retryability flag. Log only non-sensitive signer metadata (key id, algorithm) — never private material. Record <code>signature_timestamp</code> (UTC ISO8601) using injected time provider. <br><br><strong>Testing</strong>: provide a deterministic fake <code>SignerAdapter</code> to verify that signature metadata and the returned signed package structure are correct. </td></tr><tr><td data-label="Technical Breakdown — src/exporters/spt_masa_prefill.py"><strong><code>create_prefill_manifest(prefill_meta: PrefillMeta, checksums: Dict[str,str], signature_meta: Optional[SignatureMeta], runtime_meta: RuntimeMeta) -&gt; Manifest</code></strong><br><br><strong>Responsibility</strong>: assemble the manifest that accompanies the bundle (applied_rule_version, generator_version, timestamp, checksums, signature reference, file sizes, counts). Manifest must include <code>trace_id</code>, <code>source_snapshot_id</code>, <code>producer_service</code> and <code>applied_rule_version</code>. Produce stable JSON (sorted keys) and include a <code>manifest_version</code> field. <br><br><strong>Security & privacy</strong>: Do not include raw PII in manifest's free-text fields; if message fields require taxpayer identifiers for reconciliation, include hashed forms and record the hash algorithm. <br><br><strong>Testing</strong>: assert manifest deterministic ordering and that checksums/signature refs are present and correct. </td></tr><tr><td data-label="Technical Breakdown — src/exporters/spt_masa_prefill.py"><strong><code>persist_prefill_artifact(store: ObjectStoreAdapter, path: str, blob_iter: Iterable[bytes] | bytes, metadata: dict, idempotency_key: str) -&gt; StorageResult</code></strong><br><br><strong>Responsibility</strong>: write artifact to object store in an idempotent, atomic way: write to a staging key then rename/commit to the final path (or use provider atomic put semantics). Include metadata headers (content-type, content-length, checksums). Use <code>idempotency_key</code> to detect duplicate writes (store must support conditional put or underlying DB lookup). Return <code>StorageResult</code> containing final URI, size, and server-side checksum if available. <br><br><strong>Failure & retries</strong>: transient errors surfaced as <code>StorageTransientError</code> with recommended retries; duplicate write should resolve to existing <code>StorageResult</code> without error. If atomic rename is not supported, emulate with conditional put and a two-phase commit recorded in metadata table. <br><br><strong>Testing</strong>: use fake object store that simulates race conditions and returns consistent <code>StorageResult</code> for repeated idempotent calls. </td></tr><tr><td data-label="Technical Breakdown — src/exporters/spt_masa_prefill.py"><strong><code>build_export_bundle(prefill_storage_results: List[StorageResult], manifest: Manifest, extra_files: Optional[List[StorageResult]]) -&gt; ExportBundle</code></strong><br><br><strong>Responsibility</strong>: assemble the final export bundle representation used by the rest of the pipeline (URIs, manifest, applied_rule_version and publisher metadata). Do not perform further I/O; this is a pure assembly function. Include <code>bundle_signature</code> (if separate) and the <code>bundle_checksum</code> derived by hashing all artifact checksums in a stable order. <br><br><strong>Testing</strong>: unit tests assert bundle checksum is stable and ordering deterministic. </td></tr><tr><td data-label="Technical Breakdown — src/exporters/spt_masa_prefill.py"><strong><code>prepare_response_for_uploader(bundle: ExportBundle, ctx: ExportContext) -&gt; ExportResult</code></strong><br><br><strong>Responsibility</strong>: produce the <code>ExportResult</code> object consumed by the orchestrator and caller: includes artifact URIs, manifest URI, status, timestamps, and minimal exposure of PII (hashes only). Also populate <code>observability</code> fields: metrics tags, trace_id, and sampling hint for upload notifications. No network I/O here. <br><br><strong>Security</strong>: redact sensitive fields before returning to callers that are not authorized. Implement <code>to_safe_dict()</code> for CLI/debug output. </td></tr><tr><td data-label="Technical Breakdown — src/exporters/spt_masa_prefill.py"><strong><code>sanitize_for_logs(obj: Any, redaction_rules: RedactionConfig) -&gt; dict</code></strong><br><br><strong>Responsibility</strong>: produce a shallow dictionary for structured logs with PII redacted (NPWP, names, account numbers). Must be fast and avoid expensive deep-copy for large objects — operate on a shallow projection of the object. Centralize redaction rules and ensure they match logging infrastructure. <br><br><strong>Testing</strong>: tests that attempt to log objects containing PII and assert redaction placeholders. </td></tr><tr><td data-label="Technical Breakdown — src/exporters/spt_masa_prefill.py"><strong><code>expose_metrics(hooks: MetricsAdapter, stage: str, values: dict) -&gt; None</code></strong><br><br><strong>Responsibility</strong>: emit structured metric events for each major stage (render_ms, sign_ms, persist_bytes, rows_count). Keep hooks optional and non-blocking — failures in metrics must never abort the export. Use sampling for large exports to avoid metric storms. <br><br><strong>Testing</strong>: ensure metrics calls are non-blocking and that a misbehaving adapter does not propagate exceptions. </td></tr><tr><td data-label="Technical Breakdown — src/exporters/spt_masa_prefill.py"><strong>Implementation patterns & guardrails</strong><br><br>— <strong>Dependency injection</strong>: all I/O (SnapshotLoader, ObjectStoreAdapter, SignerAdapter, TemplateLoader, MetricsAdapter) must be injected from caller; no top-level imports of cloud SDKs. <br>— <strong>Pure transformations</strong>: mapping and aggregation functions must be pure and deterministic → easy to unit test. <br>— <strong>Streaming</strong>: prefer generators for snapshot reading and XML rendering. <br>— <strong>Decimal math</strong>: use <code>decimal.Decimal</code> with controlled context and explicit rounding only at the final serialization point. Document rounding policy at the head of the file (e.g., <code>ROUND_HALF_EVEN</code>, 2 decimal places). <br>— <strong>Idempotency</strong>: compute content-hash (SHA256) for final XML/signed bytes and use as idempotency key for storage. <br>— <strong>Secrets</strong>: never log private key IDs beyond a short identifier; never expose private key material; signer must be an adapter that performs signing in secure environment (KMS/HSM). <br>— <strong>Observability</strong>: attach <code>trace_id</code> to all calls and include it in manifest. Emit timing metrics for each major stage. <br>— <strong>Fail-fast vs degraded</strong>: obey <code>ctx.settings.strict</code>. When <code>strict=True</code>, validation/signing/storage failures should raise and cause export to fail. When <code>strict=False</code>, record degraded-state in manifest and continue with best-effort outputs where business allows. <br>— <strong>Logging</strong>: structured JSON logs only; sampling when logging large row sets; apply <code>sanitize_for_logs</code> before logging. <br>— <strong>Signature policy</strong>: prefer detached XML digital signatures if external verifiers expect them; support HMAC-based bundle signatures for internal integrity where private-key signing is unavailable. </td></tr><tr><td data-label="Technical Breakdown — src/exporters/spt_masa_prefill.py"><strong>Recommended tests & CI checks</strong><br><br>1. <strong>Unit</strong>: each transformation function with golden fixtures (mapping, aggregation, manifest creation, checksum computation). <br>2. <strong>Integration (fast)</strong>: in-process run of <code>make_spt_masa_prefill</code> with fake adapters (file-backed object store, deterministic signer)) asserting returned <code>ExportResult</code> and stored artifacts. <br>3. <strong>Schema</strong>: validate produced XML against the authoritative XSD for SPT Masa (golden test). <br>4. <strong>Idempotency</strong>: repeated run with same <code>ExportContext</code> produces no duplicate artifacts and identical URIs when idempotency enabled. <br>5. <strong>Security</strong>: tests that ensure PII does not appear in logs or manifest fields, and private key material never leaves signer adapter. <br>6. <strong>Performance smoke</strong>: streaming render of a large synthetic snapshot to ensure memory under target (e.g., < 100MB for 1M rows). <br>7. <strong>Chaos</strong>: signer or object store transient failure tests to assert correct retry semantics and clear error classes. <br><br>Automate tests in CI gating; include a <code>--golden</code> CI job that replays canonical fixtures to detect accidental changes in serialization. </td></tr><tr><td data-label="Technical Breakdown — src/exporters/spt_masa_prefill.py"><strong>Operational & maintenance notes</strong><br><br>— When changing serialization (XML element names, ordering), increment <code>manifest.manifest_version</code> and record migration notes. <br>— When adding fields to the prefill schema, keep backward-compatible serialization and include a <code>preview</code> mode in <code>make_spt_masa_prefill</code> that emits a <code>diff_report</code> vs prior generator version. <br>— Keep the file small: complex helper logic belongs in <code>helpers/</code> modules (templating, mapping rules, signing helpers). <br>— Document at the file head: expected schema version, rounding rules, required environment secrets for signer, and the <code>Settings</code> flags that alter behavior (<code>strict</code>, <code>dry_run</code>, <code>signing_policy</code>). <br><br><strong>Verification note</strong>: design reviewed and logically validated across the above functions and edge cases (determinism, idempotency, streaming, signing, checksums, privacy) — recommended step: pair with repository-specific fixtures (small snapshot, expected XML, expected manifest) and run integration harness before promoting to production. </td></tr></tbody></table></div><div class="row-count">Rows: 18</div></div><div class="table-caption" id="Table7" data-table="Docu_0163_07" style="margin-top:2mm;margin-left:3mm;"><strong>Table 7</strong></div>
<div class="table-wrapper" data-table-id="table-7"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown — manifest.py"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown — manifest.py</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — manifest.py"> <strong>File-level responsibilities</strong><br><br>This module is the single authoritative implementation responsible for creating, canonicalising, signing, verifying, persisting, and publishing exporter manifests for bundles produced by the export pipeline. It must codify the manifest wire-format, canonical JSON rules, checksum algorithms, signature algorithm policy, manifest versioning, and atomic publish semantics. At import time the module must be side-effect free: no network, no file or key material access, no background threads. All I/O, key access, and storage operations are performed by explicit, injected collaborators (storage client, signer/verifier factories, settings). The file must expose small, testable pure functions (canonicalization, hashing, fingerprinting) and small orchestrators that compose them. Document the manifest schema versioning strategy clearly and include an interoperability test vector. Unit-test every pure function and add integration tests for storage and cryptographic integrations. </td></tr><tr><td data-label="Technical Breakdown — manifest.py"> <strong><code>MANIFEST_SCHEMA: dict</code></strong><br><br>A single, versioned JSON Schema-like constant that defines the contract for manifests. It enumerates required top-level fields (<code>manifest_version</code>, <code>bundle_id</code>, <code>applied_rule_version</code>, <code>generator_version</code>, <code>generated_at</code>, <code>objects</code>, <code>bundle_checksum</code>, <code>signatures</code>, <code>metadata</code>) and per-object requirements (<code>path</code>, <code>size</code>, <code>checksum</code>, <code>hash_alg</code>). This schema is the authoritative validation source used by <code>validate_manifest()</code> (and by CI). The schema must be conservative: prefer additive changes; include <code>manifest_version</code> (major.minor) and an explicit compatibility policy comment. Keep a short changelog near the schema constant with each bump rationale. Unit tests must include golden valid manifests and a suite of invalid manifests to exercise each rule and boundary case. </td></tr><tr><td data-label="Technical Breakdown — manifest.py"> <strong><code>compute_file_checksum(path_or_stream, *, alg=&quot;sha256&quot;, chunk_size=8*1024*1024) -&gt; str</code></strong><br><br>Pure, stream-oriented hashing utility. Accepts either a path (string/Path) or a file-like readable stream and returns a stable digest string (recommended format <code>sha256:&lt;hex&gt;</code>). Requirements: memory-bounded (chunked reads), deterministic across platforms, and algorithm-configurable but restricted to a vetted whitelist (<code>sha256</code>, <code>sha512</code>, <code>sha3_256</code>). Should not seek gratuitously on non-seekable streams. Raise a well-typed IO-related error for unreadable sources so callers can handle retries. Unit tests: tiny file, huge stream, non-seekable stream, interrupted stream simulation, algorithm switching. Document tradeoffs for chunk size and CPU vs IO-bound hashing. </td></tr><tr><td data-label="Technical Breakdown — manifest.py"> <strong><code>compute_manifest_checksums(object_entries, storage_client=None, *, alg=&quot;sha256&quot;, concurrency=4) -&gt; List[dict]</code></strong><br><br>Given an iterable of object descriptors (local paths or storage keys), return a stable, ordered list of object records: <code>{path, size, checksum, hash_alg}</code> sorted by <code>path</code>. For local files call <code>compute_file_checksum</code>; for remote objects stream from <code>storage_client.stream(key)</code>. The function must be idempotent and deterministic (sorting ensures canonical order). Provide a configurable concurrency knob for hashing many small files but default to conservative parallelism to avoid throttling cloud stores. Expose a <code>progress_callback</code> hook for long operations. Tests: concurrency behavior, storage_client failure modes (retries/backoff), ordering invariants. </td></tr><tr><td data-label="Technical Breakdown — manifest.py"> <strong><code>canonicalize_manifest(manifest_dict, *, schema_field=&quot;manifest_version&quot;) -&gt; bytes</code></strong><br><br>Pure function that deterministically converts a manifest dict into canonical JSON bytes used for signing and fingerprinting. Rules must be explicit and stable: recursive key ordering (sorted keys at each object level), stable ordering for <code>objects</code> (sorted by <code>path</code>), normalized timestamps (strict UTC ISO8601 with millisecond precision <code>YYYY-MM-DDTHH:MM:SS.mmmZ</code>), canonical number formatting (no insignificant trailing zeros), and no insignificant whitespace. Document the chosen canonicalization algorithm (a simplified RFC8785-style approach) and include a canonicalization test vector. Unit tests must assert that semantically-equal manifests (different insertion orders or incidental metadata ordering) produce identical bytes. </td></tr><tr><td data-label="Technical Breakdown — manifest.py"> <strong><code>manifest_fingerprint(manifest_bytes, *, alg=&quot;sha256&quot;) -&gt; str</code></strong><br><br>Deterministic wrapper returning a content-addressable fingerprint of canonicalized manifest bytes (format <code>sha256:&lt;hex&gt;</code>). This fingerprint is used as the idempotency key and short manifest identifier (<code>manifest_id</code>). Keep the algorithm consistent with file/object checksum choices. Persist fingerprints in metadata stores to detect duplicate publishes. Tests: stable across formatting differences, collisions considered highly unlikely. </td></tr><tr><td data-label="Technical Breakdown — manifest.py"> <strong><code>build_manifest(bundle_id, objects: List[dict], applied_rule_version: str, generator_version: str, metadata: Optional[dict]=None, *, generated_at: Optional[datetime]=None, manifest_version: str=&quot;1.0&quot;) -&gt; dict</code></strong><br><br>Pure manifest assembly function. Validate inputs (types, required keys in <code>objects</code>), set <code>generated_at</code> to UTC now when not supplied, compute <code>bundle_checksum</code> using <code>bundle_checksum_from_objects</code>, set <code>manifest_version</code>, and return a manifest dict ready for canonicalization and signing. Avoid I/O; do not invoke storage or signing here. Provide clear error types for missing/invalid inputs. Unit tests: missing required fields, invalid object entries, metadata edge cases. </td></tr><tr><td data-label="Technical Breakdown — manifest.py"> <strong><code>bundle_checksum_from_objects(objects, *, alg=&quot;sha256&quot;, algorithm=&quot;concat&quot;) -&gt; str</code></strong><br><br>Produce a single bundle-level checksum proving the set and content of objects. Provide at least two algorithm choices with documented tradeoffs: <code>concat</code> (default) — sort objects by <code>path</code>, produce a digest over the concatenation of <code>path||size||checksum</code> lines; <code>merkle</code> — produce a Merkle root over object checksum bytes to enable partial verification. The function must be deterministic and stable. Document risk and performance characteristics (merkle supports partial proofs; concat is simpler and interoperable). Tests should assert invariants across ordering and algorithm selection, and provide sample merkle proof validation tests if merkle is supported. </td></tr><tr><td data-label="Technical Breakdown — manifest.py"> <strong><code>sign_manifest(manifest_bytes, signer: Callable[[bytes], bytes], *, key_id: str, alg: str) -&gt; dict</code></strong><br><br>Stateless helper that produces a signature record for the canonical manifest bytes. The <code>signer</code> callable is injected — it encapsulates key access and KMS interaction and must be called lazily (never load private keys at import time). The function returns a signature entry <code>{key_id, alg, sig_b64url}</code> ready to be appended to <code>manifest[&quot;signatures&quot;]</code>. Enforce explicit algorithm whitelist (Ed25519, RSA-PSS-SHA256, HMAC-SHA256 with explicit mode). Security guidance: prefer asymmetric signatures; mark HMAC signatures clearly in the metadata. Tests: roundtrip sign+verify using test signer/verifier; invalid signer returns clear errors; signing large manifests handled correctly. </td></tr><tr><td data-label="Technical Breakdown — manifest.py"> <strong><code>verify_manifest_signature(manifest_bytes, signatures, verifier_factory, *, policy=None) -&gt; List[dict]</code></strong><br><br>Verify the provided signature entries using injected verifier functions (resolved by <code>key_id</code>). Return structured verification results: list of <code>{key_id, alg, ok, error}</code>. <code>policy</code> can express acceptance rules: single valid signature required, threshold, or named roles required (<code>generator</code>, <code>auditor</code>). Support revoked-key checks and allow callers to supply a trusted keyset. Do not fetch keys at import time; resolve via <code>verifier_factory(key_id)</code>. Tests: single valid signature, missing key, revoked key, signature tampering. </td></tr><tr><td data-label="Technical Breakdown — manifest.py"> <strong><code>validate_manifest(manifest_dict, *, schema=MANIFEST_SCHEMA, strict=True) -&gt; Tuple[bool, List[str]]</code></strong><br><br>Run JSON Schema validation followed by semantic checks: timestamp parseability, object checksums syntactic form <code>alg:hex</code>, non-negative sizes, recomputed <code>bundle_checksum</code> matches stored <code>bundle_checksum</code>, presence and shape of <code>signatures</code> when <code>strict</code> is true. Return <code>(ok, errors)</code> where <code>errors</code> are human-readable diagnostics. In <code>strict=False</code> mode convert certain failures into warnings (return ok=False only for critical invariants). Tests: mismatched bundle checksum, corrupted object entries, missing signatures under strict mode. </td></tr><tr><td data-label="Technical Breakdown — manifest.py"> <strong><code>write_manifest_atomic(manifest_dict, staging_path, final_path, storage_client, *, content_type=&quot;application/json&quot;, acl=None, retry_policy=None)</code></strong><br><br>Persist a signed manifest to object storage atomically. Pattern: canonicalize → compute fingerprint → write to a unique staging path (using fingerprint and timestamp), verify written bytes checksum, then perform an atomic move/rename/copy-to-prefix to <code>final_path</code>. If the storage backend lacks atomic rename, implement safe publish semantics (write to new prefix then update a pointer/manifest index in a single metadata update). Avoid deleting existing final_path on failure. Ensure idempotency: if <code>final_path</code> already exists with the same fingerprint, the call is a no-op. Emit structured events to observability hooks. Tests: interrupted write, rename/copy failure, concurrent publishers racing to same <code>final_path</code>. </td></tr><tr><td data-label="Technical Breakdown — manifest.py"> <strong><code>load_manifest(path, storage_client, *, validate=True) -&gt; dict</code></strong><br><br>Read and parse manifest JSON from storage. Return parsed dict and attach read provenance metadata (read_ts, etag/obj-version). By default run <code>validate_manifest</code>; allow <code>validate=False</code> for fast read-only callers. Provide clear error types: <code>ManifestNotFound</code>, <code>ManifestParseError</code> (with context), <code>ManifestValidationError</code>. Tests: large/huge manifest reads, invalid encoding, missing/extra fields. </td></tr><tr><td data-label="Technical Breakdown — manifest.py"> <strong><code>verify_bundle_integrity(manifest_dict, storage_client, *, verifier_policy=None, parallelism=4, sample_rate: Optional[float]=None, timeout_per_object=None)</code></strong><br><br>End-to-end bundle verification routine for reconcilers and consumers. Steps: 1) verify manifest signatures per <code>verifier_policy</code>, 2) verify <code>bundle_checksum</code> computed from <code>objects</code>, 3) verify per-object checksum by streaming each object and comparing checksums. Support sampling mode (<code>sample_rate</code> in (0,1]) for very large bundles with documented risk. Return a structured report <code>{ok, signatures: [...], bundle_ok, object_results: [{path, ok, expected, actual, error}], metrics}</code>. Implementation must be stream-friendly, bounded-concurrency, and resilient to transient storage failures (with retry/backoff policy). Tests: full verification success path, sampling behavior, network/storage timeouts and partial failures. </td></tr><tr><td data-label="Technical Breakdown — manifest.py"> <strong><code>manifest_cli_main(argv=None) -&gt; int</code></strong><br><br>Small, safe CLI entrypoint for operators. Subcommands: <code>build</code> (assemble manifest from objects list), <code>canonicalize</code> (emit canonical bytes to stdout), <code>sign</code> (call signer factory to add signature), <code>verify</code> (validate manifest and optionally full bundle), <code>publish</code> (write_atomic). CLI must not load private keys unless explicitly allowed (<code>--use-kms</code> with explicit consent). Always support <code>--dry-run</code> and <code>--output</code> to avoid accidental publishes. Tests: CLI exit codes, dry-run no network I/O (by injecting fakes), argument parsing edge cases. Provide machine-friendly JSON output mode for automation. </td></tr><tr><td data-label="Technical Breakdown — manifest.py"> <strong><code>helpers: key_management_abstraction(), signer_factory(config), verifier_factory(config)</code></strong><br><br>Small adapter helpers that encapsulate key lookup and crypto provider integration. Must be thin: return callables that perform signing/verifying and fetch key material lazily (KMS calls only when invoked). Provide two built-in adapters for tests: <code>InsecureLocalKey</code> (file-backed, only for CI/dev) and <code>StubSigner</code>/<code>StubVerifier</code> for unit tests. Document integration with cloud KMS (example mapping: key_id -> KMS resource name) and rotation strategies. Enforce caching TTLs and safe error semantics on key fetch failures. Tests: KMS failure simulation, cache TTL behavior, local dev-key path gating. </td></tr><tr><td data-label="Technical Breakdown — manifest.py"> <strong>Operational concerns & guardrails</strong><br><br>— <strong>Secrets</strong>: private keys must not be present in plaintext in repo or logs; signer must be implemented via <code>SecretRef</code>/KMS patterns. <br>— <strong>Atomic publish</strong>: always stage then swap/rename; if rename is not supported by backend, publish via prefix swap and an atomic metadata pointer. <br>— <strong>Idempotency</strong>: rely on <code>manifest_fingerprint</code> for idempotency keys; repeated publish of identical manifest must not create duplicate records. <br>— <strong>Performance</strong>: stream hashes, bounded parallelism, and sampling options for very large bundles; provide <code>progress_callback</code> and metrics. <br>— <strong>Security</strong>: enforce signature algorithm whitelist and minimum key sizes; redact keys and sensitive stack traces from logs. <br>— <strong>Observability</strong>: emit metrics (<code>manifest.build.duration</code>, <code>manifest.sign.duration</code>, <code>manifest.publish.*</code>, <code>manifest.verify.*</code>) and structured audit events containing <code>trace_id</code>, <code>bundle_id</code>, <code>applied_rule_version</code>, and <code>actor</code>. <br>— <strong>Error handling</strong>: implement retry with exponential backoff for transient storage errors and fail-fast for critical signing/key errors under <code>strict</code> settings. </td></tr><tr><td data-label="Technical Breakdown — manifest.py"> <strong>Testing & CI</strong><br><br>Unit tests for canonicalization, checksums, fingerprinting, bundle checksum algorithms, signature record shape, and validation. Crypto tests: sign/verify roundtrips using test keys and stubbed KMS. Integration tests: publish-to-staging and atomic rename against an object-storage emulator (MinIO or local filesystem abstraction). E2E tests: worker produces objects → build manifest → sign → publish → consumer verifies. Property tests: canonicalization idempotence across many permutations; interoperability vector for third-party consumers (manifest bytes + expected fingerprint + expected signature). Security tests: assert no plaintext keys in logs or artifacts. Add <code>manifest-smoke</code> CI job that runs the full path with ephemeral storage and test keys. </td></tr><tr><td data-label="Technical Breakdown — manifest.py"> <strong>Backward/forward compatibility & migration strategy</strong><br><br>Use <code>manifest_version</code> as the definitive compatibility marker. Follow semver-like rules: minor bumps are additive and backwards-compatible; major bumps are breaking. Consumers must ignore unknown top-level fields. When introducing incompatible changes provide a migration adapter or transform path in code (e.g., <code>upgrade_manifest_v1_to_v2(manifest)</code>). Keep <code>generator_version</code> in manifest to allow reproducers to select correct verification behavior. Tests must include older-version manifests to ensure verify logic can opt-in to legacy validation when necessary. </td></tr><tr><td data-label="Technical Breakdown — manifest.py"> <strong>Integrations & handoffs</strong><br><br>— <strong>Recorder/Audit</strong>: after successful publish, write an append-only audit entry with <code>manifest_fingerprint</code>, <code>bundle_id</code>, signer <code>key_id</code>, <code>applied_rule_version</code>, object counts and sizes, and the actor identity. <br>— <strong>Reconcile</strong>: expose <code>verify_bundle_integrity()</code> for reconciliation jobs to assert published vs expected state. <br>— <strong>Exporters/Consumers</strong>: exporters (e.g., <code>bukti_potong_csv</code>, <code>spt_masa_prefill</code>) must call <code>build_manifest()</code> → <code>canonicalize_manifest()</code> → <code>sign_manifest()</code> → <code>write_manifest_atomic()</code> and persist the manifest fingerprint in metadata DB. Provide a small consumer helper to download+verify before import. </td></tr><tr><td data-label="Technical Breakdown — manifest.py"> <strong>Documentation & developer notes</strong><br><br>Include a short <code>manifest_spec.md</code> alongside the code with: canonicalization rules (example bytes), manifest JSON example (annotated), fingerprint example, signing sequence, key rotation runbook, and operational runbook for failed publish and key compromise. Provide a small interoperability test vector (manifest JSON + canonical bytes + expected fingerprint + signature produced by a test key) to be used by other-language implementers. When modifying schema, append a changelog entry and add migration helpers. </td></tr><tr><td data-label="Technical Breakdown — manifest.py"> <strong>Recommended tests & CI checks</strong><br><br>1. Unit: canonicalize, compute checksums, fingerprint stability. <br>2. Crypto: sign/verify with test keys and stubs. <br>3. Integration: publish/publish-race/atomic-rename in storage emulator. <br>4. E2E: worker → manifest → sign → publish → verify. <br>5. Chaos: interrupted writes, storage flakiness, partial verification failures. <br>6. Security: secret-scan, ensure no key material in logs or artifacts. <br>7. Compatibility: canonicalization parity across languages and schema-version compatibility tests. </td></tr><tr><td data-label="Technical Breakdown — manifest.py"> <strong>Operational checklist before production</strong><br><br>— Ensure production signer uses a KMS-backed signer and <code>signer_factory</code> is configured accordingly. <br>— Set <code>manifest_version</code> policy and validate consumers tolerate it. <br>— Configure storage ACLs and public/private access policy for manifests and objects. <br>— Enable reconcile full-verification jobs (or a robust sampling policy) and configure alerts for verification failures. <br>— Prepare runbook for failed publish, staged cleanup, and key rotation/compromise handling. </td></tr></tbody></table></div><div class="row-count">Rows: 23</div></div><div class="table-caption" id="Table8" data-table="Docu_0163_08" style="margin-top:2mm;margin-left:3mm;"><strong>Table 8</strong></div>
<div class="table-wrapper" data-table-id="table-8"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown — recorder.py"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown — recorder.py</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — recorder.py"> <strong>File-level responsibilities</strong><br><br>This module provides a robust, append-only audit recorder used by the calculation pipeline to persist immutable audit events (JSONL) and to offer replayable, queryable access to those events. It is the single source-of-truth for audit durability, ordering, and basic tamper-evidence for pipeline runs. Keep the module focused: serialization/validation/signing/atomic-write semantics, rotation/archival, read/replay helpers, light indexing for lookups, and strictly-limited helpers for compact/repair. Do not embed heavy external dependencies at import time; expose factory functions so tests can inject file-system, object-store, signing, and crypto fakes. Expect both synchronous and async callers — the module should provide a clear sync API and an async adapter if the runtime needs it. Document configuration knobs at the top (path, max_segment_size, fsync_on_write, signer, encryptor, retention_policy, index_enabled, concurrency_model). Unit tests should treat this module as the contract boundary for audit storage. </td></tr><tr><td data-label="Technical Breakdown — recorder.py"> <strong><code>Recorder</code> (class)</strong><br><br>Primary abstraction exposing durable append and read operations. Responsibilities: manage the active write stream (segment), enforce append-only invariants, maintain a tiny index (optional) mapping idempotency keys/job_ids → segment offsets, provide atomic rotation and archival, support idempotent append semantics, and expose replay/scan primitives.  Construction must be side-effect free (no open file) — <code>open()</code> or <code>start()</code> performs IO. The class must be explicitly closeable (<code>close()</code>/<code>__aexit__</code>) and idempotent in close. Internals: private thread-safe/async-safe write queue, a file-lock abstraction for multi-process protection (where required), and a pluggable persistence backend (local FS, S3-like object-store via staged uploads). Avoid implicit background threads unless created by an explicit <code>start_background_workers()</code> factory and stop them on <code>close()</code>. Unit tests should instantiate Recorder with a fake backend and assert every public method's contract. </td></tr><tr><td data-label="Technical Breakdown — recorder.py"> <strong><code>Recorder.__init__(path: str, *, signer=None, encryptor=None, max_segment_size=int, fsync_on_write: bool=False, index_enabled: bool=True, retention_policy=None, strict: bool=True)</code></strong><br><br>Constructor must only store configuration and create in-memory helpers. Do not open files or perform network I/O. Validate config shapes and provide helpful error messages for badly-typed arguments. <code>signer</code> and <code>encryptor</code> must be simple callables/interfaces (e.g. <code>sign(bytes) -&gt; signature</code> and <code>encrypt(bytes) -&gt; bytes</code>) — accept <code>None</code> for no-op implementations. <code>max_segment_size</code> controls rotation; <code>fsync_on_write</code> is conservative default <code>False</code> (configurable to True for highest durability). <code>strict</code> controls whether missing signer/encryptor when required should raise at <code>open()</code> or be tolerated with a warning. Tests: passing invalid paths, negative sizes, or wrong-type signers should raise <code>ValueError</code>. </td></tr><tr><td data-label="Technical Breakdown — recorder.py"> <strong><code>open() / start()</code></strong><br><br>Open the current segment for append and initialize runtime state (active segment file handle, current offset, index load, segment metadata). This is when the module should acquire the file-level lock (if using file locks) and validate the directory exists. Behavior: if the active segment exists, open in append mode and compute current offset; if not, create a new segment with a stable name that encodes start timestamp and monotonically-incremented sequence. Must detect if previous shutdown was unclean and set <code>recorder.state = &quot;degraded&quot;</code> but still allow reads; if <code>strict</code> is true, raise on unclean shutdown. Unit tests: simulate pre-existing segment files and validate offset calculation. </td></tr><tr><td data-label="Technical Breakdown — recorder.py"> <strong><code>append(entry: Dict[str, Any], *, idempotency_key: Optional[str]=None) -&gt; AuditRecordRef</code></strong><br><br>Canonical append API. Contract: accepts a validated <code>entry</code> (plain Python types serializable to JSON) and returns a stable reference <code>{segment, offset, length, entry_hash}</code> on success. Guarantees: atomic append (line written completely or not at all), durable as-per-configuration (fsync on write if <code>fsync_on_write</code>), idempotent when caller supplies <code>idempotency_key</code> — duplicate key returns identical <code>AuditRecordRef</code> and does not double-write. Implementation notes: 1) validate entry schema with <code>_validate_schema</code>, 2) attach required metadata (<code>timestamp</code> RFC3339, <code>service</code>, <code>trace_id</code>, <code>job_id</code>, <code>applied_rule_version</code>, <code>runtime_fingerprint</code> etc.) <em>without</em> mutating caller's object (copy-on-write), 3) compute canonical serialization with stable key ordering and <code>ensure_ascii=False</code>, 4) compute <code>entry_hash = sha256(serialized)</code>, 5) optionally sign and/or encrypt the persisted bytes, 6) call <code>_atomic_write(serialized_line)</code> under a write lock, 7) update in-memory index (if enabled) and metrics. Errors: raise <code>ValidationError</code>, <code>RecorderError</code> (IO), or <code>IdempotencyConflict</code> if idempotency database is corrupted. Tests: idempotency key semantics, concurrent appends, failure during write (simulate OSError and assert no partial line persisted). </td></tr><tr><td data-label="Technical Breakdown — recorder.py"> <strong><code>append_batch(entries: Iterable[Dict], *, idempotency_keys: Optional[Iterable[str]]=None, batch_size: int=1000) -&gt; List[AuditRecordRef]</code></strong><br><br>Bulk append optimizing throughput. Should use the same per-entry pipeline as <code>append</code> (validate → serialize → sign/encrypt → atomic write) but can buffer and issue fewer fsyncs. Guarantees: per-entry atomicity and idempotency; partial failure semantics must be explicit: by default operate in best-effort mode (return list of successes and failures) and offer a <code>strict=True</code> mode to rollback (dangerous — implement only if underlying backend supports transactional semantics). Batch ordering must be preserved. Tests: large batches to assert performance, partial failures handling, memory usage under large batched payloads. </td></tr><tr><td data-label="Technical Breakdown — recorder.py"> <strong><code>_validate_schema(entry: Dict) -&gt; None</code></strong><br><br>Pure function enforcing the audit event contract. Must be fast and strict: verify top-level fields or their allowed absence (e.g., <code>event_type: str</code>, <code>ts: optional</code>, <code>payload: dict</code>) and assert no unexpected binary blobs. Validation should be schema-driven (pydantic or small custom validator) and strictly deterministic. This function should not mutate input. Provide <code>to_safe_dict()</code> helper that returns redacted view for logging. Tests: feed malformed shapes, additional unknown keys, very large payloads, and assert clear <code>ValidationError</code> with <code>error_path</code> hints. </td></tr><tr><td data-label="Technical Breakdown — recorder.py"> <strong><code>_serialize_entry(entry: Dict, *, canonicalize: bool=True) -&gt; bytes</code></strong><br><br>Serialize the entry to a stable bytes line (UTF-8 newline-terminated JSONL). Use deterministic key ordering and canonical formatting to make hashing and diffs stable. Include a lightweight length-prefix policy for optional binary-safe storage (store <code>len\n{json}\n</code> or store JSONL with newline and maintain offsets). Return the bytes that will be written to the file. Do not perform signing/encryption here. Tests: verify two equivalent dicts produce exactly identical output and identical <code>sha256</code>. </td></tr><tr><td data-label="Technical Breakdown — recorder.py"> <strong><code>_sign_and_encrypt(payload: bytes) -&gt; bytes</code></strong><br><br>Optional transformation pipeline that applies signer and then encryptor (or encrypt then sign depending on threat model; document choice). It must be pluggable and side-effect free. Signing should produce a compact metadata envelope attached as JSON metadata line or appended header; encryptor returns the bytes to be persisted. When present, also persist detached signatures in an adjacent index file to allow offline verification. <code>strict</code> controls whether missing signer when <code>signer_required=True</code> raises. Tests: ensure round-trip verification with provided signer/encryptor fakes and guard against double-encoding. </td></tr><tr><td data-label="Technical Breakdown — recorder.py"> <strong><code>_atomic_write(bytes_line: bytes) -&gt; (segment, offset, length)</code></strong><br><br>Perform atomic append to the active segment. Implementation must: obtain a thread/process-safe write lock, seek to end, write bytes, optionally <code>fsync()</code> the file descriptor (configurable), flush buffers, compute start offset and length, and release lock. Must avoid partial-line visibility to readers; if the underlying FS does not guarantee atomic appends, use a write-then-rename segment staging technique: write to temporary file then <code>os.rename</code> to append-merge (complex — prefer platform appends with advisory locking). Always return the segment id/filename and offset. Tests: simulate abrupt process kill during write (where possible) and assert subsequent startup detects and either truncates or marks the segment as suspect. </td></tr><tr><td data-label="Technical Breakdown — recorder.py"> <strong><code>rotate(reason: Optional[str]=None) -&gt; SegmentMeta</code></strong><br><br>Trigger manual rotation of the active segment. Responsibilities: close the active segment cleanly, write a segment footer with metadata (counts, checksums, applied_rule_version), record rotation reason in segment manifest, publish an event to observers, and start a new segment. Rotation must be atomic from the client perspective: no writes should be lost and race conditions must be handled by write locks. Rotation is also invoked automatically when <code>max_segment_size</code> is exceeded. Tests: strike rotation under load and assert no lost or corrupted lines, verify segment footers. </td></tr><tr><td data-label="Technical Breakdown — recorder.py"> <strong><code>flush()</code></strong><br><br>Force-perform any in-memory buffered writes to durable storage and update persisted index. This is the user-facing durability hook. Should return a checkpoint object that clients can store for later replay (<code>{segment, offset}</code>). <code>flush()</code> must be idempotent and safe to call frequently. Tests: call flush repeatedly under concurrent load and assert no errors and that checkpoint monotonicity holds. </td></tr><tr><td data-label="Technical Breakdown — recorder.py"> <strong><code>checkpoint() -&gt; Checkpoint</code></strong><br><br>Return a compact, stable checkpoint representing the last-consistent read position. Format: <code>{version:int, segment_id:str, offset:int, checksum:str}</code>. Checkpoints are used by downstream workers to resume processing without reprocessing already-recorded audit entries. The recorder must be able to validate a checkpoint on <code>open()</code> and optionally fast-forward internal indices. Tests: corrupt checkpoint scenario and strict/lenient behavior. </td></tr><tr><td data-label="Technical Breakdown — recorder.py"> <strong><code>replay(from_checkpoint: Optional[Checkpoint]=None, to_checkpoint: Optional[Checkpoint]=None, filter: Optional[Callable]=None) -&gt; Iterator[Dict]</code></strong><br><br>Stream audit entries in order from <code>from_checkpoint</code> (exclusive) to <code>to_checkpoint</code> (inclusive/exclusive — document precisely). Provide both an iterator (sync) and an async generator adapter. Must perform on-the-fly validation of each entry, yield deserialized, <em>unmodified</em> entries, and present metadata (segment, offset, entry_hash). Behavior on encountering a corrupt line: configurable (<code>stop_on_error</code>, <code>skip_corrupt</code>, <code>raise</code>) default to <code>skip_corrupt</code> and emit a warning metric. Offer an optional <code>verify_signatures=True</code> to validate signatures as lines are replayed. Tests: replay while appending new lines concurrently; verify ordering and exactly-once semantics for a consumer using checkpointing. </td></tr><tr><td data-label="Technical Breakdown — recorder.py"> <strong><code>load_range(segment_id: str, offset: int, limit: int=1000) -&gt; List[Dict]</code></strong><br><br>Convenience to read a contiguous block starting at offset. Implement efficient random access by using file seeks and by persisting per-segment offset index (every N lines record offset). If index is missing, fallback to scanning. Must be robust to truncated segments (return partial list and a flag). Tests: large segment with sparse index; ensure correctness and performance. </td></tr><tr><td data-label="Technical Breakdown — recorder.py"> <strong><code>search(predicate: Callable[[Dict], bool], *, limit: int=1000, reverse: bool=False) -&gt; Iterator[Dict]</code></strong><br><br>Best-effort scanning search across segments. Not intended for heavy OLAP queries — recommend external indexing service for heavy use. The function should use the mini-index (if enabled) to accelerate lookups by idempotency_key or job_id; otherwise perform linear scan. Provide a documented complexity note in the docstring. Tests: predicate correctness and performance metrics. </td></tr><tr><td data-label="Technical Breakdown — recorder.py"> <strong><code>verify_integrity(segment_id: Optional[str]=None) -&gt; IntegrityReport</code></strong><br><br>Run end-to-end integrity checks: per-line JSON parse, recompute hash and compare to stored hash (if present), verify signatures (if enabled), and validate segment footer checksums. Return a structured report listing counts, errors, and locations. Implementation: avoid loading entire large segments into memory; stream and sample. Provide <code>fast</code> and <code>full</code> modes. Tests: introduce intentional corruption and assert detection. </td></tr><tr><td data-label="Technical Breakdown — recorder.py"> <strong><code>compact(older_than_ts: datetime, *, threshold_bytes: int=None) -&gt; CompactionResult</code></strong><br><br>Optional compaction to rewrite older segments into compact archives (e.g., compressing, deduplicating attachments, removing non-essential debugging fields) while preserving a full audit trail through the archive index. Compaction must not alter the effective replay semantics; it must produce an archive segment that is replay-equivalent with new offsets and must preserve original checksums and mapping metadata. Prefer offline compaction run by orchestration (do not auto-run in critical hot path). Tests: ensure replay equivalence and mapping from old offsets to new archive offsets. </td></tr><tr><td data-label="Technical Breakdown — recorder.py"> <strong><code>archive_segment(segment_id: str, destination: ObjectStoreBackend) -&gt; ArchiveResult</code></strong><br><br>Stage segment to external object store with strong atomic semantics: upload to a staging location, verify checksum on remote, then promote (rename) to final path. Write archive manifest containing generator version, applied_rule_version, segment checksum, and signer signature. On success, update local metadata to mark <code>archived=True</code>. Tests: simulate partial upload and network interruption and ensure no half-promoted archives. </td></tr><tr><td data-label="Technical Breakdown — recorder.py"> <strong><code>close()</code> / <code>__aexit__</code></strong><br><br>Close gracefully: flush buffers, close file descriptors, stop background workers, release locks, and set state to closed. On error during close, swallow non-fatal exceptions and log them, but provide a <code>CloseError</code> list for diagnostics. Close must be idempotent. Tests: call close concurrently with append/rotate and assert safe termination. </td></tr><tr><td data-label="Technical Breakdown — recorder.py"> <strong><code>_acquire_lock()</code> / <code>_release_lock()</code></strong><br><br>Portable advisory locking abstraction (POSIX <code>fcntl</code> or <code>flock</code>, and a no-op on systems without support), with an optional <code>lockfile</code> fallback. Document process vs thread lock semantics. Prefer a lock implementation that supports timeouts and can be configured to raise quickly in high-contention environments. Tests: multi-process lock contention (use subprocess in integration tests) verifying only one process owns the write lock. </td></tr><tr><td data-label="Technical Breakdown — recorder.py"> <strong><code>get_segment_path(segment_id: str) -&gt; Path</code></strong><br><br>Helper producing canonical on-disk/object-store key names. Segment naming policy should include <code>prefix + yyyyMMddTHHMMSS + seq + .audit</code> to allow lexicographic ordering. Provide functions to parse segment names back into metadata. Tests: ensure round-trip parsing and ordering. </td></tr><tr><td data-label="Technical Breakdown — recorder.py"> <strong><code>make_audit_entry(payload: Dict, *, event_type: str, job_id: str, trace_id: Optional[str]=None, extra_meta: Optional[Dict]=None) -&gt; Dict</code></strong><br><br>High-level helper used by callers to produce a compliant audit dict. It must attach stable metadata (<code>ts</code>, <code>service</code>, <code>version</code>, <code>runtime_fingerprint</code>, <code>applied_rule_version</code>) and normalise fields. This helper simplifies upstream code and centralises the canonical field names. It must not perform persistence. Tests: assert canonical fields are present and immutably typed. </td></tr><tr><td data-label="Technical Breakdown — recorder.py"> <strong><code>subscribe(callback: Callable[[Dict], None], from_checkpoint: Optional[Checkpoint]=None) -&gt; SubscriberHandle</code></strong><br><br>Lightweight wrapper to push events to in-process subscribers for real-time monitoring or sidecars (not a durable pub/sub). Deliver events in order and ensure slow subscribers do not block writes by using bounded buffers and backpressure policies. Provide <code>unsubscribe()</code> and health checks for subscribers. Do not rely on subscribers for durability. Tests: slow subscriber scenarios and backpressure behavior. </td></tr><tr><td data-label="Technical Breakdown — recorder.py"> <strong><code>module-level helpers: ensure_dir(path), atomic_rename(src, dst), compute_hash(bytes) -&gt; str</code></strong><br><br>Small deterministic helpers used across the module. Keep them trivial and well-tested. <code>compute_hash</code> must be stable (sha256 hex). <code>atomic_rename</code> must use platform-appropriate semantics and raise on cross-filesystem moves unless a safe fallback is implemented. Tests: cross-platform behavior (mocking where required). </td></tr><tr><td data-label="Technical Breakdown — recorder.py"> <strong>Observability & metrics</strong><br><br>Expose metrics hooks (counters, histograms, gauges): <code>writes_total</code>, <code>writes_failed</code>, <code>write_latency_ms</code>, <code>current_segment_size_bytes</code>, <code>segments_rotated_total</code>, <code>replay_throughput</code>, <code>integrity_errors_total</code>. Integrate with the project's telemetry exporter via a pluggable <code>metrics</code> client; avoid importing heavy telemetry SDKs at module import time. Emit structured audit-level logs for rotation, archive, verify failures, and critical integrity errors including <code>trace_id</code> and <code>runtime_fingerprint</code>. Provide a <code>dry_run</code> mode to record metrics to an in-memory sink for unit tests. </td></tr><tr><td data-label="Technical Breakdown — recorder.py"> <strong>Error types & semantics</strong><br><br>Define typed exceptions: <code>RecorderError</code> (base), <code>ValidationError</code>, <code>IdempotencyConflict</code>, <code>DurabilityError</code> (fsync/write failures), <code>IntegrityError</code>, <code>CloseError</code>. All raised exceptions should be narrow and documented. Prefer returning structured result objects for partial failures instead of raising for each non-fatal condition. </td></tr><tr><td data-label="Technical Breakdown — recorder.py"> <strong>Concurrency model & process-safety</strong><br><br>Document that Recorder is safe for multi-threaded use within a single process and provides file-lock based mutual exclusion for multi-process writers. For multi-host writers writing to the same object store, recommend leader-election or external coordination — do <em>not</em> guarantee cross-host append atomicity without external coordination. For very high-throughput producers, recommend batching and an intermediary write-ahead queue. Provide configuration knobs for lock timeouts and contention backoff. </td></tr><tr><td data-label="Technical Breakdown — recorder.py"> <strong>Performance characteristics & tuning</strong><br><br>Provide guidance: small writes (hundreds of bytes) favor batching to amortize fsync; for extremely high write rates, route through a local write-ahead log (e.g., lightweight queue) and have a single writer persist to segments. Recommend default <code>max_segment_size</code> (e.g., 256MB–1GB) and <code>index_sampling_interval</code> (e.g., every 512 lines). Document memory vs latency tradeoffs for buffering in <code>append_batch</code>. Include microbenchmarks in repo to establish baselines and to catch regressions. </td></tr><tr><td data-label="Technical Breakdown — recorder.py"> <strong>Retention, rotation & archival policy</strong><br><br>Recorder should not implement complex retention rules inline — provide hooks to a <code>retention</code> module. Provide a safe default: rotate when <code>max_segment_size</code> reached or daily at midnight; archive rotated segments to object-store and mark locally eligible for deletion per <code>retention_policy</code>. Never delete a segment that is referenced by an unreconciled checkpoint. Tests: retention edge cases where a long-running job holds an old checkpoint. </td></tr><tr><td data-label="Technical Breakdown — recorder.py"> <strong>Security & tamper-evidence</strong><br><br>Support optional signing of entries and signing of segment manifests. Keep signatures detached or included in a manifest so replay can verify integrity. If encrypting entries, ensure key management is handled externally — Recorder accepts an <code>encryptor</code> interface; it must not manage keys. Redact sensitive fields before writing if required by policy (configurable field list). Ensure logs never output raw persisted payloads unless <code>settings.env == &quot;dev&quot;</code>. Provide a <code>verify_integrity</code> and <code>verify_signature</code> command-line hook for operators. </td></tr><tr><td data-label="Technical Breakdown — recorder.py"> <strong>Testing recommendations</strong><br><br>Unit tests: validation, serialization canonicalization, idempotency, lock semantics (thread-level), rotation logic, checkpoint behavior. Integration tests: concurrent writer processes (subprocesses), crash-recovery simulation (truncate files, partial writes), replay-consumer checkpoint resume, signing/encryption roundtrip. Property tests: append-only invariant (no sequence numbers skipped/duplicated under concurrency), deterministic serialization. GitHub Actions: add a <code>recorder-smoke</code> job that runs crash + replay scenarios. </td></tr><tr><td data-label="Technical Breakdown — recorder.py"> <strong>Failure modes & operational runbook</strong><br><br>Common failures: partial writes/truncation, disk full, locked segment (stale lockfile), object-store promotion failure, signature verification failures. For each mode document detection, immediate remediation steps, and longer-term follow-ups. Examples: 1) on detection of truncated segment, stop writes, create a recovery snapshot, attempt safe truncation (truncate to last known-good offset), and then reopen in degraded mode; 2) on object-store upload failure, retry with exponential backoff and promote alert to SRE after N failures. Provide curated shell commands to inspect the active segment and checksums. </td></tr><tr><td data-label="Technical Breakdown — recorder.py"> <strong>CLI & admin utilities</strong><br><br>Provide small CLI helpers (in a separate admin module) that call into <code>recorder</code> functions: <code>recorder verify --segment &lt;id&gt;</code>, <code>recorder replay --from &lt;checkpoint&gt; --to &lt;checkpoint&gt;</code>, <code>recorder compact --older-than YYYY-MM-DD</code>, <code>recorder tail --follow</code>. These should be small wrappers that use the same underlying objects to avoid duplication. </td></tr><tr><td data-label="Technical Breakdown — recorder.py"> <strong>Backward compatibility & versioning</strong><br><br>Embed <code>schema_version</code> and <code>recorder_version</code> in every audit line/segment manifest. Provide migration helpers that can read old schema versions and coerce them into the canonical runtime schema. When changing serialization surface, bump <code>schema_version</code> and maintain code to read older versions for at least one release cycle. Tests: roundtrip old-version fixtures through current <code>replay()</code> and ensure correctness. </td></tr><tr><td data-label="Technical Breakdown — recorder.py"> <strong>Maintenance & developer notes</strong><br><br>— Keep the write-path minimal and well-optimized; move complex ops (encryption, archiving, compaction) to background or external processes. <br>— Add detailed docstrings and an architecture diagram showing segment lifecycle. <br>— When adding new fields to the audit event, update <code>make_audit_entry</code> and the schema validator; preserve read compatibility. <br>— If adding major features (live replication, multi-writer coordination), prefer separate modules and keep recorder's contract stable. <br>— Add <code>smoke</code> integration tests and keep a small set of golden JSONL fixtures for replay tests. </td></tr><tr><td data-label="Technical Breakdown — recorder.py"> <strong>Recommended tests & CI checks</strong><br><br>1. Unit: schema validator, serializer determinism, hashing. <br>2. Concurrency: threaded append with assertions on final offsets. <br>3. Process-safety: subprocess writer competing for lock. <br>4. Crash-recovery: simulate partial write & verify <code>open()</code> behavior. <br>5. Integrity: signature verification against stored manifests. <br>6. Performance: append throughput with/without fsync; compare batch vs per-write. <br>7. E2E: upload → calc → record → replay → export. <br><br>Automate as GitHub Actions matrix (py versions, FS backends). </td></tr><tr><td data-label="Technical Breakdown — recorder.py"> <strong>Operational & security checklist before production</strong><br><br>— <code>strict=True</code> by default for production. <br>— Set <code>fsync_on_write=True</code> if the system cannot tolerate recent loss (cost: latency). <br>— Ensure signer/encryptor secrets are provided via secret manager and not config files. <br>— Configure retention to never remove segments referenced by allowed checkpoints. <br>— Ensure monitoring alerts for <code>write_failures</code>, <code>integrity_errors</code>, and <code>segment_rotation_failures</code>. <br>— Verify that <code>verify_integrity</code> runs as part of nightly CI and that operators can run <code>recorder verify</code> manually. </td></tr><tr><td data-label="Technical Breakdown — recorder.py"> <strong>Short list of recommended public tests & examples in repo</strong><br><br>— <code>tests/test_recorder_serialization.py</code>: serialization determinism and hashing. <br>— <code>tests/test_recorder_append_idempotency.py</code>: idempotency semantics. <br>— <code>tests/test_recorder_rotation_and_archive.py</code>: rotate and archive flows with fake object-store. <br>— <code>integration/test_recorder_crash_recovery.py</code>: simulate partial write & recover. <br>— <code>ci/bench_recorder_write.py</code>: microbenchmark script for tuning. </td></tr><tr><td data-label="Technical Breakdown — recorder.py"> <strong>Final implementation guardrails</strong><br><br>— Keep side-effects out of <code>__init__</code>. <br>— Avoid heavy deps at top-level (pydantic optional via extras). <br>— Make every registration/wiring idempotent. <br>— Favor testable small helpers over monolith functions. <br>— Preserve public function signatures if other modules depend on them; prefer new functions for incompatible changes and mark old ones deprecated. </td></tr></tbody></table></div><div class="row-count">Rows: 40</div></div><div class="table-caption" id="Table9" data-table="Docu_0163_09" style="margin-top:2mm;margin-left:3mm;"><strong>Table 9</strong></div>
<div class="table-wrapper" data-table-id="table-9"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown — src/auditor.py"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown — src/auditor.py</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — src/auditor.py"> <strong>File-level responsibilities</strong><br><br>This module is the single-authority for producing, validating, persisting, and exposing audit records generated by the calculation and export pipelines. It must provide a small, well-documented surface that (1) constructs canonical audit envelopes for every meaningful processing step, (2) enforces PII redaction and signing policies, (3) exposes efficient read/query/streaming helpers for downstream consumers (reconciliation, exports, retention), and (4) integrates with observability and retention policies. Keep the module purely focused on audit semantics and storage format — any heavy storage adapters, cryptographic keys, or transport code must be injected via factories. Import-time behavior must be side-effect free. Attach a clear schema version identifier and migration guidance in the module top docstring. Unit tests should exercise all edge cases of signing, redaction, validation failures, and idempotent writes. </td></tr><tr><td data-label="Technical Breakdown — src/auditor.py"> <strong><code>init_auditor(settings: Settings, storage_factory: Callable, signer: Optional[Signer]=None) -&gt; Auditor</code></strong><br><br>Factory that builds and returns a configured <code>Auditor</code> instance. Responsibilities: validate required settings (audit path, schema_version, write_mode, retention policy), build storage client via <code>storage_factory</code> (object store or local append-only file writer), and attach a <code>Signer</code>/<code>Verifier</code> if signing is enabled. Must not open persistent connections; storage factories should expose <code>connect()</code>/<code>close()</code> and be called later by startup handlers. Validate configuration ranges (max_record_size, flush_interval). On missing non-critical settings populate with safe defaults and log warnings; when <code>settings.strict</code> is true, raise on missing critical settings (e.g., <code>audit_bucket</code> when exports require signed audit trails). Attach small baked-in metadata: <code>auditor.runtime_fingerprint</code> and <code>auditor.schema_version</code>. Unit tests: supply fake storage and signer and assert the returned object exposes expected methods and state. </td></tr><tr><td data-label="Technical Breakdown — src/auditor.py"> <strong><code>Auditor</code> (main runtime object)</strong><br><br>The runtime abstraction that encapsulates state and operations. Minimal, testable API surface should include: <code>write(entry: dict) -&gt; str</code>, <code>stream(path_or_range) -&gt; Iterator[dict]</code>, <code>find(filter_spec) -&gt; Iterator[dict]</code>, <code>replay(path, sink) -&gt; int</code>, <code>rotate() -&gt; None</code>, <code>close() -&gt; None</code>, and <code>health_check() -&gt; dict</code>. The <code>Auditor</code> must be thread-safe and concurrency-friendly: use an internal asyncio-friendly queue or a dedicated append lock depending on implementation; never allow partial writes to be visible. Attach <code>auditor._metrics</code> for counters (written, failed, signed, redacted). The object should not assume a particular encoding — marshalling is delegated to <code>marshal_entry</code>. Provide a clear contract for error propagation: transient storage errors raise <code>TransientAuditError</code> (retryable), fatal errors raise <code>AuditFatalError</code>. Unit tests should validate idempotence, concurrency, and health endpoint behavior. </td></tr><tr><td data-label="Technical Breakdown — src/auditor.py"> <strong><code>make_audit_entry(origin: str, input_record: dict, result: dict, applied_rules: dict, meta: Optional[dict]=None) -&gt; dict</code></strong><br><br>Canonical producer of an audit envelope. This function constructs the canonical shape used everywhere in the system: at minimum include <code>id</code> (content-hash or UUID), <code>ts</code> (ISO8601 UTC), <code>origin</code> (uploader/adapter/job id), <code>schema_version</code>, <code>runtime_fingerprint</code>, <code>applied_rule_version</code>, <code>input</code> (canonicalized), <code>result</code> (calculation output), <code>trace</code> (deterministic trace of rule decisions), <code>meta</code> (free-form metadata), and <code>signatures</code> (populated later). Responsibilities: normalize timestamps, attach runtime fingerprint and schema_version, compute stable id (prefer content-hash of normalized payload) to allow idempotent writes, and keep the envelope minimal (store heavy payloads as references if <code>settings.small_audits=True</code>). Must be pure and easy to unit-test. Provide deterministic canonicalization rules (ordered keys, stable serialization) documented at top of module. Unit tests: input permutations produce identical <code>id</code> and canonicalized payload. </td></tr><tr><td data-label="Technical Breakdown — src/auditor.py"> <strong><code>validate_audit_entry(entry: dict) -&gt; None</code></strong><br><br>Schema and invariant checker that raises a controlled exception on violation. Responsibilities: enforce required top-level keys, types, timestamp format, allowable field sizes, and that <code>applied_rule_version</code> matches expected semver form (or tag). Also validate that <code>input</code> and <code>result</code> fields are free of unexpected binary blobs; if large binary references are allowed they must be recorded as references (<code>{ &quot;ref&quot;: &lt;object_store_uri&gt;, &quot;size&quot;: int }</code>). Avoid deep validation of domain business rules (that belongs in rules tests), but ensure integrity prerequisites for persistence. Provide clear exception types for missing-required, size-violation, and malformed-timestamp. Unit tests: each failure mode and success path. </td></tr><tr><td data-label="Technical Breakdown — src/auditor.py"> <strong><code>redact_sensitive_fields(entry: dict, redact_config: RedactConfig) -&gt; dict</code></strong><br><br>Apply deterministic, auditable redaction rules before persisting. Responsibilities: walk the audit envelope and redact keys matching configured patterns (explicit keys, nested JSON paths, regex), replace redacted values with stable placeholders that preserve type and length hints when configured (e.g., <code>&quot;&lt;redacted:email&gt;&quot;</code> or hashed token). Must be idempotent (running twice yields same result), fast (avoid regex over large blobs), and safe (never accidentally leak full values in metadata fields). Support sampling (log a hashed sample for debug when <code>settings.audit_debug_sample_rate&gt;0</code>) but only for dev mode; samples must not include secrets. Provide explicit unit tests for nested structures, lists, dictionaries, and edge-cases (nulls, numbers, large text). </td></tr><tr><td data-label="Technical Breakdown — src/auditor.py"> <strong><code>sign_entry(entry: dict, signer: Signer) -&gt; dict</code></strong><br><br>Create an integrity signature for an audit envelope. Responsibilities: compute a canonical serialization (must match <code>make_audit_entry</code> canonicalization rules), then call <code>signer.sign(bytes)</code> to produce a signature entry (e.g., <code>{alg, key_id, signature, ts}</code>) that is appended to <code>entry[&#x27;signatures&#x27;]</code>. The function must not mutate signing keys in memory; signer interface should provide ephemeral handles or HSM-backed signing. Ensure signing is tolerant of transient signer unavailability: by default fail the write when <code>settings.strict_signing</code> is true, otherwise attach a marker <code>signature_status: &quot;pending&quot;</code> and schedule background signing (but prefer not to implement scheduling inside auditor — prefer external worker). Tests: signatures are verifiable with <code>verify_entry_signature</code>. </td></tr><tr><td data-label="Technical Breakdown — src/auditor.py"> <strong><code>verify_entry_signature(entry: dict, verifier: Verifier, allowed_key_ids: Optional[Set[str]] = None) -&gt; bool</code></strong><br><br>Validate one or more signatures embedded in an audit envelope. Responsibilities: ensure at least one signature matches an allowed key and is within acceptable clock skew. Return boolean and optionally raise detailed errors when verbose mode is requested. Must be fast (stop after first valid signature). When verification fails, include contextual hints (<code>key_id</code> mismatch, signature corrupt, canonicalization mismatch) to help SRE. Provide unit tests with golden fixtures and mutated payload tests to assert detection of tampering. </td></tr><tr><td data-label="Technical Breakdown — src/auditor.py"> <strong><code>marshal_entry(entry: dict, compress: bool=False, encoding: str=&#x27;utf-8&#x27;) -&gt; bytes</code></strong><br><br>Serialize an audit envelope into a durable on-wire/binary form for storage. Responsibilities: enforce canonical ordering, stable whitespace, and deterministic numeric formatting. Optionally compress (e.g., gzip) when <code>compress=True</code>. Return a bytes buffer and computed <code>content_hash</code> alongside size metadata. The function must guarantee that the same logical <code>entry</code> always marshals to the same bytes (given same flags), enabling idempotent writes by content-hash. Unit tests: round-trip serialization/parsing and hash stability. </td></tr><tr><td data-label="Technical Breakdown — src/auditor.py"> <strong><code>persist_entry(auditor: Auditor, marshalled: bytes, destination_hint: Optional[str]=None) -&gt; str</code></strong><br><br>Lowest-level persistence primitive that writes a marshalled audit record to storage and returns the stable storage URI (or local path). Responsibilities: write in an atomic, append-only manner; when using file-backed storage ensure writes are flushed and <code>fsync</code>-equivalent is used before returning success. When using object store, write to a staging path then rename/compose to a final path to avoid partial visibility. Provide idempotency: if <code>content_hash</code> already exists, return existing URI and avoid duplication. Return a stable identifier used by manifests. Error handling: raise <code>TransientAuditError</code> for retryable network/storage errors; raise <code>AuditFatalError</code> for non-retryable issues (permissions, quota). Tests: simulate transient failures and assert retry/backoff behavior (with mocked storage). </td></tr><tr><td data-label="Technical Breakdown — src/auditor.py"> <strong><code>write(entry: dict, *, signing: bool=True, redact: bool=True, compress: bool=False) -&gt; str</code></strong><br><br>High-level convenience method (usually on <code>Auditor</code>) that runs the end-to-end persist pipeline: <code>validate_audit_entry</code> → <code>redact_sensitive_fields</code> → (optional) <code>sign_entry</code> → <code>marshal_entry</code> → <code>persist_entry</code>. Responsibilities: transaction-like semantics (either the entry is fully persisted and visible, or no partial artifact is left), idempotent writes keyed by content-hash, instrumentation of metrics (latency, success/failure), and structured logging of write attempts with trace_id. Must coordinate locking if necessary to prevent concurrent attempts to write the same id from causing duplicate writes. Return the canonical storage URI or raise on fatal failure. Provide a <code>dry_run</code> path for tests that returns a canonical marshalled payload without writing. Unit tests should validate all permutations including strict vs lenient signing. </td></tr><tr><td data-label="Technical Breakdown — src/auditor.py"> <strong><code>stream_entries(source: str|Iterator[str], chunk_size: int=1024) -&gt; Iterator[dict]</code></strong><br><br>Streaming reader that yields parsed audit envelopes from a source (object store prefix, file path, or iterator of lines). Responsibilities: be memory-efficient (yield per-record), robust to partial corrupt records (log and skip by default, configurable to stop on error), expose back-pressure friendly behavior (for asyncio consumers provide an async variant or adapter). Support filters at read-time (time-range, origin, rule_version) to avoid loading irrelevant data. Document that consumers must not mutate yielded dicts. Unit tests: large-file streaming, corrupted-entry handling, and filtering. </td></tr><tr><td data-label="Technical Breakdown — src/auditor.py"> <strong><code>find(filter_spec: FilterSpec) -&gt; Iterator[dict]</code></strong><br><br>Query helper that finds audit entries matching a filter (time-range, origin, applied_rule_version, id, metadata predicates). Responsibilities: translate high-level filters into efficient storage queries where possible (object-store prefix, index lookups) and fallback to streaming + in-memory filtering only when indexed queries aren't available. Must expose a cost model in docstring: prefer queries that can be satisfied by prefix/time-shard. Return a lazy iterator. Tests: mock indexed store and streaming fallback paths; assert performance characteristics for large datasets. </td></tr><tr><td data-label="Technical Breakdown — src/auditor.py"> <strong><code>replay(path: str, sink: Callable[[dict], None], concurrency: int=1, checkpoint: Optional[Callable]=None) -&gt; int</code></strong><br><br>Replay audit records into a downstream sink (reconciliation, analytics). Responsibilities: preserve envelope ordering where required (configurable), support concurrency with deterministic partitioning, and optionally call <code>checkpoint</code> after each successful batch to allow resumption. Replay must verify signatures before delivering unless explicitly skipped, and should provide a configurable mode for “verify-only” to preflight archives. Return number of records successfully delivered. Tests: replay resume after simulated crash using checkpoint; replay with and without signature verification. </td></tr><tr><td data-label="Technical Breakdown — src/auditor.py"> <strong><code>audit_context(origin: str, meta: Optional[dict]=None) -&gt; ContextManager</code></strong><br><br>Context manager to simplify recording of a block of work. Behavior: yields an ephemeral context where <code>ctx.add_input()</code>, <code>ctx.add_output()</code>, <code>ctx.add_trace()</code> methods are available; on exit the context will call <code>make_audit_entry</code> and <code>auditor.write</code>. Ensure exceptions inside the block are captured into the audit envelope (error/warning fields) but do not obscure the original exception (unless swallowed explicitly). Provide <code>suppress_exceptions</code> option only for specific operational flows. Must be lightweight and suitable for high-throughput workers. Unit tests: success path, exception path (both suppressed and re-raised). </td></tr><tr><td data-label="Technical Breakdown — src/auditor.py"> <strong><code>audit_decorator(origin: str, meta: Optional[dict]=None, on_error: str=&#x27;record&#x27;) -&gt; Callable</code></strong><br><br>Decorator for instrumenting functions to automatically emit audit entries for each invocation. Responsibilities: the wrapped function's inputs and outputs should be recorded, exceptions captured into the audit envelope based on <code>on_error</code> policy (<code>record_and_raise</code>, <code>record_and_suppress</code>, <code>record_only</code>). The decorator must preserve original function metadata (name, docstring) and should support coroutine functions (async wrappers). Avoid capturing large objects by default; allow a <code>payload_selector</code> in <code>meta</code> to pick what to record. Unit tests: sync/async functions, exception handling modes, serialization of typical inputs. </td></tr><tr><td data-label="Technical Breakdown — src/auditor.py"> <strong><code>rotate(target_shard: Optional[str]=None) -&gt; None</code></strong><br><br>Manage audit storage rotation: flush in-memory buffers, finalize current shard/file, and start a new shard with a deterministic name schema (e.g., <code>audits/YYYY/MM/DD/HH-&lt;fingerprint&gt;-&lt;seq&gt;.jsonl</code>). Responsibilities: ensure atomic handover so readers don't observe partially-rotated shards, and optionally trigger signing/manifest generation for the closed shard. Rotation must be idempotent and safe to call concurrently (use lock or leader-elect mechanism). Expose <code>rotate_if_needed()</code> that checks size/time thresholds before calling <code>rotate</code>. Unit tests: rotation under concurrent writers, rotation atomicity guarantees. </td></tr><tr><td data-label="Technical Breakdown — src/auditor.py"> <strong><code>export_audit_bundle(audit_range: TimeRange|List[str], signer: Optional[Signer]=None, packing: str=&#x27;tar.gz&#x27;) -&gt; ExportManifest</code></strong><br><br>Produce an exportable bundle of audit records for external review or regulatory submission. Responsibilities: collect matching audit records, optionally verify signatures, produce a manifest containing schema_version, applied_rule_version, runtime_fingerprint, record count, checksums, and optional bundle signature. Ensure bundle creation is deterministic and reproducible (sort by id/time). Do not embed secrets in the manifest. When signing the bundle, use the same signer interface as individual records. Tests: validate manifest contents, checksum correctness, and ability to reconstitute bundle into original records. </td></tr><tr><td data-label="Technical Breakdown — src/auditor.py"> <strong><code>cleanup_retention(retention_policy: RetentionPolicy) -&gt; RetentionReport</code></strong><br><br>Run retention rules to prune or archive old audit shards in accordance with policy. Responsibilities: identify shards that qualify for deletion or cold-archive, ensure deletions are logged (audit trail of deletion decisions stored separately), and support a “dry-run” mode that returns the set of candidates without performing deletion. Ensure compliance constraints: if legal-hold flags exist for certain records, never delete them. Provide metrics (deleted_count, archived_count) and a safety window to avoid accidental mass-deletion. Tests: respect legal-hold, simulate concurrent retention and writes. </td></tr><tr><td data-label="Technical Breakdown — src/auditor.py"> <strong><code>health_check() -&gt; dict</code></strong><br><br>Small diagnostic helper used by <code>server.health</code>. Responsibilities: verify storage connectivity (lightweight), signer availability (if signing is required), recent successful write timestamp, and basic integrity checks (e.g., last N records verifiable). Return a structured dict <code>{ok: bool, checks: {storage: {...}, signer: {...}, last_write: {...}}}</code> and include actionable hints on failure. Do not perform heavy scans; keep it sub-second. Tests: failing storage and signer paths. </td></tr><tr><td data-label="Technical Breakdown — src/auditor.py"> <strong><code>_internal_index_writer</code> / <code>index_entry(entry: dict) -&gt; None</code></strong><br><br>Optional helper that writes lightweight index records to an indexing backend (database, search service). Responsibilities: write id, ts, origin, applied_rule_version, minimal metadata to enable fast <code>find()</code> queries. The module should keep index writes asynchronous and best-effort: failures to index must not prevent audit persistence, but must be recorded to a monitoring channel and emit a metric that triggers SRE action if sustained. Tests: index write failure does not block main write. </td></tr><tr><td data-label="Technical Breakdown — src/auditor.py"> <strong>Implementation patterns & guardrails</strong><br><br>— <strong>Side-effect free imports</strong>: heavy clients and cryptography libraries must be imported inside <code>init_auditor</code> or inside the factory, not at module top-level. <br>— <strong>Idempotence</strong>: canonicalize and content-hash entries so <code>write</code> is idempotent across retries. <br>— <strong>Atomicity</strong>: use staging+rename or fsync semantics to avoid partial records. <br>— <strong>PII</strong>: redaction rules are the single source of truth; never let other code bypass them. <br>— <strong>Signing</strong>: treat signing failures as either fatal (strict mode) or deferred (lenient mode) with explicit markers. <br>— <strong>Observability</strong>: record counters and histograms for write latency, sign latency, fail reasons, and retention actions; expose a prometheus-friendly metrics registry. <br>— <strong>Testing hygiene</strong>: provide <code>build_test_auditor</code> that returns an Auditor wired with in-memory storage and a noop signer for unit tests. <br>— <strong>Performance</strong>: keep <code>make_audit_entry</code> and <code>redact_sensitive_fields</code> CPU-efficient; prefer iterative redaction over naive deep regexes. <br>— <strong>Security</strong>: never log unredacted audit payloads; redact before any debug dumps. Use <code>SecretHandle</code> for signer keys and avoid key material in process provenance. <br>— <strong>Backward compatibility</strong>: bump <code>schema_version</code> when changing envelope shape and provide migration helpers to read older envelopes; ensure <code>verify_entry_signature</code> is version-aware. </td></tr><tr><td data-label="Technical Breakdown — src/auditor.py"> <strong>Recommended tests & CI checks</strong><br><br>1. <strong>Unit</strong>: canonicalization idempotence, redaction permutations, signing+verification round-trip, marshal/unmarshal stability. <br>2. <strong>Integration (fast)</strong>: auditor with real object-store emulator (minio/localfs) to ensure append/rename semantics and rotation. <br>3. <strong>Concurrency</strong>: concurrent writes producing identical ids—assert single persisted record and no corruption. <br>4. <strong>Failure injection</strong>: transient storage outage, signer timeout, index write failures—assert proper exception types and metrics. <br>5. <strong>Retention & export</strong>: dry-run and real deletion simulations including legal-hold. <br>6. <strong>Security</strong>: tests ensuring no secrets are present in persisted audit payloads or logs. <br>7. <strong>Compliance</strong>: manifest generation verification (checksums + bundle signature). </td></tr><tr><td data-label="Technical Breakdown — src/auditor.py"> <strong>Operational & security checklist</strong><br><br>Before enabling production signing and long retention: ensure signer keys are HSM-backed or in a protected key service; configure <code>settings.strict_signing</code> explicitly; verify redaction lists include all sensitive fields discovered in production; enable index writes to a durable DB; configure retention safe-guards (legal-hold). Test recovery: restore a bundle and verify signatures. Ensure audit storage has lifecycle policies and encryption-at-rest enabled. </td></tr><tr><td data-label="Technical Breakdown — src/auditor.py"> <strong>Maintenance & developer notes</strong><br><br>— When adding a new field to the envelope, increment <code>schema_version</code> and add a migration helper. <br>— If audit volume grows, push indexing, verification, and export to worker processes; keep <code>Auditor.write</code> synchronous-fast and non-blocking where possible. <br>— Prefer explicit factory injection for signer/storage to facilitate testing. <br>— Document the canonicalization algorithm in the module to avoid signature mismatches across languages. <br>— Any change that can alter canonical serialization must be reviewed by SRE and compliance and must include a tool to re-sign or re-hash historical archives if required. </td></tr></tbody></table></div><div class="row-count">Rows: 25</div></div><div class="table-caption" id="Table10" data-table="Docu_0163_10" style="margin-top:2mm;margin-left:3mm;"><strong>Table 10</strong></div>
<div class="table-wrapper" data-table-id="table-10"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown — src/retention.py"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown — src/retention.py</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — src/retention.py"> <strong>File-level responsibilities</strong><br><br>This module centralizes all logic for retention, archival, and safe deletion of snapshots, exports, audits, and related artifacts. It must encode policy (time-based TTLs, tiering rules, legal/forensic holds, business-critical exemptions), perform safe, idempotent operations against object stores and metadata databases, and emit auditable events for every action. Keep this module language-level small and declarative: policy/parameters belong in configuration (<code>settings.retention.*</code>) and low-level storage moves belong in adapter modules (<code>file_ops</code>, <code>object_store_adapter</code>). The module exposes pure helpers (cutoff calculation, candidate selection) plus a small set of side-effecting orchestration functions (mark_for_deletion, purge_batch, archive_and_move) and one supervisor entrypoint (retention_worker_loop / register_retention_handlers). Operational guardrails: default to <code>dry_run=True</code> for CLI and scheduled previews; require explicit <code>force=True</code> and admin-level context to perform irreversible deletions; always record a durable audit entry before & after destructive actions. Unit tests should exercise policy permutations, idempotence, and failure-recovery paths; integration tests must run against an emulated object store and a transient metadata DB. </td></tr><tr><td data-label="Technical Breakdown — src/retention.py"> <strong><code>compute_retention_key(record: Mapping[str,Any]) -&gt; str</code></strong><br><br>Deterministic classifier that maps a normalized object (snapshot/export/audit) to a retention class (e.g., <code>short_term</code>, <code>long_term</code>, <code>archive</code>, <code>permanent</code>). Inputs: canonical metadata dictionary with keys such as <code>created_at</code>, <code>object_type</code>, <code>owner</code>, <code>sensitivity_level</code>, <code>legal_hold</code>. Output: canonical string key used to look up TTL and tier in settings. Must be pure, idempotent, and stable across process restarts. Implementation notes: only derive from non-secret metadata fields; avoid using fields that are mutable (e.g., last_accessed) for classification because that destabilizes reclassification. Tests should cover classification decision table and boundary values (e.g., created exactly at policy cutoff). </td></tr><tr><td data-label="Technical Breakdown — src/retention.py"> <strong><code>compute_retention_cutoff(retention_period: str|timedelta, now: Optional[datetime]=None) -&gt; datetime</code></strong><br><br>Convert a retention policy descriptor (<code>&quot;90d&quot;</code>, <code>&quot;365d&quot;</code>, <code>timedelta</code>) to an exact cutoff <code>datetime</code> in UTC. Must validate inputs, normalize timezone to UTC, and be pure for deterministic unit tests. Important: callers must use the same <code>now</code> during a single retention pass to avoid racey boundaries across batches. Provide explicit error on unsupported strings. Unit tests: time-freeze using fixed <code>now</code> and assert arithmetic. </td></tr><tr><td data-label="Technical Breakdown — src/retention.py"> <strong><code>retention_schedule_for_snapshot(meta: Mapping[str,Any], settings: Settings) -&gt; Dict[str,Any]</code></strong><br><br>Return a policy decision for a single metadata row: retention class, cutoff timestamp, target tier (<code>hot|cold|archive|delete</code>), and any required manual-approval flags. Must respect overrides: <code>meta.get(&#x27;legal_hold&#x27;)</code>, <code>meta.get(&#x27;regulatory_retention&#x27;)</code>, and per-customer contractual retention. Do not mutate <code>meta</code>. Produce a compact returned dict suitable for recording in a retention plan table. Tests should assert precedence of holds and that exceptions are surfaced as non-fatal notices in the returned dict. </td></tr><tr><td data-label="Technical Breakdown — src/retention.py"> <strong><code>list_purge_candidates(metadata_db: MetadataDB, cutoff: datetime, batch_size: int = 1000, filter: Optional[Mapping]=None) -&gt; Iterator[Dict]</code></strong><br><br>Query the metadata store for object records whose retention-decision is <code>delete</code> or <code>archive</code> and which are older than <code>cutoff</code>. Must be streaming (yield per-row) and safe for long-running scans: use server-side cursors, <code>LIMIT</code>/<code>OFFSET</code> avoidance, and pagination tokens. The function must be idempotent if interrupted (produce the same set when re-run with the same cutoff). Provide a <code>filter</code> hook for testing and emergency scoping (e.g., customer id). Observability: instrument candidate count, scan duration, and page size. Tests: simulate large tables and ensure memory stays bounded. </td></tr><tr><td data-label="Technical Breakdown — src/retention.py"> <strong><code>mark_for_deletion(metadata_db: MetadataDB, ids: Sequence[str], actor: str, reason: str, dry_run: bool=True) -&gt; Dict[str,Any]</code></strong><br><br>Idempotent operation to create or update tombstone/retention records in the metadata DB. Behavior: for each id, record <code>retention_status=&#x27;pending_delete&#x27;</code>, <code>scheduled_at=now</code>, <code>scheduled_by=actor</code>, <code>reason</code>, and <code>operation_id</code> (idempotency key derived from content hash + actor). Must obey legal-hold and exemption checks (do not mark if <code>legal_hold==True</code> unless <code>override=True</code> and actor has <code>admin</code> role). Side effects: emits an audit event for each changed row. Returns a report with counts (<code>skipped</code>, <code>marked</code>, <code>already_marked</code>) and per-id messages for failure handling. Tests: idempotence (two calls produce same DB state & reports). </td></tr><tr><td data-label="Technical Breakdown — src/retention.py"> <strong><code>archive_and_move(snapshot_id: str, object_store: ObjectStore, archive_store: ObjectStore, metadata_db: MetadataDB, dry_run: bool=True, verify: bool=True) -&gt; Dict[str,Any]</code></strong><br><br>Move an object from primary storage to cold/archive storage while preserving integrity and metadata. Steps: (1) fetch metadata & checksum, (2) stream-copy to target with a temporary staging key, (3) atomic copy/commit (rename or server-side copy where supported), (4) optional checksum verification, (5) update metadata to point to archive location and mark <code>tier=&#x27;archive&#x27;</code>, (6) write an audit entry including pre/post checksums. Must implement strong safety: never delete source until verification passes; support <code>dry_run</code> to return the planned actions; support <code>idempotency</code> by skipping if <code>metadata.points_to == archive_store</code> and checksums match. Failure modes: network error during copy → retry with exponential backoff; verification failure → leave both copies and open a reconcile ticket. Tests: simulate partial copy, verify rollback semantics, and verify audit emission. </td></tr><tr><td data-label="Technical Breakdown — src/retention.py"> <strong><code>purge_batch(ids: Sequence[str], object_store: ObjectStore, metadata_db: MetadataDB, audit_db: AuditDB, dry_run: bool=True, concurrency: int=10, rate_limit: Optional[int]=None) -&gt; Dict[str,Any]</code></strong><br><br>Perform irreversible deletion of object blobs and corresponding metadata entries. MUST be used only when policy & approvals are present. Behavior: implement a multi-step, auditable workflow: (A) acquire per-id deletion lock, (B) preflight checks (legal_hold, export_links, retention_override), (C) move object to a temporary <code>quarantine/</code> prefix (fast rename when supported), (D) record a pre-delete audit record containing checksum & oracle, (E) schedule final deletion task (actual <code>DELETE</code> or <code>PURGE</code>) with a short grace window (configurable), (F) remove metadata pointers or mark <code>deleted_at</code>. Advantages: using quarantine enables quick rollback within the grace window. Concurrency & rate limiting to avoid overwhelming the object store. Observability: emit metrics for bytes freed, objects purged, and failures. Error handling: per-id errors are collected and returned without aborting the batch; permanent failures go to <code>exception_queue</code>. Tests: concurrency correctness, lock behavior, and quarantine rollback. </td></tr><tr><td data-label="Technical Breakdown — src/retention.py"> <strong><code>enforce_legal_holds(ids: Sequence[str], metadata_db: MetadataDB) -&gt; Dict[str,Any]</code></strong><br><br>Check and return which ids are currently subject to legal/regulatory holds or active reconciliation flags. Must consult both <code>metadata_db</code> and cross-service hold registries (via an adapter). This function should be read-only and fast; callers must interpret results and decide whether to skip, escalate (create a ticket), or clear hold (only via an explicit administrative flow elsewhere). Tests: simulated hold entries and concurrent hold-set changes. </td></tr><tr><td data-label="Technical Breakdown — src/retention.py"> <strong><code>retention_worker_loop(ctx: WorkerContext, settings: Settings) -&gt; None</code></strong><br><br>Long-running supervisor that orchestrates scheduled retention passes. Responsibilities: read schedule from <code>settings.retention.schedule</code> (e.g., daily at 03:00 UTC), claim a leader lock (single-run across cluster), compute cutoffs for every retention class (use single <code>now</code> per run), stream purge/archive candidates (batched), invoke <code>archive_and_move</code>/<code>purge_batch</code> as appropriate, and emit a concise run summary to telemetry and audit. Safety rules: run default in <code>dry_run</code> on first run after deployment, support <code>--force</code> to enable destructive mode, respect <code>settings.maintenance_mode</code> to skip runs, and abort early if the system load or storage errors exceed thresholds. Operational behavior: exponential backoff on transient errors, supervisor must not leak tasks, and it should maintain <code>app.state._retention_last_run</code> with timestamps and status. Tests: leader election mock, partial failures recovery, and telemetry expectations. </td></tr><tr><td data-label="Technical Breakdown — src/retention.py"> <strong><code>register_retention_handlers(app: FastAPI, settings: Settings) -&gt; None</code></strong><br><br>Idempotently install retention-related lifecycle hooks: startup registration for scheduled job registration, shutdown handlers to gracefully cancel running retention worker tasks, and a small endpoint for <code>GET /internal/retention/preview</code> (read-only) that returns the next-run plan for debugging. Must check <code>app.state._registered_retention</code> to avoid double registration. For frameworks without integrated task schedulers, the function should provide a <code>build_cron_job()</code> helper that returns a callable the external scheduler can call. Tests: ensure repeated registration is a no-op and that TestClient lifespan triggers worker lifecycle hooks. </td></tr><tr><td data-label="Technical Breakdown — src/retention.py"> <strong><code>retention_cli_main(argv: Optional[List[str]] = None) -&gt; int</code></strong><br><br>CLI entrypoint used by operators for previews and manual runs. Flags: <code>--dry-run</code> (default), <code>--cutoff DATE</code>, <code>--class &lt;retention_key&gt;</code>, <code>--force</code>, <code>--preview</code>, <code>--concurrency</code>, and <code>--report-out PATH</code>. Behavior: parse flags, build settings, validate <code>--force</code> requires an env var or admin role, compute plan, print both human (table) and machine (JSON) reports, and optionally execute the plan (archive/purge) when <code>--force</code>. Exit codes: <code>0</code> success, <code>1</code> validation error, <code>2</code> partial failures (some items failed), <code>3</code> fatal error (e.g., missing credentials). Must never run destructive operations unless explicit <code>--force</code> and authenticated. Tests: CLI unit tests for argument parsing and simulated dry-run output. </td></tr><tr><td data-label="Technical Breakdown — src/retention.py"> <strong><code>build_retention_report(run_id: str, stats: Mapping[str,int], items: Sequence[Mapping], dry_run: bool) -&gt; Dict[str,Any]</code></strong><br><br>Create a compact, exportable report describing what the retention run did (or would do in dry-run): number of candidates scanned, archived, quarantined, permanently deleted, bytes freed, skipped due to holds, and any failures. Include <code>run_id</code>, <code>applied_rule_version</code>, <code>runtime_fingerprint</code>, and <code>timestamp</code>. Return both human-friendly strings and structured fields for telemetry ingestion. Persist the report to <code>audit_db</code> and optionally to object store as JSONL. Tests: cross-validate counts vs detailed item lists. </td></tr><tr><td data-label="Technical Breakdown — src/retention.py"> <strong><code>reconcile_post_purge(metadata_db: MetadataDB, audit_db: AuditDB, affected_ids: Sequence[str]) -&gt; Dict[str,Any]</code></strong><br><br>After purge/archive actions, run integrity checks to ensure no leftover access paths exist (export manifests, secondary indices, search indices). Steps: remove secondary index entries, update export manifests to reflect removed artifacts, and emit reconcile audit entries. This function must take an explicit list of <code>affected_ids</code>—it must not try to discover deletions itself to avoid race conditions. Provide compensating action hooks for external systems (search, caches) via adapter calls. Tests: verify all adapters are invoked and failures are logged but do not block the completion of the retention run. </td></tr><tr><td data-label="Technical Breakdown — src/retention.py"> <strong><code>vacuum_orphaned_records(metadata_db: MetadataDB, audit_db: AuditDB, batch_size: int=1000) -&gt; Dict[str,Any]</code></strong><br><br>Safely remove metadata rows that reference non-existent blobs and are older than an admin-configured grace window. Must be conservative: for each orphan candidate, verify across both object store and archive store before removal; if ambiguous, move to <code>orphan_quarantine</code> for manual review. Produce metrics for orphans removed and quarantined. Tests: simulate inconsistent states and confirm quarantine behavior. </td></tr><tr><td data-label="Technical Breakdown — src/retention.py"> <strong><code>verify_archive_integrity(archive_store: ObjectStore, key: str, expected_checksum: str, timeout: Optional[float]=30.0) -&gt; bool</code></strong><br><br>Perform a non-destructive verification that an archived object matches the expected checksum. Use streaming reads and incremental hashing to avoid loading whole objects into memory. Return <code>True</code> on match, <code>False</code> otherwise. On mismatch, create a high-severity reconcile ticket (via <code>exception_queue</code>) and include both checksums in the audit. Tests: large-file streaming and hash mismatch path. </td></tr><tr><td data-label="Technical Breakdown — src/retention.py"> <strong><code>send_retention_audit_event(audit_db: AuditDB, event: Mapping[str,Any]) -&gt; None</code></strong><br><br>Write an append-only audit record for each retention action. Requirements: include <code>run_id</code>, <code>operation</code>, <code>object_id</code>, <code>actor</code>, <code>status</code>, <code>pre_state</code> and <code>post_state</code> diffs, <code>checksum</code>, and <code>trace_id</code>. The function must be highly reliable: on write failure, retry briefly and then write a local fallback to disk (or to <code>exception_queue</code>) so no destructive action is left undocumented. Tests: audit guarantees under simulated DB outage. </td></tr><tr><td data-label="Technical Breakdown — src/retention.py"> <strong><code>validate_retention_config(cfg: Mapping[str,Any]) -&gt; None</code></strong><br><br>Schema and semantic validation of retention configuration: validate TTL formats, tier names, required fields for archival targets, maximum batch sizes, and safety knobs (<code>dry_run_default</code>, <code>force_requires_admin</code>). Must raise descriptive exceptions on bad config and include automatic repair suggestions where feasible. CI should call this on every settings change. Unit tests: malformed config entries and helpful error messages. </td></tr><tr><td data-label="Technical Breakdown — src/retention.py"> <strong><code>restore_from_archive(snapshot_id: str, archive_store: ObjectStore, object_store: ObjectStore, metadata_db: MetadataDB, verify: bool=True, dry_run: bool=False) -&gt; Dict[str,Any]</code></strong><br><br>Reverse of <code>archive_and_move</code>: restore an archived object to hot storage and update metadata. Must check retention policy to ensure restoration is allowed (e.g., within legal/contractual windows), perform integrity checks, and create an audit entry describing prior archive and restore operations. Support <code>dry_run</code> preview and require elevated permissions to perform actual restore. Tests: round-trip archive → restore with checksum match and metadata fidelity. </td></tr><tr><td data-label="Technical Breakdown — src/retention.py"> <strong>Safety, observability & operational guardrails (module-level)</strong><br><br>— <strong>Dry-run by default:</strong> All public orchestration APIs default to non-destructive behavior unless explicitly requested. CLI enforces <code>--force</code> plus an additional guard (environment flag or admin token).<br><br>— <strong>Idempotence:</strong> All state-changing functions must be safe to re-run. Use idempotency keys for per-operation reconciliation. <br><br>— <strong>Audit-first:</strong> Before any destructive change, write a pre-change audit record and ensure on-disk persistence; after the operation write a post-change audit record including checksums and caller identity.<br><br>— <strong>Legal/Compliance holds:</strong> Always check holds and regulatory retention anchors; holds take precedence over policy TTLs. Provide an explicit exception workflow (audit + manual override) for compliance teams. <br><br>— <strong>Quarantine / two-step deletion:</strong> Prefer rename-to-quarantine + grace window before final purge to allow fast rollback. <br><br>— <strong>Backpressure & batching:</strong> Limit IOPS and API calls with concurrency and rate-limit tokens to protect object stores. <br><br>— <strong>Retries & DLQs:</strong> Transient errors use exponential backoff with capped retries; permanent failures are written to <code>exception_queue</code> with a human-friendly message and diagnostic payload including operation_id, stack trace, and relevant metadata. <br><br>— <strong>Metrics & traces:</strong> Emit metrics for candidates scanned, bytes archived, bytes deleted, time per item, and retention-run latency. Attach <code>trace_id</code> to all audit records for cross-service correlation. <br><br>— <strong>Testing:</strong> Unit tests for pure helpers; integration tests against an in-memory or fake object store and SQLite/ephemeral metadata DB; golden acceptance tests verifying idempotence, emergency rollback (quarantine recovery), and legal-hold enforcement. <br><br>— <strong>Docs & runbooks:</strong> Provide operator runbook: how to preview a run, how to force run, how to recover from a bad purge (using quarantine), and how to respond to integrity mismatches. Include examples for common compliance scenarios (e.g., extend retention for a subset of customers). </td></tr><tr><td data-label="Technical Breakdown — src/retention.py"> <strong>Recommended tests & CI checks (module-level)</strong><br><br>1. <strong>Unit</strong>: <code>compute_retention_key</code>, <code>compute_retention_cutoff</code>, <code>validate_retention_config</code>.<br>2. <strong>Integration (fast)</strong>: <code>archive_and_move</code> & <code>purge_batch</code> against local S3 emulator / local filesystem adapter. <br>3. <strong>Idempotence</strong>: repeated <code>mark_for_deletion</code> and <code>purge_batch</code> re-runs produce consistent final state. <br>4. <strong>Chaos</strong>: simulate object-store mid-copy failure; ensure quarantine left intact and audit entries present. <br>5. <strong>Legal-hold</strong>: simulate concurrent hold added during run — ensure the object is not purged and an exception was recorded. <br>6. <strong>E2E smoke</strong>: CLI <code>--preview</code> vs <code>--force</code> run in a containerized test with sample fixtures. </td></tr></tbody></table></div><div class="row-count">Rows: 21</div></div><div class="table-caption" id="Table11" data-table="Docu_0163_11" style="margin-top:2mm;margin-left:3mm;"><strong>Table 11</strong></div>
<div class="table-wrapper" data-table-id="table-11"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown — recon_service.py"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown — recon_service.py</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — recon_service.py"> <strong>File-level responsibilities</strong><br><br>This module implements the reconciliation orchestration: scheduling, job lifecycle, record-by-record comparison between internal snapshots and external sources (banks, DJP, SAP, etc.), scoring and prioritization of discrepancies, persistent storage of reconciliation outcomes, and integration points for human review and automated remediation. Keep this module focused on orchestration and IO coordination only — comparison rules, scoring heuristics, and storage adapters must be injected as small, pure components. Import-time behavior must be side-effect free: do not open network connections or persistent handles at module import. All long-running or blocking work must be asynchronous or run inside a managed worker pool. Provide clear idempotency boundaries (job idempotency keys, content-hash dedupe) and defensive guards for partial re-runs. Document operational knobs (<code>settings.recon.strict</code>, <code>settings.recon.max_workers</code>, <code>settings.recon.timeout_seconds</code>, <code>settings.recon.sample_rate</code>) at the head of the file and attach metrics and trace context on every public function. Unit-tests should be pure and fast; integration tests run in an ephemeral test DB and use recorded external source fixtures. </td></tr><tr><td data-label="Technical Breakdown — recon_service.py"> <strong><code>enqueue_reconcile_job(snapshot_ref: str, external_source: str, *, force: bool=False) -&gt; str</code></strong><br><br>Accepts a reference to an internal snapshot (content-hash or object-store path) and an external source identifier and enqueues an idempotent reconciliation job. Responsibilities: validate inputs via <code>validate_recon_job_payload</code>, compute an idempotency key (e.g., <code>sha256(snapshot_ref + external_source + normalization)</code>), persist a job record in jobs table with state <code>queued</code> if not already terminal, attach metadata (applied_rule_version, generator_version, caller), and push to the job queue. Must be transactional: writing the job record and enqueue operation must either both succeed or fail; use DB transaction with enqueue-after-commit pattern. On duplicate job (same idempotency key), return existing job id and do not re-enqueue unless <code>force=True</code>. Side-effects: DB writes, queue push, audit event. Errors: raise <code>InvalidJobPayload</code> (400-like) for malformed input; raise <code>JobEnqueueError</code> on queue failures (retryable) with exponential backoff. Tests: assert idempotency, force override, and transactional behavior with a failing queue client. Observability: emit metric <code>recon.jobs.enqueued</code> and log with <code>trace_id</code>, <code>job_id</code>, <code>snapshot_ref</code>, <code>external_source</code>. </td></tr><tr><td data-label="Technical Breakdown — recon_service.py"> <strong><code>validate_recon_job_payload(payload: Dict) -&gt; ValidatedPayload</code></strong><br><br>Pure validator for enqueue payloads. Responsibilities: ensure <code>snapshot_ref</code> exists & is accessible (lightcheck: metadata lookup, not full download), <code>external_source</code> is registered, and optional selectors (date range, tax period) are syntactically valid. Should return a normalized payload with defaults (e.g., default time window, sampling). Do not perform network I/O beyond metadata checks. On missing required fields raise <code>PayloadValidationError</code> with actionable hints. Unit-tests: cover missing fields, malformed ranges, and type coercion. Implementation note: keep validation deterministic and idempotent so CLI and API behavior match. </td></tr><tr><td data-label="Technical Breakdown — recon_service.py"> <strong><code>run_reconciliation(job_id: str) -&gt; None</code></strong><br><br>Main job runner invoked by workers. Responsibilities: load job record (verify state <code>queued</code>), transition to <code>running</code> atomically, attach a <code>runtime_fingerprint</code> and start a traced span. Steps (deterministic order): 1) load internal snapshot via <code>load_snapshot</code>, 2) resolve external source config, 3) open adapter client (via <code>app.state._extensions[&#x27;sources&#x27;][external_source]</code>) with timeboxed connect, 4) stream and pair records using <code>pairing_strategy</code> (join-by-key, time-window matching, fuzzy matching), 5) for each paired group call <code>reconcile_record</code>, 6) collect and score discrepancies via <code>score_discrepancy</code>, 7) aggregate via <code>aggregate_discrepancies</code>, 8) persist results with <code>persist_recon_results</code>, 9) emit completion events and set job state to <code>complete</code> (or <code>degraded</code>/<code>failed</code>). Must be idempotent: if job is re-run, use job-level markers to skip already-processed record ranges. Timeouts: wrap long steps with configurable <code>asyncio.wait_for</code>. Error handling: transient external errors should mark job <code>retryable</code> with backoff; fatal invariant failures raise <code>JobPermanentFailure</code> and move job to <code>failed</code>. Observability: record <code>recon.job.duration</code>, <code>recon.records.processed</code>, <code>recon.discrepancies.count</code>, and attach <code>trace_id</code>. Tests: integration test that runs small snapshot vs a recorded external fixture and asserts stored summary and audit entries. Implementation note: avoid materializing entire snapshots in memory; stream and window. </td></tr><tr><td data-label="Technical Breakdown — recon_service.py"> <strong><code>load_snapshot(snapshot_ref: str) -&gt; Iterator[Record]</code></strong><br><br>Loads an internal snapshot as a streaming iterator of canonical records. Responsibilities: resolve <code>snapshot_ref</code> to object-store path, verify checksum, stream decompress (if necessary), and yield canonical, validated records (the same canonical shape produced by csv_mapper/parsers). Must preserve stable sort order if ordering matters for pairing. Fail fast on checksum mismatch and raise <code>SnapshotIntegrityError</code>. Do not mutate snapshot; snapshots are immutable. Performance: use chunked reads, backpressure-friendly iterator that integrates with async worker loops. Tests: verify checksum check, empty snapshot handling, and partial corruption behavior. </td></tr><tr><td data-label="Technical Breakdown — recon_service.py"> <strong><code>fetch_external_records(source_config: SourceConfig, query: QuerySpec) -&gt; Iterator[Record]</code></strong><br><br	>Stream records from an external system (DJP, bank, SAP, Workday). Responsibilities: open adapter client lazily (adapter factory on <code>app.state</code>), perform short-lived queries/timeboxed pagination, normalize external fields to canonical schema (via injected normalizer), and yield records. Must handle rate-limiting and transient errors with retry/backoff at adapter level; job runner should treat persistent adapter unavailability as retryable. Security: do not log PII; scrub or hash identifiers in logs. Observability: expose <code>recon.external.calls</code>, <code>recon.external.latency</code> metrics and traces. Tests: adapter-level unit tests using recorded HTTP fixtures and a contract test asserting canonical output schema. </td></tr><tr><td data-label="Technical Breakdown — recon_service.py"> <strong><code>pairing_strategy(internal_iter: Iterator[Record], external_iter: Iterator[Record], strategy: str = &#x27;key&#x27;) -&gt; Iterator[Pair]</code></strong><br><br>Small pure-ish function that pairs internal and external records according to <code>strategy</code>: exact key-join, time-window, fuzzy-key (approx string match), or multi-key composite. Responsibilities: expose deterministic, testable behavior for each strategy and emit per-pair confidence metadata. Should be memory efficient — prefer merge-join on sorted streams; fall back to windowed buffering for fuzzy matches. Provide a small pluggable interface so new strategies can be registered. Error cases: unmatched records should still be emitted as <code>Pair(internal=None)</code> or <code>Pair(external=None)</code> rather than dropped. Tests: exhaustive permutations for small synthetic streams, including duplicates and out-of-order inputs. </td></tr><tr><td data-label="Technical Breakdown — recon_service.py"> <strong><code>reconcile_record(pair: Pair, rules: Sequence[Rule], context: ReconContext) -&gt; ReconciliationResult</code></strong><br><br>Core pure function implementing comparison logic for a single paired unit. Responsibilities: apply deterministic rules (numeric tolerances, tax-period alignment, currency conversion via injected fx adapter, rounding rules), produce a <code>ReconciliationResult</code> containing <code>ok|mismatch|missing|extra</code>, field-level diagnostics, suggested fixes (if low-risk), and a reproducible <code>trace</code> that explains decisions (rule ids, input values, intermediate normalized values). Must NOT perform persistence. Security: ensure redaction of PII included in result traces. Performance: keep CPU-bound work optimized and free of heavy allocations; vectorize where rules allow. Tests: unit tests covering rule precedence, tolerance edge-cases, currency/case normalization, and floating point stability. </td></tr><tr><td data-label="Technical Breakdown — recon_service.py"> <strong><code>score_discrepancy(result: ReconciliationResult, model: Optional[ScoringModel]=None) -&gt; ScoredDiscrepancy</code></strong><br><br>Assigns a risk/priority score to discrepancies for triage. Responsibilities: combine rule-based severity mapping (required) with optional ML model outputs (pluggable) to yield a single <code>0..1</code> score and categorical priority (<code>low|medium|high|critical</code>). Must be deterministic for rule-only paths and probabilistic when model is used; expose <code>score_explainer</code> metadata for audit. Guardrails: if ML model fails or is unavailable, fall back to rule-based heuristics and set <code>scoring_model_version=null</code> in metadata. Tests: deterministic mapping tests, model-fallback tests, adversarial input stability tests. Observability: histogram of scores and counts by severity. </td></tr><tr><td data-label="Technical Breakdown — recon_service.py"> <strong><code>aggregate_discrepancies(discrepancies: Iterable[ScoredDiscrepancy]) -&gt; ReconSummary</code></strong><br><br	>Aggregate function that reduces per-record findings into job-level summary: counts (ok, missing, extra, mismatches), top-N offending accounts, monetary deltas, aggregate risk score, and suggested actions (auto-accept, flag-for-review). Responsibilities: compute checkpoints for incremental persistence (e.g., batches of 1k results), optionally compute sampling for manual QA, and attach <code>applied_rule_version</code> and <code>scoring_model_version</code>. Performance: streaming aggregation to avoid keeping full details in memory; maintain rolling top-K data structures. Tests: aggregations with edge-case counts, extremely skewed data, and numeric sums that could overflow. </td></tr><tr><td data-label="Technical Breakdown — recon_service.py"> <strong><code>persist_recon_results(job_id: str, summary: ReconSummary, details_iter: Iterator[ScoredDiscrepancy], *, batch_size: int=1000) -&gt; None</code></strong><br><br>Persist aggregated and detailed results atomically and append-only. Responsibilities: write summary to job record, write details to audit store (append-only JSONL or dedicated recon table), update reconciliation index (for queries by external id, account, date), and emit metrics. Implement batched writes and checkpointing so long jobs can resume without duplicating persisted rows — use per-batch idempotency keys. Ensure persistence is durable before marking job <code>complete</code>. Error handling: on partial DB failure, mark job <code>degraded</code> and persist retry metadata to dead-letter. Tests: durability tests, idempotent re-run asserts, and partial-failure recovery. Security: encrypt PHI at rest if present. </td></tr><tr><td data-label="Technical Breakdown — recon_service.py"> <strong><code>build_recon_manifest(job_meta: Dict, summary: ReconSummary, checksums: Dict[str,str]) -&gt; Manifest</code></strong><br><br	>Construct a canonical manifest describing the reconciliation job and results for audit/export. Include metadata: job_id, applied_rule_version, scoring_model_version, snapshot_refs with checksums, start/end timestamps, summary metrics, provider signatures (if signing enabled), and an optional <code>bundle_signature</code>. Responsibility: canonicalize manifest JSON deterministically (stable key order) for reproducible signatures. Tests: ensure manifest canonicalization yields stable bytes and signing/verification round-trips succeed. </td></tr><tr><td data-label="Technical Breakdown — recon_service.py"> <strong><code>notify_recon_outcome(job_id: str, channels: Sequence[str], summary: ReconSummary) -&gt; None</code></strong><br><br>Outbound notification helper. Responsibilities: publish events (internal event bus, webhook, email tasks, Slack) with sanitized payloads (no PII), attach trace/context, and support synchronous and asynchronous notify backends. Respect rate-limits and retry semantics of each notifier. Side-effects: network calls. Errors: log but do not cause job re-run; if delivery to critical systems fails and <code>settings.recon.strict</code> is true, raise a <code>NotificationFailure</code> to be surfaced to operator dashboards. Tests: mock notifiers and assert payloads and redaction. </td></tr><tr><td data-label="Technical Breakdown — recon_service.py"> <strong><code>make_recon_handler(app) -&gt; Callable</code></strong><br><br>HTTP/CLI entrypoint factory that returns a request handler for triggering reconciliation via API. Responsibilities: parse and validate request, enforce RBAC/authorization (who may request recon), run <code>enqueue_reconcile_job</code>, and return a stable response shape (<code>{job_id, status_url}</code>). Must perform shallow checks only (no heavy IO on request path). For admin debug endpoints, support synchronous <code>?wait=true</code> mode that calls <code>run_reconciliation</code> in-process but only in dev/test environments. Tests: route-level unit tests covering auth, payload validation, and synchronous vs async modes. Logging: include <code>request_id</code>, <code>user_id</code>, and avoid secrets in logs. </td></tr><tr><td data-label="Technical Breakdown — recon_service.py"> <strong><code>start_worker_pool(settings: Settings) -&gt; WorkerPool</code></strong><br><br	>Lifecycle management for reconciliation workers. Responsibilities: create bounded worker pool sized by <code>settings.recon.max_workers</code>, wire graceful shutdown handlers, implement task supervision (restart policy with capped restarts), and provide observability hooks (worker heartbeat, in-flight tasks gauge). Workers should acquire jobs from queue in a fair manner and call <code>run_reconciliation</code>. Also implement backpressure: if queue backlog exceeds threshold, reduce concurrency. Tests: integration tests that assert graceful shutdown awaits running jobs up to <code>settings.shutdown_grace_period</code> and that workers restart on transient failures. </td></tr><tr><td data-label="Technical Breakdown — recon_service.py"> <strong><code>stop_worker_pool(pool: WorkerPool, *, force: bool=False) -&gt; None</code></strong><br><br>Graceful shutdown. Responsibilities: stop accepting new jobs, signal workers to stop, wait with timeout, and optionally force-cancel. On shutdown, ensure open adapters and DB clients are closed in reverse order. Always persist worker state to allow safe restart. Catch and log all exceptions; never raise during normal shutdown. Tests: simulate stuck workers and verify forced termination path and state persisted for restart. </td></tr><tr><td data-label="Technical Breakdown — recon_service.py"> <strong><code>backfill_reconcile(range_spec: RangeSpec, external_source: str, *, parallel: int=4) -&gt; Generator[str]</code></strong><br><br>Utility to schedule a large backfill over historical snapshots or date ranges. Responsibilities: divide range into idempotent chunks, schedule <code>enqueue_reconcile_job</code> for each chunk, respect concurrency limits, apply rate-limiters to external sources, and provide progress iterator of scheduled job ids. Ensure chunk boundaries avoid overlap and duplicate work. Provide safe-guards to pause/resume backfill by storing progress markers. Tests: chunking logic tests, idempotency across restarts, and stress tests for large ranges. </td></tr><tr><td data-label="Technical Breakdown — recon_service.py"> <strong><code>reconcile_health_check() -&gt; Dict</code></strong><br><br>Lightweight health handler used by server /health or admin endpoints. Responsibilities: confirm job queue is reachable, a DB readonly probe responds, and at least one worker heartbeat exists. Must avoid heavy external calls. Return shape: <code>{status, subsystems:{db:ok|fail, queue:ok|fail, workers:ok|degraded}, last_heartbeat}</code>. Tests: unit test with mocked subsystems, and an integration test that toggles subsystems to produce degraded state. </td></tr><tr><td data-label="Technical Breakdown — recon_service.py"> <strong>Implementation patterns & guardrails</strong><br><br>— <strong>Dependency injection</strong>: adapters (external fetchers), storage clients, rule engines, and scoring models must be injected via <code>app.state._extensions</code> or constructor parameters. <br>— <strong>Idempotency</strong>: compute and persist job-level and batch-level idempotency keys. Re-run must be safe. <br>— <strong>Streaming & memory</strong>: always prefer streaming iterators for snapshots and external feeds. Use merge-join or limited windows, never load millions of records into memory. <br>— <strong>Observability</strong>: record <code>recon.job.*</code> metrics, per-step durations, and expose traces with <code>trace_id</code>. Attach short human-readable hints to logs. <br>— <strong>Security & privacy</strong>: redact PII in logs and event payloads; enable at-rest encryption for sensitive fields; access to raw external records limited to authorized roles. <br>— <strong>Fail-fast vs degraded</strong>: <code>settings.recon.strict</code> toggles whether external adapter failures mark job failed vs degraded. Document which systems are critical. <br>— <strong>Testing hygiene</strong>: provide <code>build_test_recon_job</code> helper returning a job record and small in-memory adapters for deterministic tests. Include golden fixtures and small corpus tests for pairing strategies. </td></tr><tr><td data-label="Technical Breakdown — recon_service.py"> <strong>Recommended tests & CI checks</strong><br><br>1. <strong>Unit</strong>: pairing strategies, reconcile_record rule precedence, scoring deterministic mapping, manifest canonicalization. <br>2. <strong>Integration (fast)</strong>: run a single job with small snapshots and a mocked external adapter; assert persisted summary and notifications. <br>3. <strong>Load</strong>: simulate large snapshot streaming, verify memory usage stays bounded and worker concurrency limits respected. <br>4. <strong>Chaos</strong>: adapter intermittent failures, snapshot checksum mismatch, and queue downtime; assert correct <code>degraded</code> or <code>retryable</code> transitions. <br>5. <strong>Security</strong>: tests that PII is redacted in logs and that manifest/noified payloads do not leak secrets. <br>6. <strong>E2E</strong>: full pipeline from upload → validate → enqueue → worker → export. Automate these in CI gating. </td></tr><tr><td data-label="Technical Breakdown — recon_service.py"> <strong>Operational checklist before production</strong><br><br>— Ensure <code>strict</code> flags and retention policies set. <br>— Configure backpressure thresholds and <code>max_workers</code> appropriate to host resources. <br>— Ensure external adapters have credentials stored as secrets and rotate them. <br>— Wire observability: Prometheus metrics, structured logs, and tracing. <br>— Test failover: worker restarts, queue lag surge, and partial persistence failures. <br>— Prepare runbooks for <code>degraded</code> jobs and manual reconciliation UI steps. </td></tr></tbody></table></div><div class="row-count">Rows: 21</div></div><div class="table-caption" id="Table12" data-table="Docu_0163_12" style="margin-top:2mm;margin-left:3mm;"><strong>Table 12</strong></div>
<div class="table-wrapper" data-table-id="table-12"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown — src/exception_queue.py"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown — src/exception_queue.py</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — src/exception_queue.py"> <strong>File-level responsibilities</strong><br><br>This module implements the system's canonical exception handling / human-review queue: a durable, searchable, and observable place where "business exceptions" (recon mismatches, exporter failures, validation edge-cases, permanent rule failures) are parked for automated retry, enrichment, or human action. Its responsibilities are: (1) provide a small, well-documented adapter surface that supports multiple backing stores (RDBMS queue table, Redis streams, SQS-like, or a hybrid), (2) implement robust claim/ack/release semantics (visibility timeout semantics), (3) encapsulate retry and dead-letter policies, (4) enforce idempotency and deduplication for incoming exception records, (5) provide APIs for listing, searching, and updating exception entries for automation and UI, (6) integrate with auditor/recorder/recon_service and notification systems (email, Slack, webhook), and (7) expose instrumentation and health checks. Keep this file focused on orchestration and policies — heavy implementations of storage drivers, notification senders, or complex search indexes must live in small testable adapters that this module composes. Document configuration knobs (retention, retry_policy, visibility_timeout, max_attempts, dedupe_window) at top-of-file. Ensure import-time safety (no blocking I/O at import) and idempotent initialization. Unit/contract tests should be possible against an in-memory backend implementation. </td></tr><tr><td data-label="Technical Breakdown — src/exception_queue.py"> <strong><code>init_exception_queue(settings, storage_adapter) -&gt; ExceptionQueue</code></strong><br><br><strong>Responsibility:</strong> Factory that returns a configured <code>ExceptionQueue</code> object (or singleton) wired with settings (retry policy, retention rules, visibility timeout, dead-letter thresholds) and a <code>storage_adapter</code> implementing the persistence primitives. Must not perform network I/O at import. Validate settings early and attach a lightweight <code>health_check</code> closure to the returned object's public API. Persisted state (e.g., connection pools) must not be opened here — <code>connect()</code>/<code>close()</code> methods on adapters are invoked by application startup/shutdown hooks. Unit tests should create the queue with a fake adapter and assert the policy properties are set and that repeated calls are idempotent. </td></tr><tr><td data-label="Technical Breakdown — src/exception_queue.py"> <strong><code>enqueue(exception_record: dict, dedupe: bool = True) -&gt; str</code></strong><br><br><strong>Responsibility:</strong> Accept a normalized exception record and persist it into the queue. Must: validate payload shape (using shared validators), compute dedupe key when requested, and either create-or-return existing id if dedupe hits within configured window. Inputs: a dictionary describing the exception (required fields: <code>source</code>, <code>type</code>, <code>payload_hash</code>, <code>first_seen_ts</code>, <code>severity</code>, optional <code>suggestions</code>, <code>metadata</code>). Output: canonical queue id (string/UUID). Side-effects: write to storage adapter, increment metrics (<code>exceptions.enqueued</code>), emit a compact enqueue audit event. Important: this function must be idempotent for repeated calls with the same dedupe key. Error handling: transient adapter errors bubble as a <code>TransientQueueError</code> (retry at caller) while schema errors raise <code>InvalidExceptionRecord</code>. Tests: unit tests that call <code>enqueue</code> twice with same dedupe key assert single record created and idempotent response. </td></tr><tr><td data-label="Technical Breakdown — src/exception_queue.py"> <strong><code>bulk_enqueue(records: Iterable[dict], dedupe=True, batch_size=100) -&gt; List[str]</code></strong><br><br><strong>Responsibility:</strong> Efficiently enqueue many exceptions with batching semantics. Should validate each record, compute dedupe keys, and call adapter batch-write if available. Returns list of created/returned ids in input order. Must handle partial failures: on partial adapter failure return created ids and a structured error summarizing failed items; caller decides retry policy. Observability: record <code>exceptions.bulk_enqueue.size</code> histogram and overall latency. Tests: stress test with mixed valid/invalid records and ensure partial failures handled predictably. </td></tr><tr><td data-label="Technical Breakdown — src/exception_queue.py"> <strong><code>claim_next(consumer_id: str, limit: int = 10, visibility_timeout: Optional[int] = None, priority_filter: Optional[List[str]] = None) -&gt; List[ExceptionClaim]</code></strong><br><br><strong>Responsibility:</strong> Atomically select up to <code>limit</code> ready exceptions for processing and mark them as claimed by <code>consumer_id</code> with a configurable visibility timeout. Must support priority and type filtering. The returned <code>ExceptionClaim</code> includes <code>exception_id</code>, <code>payload</code>, <code>attempt_count</code>, <code>claimed_at</code>, <code>visibility_deadline</code>. Concurrency: implement claim atomically — rely on the storage adapter for compare-and-set or DB row-locking. Guarantee: best-effort FIFO within priority buckets; document that strict global FIFO is not guaranteed under distributed/partitioned stores. If visibility_timeout is omitted, use the queue default. Errors: if no items available, return empty list. Tests: concurrency tests with multiple claimers to assert single-claim semantics and visibility expiry re-appearing the item. </td></tr><tr><td data-label="Technical Breakdown — src/exception_queue.py"> <strong><code>ack(exception_id: str, consumer_id: str, result_metadata: Optional[dict] = None) -&gt; None</code></strong><br><br><strong>Responsibility:</strong> Acknowledge successful processing of a claimed exception. Must verify <code>consumer_id</code> matches claim owner (or allow <code>force</code> mode only for admin operations). Side-effects: mark as <code>resolved</code>, persist <code>resolved_at</code>, append <code>result_metadata</code> to audit/recorder, increment success metrics, remove or move record to an archive table depending on retention config. Idempotence: ack should be idempotent — repeated acks are no-op. Failure semantics: if the item is already unclaimed or assigned to another consumer, raise <code>ClaimOwnershipError</code>. Tests: assert ack transitions, idempotence, correct audit entry. </td></tr><tr><td data-label="Technical Breakdown — src/exception_queue.py"> <strong><code>release(exception_id: str, consumer_id: str, reason: Optional[str] = None, backoff_strategy: Optional[str] = None, escalate: bool = False) -&gt; None</code></strong><br><br><strong>Responsibility:</strong> Release a previously claimed exception back to the queue for retry (or escalate to DLQ/human review). Behavior: increment attempt_count, compute next visible time using <code>backoff_strategy</code> (exponential, linear, fixed) or explicit <code>next_attempt_at</code>, attach <code>last_error</code>/<code>trace</code> metadata, and reinsert into ready set with new visibility deadline or move to dead-letter if attempts >= max_attempts or <code>escalate=True</code>. Side-effects: emit retry metric and audit entry. Concurrency: ensure atomic increment of attempt_count. Safety: avoid resetting original <code>first_seen_ts</code>. Tests: assert release increments attempt_count, computes backoff correctly, and escalates to DLQ at threshold. </td></tr><tr><td data-label="Technical Breakdown — src/exception_queue.py"> <strong><code>mark_dead_letter(exception_id: str, consumer_id: Optional[str], reason: str, metadata: Optional[dict] = None) -&gt; None</code></strong><br><br><strong>Responsibility:</strong> Forced move of an item to dead-letter state for manual inspection. Must preserve the full history of attempts, store <code>dead_lettered_at</code>, <code>dead_letter_reason</code>, and optionally a <code>resolution_hint</code>. Integrations: publish a notification (email/webhook) to configured channels and optionally create a <code>recon_service</code> entry for reconciliation. Side-effects: increment <code>exceptions.dead_lettered</code> metric. Tests: ensure move is atomic and DLQ entries are queryable and immutable except for admin annotations. </td></tr><tr><td data-label="Technical Breakdown — src/exception_queue.py"> <strong><code>requeue_dead_letter(exception_id: str, admin_user: str, reason: Optional[str] = None) -&gt; None</code></strong><br><br><strong>Responsibility:</strong> Admin operation to move a dead-lettered item back into the ready queue (resetting attempt_count or preserving it per policy). Must require elevated ACL and emit an audit event including <code>admin_user</code>. Safety: document that requeueing may create duplicates if original processing completed externally; caller should validate idempotency keys. Tests: ACL enforcement and correct state transition. </td></tr><tr><td data-label="Technical Breakdown — src/exception_queue.py"> <strong><code>get(exception_id: str) -&gt; dict</code></strong><br><br><strong>Responsibility:</strong> Read-only fetch of the canonical exception record including its processing history (<code>attempts[]</code>), current state, assigned consumer, and audit trace. Must redact PII from returned payloads depending on <code>settings.redaction_rules</code> unless caller has <code>unredacted</code> permission. Tests: verify redaction and full detail when allowed. </td></tr><tr><td data-label="Technical Breakdown — src/exception_queue.py"> <strong><code>list(filter: dict = None, limit: int = 50, cursor: Optional[str] = None, sort: Optional[str] = None) -&gt; (List[dict], Optional[str])</code></strong><br><br><strong>Responsibility:</strong> Paginated listing API used by UI and automation. Support filtering by <code>state</code> (ready, claimed, dead_letter, resolved), <code>source</code>, <code>severity</code>, date ranges, and free-text on <code>summary</code>/<code>normalized_message</code> using the storage adapter's index. Return opaque cursor tokens for continuation. Performance: ensure queries use appropriate indexes; avoid full-table scans. Tests: pagination correctness, filter behavior, and cursor stability under concurrent writes. </td></tr><tr><td data-label="Technical Breakdown — src/exception_queue.py"> <strong><code>update(exception_id: str, updater: dict, actor: str) -&gt; dict</code></strong><br><br><strong>Responsibility:</strong> Controlled updates to mutable fields (annotations, priority, assigned_owner, suggested_resolution). Preserve immutable core fields (<code>first_seen_ts</code>, <code>payload_hash</code>). Must persist an audit record citing <code>actor</code>. ACL: restrict who can change priority/assignment. Idempotence: repeated identical updates are no-op but must still record audit. Tests: ACL checks, audit entries, and field immutability. </td></tr><tr><td data-label="Technical Breakdown — src/exception_queue.py"> <strong><code>delete(exception_id: str, actor: str, hard: bool = False) -&gt; None</code></strong><br><br><strong>Responsibility:</strong> Remove or soft-delete an exception. Default is soft-delete (mark <code>deleted=True</code>, <code>deleted_at</code>, <code>deleted_by</code>) to preserve history; <code>hard=True</code> is restricted and permanently removes storage. Always append an audit entry. For compliance, deletion must preserve anonymized audit trails unless retention policy allows full wipe. Tests: soft/hard delete behavior and permission gating. </td></tr><tr><td data-label="Technical Breakdown — src/exception_queue.py"> <strong><code>schedule_retry(exception_id: str, next_attempt_at: datetime, actor: str) -&gt; None</code></strong><br><br><strong>Responsibility:</strong> Administrative scheduling of next retry for a specific exception (useful for after manual patching or external dependency recovery). Must accept <code>next_attempt_at</code> and optionally <code>attempt_count</code> override. Validate <code>next_attempt_at</code> is in future. Tests: scheduling works and enforces future timestamp. </td></tr><tr><td data-label="Technical Breakdown — src/exception_queue.py"> <strong><code>compute_backoff(attempt_count: int, policy: str | dict) -&gt; int</code></strong><br><br><strong>Responsibility:</strong> Pure utility computing next delay in seconds. Support policies: <code>exponential</code> (base, cap), <code>fixed</code>, <code>linear</code>, <code>jittered</code>, and advanced scripts referenced by name. Must return safe bounded values and never exceed <code>settings.max_backoff_seconds</code>. Unit tests should exhaustively check boundaries and jitter distributions. </td></tr><tr><td data-label="Technical Breakdown — src/exception_queue.py"> <strong><code>dedupe_key(payload: dict, keys: Optional[List[str]] = None) -&gt; str</code></strong><br><br><strong>Responsibility:</strong> Deterministic computation of a dedupe key (hash) from the exception payload using canonicalization (sorted keys, normalized whitespace, redaction of variable fields unless included). This function must be deterministic across language versions and stable across deployments. Document which payload fields are included by default (e.g., <code>source</code>, <code>payload_hash</code>, <code>record_id</code>, <code>applied_rule_version</code>). Tests: canonicalization stability, collision probability checks. </td></tr><tr><td data-label="Technical Breakdown — src/exception_queue.py"> <strong><code>archive_resolved(exception_id: str, archive_adapter=None) -&gt; None</code></strong><br><br><strong>Responsibility:</strong> Move resolved/acknowledged items to long-term archive (cold storage or object store) for retention policies. Archive must contain complete JSONL record with attempts history and manifest metadata (applied_rule_version, generator_version). Side-effects: write to archive adapter, then optionally delete original depending on retention. Ensure transactional semantics: archive success before deletion. Tests: failover handling when archive backend is transient. </td></tr><tr><td data-label="Technical Breakdown — src/exception_queue.py"> <strong><code>purge_older_than(cutoff_ts: datetime, dry_run: bool = True, force: bool = False) -&gt; int</code></strong><br><br><strong>Responsibility:</strong> Administrative cleanup of old exceptions according to retention policy. Must default to <code>dry_run</code> for safety and require <code>force</code> for irreversible operations. Return number of items affected. Respect legal holds and compliance flags; skip entries with <code>legal_hold=True</code>. Tests: respect dry_run and legal hold. </td></tr><tr><td data-label="Technical Breakdown — src/exception_queue.py"> <strong><code>notify_human_review(exception_id: str, channels: List[str], severity: str = &#x27;high&#x27;) -&gt; None</code></strong><br><br><strong>Responsibility:</strong> Create and dispatch human-review notifications, including a pre-computed summary, risk score, and direct deep-link into UI. Must use notifier adapter(s) and be resilient (exponential local retry on transient failure). Notifications must not include raw PII unless recipient has permission; otherwise include redacted snapshot and link to secure UI. Tests: verify notification payloads, redaction behavior, and retry logic. </td></tr><tr><td data-label="Technical Breakdown — src/exception_queue.py"> <strong><code>health_check() -&gt; dict</code></strong><br><br><strong>Responsibility:</strong> Lightweight synchronous health probe for the queue: checks storage adapter connectivity (lightweight query), recent enqueue/ack metrics, and DLQ size. Return structured result <code>{ok: bool, details: {adapter_ok, dlq_count, queue_depth, last_enqueue_age_seconds}}</code>. This is attached to app health endpoints. Keep this function fast and non-blocking. Tests: simulate adapter failure and ensure health_report toggles <code>ok=false</code>. </td></tr><tr><td data-label="Technical Breakdown — src/exception_queue.py"> <strong>Observability helpers</strong> (<code>_record_metric</code>, <code>_emit_audit_event</code>, <code>_log_event</code>)<br><br><strong>Responsibility:</strong> Internal helpers that wrap instrumentation and recorder/auditor integrations. Must: include trace identifiers, <code>service=pph21</code>, and redact sensitive fields in emitted events. Expose a <code>dry_run</code> mode where metrics/audit writes are captured for unit tests. Ensure structured logging (JSON lines) and provide sampling controls for high-volume events. Tests: ensure redaction rules and sampling apply. </td></tr><tr><td data-label="Technical Breakdown — src/exception_queue.py"> <strong>Integration & adapter contracts</strong><br><br><strong>Responsibility:</strong> Define the minimal adapter interface the queue expects from a persistence backend: <code>connect()</code>, <code>close()</code>, <code>insert(record)</code>, <code>bulk_insert(records)</code>, <code>claim(consumer_id, limit, visibility)</code>, <code>ack(id, consumer_id)</code>, <code>release(id, consumer_id, next_visible_at)</code>, <code>move_to_dead_letter(id, reason, metadata)</code>, <code>get(id)</code>, <code>list(filters, limit, cursor)</code>, <code>update(id, patch)</code>, <code>delete(id, hard=False)</code>. Keep adapters small — prefer composition for features (e.g., RedisStreamAdapter wraps Redis primitives). Tests: provide an in-memory adapter implementing this interface and a contract test-suite that all adapters must pass. </td></tr><tr><td data-label="Technical Breakdown — src/exception_queue.py"> <strong>Security & privacy</strong><br><br>— Ensure PII is redacted by default when stored and when emitted to logs/notifications; provide <code>reveal()</code> accessor guarded by ACL/elevated token and audit. <br>— Staff-facing actions (<code>requeue_dead_letter</code>, <code>hard_delete</code>, <code>unredact</code>) must require an authenticated actor and be recorded. <br>— Stored sensitive data must be encrypted at rest (delegated to storage_adapter or use envelope encryption with <code>crypto</code> helper). <br>— Rate-limit public APIs that can create exceptions to avoid spam or backfill storms. Include tests for redaction and ACL enforcement. </td></tr><tr><td data-label="Technical Breakdown — src/exception_queue.py"> <strong>Failure modes & semantics</strong><br><br>— <strong>At-least-once vs at-most-once</strong>: document that claim/ack model yields <em>at-least-once</em> processing guarantees; callers must write idempotent processors or use idempotency keys in exception payloads. <br>— <strong>Visibility timeouts</strong>: if a consumer fails to ack before visibility_deadline, the item returns to ready state; implement a dead-claim guard to avoid tight re-delivery loops. <br>— <strong>Partial-batch failures</strong>: bulk operations must use per-item outcome reporting so callers can retry specific failures. <br>— <strong>Adapter outages</strong>: transient adapter errors should surface as <code>TransientQueueError</code>; the module must not swallow them silently. Provide a circuit-breaker or backoff at caller level for prolonged outages. </td></tr><tr><td data-label="Technical Breakdown — src/exception_queue.py"> <strong>Operational knobs & configuration</strong><br><br>List and document top-level knobs: <code>retry_policy</code> (type, base_seconds, cap_seconds, max_attempts), <code>dead_letter_strategy</code> (immediate/escalate-after-n attempts), <code>visibility_timeout_default</code>, <code>dedupe_window_seconds</code>, <code>retention_days</code>, <code>archive_batch_size</code>, <code>notification_channels</code>, <code>max_claim_batch_size</code>, <code>backoff_jitter</code>. Each knob must have sane defaults and be validated by <code>init_exception_queue</code>. Unit tests should assert behavior for representative knob values. </td></tr><tr><td data-label="Technical Breakdown — src/exception_queue.py"> <strong>Testing guidance</strong><br><br>— <strong>Unit</strong>: pure functions (<code>compute_backoff</code>, <code>dedupe_key</code>, <code>validate_exception_record</code>), and adapter contract mocks. <br>— <strong>Integration (fast)</strong>: in-memory adapter running in-process: test enqueue → claim → release → ack cycle, DLQ path, requeue path, and health_check. <br>— <strong>Concurrency</strong>: simulate multiple consumers claiming simultaneously to assert single-claim semantics and visibility expiry re-delivery. <br>— <strong>Chaos</strong>: adapter transient failures during claim/ack and ensure the module surfaces correct transient vs permanent errors. <br>— <strong>Security</strong>: test redaction and ACL enforcement for admin functions. <br>— <strong>E2E</strong>: run a job that ingests a deterministic failure and assert it moves through the queue to human-review notification and back to resolved after manual patch + requeue. </td></tr><tr><td data-label="Technical Breakdown — src/exception_queue.py"> <strong>Operational runbooks & run-time notes</strong><br><br>— Monitor queue depth, DLQ growth rate, average time-to-resolve, and claim/ack latency. <br>— Alert on sustained increase in <code>dead_letter_rate</code> or <code>avg_attempts_before_resolve</code>. <br>— Provide an admin endpoint to re-run DLQ reconciliation in bulk with dry-run mode. <br>— Document manual steps to inspect a DLQ item safely (redacted view vs unredacted) and how to requeue after remediation. <br>— Keep retention and legal-hold controls surfaced in the UI and accessible only to compliance roles. </td></tr><tr><td data-label="Technical Breakdown — src/exception_queue.py"> <strong>Implementation patterns & guardrails</strong><br><br>— <strong>Small functions</strong>: keep the module orchestrator-like; delegate storage, notification, and archiving to adapters. <br>— <strong>Idempotency</strong>: design for at-least-once delivery — processors should be idempotent and exception payloads should include idempotency tokens where possible. <br>— <strong>No import-time I/O</strong>: adapters must be lazy-initialized. <br>— <strong>Audit trail</strong>: every state change must append an audit record (actor, timestamp, before/after snapshot). <br>— <strong>Back-pressure</strong>: provide an overload strategy (reject with <code>429</code>/enqueue-shed) if queue depth exceeds configured safety thresholds. <br>— <strong>Testing</strong>: provide an <code>InMemoryExceptionQueue</code> used by unit/integration tests to avoid dependence on external services. </td></tr><tr><td data-label="Technical Breakdown — src/exception_queue.py"> <strong>Recommended metrics & logs</strong><br><br>Emit time-series and counters: <code>exceptions.enqueued_total</code>, <code>exceptions.acked_total</code>, <code>exceptions.released_total</code>, <code>exceptions.dead_letter_total</code>, <code>exceptions.requeued_total</code>, <code>exceptions.archive_failures</code>, <code>exceptions.queue_depth</code>, <code>exceptions.dlq_depth</code>, <code>exceptions.claim_latency_ms</code>. Logs must include <code>trace_id</code>, <code>request_id</code>, <code>service=pph21</code>, <code>exception_id</code>, <code>state</code>, and a redacted payload snapshot when helpful. Provide dashboards (Queue depth over time, Time-to-resolution percentiles, DLQ rate by source) and create an SLO around median time-to-resolve for high-severity exceptions. </td></tr><tr><td data-label="Technical Breakdown — src/exception_queue.py"> <strong>Maintenance notes</strong><br><br>— When changing the shape of <code>exception_record</code>, provide migration scripts that: (a) map old fields, (b) rehydrate dedupe keys, and (c) re-index search fields. <br>— When adding a new adapter, run the adapter contract tests and smoke test claim/ack cycles under load. <br>— When tuning retry/backoff, measure worker concurrency to avoid retry storms. <br>— Keep operational runbooks for DLQ investigations and ensure a “safety net” rollback (requeue in dry-run mode) is available. </td></tr></tbody></table></div><div class="row-count">Rows: 30</div></div><script src="assets/xlsx.full.min.js?v=1758605028" defer></script>
<script src="assets/script.js?v=1759748863" defer></script>
<script src="assets/worker.js?v=1758331710" defer></script>
<script>
(function(){
  const template = "{table}_{date}";
  const userVal = "";
  const hrefPrefix = "assets";
  function formatName(tableName) {
    const date = (new Date()).toISOString().slice(0,10);
    return template.replace('{table}', tableName).replace('{date}', date).replace('{user}', userVal);
  }
  const btn = document.getElementById('exportBtn');
  if(btn) {
    btn.addEventListener('click', async function() {
      try {
        const html = document.documentElement.outerHTML;
        if(html.length > 2000000) { alert('Export refused: html too large'); return; }
        if(window.Worker) {
          const workerUrl = (function(){ try{ return hrefPrefix + '/worker.js'; }catch(e){ return null; } })();
          if(workerUrl) {
            try {
              const worker = new Worker(workerUrl);
              worker.postMessage({html: html, format: 'pdf'});
              worker.onmessage = function(e) { console.log('worker:', e.data); alert('Worker replied: '+(e.data.msg||e.data.status)); };
            } catch(err) { console.warn('Worker create failed', err); alert('Worker not available'); }
          } else { alert('Worker not available'); }
        } else { alert('Export worker not supported in this environment.'); }
      } catch(err) { console.warn('Export failed', err); alert('Export worker not available. See console for details.'); }
    });
  }
})();
window.addEventListener('load', function(){ try{ document.querySelectorAll('.table-wrapper').forEach(function(e){ e.style.opacity='1'; }); }catch(e){} });
</script>
</div>
</body>
</html>