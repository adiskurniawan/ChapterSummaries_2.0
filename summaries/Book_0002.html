<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='utf-8'><meta name='viewport' content='width=device-width,initial-scale=1'>
<title>Tables Viewer v2.1</title>
<style>:root{--main-header-height:56px;} .table-wrapper{opacity:0;min-height:24px;transition:opacity .12s linear}</style>
<link rel="stylesheet" href="assets/style.css?v=1759925496">
<link rel="stylesheet" href="assets/overrides.css?v=1764047295">
</head><body>
<div id="tables-viewer" role="region" aria-label="Tables viewer">
<div id="stickyMainHeader">
<div id="tv-header">
<div><h1>Tables Viewer v2.1</h1></div>
<div style="display:flex;gap:8px;align-items:center;">
<input id="searchBox" class="tv-search" type="search" placeholder="Search" aria-label="Search tables" />
<button id="modeBtn" type="button" onclick="toggleMode()" aria-label="Toggle theme">Theme</button>
<button id="toggleAllBtn" type="button" aria-label="Toggle all" onclick="toggleAllTables()">Collapse All Tables</button>
<button id="copyAllPlainBtn" class="copy-btn" type="button" onclick="copyAllTablesPlain()" aria-label="Copy all tables as plain text">Copy All Tables (Plain Text)</button>
<button id="copyAllTablesBtn" class="copy-all-btn" type="button" onclick="copyAllTablesMarkdown()" aria-label="Copy All Tables (Markdown)">Copy All Tables (Markdown)</button>
<button id="copyAllMdBtn" style="display:none" aria-hidden="true"></button>
<button id="resetAllBtn" type="button" onclick="resetAllTables()" aria-label="Reset All Tables">Reset All Tables</button>
</div></div>
<script>
(function(){
  function ensureDelegation(){
    try {
      var vis = document.getElementById('copyAllTablesBtn');
      var alias = document.getElementById('copyAllMdBtn');
      if(!vis || !alias) return;
      // Delegate addEventListener/removeEventListener to visible button
      alias.addEventListener = function(type, listener, options){
        vis.addEventListener(type, listener, options);
      };
      alias.removeEventListener = function(type, listener, options){
        vis.removeEventListener(type, listener, options);
      };
      // Delegate onclick assignments
      Object.defineProperty(alias, 'onclick', {
        set: function(fn){ vis.onclick = fn; },
        get: function(){ return vis.onclick; },
        configurable: true
      });
      // Delegate focus/blur
      alias.focus = function(){ vis.focus(); };
      alias.blur = function(){ vis.blur(); };
      // If legacy code used direct addEventListener earlier than this script, listeners would already exist
      // on alias element; attempt to re-dispatch those by cloning them to visible button is non-trivial.
      // This approach covers the common case where legacy scripts query the alias and bind after DOM ready.
    } catch(e) {
      try{ console && console.warn && console.warn('alias delegation failed', e); }catch(_){} 
    }
  }
  if(document.readyState === 'loading'){
    document.addEventListener('DOMContentLoaded', ensureDelegation);
  } else {
    ensureDelegation();
  }
})();
</script>
<noscript><div style='color:#b91c1c'>JavaScript is disabled. Tables will be shown statically. For large tables enable JS for virtualization.</div></noscript>
<div id="tocBar" role="navigation" aria-label="Table of contents"><ul><li class="toc-item"><a class="toc-link" href="#Table1">Table 1</a></li>
<li class="toc-item"><a class="toc-link" href="#Table2">Table 2</a></li>
<li class="toc-item"><a class="toc-link" href="#Table3">Table 3</a></li>
<li class="toc-item"><a class="toc-link" href="#Table4">Table 4</a></li>
<li class="toc-item"><a class="toc-link" href="#Table5">Table 5</a></li>
<li class="toc-item"><a class="toc-link" href="#Table6">Table 6</a></li>
<li class="toc-item"><a class="toc-link" href="#Table7">Table 7</a></li>
<li class="toc-item"><a class="toc-link" href="#Table8">Table 8</a></li>
<li class="toc-item"><a class="toc-link" href="#Table9">Table 9</a></li>
<li class="toc-item"><a class="toc-link" href="#Table10">Table 10</a></li></ul></div></div>
<div class="table-caption" id="Table1" data-table="Book_0002_01" style="margin-top:2mm;margin-left:3mm;"><strong>Chapter 1 • The Five Principles of Prompting</strong></div>
<div class="table-wrapper" data-table-id="table-1"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by **CHAPTER 1 — The Five Principles of Prompting**"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>CHAPTER 1 — The Five Principles of Prompting</strong></div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="CHAPTER 1 — The Five Principles of Prompting"> <strong>Overview — Prompt engineering purpose & payoff</strong><br>Prompt engineering is the disciplined practice of designing inputs so generative models reliably produce desired outputs; it trades token cost and complexity for repeatability, parseability, and lower downstream error rates.<br>It is model-agnostic and applies to text and image models; the goal is reproducible, auditable, and efficient prompts for production systems.<br> </td></tr><tr><td data-label="CHAPTER 1 — The Five Principles of Prompting"> <strong>Key thesis — Five principles map to common failure modes</strong><br>1. Give Direction → solves vagueness and misaligned style.<br>2. Specify Format → prevents parsing failures and inconsistent structure.<br>3. Provide Examples → converts zero-shot ambiguity into few-shot reliability.<br>4. Evaluate Quality → creates feedback loops and measurable metrics.<br>5. Divide Labor → decomposes complexity into traceable steps.<br>Each principle targets one of the original naive prompt problems and together produce robust prompting pipelines.<br> </td></tr><tr><td data-label="CHAPTER 1 — The Five Principles of Prompting"> <strong>Why tokens and probability matter</strong><br>LLMs predict the next token; each prompt changes token probabilities and therefore outcomes.<br>Prompt length, temperature, and sampling parameters alter randomness and cost; optimize for minimum tokens that preserve performance.<br>Understand tokenization to reason about cost and to engineer compact, high-information prompts.<br> </td></tr><tr><td data-label="CHAPTER 1 — The Five Principles of Prompting"> <strong>Principle 1 — Give Direction (what & who)</strong><br>• Provide persona or role (e.g., "in the style of Steve Jobs") to bias tone and conventions.<br>• Supply constraints about vocabulary, register, and creativity budget.<br>• Use prewarming/internal retrieval: ask model for best practices then instruct it to follow them.<br>• For images, supply compositional direction (format, camera, artist, focal elements).<br> </td></tr><tr><td data-label="CHAPTER 1 — The Five Principles of Prompting"> <strong>Give Direction — Practical tips & failure modes</strong><br>1. Too little direction → average internet-like output; low reproducibility.<br>2. Too much conflicting direction → no coherent sample (model cannot resolve contradictory constraints).<br>3. Prefer prioritized direction: mark what matters most and drop lower priority constraints when conflicts occur.<br> </td></tr><tr><td data-label="CHAPTER 1 — The Five Principles of Prompting"> <strong>Principle 2 — Specify Format (structure & parsability)</strong><br>• Require machine-friendly formats (JSON, YAML, CSV) when outputs feed downstream systems.<br>• Use explicit instructions: "Return only JSON" and provide an example skeleton to complete.<br>• For visual outputs, specify format tokens like <code>stock photo</code>, <code>oil painting</code>, <code>illustration</code>, or environment constraints like <code>in Minecraft</code>.<br>• Resolve clashes between style and format by choosing a single primary constraint and documenting the tradeoff.<br> </td></tr><tr><td data-label="CHAPTER 1 — The Five Principles of Prompting"> <strong>Specify Format — Implementation notes</strong><br>• Provide an uncompleted JSON/YAML skeleton to strongly bias completion format.<br>• Use system message to lock behavior (e.g., "You are a helpful assistant that only responds in JSON").<br>• Validate outputs automatically with parsers; on failure, retry or apply transformation rules rather than manual fixes.<br> </td></tr><tr><td data-label="CHAPTER 1 — The Five Principles of Prompting"> <strong>Principle 3 — Provide Examples (one-shot → few-shot)</strong><br>• Few illustrative examples guide the model to desired style, format, and edge cases.<br>• One to three diverse examples typically improves reliability while preserving creativity; too many homogeneous examples constrain novelty.<br>• For images, provide base images (img2img) as examples; for text, include positive and negative examples if needed.<br> </td></tr><tr><td data-label="CHAPTER 1 — The Five Principles of Prompting"> <strong>Examples — Design patterns</strong><br>• Positive examples: short, diverse, clearly annotated outputs.<br>• Negative examples: optional — include only if you want to prevent specific failure modes.<br>• Vary examples to cover edge cases and unusual inputs you expect in production.<br> </td></tr><tr><td data-label="CHAPTER 1 — The Five Principles of Prompting"> <strong>Principle 4 — Evaluate Quality (metrics & A/B testing)</strong><br>• Use human evaluation (thumbs up/down or N-point scales) initially for subjective tasks.<br>• Automate where possible: ground truth checks, classifiers, other LLM evaluators, similarity metrics (BLEU/ROUGE), vector distance, and domain-specific correctness checks (math, logic, formats).<br>• Implement randomized blind testing and aggregate per-variant scores to compute lift (A vs B).<br> </td></tr><tr><td data-label="CHAPTER 1 — The Five Principles of Prompting"> <strong>Evaluation — Example workflow (practical)</strong><br>1. Generate N responses per prompt variant; randomize and blind them.<br>2. Collect human labels via simple UI or <code>input()</code> loop; store feedback.<br>3. Aggregate counts and mean scores by variant to compare.<br>4. Programmatically reweight examples and iterate on prompt length vs performance.<br> </td></tr><tr><td data-label="CHAPTER 1 — The Five Principles of Prompting"> <strong>Principle 5 — Divide Labor (chaining & decomposition)</strong><br>• Break monolithic tasks into subtasks: generate candidates → self-rate → filter → enrich → format.<br>• Use chain-of-thought prompts when internal reasoning improves consistency, or split steps into separate calls for observability.<br>• Automate self-evaluation by asking the model to score its own outputs, then verify with a secondary model or rule set.<br> </td></tr><tr><td data-label="CHAPTER 1 — The Five Principles of Prompting"> <strong>Divide Labor — Chains & agents</strong><br>• Use progressive summarization for large texts: chunk → summarize → summarize summaries.<br>• Use LangChain / agent frameworks to coordinate multi-step flows and external data calls.<br>• Be cautious with autonomous agents; they can be brittle and may introduce compounding errors if not strictly monitored.<br> </td></tr><tr><td data-label="CHAPTER 1 — The Five Principles of Prompting"> <strong>Product-naming example — applied principles</strong><br>• Direction: "in the style of Steve Jobs" or industry persona.<br>• Format: "Return as comma-separated list; Product description: … Product names: [list]" (or JSON skeleton).<br>• Examples: include 2–3 sample product descriptions and desired names as few-shot context.<br>• Evaluate: blind human ratings or rule checks for readability, trademark risk, and emotional valence.<br> </td></tr><tr><td data-label="CHAPTER 1 — The Five Principles of Prompting"> <strong>Image prompting example — applied principles</strong><br>• Direction: specify scene, objects, mood, camera, and priority (e.g., Van Gogh style vs stock photo realism).<br>• Format: designate <code>stock photo</code> or <code>oil painting</code> to control output distribution.<br>• Examples: use base images for img2img to control composition and lighting; remove conflicting camera meta if artist style is primary.<br> </td></tr><tr><td data-label="CHAPTER 1 — The Five Principles of Prompting"> <strong>Evaluation instruments & automation</strong><br>• Thumbs up/down UI or <code>input()</code> replacement for notebooks; store responses in CSV for reproducibility.<br>• Secondary LLM as evaluator: use a stronger model to score weaker model outputs when human labeling budget is constrained.<br>• Regex and JSON schema validation for format checks; fail fast and retry prompts on parse errors.<br> </td></tr><tr><td data-label="CHAPTER 1 — The Five Principles of Prompting"> <strong>Common pitfalls & mitigation</strong><br>• Hallucination: add grounding context, citations, or retrieval augmentation.<br>• Overfitting to examples: diversify example set and occasionally run zero-shot to test generality.<br>• Shadow IT & secrecy risk: track provenance, log prompts, and classify sensitive data to avoid leaks.<br>• Format flips: always provide strict skeletons and parse checks before accepting outputs.<br> </td></tr><tr><td data-label="CHAPTER 1 — The Five Principles of Prompting"> <strong>Governance & safety considerations</strong><br>• Log prompts and outputs for auditability and bias detection.<br>• Require human sign-off for high-stakes outputs; define liability boundaries and provenance tags.<br>• Apply content filters and refusal checks as part of the chain; test adversarial prompt injections periodically.<br> </td></tr><tr><td data-label="CHAPTER 1 — The Five Principles of Prompting"> <strong>Tools, models & context windows</strong><br>• Select models by task: large context windows for long documents; smaller cheaper models for routine classification.<br>• Track costs: token count × model price; optimize by trimming nonessential context and caching shared artifacts.<br>• Monitor model updates and revalidate prompts after major model upgrades or provider changes.<br> </td></tr><tr><td data-label="CHAPTER 1 — The Five Principles of Prompting"> <strong>Testing and validation checklist (condensed)</strong><br>• Blind A/B testing, JSON schema validation, automated regex tests, sample size estimation, and repeatability checks.<br>• Confirm rendering in target tools (GitHub, Obsidian, VS Code) for final format sensitive outputs.<br>• Maintain a 15-point validator (see Prompt_13 specification) and run it before release.<br> </td></tr><tr><td data-label="CHAPTER 1 — The Five Principles of Prompting"> <strong>Actionable prompt templates (high-utility)</strong><br>• Template A — Few-shot JSON completion skeleton for product lists: include 2 examples + "Return only JSON" directive.<br>• Template B — Img2img wrapper: include base image URL, primary style, and one priority attribute.<br>• Template C — Chain starter: "Step 1: generate candidates. Step 2: self-rate out of 10. Step 3: filter top 5. Step 4: format as JSON." Use separate calls per step for observability.<br> </td></tr><tr><td data-label="CHAPTER 1 — The Five Principles of Prompting"> <strong>Operational checklist before productionizing a prompt</strong><br>1. Token budget review and cost estimate.<br>2. Five principle coverage audit (Direction, Format, Examples, Evaluate, Divide).<br>3. Blind sample evaluation with N≥10 per variant where practical.<br>4. Format parser and schema enforcement in pipeline.<br>5. Logging, provenance, and privacy redaction rules enabled.<br> </td></tr><tr><td data-label="CHAPTER 1 — The Five Principles of Prompting"> <strong>Versioning, templates, and reusability</strong><br>• Store canonical prompt templates with metadata: version, model tested, date, test metrics, and known failure modes.<br>• Use semantic naming and inherit defaults (e.g., Prompt_01C) so downstream teams can opt into or override safely.<br> </td></tr><tr><td data-label="CHAPTER 1 — The Five Principles of Prompting"> <strong>Future-proofing prompts</strong><br>• Expect model behavior drift with new model releases; schedule periodic revalidation and automated regression tests.<br>• Keep prompts modular so you can swap model backends or recompose chains with minimal edits.<br> </td></tr><tr><td data-label="CHAPTER 1 — The Five Principles of Prompting"> <strong>Concise managerial playbook (3 steps)</strong><br>1. Pilot transparently: allow safe worker-level experimentation and log results.<br>2. Measure objectively: use human and automated metrics to decide which prompt variants scale.<br>3. Protect workers: redeploy time saved toward higher-value work and maintain audit trails for governance.<br> </td></tr></tbody></table></div><div class="row-count">Rows: 25</div></div><div class="table-caption" id="Table2" data-table="Book_0002_02" style="margin-top:2mm;margin-left:3mm;"><strong>Chapter 2 • Introduction to Large Language Models for Text Generation</strong></div>
<div class="table-wrapper" data-table-id="table-2"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by **CHAPTER 2 — Introduction to Large Language Models (Text Generation)**"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>CHAPTER 2 — Introduction to Large Language Models (Text Generation)</strong></div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="CHAPTER 2 — Introduction to Large Language Models (Text Generation)"> <strong>Overview — Purpose & payoff</strong><br>Large language models (LLMs) learn statistical patterns from massive text corpora to generate human-like text, enabling content creation, code generation, conversational agents, and automation of language tasks at scale.<br>LLMs trade computation and data for emergent capabilities that generalize across domains, improving productivity and enabling new products when coupled with retrieval and safety controls.<br> </td></tr><tr><td data-label="CHAPTER 2 — Introduction to Large Language Models (Text Generation)"> <strong>Core concept — Tokens & tokenization</strong><br>Text is processed as tokens (words, subwords, or characters); token counts map roughly to word counts and determine context-window usage and cost.<br>Common tokenization methods include Byte-Pair Encoding (BPE), WordPiece, and SentencePiece; each balances vocabulary coverage versus token length.<br> </td></tr><tr><td data-label="CHAPTER 2 — Introduction to Large Language Models (Text Generation)"> <strong>BPE explained — practical intuition</strong><br>BPE starts from characters and iteratively merges frequent symbol pairs into larger tokens, allowing the model to represent common morphemes or words compactly while still decomposing rare words.<br>This enables LLMs to generalize to unseen words by composing known subword tokens.<br> </td></tr><tr><td data-label="CHAPTER 2 — Introduction to Large Language Models (Text Generation)"> <strong>Vector representations — embeddings</strong><br>Tokens are mapped to numeric vectors (embeddings) that encode semantic and syntactic relationships in high-dimensional space.<br>Nearby vectors correspond to semantically related tokens; embeddings enable similarity search, clustering, and as inputs to downstream transformer layers. <br> </td></tr><tr><td data-label="CHAPTER 2 — Introduction to Large Language Models (Text Generation)"> <strong>Transformer architecture — high-level</strong><br>Transformers convert token embeddings into context-aware vectors by applying layers of self-attention and feed-forward transformations; they scale efficiently to long-range dependencies compared with recurrent models.<br>Two canonical variants: encoder-focused (e.g., BERT) for representation tasks and decoder-focused (e.g., GPT) for autoregressive generation. <br> </td></tr><tr><td data-label="CHAPTER 2 — Introduction to Large Language Models (Text Generation)"> <strong>Self-attention — mechanism & benefit</strong><br>Self-attention lets each token weight information from every other token in the context window, producing contextually enriched token representations.<br>This permits modeling of long-range dependencies and flexible contextualization without strictly sequential computation. <br> </td></tr><tr><td data-label="CHAPTER 2 — Introduction to Large Language Models (Text Generation)"> <strong>Probabilistic text generation — decision rule</strong><br>Generation selects next tokens by estimating P(token and prior tokens); deterministic argmax or stochastic sampling (temperature, top-k, top-p) control creativity and diversity.<br>Repeated selection produces fluent text but requires decoding strategies and safety/consistency filters for production use.<br> </td></tr><tr><td data-label="CHAPTER 2 — Introduction to Large Language Models (Text Generation)"> <strong>Training phases — pretraining & fine-tuning</strong><br>Pretraining: models absorb vast unlabeled corpora to learn general linguistic patterns via next-token prediction or masked objectives.<br>Fine-tuning: supervised and RLHF (reinforcement learning from human feedback) tailor pretrained models for specific behaviors, improving alignment, helpfulness, and safety. <br> </td></tr><tr><td data-label="CHAPTER 2 — Introduction to Large Language Models (Text Generation)"> <strong>ChatGPT fine-tuning pipeline (condensed)</strong><br>1. Collect human demonstration data (labelers show desired behavior).<br>2. Train supervised policy on demonstrations to align outputs with instructions.<br>3. Gather comparison data, train a reward model, then apply PPO-style RL to optimize for human-preferred outputs.<br> </td></tr><tr><td data-label="CHAPTER 2 — Introduction to Large Language Models (Text Generation)"> <strong>Multimodality & vision-enabled models</strong><br>Recent models incorporate multiple modalities (text, image, audio) enabling unified reasoning over mixed inputs; multimodal models facilitate tasks like image captioning, visual question answering, and multimodal instruction following.<br> </td></tr><tr><td data-label="CHAPTER 2 — Introduction to Large Language Models (Text Generation)"> <strong>Historical turning point — Attention Is All You Need</strong><br>The transformer paper introduced attention as a scalable, parallelizable mechanism that replaced sequential recurrence and launched modern LLM progress, enabling large-context modeling and efficient scaling. <br> </td></tr><tr><td data-label="CHAPTER 2 — Introduction to Large Language Models (Text Generation)"> <strong>Compute, parameters, and cost</strong><br>Model capability scales with data, parameters, and compute; large models (hundreds of billions to trillions of parameters) require substantial GPU/TPU fleets and investment, creating high fixed costs and concentration among major cloud providers.<br> </td></tr><tr><td data-label="CHAPTER 2 — Introduction to Large Language Models (Text Generation)"> <strong>Hardware ecosystem — GPUs, TPUs, and the AI supply chain</strong><br>NVIDIA GPUs (e.g., H100) dominate high-performance training due to tensor-core acceleration; TPUs and custom accelerators are alternatives optimized for large-scale tensor operations.<br>Supply-demand imbalances drive innovation in quantization, sparsity, and hardware specialization. <br> </td></tr><tr><td data-label="CHAPTER 2 — Introduction to Large Language Models (Text Generation)"> <strong>OpenAI & GPT family — trajectory</strong><br>GPT lineage progressed from GPT → GPT-2 (wider public access, ethical debate) → GPT-3 (scale and capability) → GPT-3.5-turbo (efficiency) → GPT-4 (advanced reasoning and multimodality); productization included ChatGPT with RLHF-based alignment.<br> </td></tr><tr><td data-label="CHAPTER 2 — Introduction to Large Language Models (Text Generation)"> <strong>Competitors & ecosystem dynamics</strong><br>Google (Gemini/Bard/PaLM), Anthropic (Claude series), Meta (Llama family), Mistral, and others created a competitive landscape of closed and open models, accelerating capability and specialty trade-offs (cost vs. openness vs. safety).<br> </td></tr><tr><td data-label="CHAPTER 2 — Introduction to Large Language Models (Text Generation)"> <strong>Open source movement — LLaMA, Mistral, Mixtral</strong><br>Open-source models (LLaMA, Mistral 7B, Mixtral variants) enable research and local fine-tuning; quantization and LoRA adapters make them feasible on smaller hardware while raising governance concerns around misuse. <br> </td></tr><tr><td data-label="CHAPTER 2 — Introduction to Large Language Models (Text Generation)"> <strong>Model efficiency techniques — quantization & LoRA</strong><br>Quantization reduces numerical precision of weights (e.g., 16→8→4-bit) to shrink memory and inference cost with modest accuracy loss; LoRA adds low-rank adapters for efficient task-specific fine-tuning without updating full model weights. <br> </td></tr><tr><td data-label="CHAPTER 2 — Introduction to Large Language Models (Text Generation)"> <strong>Safety, bias, and governance considerations</strong><br>Training data biases propagate into model behavior; mitigation requires data curation, instruction tuning, human oversight, and monitoring pipelines to detect hallucinations, toxicity, and misuse risks.<br>High-stakes applications demand human-in-the-loop sign-off and auditable provenance. <br> </td></tr><tr><td data-label="CHAPTER 2 — Introduction to Large Language Models (Text Generation)"> <strong>Model comparison — practical guidance</strong><br>Evaluate multiple models on the same prompt to identify strengths: instruction following, format fidelity, latency, cost, and safety behavior; prefer smaller specialized models for constrained tasks and larger models for open-ended reasoning. <br> </td></tr><tr><td data-label="CHAPTER 2 — Introduction to Large Language Models (Text Generation)"> <strong>Production practices — validation & monitoring</strong><br>Implement schema checks, automated validators, blind A/B testing, and continuous monitoring for drift after deployment; log prompts/outputs and perform periodic revalidation when upstream models or data change. <br> </td></tr><tr><td data-label="CHAPTER 2 — Introduction to Large Language Models (Text Generation)"> <strong>Common pitfalls & mitigations</strong><br>• Hallucinations → add grounding sources, retrieval augmentation, and verification steps.<br>• Overfitting to examples → diversify few-shot contexts and test zero-shot generality.<br>• Format regressions → enforce strict schema validation and retries.<br> </td></tr><tr><td data-label="CHAPTER 2 — Introduction to Large Language Models (Text Generation)"> <strong>Summary — practical takeaways</strong><br>LLMs transform language tasks by combining tokenization, embedding spaces, transformer attention, and large-scale training to produce fluent, context-aware outputs; real-world success depends on alignment, compute trade-offs, efficient deployment techniques (quantization/LoRA), and rigorous safety processes.<br>Understand model limits, validate outputs, and choose the right model/size for the task while retaining human oversight for high-risk decisions.<br> </td></tr><tr><td data-label="CHAPTER 2 — Introduction to Large Language Models (Text Generation)"> <strong>Next steps — where to apply learning</strong><br>1. Prototype prompts across multiple models and compare outputs and format fidelity.<br>2. Integrate retrieval/grounding for factual tasks and add verification layers.<br>3. Use quantization/LoRA to experiment with local fine-tuning and reduce deployment cost.<br>4. Establish logging, validation, and human-review workflows before scaling to production.<br> </td></tr></tbody></table></div><div class="row-count">Rows: 23</div></div><div class="table-caption" id="Table3" data-table="Book_0002_03" style="margin-top:2mm;margin-left:3mm;"><strong>Chapter 3 • Standard Practices for Text Generation with ChatGPT</strong></div>
<div class="table-wrapper" data-table-id="table-3"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by **CHAPTER 3 — Standard Practices for Text Generation with ChatGPT**"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>CHAPTER 3 — Standard Practices for Text Generation with ChatGPT</strong></div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Overview — purpose & payoff</strong><br>Chapter 3 consolidates practical prompting patterns, parsing techniques, and production workflows for reliable text-generation with LLMs; the emphasis is on format fidelity, chunking, evaluation, and safe deployment. <br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Generating lists — pitfalls & controls</strong><br>1. Uncontrolled length and format variation can break downstream parsers.<br>2. Precede outputs with explicit format instructions (bullet vs numbered) and enforce "only return" constraints.<br>3. Provide examples (few-shot) and exact output templates to increase parsing reliability.<br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Provide examples — few-shot advantage</strong><br>• Including 1–3 diverse examples converts zero-shot ambiguity into predictable style/format behavior.<br>• Use positive and optional negative examples to show desired and forbidden outputs.<br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Hierarchical list generation — when to use</strong><br>• Use for nested outlines: articles, docs, multi-level plans.<br>• Mark explicitly "hierarchical" and request top-level count or depth to guide size. <br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Parsing hierarchical lists — regex approach</strong><br>1. Simple regex can extract headings (<code>* </code>) and subheadings (e.g., <code>a.</code>) efficiently for lightweight pipelines.<br>2. Two-pass approach: extract headings, then attach subitems while iterating lines to preserve association.<br>3. Caveat: regex becomes brittle for varied formats or noisy LLM output. <br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>When to avoid regex</strong><br>• Avoid when structure is complex or when LLM can return JSON/YAML directly; prefer strict machine formats for production. <br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Generating JSON — best practices</strong><br>• Instruct "Only return valid JSON" and forbid extra wrappers or backticks.<br>• Provide a JSON skeleton to bias keys, types, and nesting; validate with <code>json.loads()</code> before downstream use.<br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>JSON failure modes & mitigations</strong><br>• LLM adds commentary or markdown fences → pre-parse strip heuristics or re-prompt.<br>• Invalid JSON due to escaping or trailing commas → run automatic parse-then-repair loop or request corrected JSON from model. <br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>YAML — pros & tradeoffs</strong><br>• Pros: human-readable, comments allowed, less escaping than JSON.<br>• Use when humans will edit or when schema is simple; still validate with YAML parsers before consumption. <br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Filtering YAML payloads with LLMs</strong><br>• Provide canonical schema and instruct the model to return only matching items; specify exact output behavior for no matches (e.g., "No Items").<br>• Use prompts that enforce "no commentary" and pure YAML output. <br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Handling invalid YAML outputs programmatically</strong><br>1. Parse with safe loader (yaml.safe_load).<br>2. Implement validation checks and custom exceptions for type/key/value constraints.<br>3. Treat non-list or malformed outputs as failure modes and fallback to re-request or human review. <br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Diverse format generation (Mermaid, CSV, etc.)</strong><br>• LLMs can produce many structured formats; always assert the exact dialect (e.g., mermaid flowchart syntax, CSV header order).<br>• Validate with format-specific linters or renderers before using in production. <br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Explain-it-like-I'm-five (ELI5)</strong><br>• Use for simplification and accessibility; instruct the model to preserve core facts while reducing lexical and syntactic complexity. <br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Universal translation & fidelity limits</strong><br>• LLMs are strong multilingual translators for high-resource languages; expect quality degradation for low-resource languages and for nuance-sensitive content. <br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Ask for context — decision quality</strong><br>• Always request or define a context checklist when user inputs are underspecified (data shape, constraints, SLAs, scale, privacy).<br>• Provide a standard "what I need to decide" list so the model prompts the user when necessary. <br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Allow LLM to ask for context by default</strong><br>• Include a default sentence such as "If you need more context, specify what would help" to make the model request clarifying details automatically. <br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Text style unbundling — method</strong><br>1. Analyze sample text to extract tone, vocabulary, structure, and length.<br>2. Return a concise style guide (tone bullets, punctuation norms, preferred lexical items).<br>3. Use guide as prompt prefix for new content. <br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Extracting textual features — practical checklist</strong><br>• Tone of voice, length target, vocabulary level, structural patterns (subheadings, bullets), typical transitions and examples of phrases to avoid/embrace.<br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Generating new content from extracted features</strong><br>• Seed new prompts with the style guide and representative examples; enforce "follow this style" and validate by automated checks comparing n-grams and tone features. <br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Summarization — types & tailoring</strong><br>• Produce different summary types: Key-insights (A), Decision-focused (B), Shareable briefing (C).<br>• For long docs use chunk→summarize→merge pipeline to respect context limits. <br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Chunking text — why it matters</strong><br>• Chunking ensures content fits context windows, reduces cost, and enables per-chunk processing such as topic extraction or focused summarization. <br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Chunking strategies — selection guide</strong><br>1. Sentence-based: preserves local semantics, good for summarization.<br>2. Paragraph-based: cohesive units for longer content.<br>3. Topic/section-based: best when semantic segmentation is available.<br>4. Length/token-based: deterministic control for token budgets.<br>5. Sliding-window: overlap to avoid boundary loss when accuracy matters.<br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Poor chunking pitfalls</strong><br>• Splitting at arbitrary character boundaries loses context and harms model output quality; prefer linguistically-aware splits. <br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Sliding-window chunking — parameters</strong><br>• Choose window size and step size to balance overlap vs cost; larger overlap increases contextual continuity at expense of extra tokens. <br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Chunking tools — spaCy, NLTK, tiktoken</strong><br>• Use spaCy for sentence detection; use tiktoken for accurate token counts aligned with OpenAI models; combine strategies for best results. <br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Tiktoken & encodings — practical notes</strong><br>• Use cl100k_base for GPT-4/3.5-turbo tokenization; count tokens to ensure input+output fit model context length; implement token heuristics before API calls. <br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Estimating tokens for chat messages</strong><br>• Account for token-per-message overhead, name fields, and per-reply priming; use established helper functions to compute message token budgets. <br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Sentiment analysis — prompt design</strong><br>• Provide clear labels and examples in prompt; ask for single-word outputs when integrating into pipelines to minimize post-processing. <br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Improving sentiment accuracy</strong><br>• Preprocess text (lowercase, remove noise), supply domain examples, and instruct model how to treat mixed sentiments or sarcasm. <br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Least-to-most technique — staged reasoning</strong><br>• Build answers progressively: ask for coarse answer then refine; useful for complex tasks, code generation, or instructional decomposition. <br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Planning with least-to-most</strong><br>• Use chained prompts where each step uses the previous step's output as context; monitor context-length limits and summarize intermediate state when necessary. <br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Role prompting — definition & use cases</strong><br>• Assign domain persona via the system message to bias tone, depth, and perspective; suitable for reviews, expert summaries, or creative personas. <br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Role prompting challenges</strong><br>• Monitor for role drift and bias; periodically re-anchor role with short system reminders and validate content against factual checks. <br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Avoiding hallucinations with reference text</strong><br>• Supply authoritative excerpts and instruct the model to answer only from provided sources; define explicit fallback (e.g., "I could not find an answer"). <br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Cite-sourced answers — format guidance</strong><br>• Ask for inline passage citations using exact-quote blocks or JSON citation fields to enable automated provenance matching. <br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Give GPTs thinking time — structured internal reasoning</strong><br>• Ask the model to derive from first principles before answering to improve correctness; use internal monologue markers that are stripped before user-facing output when needed. <br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Inner monologue tactic — safe usage pattern</strong><br>• Encapsulate chain-of-thought or working notes in clearly delimited sections for removal; avoid exposing raw chain-of-thought in high-stakes contexts. <br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Self-eval and iterative refinement</strong><br>• Ask the LLM to critique and improve its own outputs; use a stronger model as an evaluator for outputs from a weaker model and repeat until quality thresholds are met. <br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Classification — zero-shot & few-shot</strong><br>• Few-shot examples stabilize label semantics; require single-token/class outputs for deterministic downstream use and consider majority-vote ensemble for robustness. <br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Majority vote & ensemble methods</strong><br>• Run multiple inferences and take the most frequent label to reduce stochastic variance; weigh cost vs reliability when choosing number of repeats. <br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Criteria evaluation & synthetic evaluators</strong><br>• Use GPT-4 as an automated rater for smaller models after validating evaluator accuracy on known test cases; measure false positives/negatives before relying on synthetic evaluation. <br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Meta prompting — prompts that create prompts</strong><br>• Use meta prompts to generate reusable, high-quality prompts for downstream tasks (copywriting, image prompts, test-case generation); provide sample outputs for grounding. <br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Combining web content with meta prompts</strong><br>• When augmenting style guides with web examples, provide URLs or excerpts and instruct the model to synthesize a refined prompt that cites salient stylistic features. <br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Operational checklist — pre-production</strong><br>1. Define expected format and schema.<br>2. Provide canonical examples and negative examples.<br>3. Implement token budgeting and chunking strategy.<br>4. Add automated validators (JSON schema, YAML lint, mermaid render check).<br>5. Deploy evaluator and logging for drift detection.<br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Common pitfalls & mitigations (condensed)</strong><br>• Hallucination → retrieval/grounding and citation enforcement.<br>• Format regressions → strict skeletons and parse checks.<br>• Role drift → periodic system re-anchoring and sampling checks.<br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Summary — chapter essence</strong><br>Chapter 3 presents hands-on patterns for producing predictable, parsable, and safe LLM outputs: control format via strict prompts and examples, prefer machine formats (JSON/YAML) when possible, chunk intelligently to fit token budgets, evaluate via synthetic and human pipelines, and apply governance around hallucinations and sensitive data. <br> </td></tr><tr><td data-label="CHAPTER 3 — Standard Practices for Text Generation with ChatGPT"> <strong>Next steps — practical experiments</strong><br>1. Prototype a JSON-skeleton prompt and validate with an automated parse-retry loop.<br>2. Implement tiktoken-based chunker with sliding-window overlap for a sample long document.<br>3. Build an evaluator pipeline using GPT-4 plus a small test-suite to measure false positives/negatives.<br>4. Create a meta-prompt to generate image prompts from article sections and validate on a rendering tool. <br> </td></tr></tbody></table></div><div class="row-count">Rows: 47</div></div><div class="table-caption" id="Table4" data-table="Book_0002_04" style="margin-top:2mm;margin-left:3mm;"><strong>Chapter 4 • Advanced Techniques for Text Generation with LangChain</strong></div>
<div class="table-wrapper" data-table-id="table-4"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by **Chapter 4: surveys LangChain for advanced LLM applications**"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>Chapter 4: surveys LangChain for advanced LLM applications</strong></div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Chapter 4: surveys LangChain for advanced LLM applications"> <strong>Summary</strong><br>Chapter 4 surveys LangChain for advanced LLM applications, covering environment setup, chat models and streaming, prompt templates and LCEL, output parsers (notably Pydantic), function calling (OpenAI and LangChain Pydantic tools), chains (sequential, parallel, document chains: stuff/refine/map-reduce/map-re-rank), few-shot example selection, text/document loaders, text splitters, query planning, task decomposition, evaluators (LangChain Evals), and practical best practices for production workflows.<br> </td></tr><tr><td data-label="Chapter 4: surveys LangChain for advanced LLM applications"> <strong>Purpose & Value Proposition</strong><br>Use LangChain to standardize model I/O, retrieval, chains, agents, memory, and callbacks so LLM applications can be modular, data-aware, and model-agnostic. Implement safe, testable pipelines for complex tasks that exceed single-prompt capabilities.<br> </td></tr><tr><td data-label="Chapter 4: surveys LangChain for advanced LLM applications"> <strong>Quick Setup</strong><br>1. Create virtualenv and activate it: <code>python -m venv venv</code> then <code>source venv/bin/activate</code>.<br>2. Install <code>langchain</code>, provider SDKs (e.g., <code>openai</code>), and extras via <code>pip</code> or <code>conda</code>.<br>3. Store API keys in environment variables (do not hardcode).<br> </td></tr><tr><td data-label="Chapter 4: surveys LangChain for advanced LLM applications"> <strong>LangChain Core Modules</strong><br>• Model I/O — unified model interfaces for multiple providers.<br>• Retrieval — connect LLMs to external data sources.<br>• Chains — runnable compositions of prompts and models.<br>• Agents — decision-making tool orchestration.<br>• Memory — persist state across runs.<br>• Callbacks — hook events like token streaming.<br> </td></tr><tr><td data-label="Chapter 4: surveys LangChain for advanced LLM applications"> <strong>Chat Models & Messages</strong><br>SystemMessage, HumanMessage, AIMessage are primary types; use SystemMessage for behavior constraints and role guidance.<br>Streaming reduces latency; <code>.stream()</code> yields incremental chunks for UI responsiveness.<br> </td></tr><tr><td data-label="Chapter 4: surveys LangChain for advanced LLM applications"> <strong>Prompt Templates & LCEL</strong><br>Use PromptTemplate / ChatPromptTemplate for parameterized prompts; LCEL <code>|</code> pipes runnables to build chains like <code>prompt | model | parser</code>.<br>LCEL supports itemgetter, RunnablePassthrough, RunnableLambda, RunnableParallel for robust data flow and selection of inputs across steps.<br> </td></tr><tr><td data-label="Chapter 4: surveys LangChain for advanced LLM applications"> <strong>Prompt Engineering Patterns</strong><br>• Role prompting — set system role for tone and expertise.<br>• Least-to-most chaining — iterative deepening for complex tasks.<br>• Meta-prompting — generate prompts for other models (image prompts, style guides).<br>• Few-shot templates — fixed examples or example selectors (LengthBasedExampleSelector).<br> </td></tr><tr><td data-label="Chapter 4: surveys LangChain for advanced LLM applications"> <strong>Output Parsers</strong><br>PydanticOutputParser for structured JSON → validated Pydantic models is preferred for robustness.<br>Other parsers: List, Datetime, Enum, Auto-fixing, Retry, Structured, XML, StrOutputParser.<br>Always include <code>parser.get_format_instructions()</code> in prompts for reliable parsing.<br> </td></tr><tr><td data-label="Chapter 4: surveys LangChain for advanced LLM applications"> <strong>Function Calling (OpenAI & LangChain)</strong><br>OpenAI function calling: define JSON schemas for functions and pass to model; model returns name+args; execute and re-inject result into conversation.<br>LangChain: convert Pydantic models to tools with <code>convert_to_openai_tool</code> and use <code>PydanticToolsParser</code> for end-to-end extraction without manual schema JSON.<br> </td></tr><tr><td data-label="Chapter 4: surveys LangChain for advanced LLM applications"> <strong>Parallel & Sequential Calls</strong><br>• <code>.batch()</code> and RunnableParallel allow parallel generation and tool calls for speed.<br>• Sequential chains (e.g., character → plot → scenes) preserve provenance and allow previous outputs to inform downstream steps; sequential is simpler but slower.<br> </td></tr><tr><td data-label="Chapter 4: surveys LangChain for advanced LLM applications"> <strong>Chains: Types & Tradeoffs</strong><br>• Stuff: simplest, place all docs into one prompt; cheap to implement but fails on large inputs.<br>• Refine: iterative update of an answer across documents; good for progressive extraction but higher latency.<br>• Map-Reduce: map each doc to an intermediate summary then reduce; scalable and suitable for long corpora.<br>• Map Re-rank: score per-doc answers and pick highest-confidence result; useful when one-best answer is expected.<br> </td></tr><tr><td data-label="Chapter 4: surveys LangChain for advanced LLM applications"> <strong>Document Loaders & Connectors</strong><br>Load PDFs, DOCX, CSV, web pages, cloud buckets via community loaders (PyPDFLoader, Docx2txtLoader, CSVLoader, etc.).<br>Add metadata (source, page, description) to docs for provenance and retrieval relevance.<br> </td></tr><tr><td data-label="Chapter 4: surveys LangChain for advanced LLM applications"> <strong>Text Splitters & Chunking Strategies</strong><br>Options: CharacterTextSplitter, TokenTextSplitter, RecursiveCharacterTextSplitter, sliding window, length-based selectors using <code>tiktoken</code> for token-aware chunking.<br>Key rules: preserve sentences/paragraphs when possible; choose chunk_size and overlap to balance context integrity vs token limits.<br> </td></tr><tr><td data-label="Chapter 4: surveys LangChain for advanced LLM applications"> <strong>Chunking Heuristics</strong><br>• Preserve whole sentences/paragraphs when possible.<br>• Use overlap (e.g., 200 tokens) to retain continuity across chunks.<br>• Token-based length functions (tiktoken) give predictable context usage.<br> </td></tr><tr><td data-label="Chapter 4: surveys LangChain for advanced LLM applications"> <strong>Query Planning & Task Decomposition</strong><br>Convert complex user intents into a QueryPlan (Pydantic) — a dependency graph of subqueries that the system executes in order; useful for multi-intent workflows and safe orchestration of steps.<br> </td></tr><tr><td data-label="Chapter 4: surveys LangChain for advanced LLM applications"> <strong>Few-Shot Example Selection</strong><br>• Fixed few-shot: place curated examples into prompt.<br>• Dynamic selector: LengthBasedExampleSelector picks examples by token budget to avoid context overflow.<br>Use example selectors when user input length varies widely.<br> </td></tr><tr><td data-label="Chapter 4: surveys LangChain for advanced LLM applications"> <strong>LangChain Evals & Metrics</strong><br>Use built-in evaluators (pairwise, labeled_pairwise_string) or custom metrics (Levenshtein, embedding distance) to compare model outputs and track improvements.<br>Store evals to LangSmith or WandB for traceability and debugging.<br> </td></tr><tr><td data-label="Chapter 4: surveys LangChain for advanced LLM applications"> <strong>Common Pitfalls & Mitigations</strong><br>• Hallucinated function args — make schema/instructions explicit.<br>• Backslashes / JSON escape issues from some models — use intermediate StrOutputParser and cleanup functions.<br>• Context overflow — enforce token counting and chunking strategies.<br>• Overfitting to few-shot examples — instruct model to treat examples as formatting guidance if needed.<br> </td></tr><tr><td data-label="Chapter 4: surveys LangChain for advanced LLM applications"> <strong>Practical Example Workflows</strong><br>1. Story pipeline: character_gen → plot_gen → scene_gen → per-scene character scripts → summarize → compile.<br>2. Transaction classification: loader → chunker → model with Pydantic parser → save outputs to dataframe → eval vs reference.<br>3. Extraction via function calling: convert Pydantic → bind tools → parallel extraction → parsed Pydantic results.<br> </td></tr><tr><td data-label="Chapter 4: surveys LangChain for advanced LLM applications"> <strong>Best Practices for Production</strong><br>• Never hardcode API keys; use env vars and secrets management.<br>• Add automated validators for parser outputs and structure checks.<br>• Use smaller models for cheap generation and larger models for evaluation or difficult tasks.<br>• Instrument with logging, callbacks, and streaming for observability.<br> </td></tr><tr><td data-label="Chapter 4: surveys LangChain for advanced LLM applications"> <strong>Validation & QA Checklist (applies to pipelines)</strong><br>• Token budget validated via <code>tiktoken</code> before calls.<br>• Output parsed and schema-validated (Pydantic) with graceful fallback.<br>• Retries and auto-fix parsers for robust parsing.<br>• Eval metrics and A/B comparisons logged and reviewed regularly.<br> </td></tr><tr><td data-label="Chapter 4: surveys LangChain for advanced LLM applications"> <strong>Security & Privacy Notes</strong><br>• Avoid sending sensitive PII to third-party APIs unless allowed by policy.<br>• Local/self-hosted models are preferable for highly sensitive data.<br>• Anonymize or redact data during ingestion when feasible.<br> </td></tr><tr><td data-label="Chapter 4: surveys LangChain for advanced LLM applications"> <strong>Actionable Next Steps</strong><br>1. Wire up environment and provider keys in a secrets manager.<br>2. Build document loaders for target corpus and add metadata.<br>3. Design Pydantic schemas for structured outputs and create extraction chains.<br>4. Implement chunking with token-aware splitters and test map-reduce summarizer.<br>5. Add LangChain evals for continuous measurement and regression testing.<br> </td></tr><tr><td data-label="Chapter 4: surveys LangChain for advanced LLM applications"> <strong>Reference Patterns & Code Snippets to Keep</strong><br>• <code>parser.get_format_instructions()</code> insertion pattern for reliable structured output.<br>• LCEL chaining: <code>chat_prompt | model | parser</code>.<br>• Pydantic → tools conversion: <code>convert_to_openai_tool</code> + <code>PydanticToolsParser</code> for parallel extraction.<br>• Use <code>RunnablePassthrough</code>, <code>itemgetter</code>, and <code>RunnableParallel</code> to manage input wiring.<br> </td></tr><tr><td data-label="Chapter 4: surveys LangChain for advanced LLM applications"> <strong>Delivery & Output Policy (Prompt_13 alignment)</strong><br>All outputs should be rendered as single-column Markdown tables with bold section headers and <code>&lt;br&gt;</code> terminated lines, validated against the 15-point checklist; final artifacts must be wrapped in a Markdown code block for easy copy/paste into repos or editors.<br> </td></tr><tr><td data-label="Chapter 4: surveys LangChain for advanced LLM applications"> <strong>Concise TL;DR</strong><br>LangChain provides modular abstractions to convert single-prompt LLM tasks into robust, testable pipelines using prompt templates, LCEL chains, Pydantic parsers, function calling, document loaders, token-aware chunking, and evaluators — enabling scalable production LLM applications.<br> </td></tr><tr><td data-label="Chapter 4: surveys LangChain for advanced LLM applications"> <strong>Metadata / Version</strong><br><em>Prompt_13 summary generated from Chapter 4 content; format: single-column Prompt_13 table; validation: structural rules applied; version: Prompt_13 v1.0 2025-11-25</em><br> </td></tr></tbody></table></div><div class="row-count">Rows: 27</div></div><div class="table-caption" id="Table5" data-table="Book_0002_05" style="margin-top:2mm;margin-left:3mm;"><strong>Chapter 5 • Vector Databases with FAISS and Pinecone</strong></div>
<div class="table-wrapper" data-table-id="table-5"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by **Chapter 5 — Vector Databases with FAISS and Pinecone**"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>Chapter 5 — Vector Databases with FAISS and Pinecone</strong></div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Chapter 5 — Vector Databases with FAISS and Pinecone"> <strong>Summary</strong><br>Explains embeddings, vector search, RAG (Retrieval-Augmented Generation), document loading and chunking, FAISS local vector store usage, Pinecone hosted vector database workflows, metadata strategies, self-querying retrievers, alternative retrievers, and operational best practices for production systems.<br> </td></tr><tr><td data-label="Chapter 5 — Vector Databases with FAISS and Pinecone"> <strong>Purpose</strong><br>Show how to convert text into vector embeddings, index them for semantic search, retrieve relevant context at query-time, and inject that context into prompts to reduce hallucinations and scale LLM applications.<br> </td></tr><tr><td data-label="Chapter 5 — Vector Databases with FAISS and Pinecone"> <strong>Key Concepts — Vectors & Embeddings</strong><br>1. Embeddings are fixed-length numeric vectors representing semantic content.<br>2. Similar texts map to nearby points in high-dimensional space.<br>3. Modern embeddings are contextual and model-dependent — keep consistency per dataset.<br> </td></tr><tr><td data-label="Chapter 5 — Vector Databases with FAISS and Pinecone"> <strong>Common Embedding Models</strong><br>1. OpenAI text-embedding-ada-002 (1536 dims) — common production choice.<br>2. Hugging Face Sentence-Transformers (e.g., all-MiniLM-L6-v2, 384 dims) — open-source alternative.<br>3. Classical options: word2vec, GloVe for domain-specific training.<br> </td></tr><tr><td data-label="Chapter 5 — Vector Databases with FAISS and Pinecone"> <strong>Embedding Properties & Tradeoffs</strong><br>1. Dense embeddings capture semantics; sparse approaches capture keyword signals.<br>2. Different models produce incompatible vector spaces — do not mix models for same index.<br>3. Higher dimensionality can capture more nuance but increases storage and compute cost.<br> </td></tr><tr><td data-label="Chapter 5 — Vector Databases with FAISS and Pinecone"> <strong>When to Train Custom Embeddings</strong><br>1. Domain-specific vocabulary or rapidly evolving lexicons.<br>2. Need control over semantics not present in pretrained models.<br>3. Justified when dataset size and budget allow for retraining and validation.<br> </td></tr><tr><td data-label="Chapter 5 — Vector Databases with FAISS and Pinecone"> <strong>RAG Overview</strong><br>1. Retrieve top-k semantically similar chunks for a query.<br>2. Insert retrieved chunks into prompt as context with strict system instruction to prefer "I don't know" when context is insufficient.<br>3. Reduces hallucination and token waste compared to passing full corpora into prompts.<br> </td></tr><tr><td data-label="Chapter 5 — Vector Databases with FAISS and Pinecone"> <strong>RAG Typical Pipeline</strong><br>1. Chunk documents into semantically coherent pieces.<br>2. Compute embeddings for chunks and persist vectors.<br>3. Index vectors into vectorstore (FAISS/Pinecone/etc.).<br>4. At query time, embed query, retrieve top-k chunks, inject into prompt, call LLM, parse results.<br> </td></tr><tr><td data-label="Chapter 5 — Vector Databases with FAISS and Pinecone"> <strong>Document Loading & Metadata</strong><br>1. Use document loaders for PDFs, DOCX, CSV, HTML, and other sources.<br>2. Attach metadata (source, page, chapter, timestamp, batch) to each chunk for filtering and provenance.<br>3. Keep original text for re-indexing when changing embedding models.<br> </td></tr><tr><td data-label="Chapter 5 — Vector Databases with FAISS and Pinecone"> <strong>Chunking Strategy</strong><br>1. Chunk to paragraph or semantic unit granularity as baseline.<br>2. Use overlap (e.g., 20–200 tokens) to preserve context across chunk boundaries.<br>3. Smaller chunks -> more precise matches; larger chunks -> more context but risk averaging semantics.<br> </td></tr><tr><td data-label="Chapter 5 — Vector Databases with FAISS and Pinecone"> <strong>Text Splitters</strong><br>1. CharacterTextSplitter — simple character/window-based splitting.<br>2. TokenTextSplitter — token-aware splitting (recommended with tiktoken).<br>3. RecursiveCharacterTextSplitter — split by paragraph -> sentence -> word recursively to preserve semantics.<br> </td></tr><tr><td data-label="Chapter 5 — Vector Databases with FAISS and Pinecone"> <strong>Token Budgeting & tiktoken</strong><br>1. Count tokens with tiktoken to ensure prompt + context + completion fit model limits.<br>2. Use token-aware chunk size to control per-query token cost and avoid exceeding context windows.<br> </td></tr><tr><td data-label="Chapter 5 — Vector Databases with FAISS and Pinecone"> <strong>Embedding Costs & Performance</strong><br>1. OpenAI embedding cost example: \$0.0004 per 1,000 tokens (example in text — confirm current pricing before production).<br>2. Batch embedding calls for throughput and lower rate-limit issues.<br>3. Cache embeddings; do not recompute unchanged vectors per query.<br> </td></tr><tr><td data-label="Chapter 5 — Vector Databases with FAISS and Pinecone"> <strong>FAISS — Local Vector Store</strong><br>1. FAISS is an open-source library for efficient nearest-neighbor search on dense vectors.<br>2. Common index: IndexFlatL2 for brute-force L2 search; many other index types for approximate search and scaling.<br>3. Workflow: create index, add vectors, search(query_vector, k).<br> </td></tr><tr><td data-label="Chapter 5 — Vector Databases with FAISS and Pinecone"> <strong>FAISS Code Pattern (essentials)</strong><br>1. vectors = np.array([get_vector_embeddings(chunk) for chunk in chunks])<br>2. index = faiss.IndexFlatL2(vectors.shape[1])<br>3. index.add(vectors); distances, indices = index.search(np.array([qvec]), k)<br> </td></tr><tr><td data-label="Chapter 5 — Vector Databases with FAISS and Pinecone"> <strong>Persisting FAISS Index</strong><br>1. faiss.write_index(index, "path.index") to save.<br>2. index = faiss.read_index("path.index") to load later.<br>3. Persist external mapping of index positions -> metadata because IDs may not be preserved internally.<br> </td></tr><tr><td data-label="Chapter 5 — Vector Databases with FAISS and Pinecone"> <strong>Merging FAISS Indices & ID Concerns</strong><br>1. Merge by reconstructing vectors from index2 and adding to index1 via index1.add(index2.reconstruct_n(0, index2.ntotal)).<br>2. Vector IDs are not preserved; maintain external ID->metadata mapping to track origin and stable identifiers.<br>3. Some FAISS index types do not support reconstruct; for those, rebuild from original vectors instead.<br> </td></tr><tr><td data-label="Chapter 5 — Vector Databases with FAISS and Pinecone"> <strong>FAISS Advantages & Limitations</strong><br>1. Advantages: free, fast for local use, low-latency, flexible index types.<br>2. Limitations: ops overhead for scaling, sharding, persistence, monitoring, and lack of managed metadata filtering APIs.<br> </td></tr><tr><td data-label="Chapter 5 — Vector Databases with FAISS and Pinecone"> <strong>Pinecone — Hosted Vector DB</strong><br>1. Managed, scalable vector database with indexing, metadata filters, namespaces, and durability.<br>2. Create index with dimension matching embeddings and chosen similarity metric (cosine or euclidean).<br>3. Upsert vectors in batches with ids and metadata; query with top_k and optional filters.<br> </td></tr><tr><td data-label="Chapter 5 — Vector Databases with FAISS and Pinecone"> <strong>Pinecone Initialization Pattern</strong><br>1. Set PINECONE_API_KEY in environment variables.<br>2. pc = Pinecone(); check/create index with pc.create_index(index_name, dimension=1536, metric='cosine', spec=ServerlessSpec(...)).<br>3. index = pc.Index(index_name); index.describe_index_stats() for health stats.<br> </td></tr><tr><td data-label="Chapter 5 — Vector Databases with FAISS and Pinecone"> <strong>Batch Upsert Pattern (Pinecone)</strong><br>1. Batch_size e.g., 10–100; create ids, fetch embeddings, form (id, vector, metadata) tuples.<br>2. Retry/backoff logic around embedding calls to avoid rate limits.<br>3. index.upsert(vectors=to_upsert) to persist batches.<br> </td></tr><tr><td data-label="Chapter 5 — Vector Databases with FAISS and Pinecone"> <strong>Pinecone Query Example</strong><br>1. xq = get_vector_embeddings(query_text); res = index.query(vector=xq, top_k=k, include_metadata=True).<br>2. Use filter argument for metadata-scoped queries, e.g., filter={'batch': {'$eq': 1}}.<br> </td></tr><tr><td data-label="Chapter 5 — Vector Databases with FAISS and Pinecone"> <strong>Metadata Types & Usage</strong><br>1. Supported metadata: strings, numbers, booleans, list[str].<br>2. Use metadata to filter by document, date, user, product, or page; crucial for multi-tenant or time-sensitive searches.<br> </td></tr><tr><td data-label="Chapter 5 — Vector Databases with FAISS and Pinecone"> <strong>Self-Querying Retriever</strong><br>1. SelfQueryRetriever uses an LLM to convert natural language filters into structured metadata filters and vector queries.<br>2. Define AttributeInfo schema (name, description, type) for expected metadata fields to guide parsing.<br>3. Best paired with temperature=0 for deterministic filter extraction.<br> </td></tr><tr><td data-label="Chapter 5 — Vector Databases with FAISS and Pinecone"> <strong>Self-Querying Workflow</strong><br>1. Build Chroma/other vectorstore from Document objects with metadata.<br>2. Define metadata_field_info (AttributeInfo list) with basic, detailed, and analysis attributes.<br>3. retriever = SelfQueryRetriever.from_llm(llm, vectorstore, document_content_description, metadata_field_info).<br>4. retriever.invoke or retriever.get_relevant_documents with natural query strings that may include filters.<br> </td></tr><tr><td data-label="Chapter 5 — Vector Databases with FAISS and Pinecone"> <strong>Alternative Retriever Patterns</strong><br>1. MultiQueryRetriever — generate multiple reformulations for broader recall.<br>2. ContextualCompressionRetriever — compress long docs to relevant summaries before retrieval.<br>3. Ensemble Retriever — combine sparse (BM25/TF-IDF) + dense retrievers to improve recall and precision.<br>4. ParentDocumentRetriever — rehydrate full parent docs from chunk hits for richer context.<br>5. Time-Weighted Retriever — apply temporal decay to bias recent documents.<br> </td></tr><tr><td data-label="Chapter 5 — Vector Databases with FAISS and Pinecone"> <strong>TF-IDF & Classical Alternatives</strong><br>1. TF-IDF + cosine similarity is a low-cost alternative for keyword or small-corpus scenarios.<br>2. Useful baseline for retrieval quality and inexpensive experimentation before adding embedding costs.<br> </td></tr><tr><td data-label="Chapter 5 — Vector Databases with FAISS and Pinecone"> <strong>Word2Vec / Gensim Use Cases</strong><br>1. Train custom embeddings when domain-specific vectors are required and data volume is sufficient.<br>2. Use Word2Vec/Gensim for exploratory analysis or small-to-medium corpora where transformer embeddings are not feasible.<br> </td></tr><tr><td data-label="Chapter 5 — Vector Databases with FAISS and Pinecone"> <strong>Evaluation & Evals Integration</strong><br>1. Automate evals: exact match, Levenshtein, embedding-distance metrics; use pairwise evaluators for model comparison.<br>2. Log reasoning when using stronger evaluator models and perform manual review of ambiguous cases to avoid evaluator bias.<br>3. Use LangSmith or W&B for traceability and historical comparison.<br> </td></tr><tr><td data-label="Chapter 5 — Vector Databases with FAISS and Pinecone"> <strong>Operational Best Practices</strong><br>1. Batch embedding requests and implement retry with exponential backoff for rate-limit stability.<br>2. Persist raw texts and metadata for reindexing; use deterministic stable IDs for items.<br>3. Monitor cost per query, latency, retrieval hit-rate, and hallucination incidents; instrument with telemetry.<br>4. Lock embedding model for a dataset to prevent vector drift; re-embed entire corpus when intentionally upgrading models.<br> </td></tr><tr><td data-label="Chapter 5 — Vector Databases with FAISS and Pinecone"> <strong>Security & Privacy Considerations</strong><br>1. Never hardcode API keys; use environment variables or secret managers.<br>2. Consider data residency, encryption, and legal constraints when using hosted vector stores.<br>3. Limit PII or sensitive content in hosted indices unless compliance is cleared.<br> </td></tr><tr><td data-label="Chapter 5 — Vector Databases with FAISS and Pinecone"> <strong>Cost vs Performance Decisions</strong><br>1. FAISS: low cost, ops complexity for scale; good for prototyping and self-hosted production with ops capacity.<br>2. Pinecone: higher cost, low ops overhead, easier vertical scaling and metadata filters; good for production teams needing managed DBs.<br>3. Hybrid approach: prototype on FAISS then migrate to hosted provider for scale and reliability.<br> </td></tr><tr><td data-label="Chapter 5 — Vector Databases with FAISS and Pinecone"> <strong>Common Failure Modes & Mitigations</strong><br>1. Poor retrieval quality -> re-evaluate chunking, embedding model, or try hybrid sparse+dense retriever.<br>2. Token budget exceeded -> reduce context size or retrieve fewer chunks; use token-aware chunking.<br>3. Hallucinations remain -> tighten system message, include provenance snippets, return "I don't know" default.<br>4. Rate limits on embeddings -> batch calls, implement backoff and local caching of embeddings.<br> </td></tr><tr><td data-label="Chapter 5 — Vector Databases with FAISS and Pinecone"> <strong>Developer Checklist (ingest -> serve)</strong><br>1. Ingest and clean source documents with rich metadata.<br>2. Token-aware chunking with overlap; store chunk->metadata mapping.<br>3. Batch-compute embeddings; persist vectors and mapping table.<br>4. Index vectors into FAISS or Pinecone with deterministic IDs.<br>5. At query time compute query embedding, retrieve top-k, attach context to prompt, call LLM, parse with Pydantic/StrOutputParser.<br>6. Run automated evals against labeled test set; iterate on chunking, prompts, and model choices.<br> </td></tr><tr><td data-label="Chapter 5 — Vector Databases with FAISS and Pinecone"> <strong>Concise Code Patterns</strong><br>• get_vector_embeddings(text) -> vector<br>• FAISS: index = faiss.IndexFlatL2(dim) -> index.add(vectors) -> index.search(qvec, k)<br>• Pinecone: index.upsert(list_of(id, vector, metadata)) -> index.query(vector=xq, top_k=k, include_metadata=True)<br> </td></tr><tr><td data-label="Chapter 5 — Vector Databases with FAISS and Pinecone"> <strong>Provenance & Explainability</strong><br>1. Return source snippets and metadata (source, page, score) with model answers to enable traceability.<br>2. Store mapping and maintain reproducible pipeline for audits and debugging.<br> </td></tr><tr><td data-label="Chapter 5 — Vector Databases with FAISS and Pinecone"> <strong>When to Rebuild Indexes</strong><br>1. Embedding model upgrades or swapping model family.<br>2. Major document corpus changes or schema revisions for metadata.<br>3. Detected drift in retrieval quality after production changes; schedule reindex and validation runs.<br> </td></tr><tr><td data-label="Chapter 5 — Vector Databases with FAISS and Pinecone"> <strong>Short Recommended Defaults</strong><br>1. Chunk size: paragraph-level or 500 tokens as starting heuristics, refine per dataset.<br>2. Overlap: 20–200 tokens depending on chunk size and context continuity needs.<br>3. Top-k retrieval: start k between 3–5 and tune for recall vs token budget.<br>4. Temperature for metadata parsing: 0; for creative responses: tune higher as needed.<br> </td></tr><tr><td data-label="Chapter 5 — Vector Databases with FAISS and Pinecone"> <strong>Final Notes</strong><br>Chapter 5 provides an operational playbook for building production RAG systems: choose and fix an embedding model, design chunking and metadata strategies, persist vectors and mappings, select FAISS for local control or Pinecone for hosted scale, add robust batching/retries and monitoring, and validate continuously with automated evals and human spot checks.<br> </td></tr></tbody></table></div><div class="row-count">Rows: 39</div></div><div class="table-caption" id="Table6" data-table="Book_0002_06" style="margin-top:2mm;margin-left:3mm;"><strong>Chapter 6 • Autonomous Agents with Memory and Tools</strong></div>
<div class="table-wrapper" data-table-id="table-6"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by **Summary**"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>Summary</strong></div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th><th class="tv-col" role="button" aria-label="Sort by **Notes**"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>Notes</strong></div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Summary"> <strong>Introduction to Autonomous Agents</strong> </td><td data-label="Notes"> Autonomous agents are self-directed programs that perceive their environment, reason about actions, and execute tasks with minimal human intervention. In the context of LLMs, they leverage memory and tools to perform complex workflows, maintain context over multiple steps, and achieve goals dynamically. Agents extend the capabilities of LLMs beyond single-turn interactions, allowing them to plan, execute, and revise actions autonomously.                                                                                                                                                                                                                                                                                                     </td></tr><tr><td data-label="Summary"> <strong>Agent Components</strong>                  </td><td data-label="Notes"> Key components include:<br>1. Observation – the agent perceives the environment or receives input data.<br>2. Memory – stores conversation history, state, and summary context to enable multi-turn reasoning.<br>3. Decision-Making / Reasoning – the agent chooses an action based on observations and goals, often leveraging LLM reasoning.<br>4. Actions / Tools – APIs, functions, or external programs the agent can invoke to affect the environment.<br>5. Reward / Feedback – optional reinforcement signals guide agent performance.<br>Example: A research assistant agent reads documents (observation), tracks previous queries (memory), chooses to search a database or summarize content (actions/tools), and logs success metrics (reward). </td></tr><tr><td data-label="Summary"> <strong>Memory in Agents</strong>                  </td><td data-label="Notes"> Memory allows agents to retain and retrieve information over multiple steps:<br>- Short-term / Buffer memory: keeps recent interactions.<br>- Summary / Condensed memory: stores key points from past interactions.<br>- Custom / External memory: connects to databases or vector stores for long-term retrieval.<br>Example: When a user asks a multi-part question, the agent recalls earlier steps from buffer memory to provide a coherent final answer. Memory is critical for multi-turn reasoning and avoiding repetition.                                                                                                                                                                                                                            </td></tr><tr><td data-label="Summary"> <strong>Tools in Agents</strong>                   </td><td data-label="Notes"> Tools allow agents to interact with external systems and extend capabilities beyond LLM text generation:<br>- APIs – e.g., search engines, weather data, or product databases.<br>- Functions / Scripts – pre-defined Python functions or scripts the agent can call.<br>- Vector Stores / Embeddings – retrieve semantically relevant context.<br>Example: Agent decides to call <code>getweather(location)</code> when a user asks about local weather conditions. Tools are integrated via structured action templates.                                                                                                                                                                                                                                               </td></tr><tr><td data-label="Summary"> <strong>ReAct Framework for Agents</strong>        </td><td data-label="Notes"> ReAct (Reason + Act) framework combines LLM reasoning with actionable steps:<br>- Observation: agent perceives input or environment.<br>- Thought: agent generates reasoning about next step.<br>- Action: agent decides which tool or function to invoke.<br>- Action\Input: specific parameters for the chosen action.<br>- Loop: observation → thought → action → observation, repeated until task completion.<br>Example Prompt Pattern: <code>Observation: User asked about stock prices</code> <code>Thought: I need to fetch the latest stock data</code> <code>Action: getstockprice</code> <code>ActionInput: &quot;AAPL&quot;</code> <code>Observation: Returned stock price is $150</code> <code>Thought: I now can answer the user</code> <code>Final Answer: The current price of AAPL is $150.</code>                                  </td></tr><tr><td data-label="Summary"> <strong>Agent Loops / Execution Flow</strong>      </td><td data-label="Notes"> The agent loop follows a cyclical process:<br>1. Receive input / observe environment.<br>2. Generate reasoning (thought).<br>3. Select action or tool.<br>4. Execute action with parameters.<br>5. Record results in memory.<br>6. Repeat until goal achieved or termination condition met.<br>Example Pseudocode:<br><code>python  while not done:    observation = getinput()    thought = llmreason(observation, memory)    action, params = parseaction(thought)    result = execute(action, params)    memory.update(result)    done = checkgoal(memory)  </code>                                                                                                                                                                                                   </td></tr><tr><td data-label="Summary"> <strong>Memory-Augmented Action</strong>           </td><td data-label="Notes"> Agents leverage memory for decision-making and context retrieval:<br>- Retrieve relevant past interactions from buffer or vector memory.<br>- Condense long histories into summaries to reduce token usage.<br>- Use retrieved memory to influence next reasoning step.<br>Example: For a multi-step research query, agent recalls prior retrieved documents to refine search and avoid redundant API calls.                                                                                                                                                                                                                                                                                                                                                  </td></tr><tr><td data-label="Summary"> <strong>Tool Integration and API Calls</strong>    </td><td data-label="Notes"> Structured tool calls ensure deterministic behavior:<br>- Tools are registered with the agent with clear input/output specifications.<br>- Agents generate actions referencing tool names and arguments.<br>- Outputs are captured in memory for next reasoning step.<br>Example Tool Definition:<br><code>python  def getweather(location):    # call weather API and return forecast  </code><br>Agent generates: <code>Action: getweather</code> <code>ActionInput: &quot;Jakarta&quot;</code> Memory logs the result for follow-up queries.                                                                                                                                                                                                                                                          </td></tr><tr><td data-label="Summary"> <strong>Planning and Multi-Step Tasks</strong>     </td><td data-label="Notes"> Advanced agents can plan sequences of actions using memory and reasoning:<br>- Break down complex goals into sub-goals.<br>- Predict dependencies between actions.<br>- Adjust plan dynamically based on observation results.<br>Example: Agent tasked with booking travel:<br>1. Search flights.<br>2. Search hotels.<br>3. Reserve transportation.<br>Each step uses results from the previous action and stores them in memory for planning the next step.                                                                                                                                                                                                                                                                                                 </td></tr><tr><td data-label="Summary"> <strong>Safety, Logging, and Feedback</strong>     </td><td data-label="Notes"> Agents must include monitoring and safety mechanisms:<br>- Log all observations, thoughts, actions, and results.<br>- Use feedback or rewards to refine reasoning and action selection.<br>- Handle errors gracefully (tool failures, API errors).<br>Example: If a tool fails, agent logs failure, chooses alternate action, and updates memory with outcome.                                                                                                                                                                                                                                                                                                                                                                                                </td></tr><tr><td data-label="Summary"> <strong>Applications of Autonomous Agents</strong> </td><td data-label="Notes"> - Customer support chatbots capable of multi-turn problem solving.<br>- Research assistants retrieving, summarizing, and cross-referencing documents.<br>- Workflow automation, e.g., scheduling, monitoring, or data extraction.<br>- Multi-modal agents combining text, images, and API-based reasoning.<br>Example: A medical research agent queries databases, extracts relevant studies, summarizes key points, and outputs an integrated report for the user.                                                                                                                                                                                                                                                                                           </td></tr><tr><td data-label="Summary"> <strong>Key Takeaways / Summary</strong>           </td><td data-label="Notes"> Autonomous agents with memory and tools extend LLM capabilities by enabling multi-step reasoning, action execution, and dynamic task management. Memory allows agents to maintain context, while tools provide external interaction and functional augmentation. The ReAct framework structures reasoning and actions in a loop, ensuring clarity and traceability. Proper design, logging, and error handling make agents reliable, scalable, and safe for complex workflows. Integrating agents with LangChain, vector databases, and custom APIs enables powerful, context-aware, autonomous text generation and task execution.                                                                                                                           </td></tr></tbody></table></div><div class="row-count">Rows: 12</div></div><div class="table-caption" id="Table7" data-table="Book_0002_07" style="margin-top:2mm;margin-left:3mm;"><strong>Chapter 7 • Introduction to Diffusion Models for Image Generation</strong></div>
<div class="table-wrapper" data-table-id="table-7"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by **Summary**"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>Summary</strong></div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th><th class="tv-col" role="button" aria-label="Sort by Notes"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Notes</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Summary"> <strong>Introduction to MidJourney</strong>          </td><td data-label="Notes"> MidJourney is an AI-powered image generation platform using diffusion models. Converts text prompts into visual art. Used by artists, designers, and creators for efficient concept exploration.                                                                                     </td></tr><tr><td data-label="Summary"> <strong>Prompt Structure and Best Practices</strong> </td><td data-label="Notes"> Prompts should include: <br> 1) Content description (subject). <br> 2) Style keywords (e.g., “oil painting,” “cinematic”). <br> 3) Lighting/mood/environment cues (e.g., “sunset,” “foggy”). Example: <em>“A futuristic city skyline at sunset, cyberpunk style, cinematic lighting, ultra-detailed.”</em> </td></tr><tr><td data-label="Summary"> <strong>Prompt Weighting and Parameters</strong>     </td><td data-label="Notes"> Parameters include: <code>--ar</code> (aspect ratio), <code>--q</code> (quality vs. speed), <code>--v</code> (model version). Weighting with <code>::</code> emphasizes words (e.g., <em>“castle::2, fog::1”</em>).                                                                                                                     </td></tr><tr><td data-label="Summary"> <strong>Stylistic Controls</strong>                  </td><td data-label="Notes"> Invoke styles (“digital art,” “watercolor,” “3D render”) and refine with adjectives or artist references. <br> Example: <em>“portrait of a lion, in the style of Rembrandt, dramatic chiaroscuro, highly detailed”</em> → baroque effect.                                                        </td></tr><tr><td data-label="Summary"> <strong>Aspect Ratio and Composition</strong>        </td><td data-label="Notes"> Aspect ratio affects framing: <code>--ar 1:1</code> (square), <code>--ar 16:9</code> (cinematic), <code>--ar 3:2</code> (panoramic). Impacts spatial balance and object placement.                                                                                                                                    </td></tr><tr><td data-label="Summary"> <strong>Image Variations and Upscaling</strong>      </td><td data-label="Notes"> Options: Variations (V) → new similar outputs. Upscaling (U) → higher resolution. Light/Heavy upscaling to balance stylization vs. realism. <br> Workflow: thumbnails → select → upscale → refine variations.                                                                             </td></tr><tr><td data-label="Summary"> <strong>Seed Control and Reproducibility</strong>    </td><td data-label="Notes"> <code>--seed</code> ensures reproducibility. Same prompt + seed = consistent outputs. Example: <code>--seed 12345</code>.                                                                                                                                                                                  </td></tr><tr><td data-label="Summary"> <strong>Blend and Remix Features</strong>            </td><td data-label="Notes"> Blend = merge multiple input images with text prompts. Remix = modify prior generations. <br> Example: upload sketch → prompt “enhance with cyberpunk city style.”                                                                                                                        </td></tr><tr><td data-label="Summary"> <strong>Iterative Refinement Practices</strong>      </td><td data-label="Notes"> Steps: <br> 1) Start broad.<br> 2) Generate variations. <br> 3) Upscale + tweak style/lighting. <br> 4) Blend/compose final render. <br> Workflow: concept → adjust → refine → final select.                                                                                                                 </td></tr><tr><td data-label="Summary"> <strong>Avoiding Common Pitfalls</strong>            </td><td data-label="Notes"> Pitfalls: too complex prompts = incoherence, conflicting style cues = mixed aesthetics, extreme ratios = distortions. <br> Best practices: concise but descriptive prompts, iterative testing, parameter adjustment.                                                                      </td></tr><tr><td data-label="Summary"> <strong>Advanced Prompt Techniques</strong>          </td><td data-label="Notes"> Use multi-prompt weighting, artist/medium references, image+text combo. Example: <em>“dragon::2, castle::1, foggy::0.5 --ar 16:9 --v 5”</em> → dragon-focused scene with subtle fog.                                                                                                        </td></tr><tr><td data-label="Summary"> <strong>Community Models and Versions</strong>       </td><td data-label="Notes"> MidJourney evolves with versions and community models. Versions differ in realism vs. stylization. <br> Example: V5 = realism and high detail, earlier = more stylized.                                                                                                                   </td></tr><tr><td data-label="Summary"> <strong>Applications of MidJourney</strong>          </td><td data-label="Notes"> Use cases: concept art, marketing visuals, storyboarding, game prototyping, education, creative exploration. <br> Example: rapid mood boards without manual rendering.                                                                                                                    </td></tr><tr><td data-label="Summary"> <strong>Key Takeaways / Summary</strong>             </td><td data-label="Notes"> Success with MidJourney = structured prompts, parameter control, iterative refinement. <br> Core practices: prompt clarity, weighting, style/ratio guidance, seed reproducibility, variations/upscaling. Enables reproducible, high-quality, stylistically coherent images.               </td></tr></tbody></table></div><div class="row-count">Rows: 14</div></div><div class="table-caption" id="Table8" data-table="Book_0002_08" style="margin-top:2mm;margin-left:3mm;"><strong>Chapter 8 • Standard Practices for Image Generation with Midjourney</strong></div>
<div class="table-wrapper" data-table-id="table-8"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by **Summary**"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>Summary</strong></div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th><th class="tv-col" role="button" aria-label="Sort by Notes"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Notes</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Summary"> <strong>Introduction to Stable Diffusion</strong>        </td><td data-label="Notes"> Stable Diffusion is an open-source text-to-image generation model based on latent diffusion. Produces high-quality images from text prompts and allows fine-grained control over styles, composition, and content. Often run locally for privacy and faster iteration; used for research, art, and prototyping.                                                                                                                          </td></tr><tr><td data-label="Summary"> <strong>Prompt Engineering for Stable Diffusion</strong> </td><td data-label="Notes"> Effective prompts include: <br>- Content description (objects, characters, scene elements). <br>- Style cues (e.g., “digital painting,” “watercolor,” “photorealistic”). <br>- Lighting/mood/environment (e.g., “sunset,” “cinematic lighting,” “foggy forest”). <br>Example: <code>&quot;A futuristic cityscape at night, cyberpunk aesthetic, neon lights, ultra-detailed, photorealistic&quot;</code>                                                      </td></tr><tr><td data-label="Summary"> <strong>Control of Image Generation Parameters</strong>  </td><td data-label="Notes"> Adjustable parameters: <br>- Steps: number of diffusion iterations (more → more detail). <br>- CFG (classifier-free guidance) scale: balances prompt adherence vs creativity. <br>- Seed: reproducibility. <br>Example: <code>steps=50, CFG=7.5, seed=12345</code> for a high-fidelity, reproducible image.                                                                                                                                         </td></tr><tr><td data-label="Summary"> <strong>Latent Space Manipulation</strong>               </td><td data-label="Notes"> Techniques: <br>- Interpolation: blend two latent vectors to create hybrids. <br>- Noise injection: control randomness for variations. <br>- Vector arithmetic: e.g., <code>latentdragon - latentcat + latentwolf</code> to create novel composites.                                                                                                                                                                                                </td></tr><tr><td data-label="Summary"> <strong>Image-to-Image Generation (img2img)</strong>     </td><td data-label="Notes"> Img2img modifies an existing image guided by a prompt: <br>- Upload base image → define prompt → adjust strength (noise control). <br>- Strength: 0 → minor edits; 1 → full regeneration. <br>Example: Transform a rough sketch into a fully rendered digital painting with <code>strength=0.7</code>.                                                                                                                                              </td></tr><tr><td data-label="Summary"> <strong>Inpainting Techniques</strong>                   </td><td data-label="Notes"> Inpainting edits specific regions while keeping the rest intact: <br>- Mask areas to edit → provide prompt for desired change. <br>- Useful for repairs, adding objects, or changing backgrounds. <br>Example: Mask sky → prompt <code>&quot;sunset with dramatic clouds&quot;</code> → regenerate sky without affecting foreground.                                                                                                                          </td></tr><tr><td data-label="Summary"> <strong>Outpainting for Extended Scenes</strong>         </td><td data-label="Notes"> Outpainting expands canvas beyond original borders: <br>- Extends scene consistently based on context. <br>- Useful for panoramas or immersive environments. <br>Example: Expand a castle painting left/right → prompt <code>&quot;surrounding landscape, misty mountains&quot;</code> → panoramic view.                                                                                                                                                      </td></tr><tr><td data-label="Summary"> <strong>Fine-Tuning and Custom Models</strong>           </td><td data-label="Notes"> Options to adapt model to specific styles/subjects: <br>- LoRA (Low-Rank Adaptation): add styles/characters without full retrain. <br>- DreamBooth: personalize model for specific subjects. <br>- Custom checkpoints: control style, characters, or domain. <br>Example: Fine-tune on fantasy creatures to generate unique, consistent monsters.                                                                                        </td></tr><tr><td data-label="Summary"> <strong>Prompt Weighting and Advanced Syntax</strong>    </td><td data-label="Notes"> Weighting syntax for nuanced control: <br>- Parentheses to increase weight: <code>(castle)</code> or <code>((castle))</code>. <br>- Square brackets to reduce weight: <code>[fog]</code>. <br>- Multi-prompt concatenation for complex scenes. <br>Example: <code>&quot;((dragon)) flying over castle, [fog], cinematic lighting, photorealistic&quot;</code> emphasizes dragon and castle, downplays fog.                                                                                     </td></tr><tr><td data-label="Summary"> <strong>Iterative Refinement Practices</strong>          </td><td data-label="Notes"> Workflow: <br>1. Start with a broad prompt. <br>2. Generate multiple outputs → pick promising ones. <br>3. Apply img2img / inpainting / outpainting → tweak prompt/params. <br>4. Upscale and finalize. <br>Example: Base prompt → variations → inpaint sky → upscale → final render.                                                                                                                                                    </td></tr><tr><td data-label="Summary"> <strong>Upscaling and Super-Resolution</strong>          </td><td data-label="Notes"> Techniques to improve final quality: <br>- Use ESRGAN or Real-ESRGAN for detail-preserving upscales. <br>- Combine with inpainting to refine edges/textures. <br>Example: Generate 512×512 → upscale to 2048×2048 → apply inpainting for sharpness.                                                                                                                                                                                      </td></tr><tr><td data-label="Summary"> <strong>Safety and Filtering Controls</strong>           </td><td data-label="Notes"> Some builds include NSFW/harmful-content filters: <br>- Ensures responsible generation. <br>- Filters can be adjusted/disabled locally with caution. <br>- Important for research or curated artistic workflows.                                                                                                                                                                                                                         </td></tr><tr><td data-label="Summary"> <strong>Applications of Stable Diffusion</strong>        </td><td data-label="Notes"> Use cases: <br>- Concept art, illustration, visual storytelling. <br>- Game asset generation and prototyping. <br>- Scientific visualization and educational imagery. <br>- Personalized marketing or social media content. <br>Example: Artist generates a fantasy world map → refines characters, landscapes, and lighting for game assets.                                                                                            </td></tr><tr><td data-label="Summary"> <strong>Key Takeaways / Summary</strong>                 </td><td data-label="Notes"> Stable Diffusion is flexible and powerful for text→image generation. Best practices: <br>- Structured prompt engineering (style + content clarity). <br>- Control generation params (<code>steps</code>, <code>CFG</code>, <code>seed</code>). <br>- Use latent manipulation, img2img, inpainting, outpainting. <br>- Fine-tune with LoRA/DreamBooth for specific styles. <br>- Iterate, upscale, and apply responsible filtering for high-quality, reproducible outputs. </td></tr></tbody></table></div><div class="row-count">Rows: 14</div></div><div class="table-caption" id="Table9" data-table="Book_0002_09" style="margin-top:2mm;margin-left:3mm;"><strong>Chapter 9 • Advanced Techniques for Image Generation with Stable Diffusion</strong></div>
<div class="table-wrapper" data-table-id="table-9"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by **Summary**"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>Summary</strong></div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th><th class="tv-col" role="button" aria-label="Sort by Notes"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Notes</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Summary"> <strong>Introduction to AI-Powered Applications</strong> </td><td data-label="Notes"> AI-powered applications integrate ML, NLP, computer vision, or other AI capabilities into software to automate tasks, provide insights, and enhance UX. Examples: chatbots, recommendation engines, image recognition tools, autonomous agents.                                                                                                                                                     </td></tr><tr><td data-label="Summary"> <strong>Architecture Overview</strong>                   </td><td data-label="Notes"> Common layers: <br>- Frontend: web/mobile/desktop UI. <br>- Backend: data processing, model inference, APIs. <br>- Model Layer: hosts models (local or cloud). <br>- Data Layer: storage, retrieval, preprocessing. <br>Example: React frontend + FastAPI backend + GPT server + DB for history.                                                                                                    </td></tr><tr><td data-label="Summary"> <strong>Choosing AI Models</strong>                      </td><td data-label="Notes"> Pick by task: <br>- Text generation: GPT, LLaMA, OpenAI API. <br>- Image generation: Stable Diffusion, MidJourney API. <br>- Speech recognition: Whisper, DeepSpeech. <br>- Recommenders: collaborative filtering, embeddings. <br>Consider accuracy, latency, cost, scalability trade-offs.                                                                                                        </td></tr><tr><td data-label="Summary"> <strong>Data Preparation and Preprocessing</strong>      </td><td data-label="Notes"> Key steps: <br>- Cleaning: dedupe, fix formatting, handle missing values. <br>- Normalization / Tokenization for text/numeric inputs. <br>- Augmentation for robustness. <br>- Split: training / validation / test to avoid overfitting. <br>Example: Tokenize and embed text classifier inputs.                                                                                                    </td></tr><tr><td data-label="Summary"> <strong>Integration of AI Models</strong>                </td><td data-label="Notes"> Integration options: <br>- Local inference on own servers. <br>- Cloud APIs: OpenAI, Hugging Face, Stability.ai. <br>- SDKs/Libraries: LangChain, PyTorch, TensorFlow, ONNX. <br>Example: Expose GPT generation via FastAPI <code>/generate</code> endpoint.                                                                                                                                                   </td></tr><tr><td data-label="Summary"> <strong>Prompt and Workflow Management</strong>          </td><td data-label="Notes"> Best practices: <br>- Use structured prompts for consistent responses. <br>- Maintain conversation state/memory/context windows. <br>- Orchestrate pipelines with LangChain or similar. <br>Example: Domain-limited customer support bot using conversation history.                                                                                                                                </td></tr><tr><td data-label="Summary"> <strong>Vector Databases and Semantic Search</strong>    </td><td data-label="Notes"> Use embeddings for semantic retrieval: <br>- Store vectors in FAISS, Pinecone, Milvus. <br>- Retrieve nearest neighbors for Q\&A, recommendations. <br>Example: Query → embed → search vector DB → return docs → feed LLM.                                                                                                                                                                          </td></tr><tr><td data-label="Summary"> <strong>Autonomous Agents and Tool Integration</strong>  </td><td data-label="Notes"> Advanced apps include agents with memory/tools: <br>- Agents call APIs, perform tasks, maintain state. <br>- Integrate calculators, search, databases to extend capability. <br>Example: Travel-planning agent fetching flights, booking hotels, summarizing options.                                                                                                                               </td></tr><tr><td data-label="Summary"> <strong>Testing and Evaluation</strong>                  </td><td data-label="Notes"> Rigorous testing: <br>- Unit tests for components. <br>- Integration tests across modules. <br>- Model evaluation metrics: accuracy, BLEU, ROUGE, FID. <br>Example: Evaluate chatbot responses for relevance and correctness using sample queries.                                                                                                                                                  </td></tr><tr><td data-label="Summary"> <strong>Deployment Strategies</strong>                   </td><td data-label="Notes"> Deployment patterns: <br>- Containerization: Docker/Kubernetes for scale. <br>- Serverless: AWS Lambda/Azure Functions for lightweight. <br>- Cloud GPU hosting: AWS/GCP/Azure for inference. <br>Example: Deploy image generation service on GCP GPU instance + REST API.                                                                                                                          </td></tr><tr><td data-label="Summary"> <strong>Security, Privacy, and Compliance</strong>       </td><td data-label="Notes"> Considerations: <br>- Protect PII and sensitive data. <br>- Filter unsafe or biased outputs. <br>- Comply with GDPR, HIPAA, regional laws. <br>Example: Mask sensitive fields before sending to third-party LLMs; log user consent.                                                                                                                                                                 </td></tr><tr><td data-label="Summary"> <strong>Monitoring and Maintenance</strong>              </td><td data-label="Notes"> Continuous monitoring: <br>- Track usage, errors, latency. <br>- Monitor model drift and performance degradation. <br>- Retrain/update models as needed. <br>Example: Use Prometheus + Grafana to monitor API latency and trigger retraining on drop in accuracy.                                                                                                                                   </td></tr><tr><td data-label="Summary"> <strong>Scalability and Optimization</strong>            </td><td data-label="Notes"> Optimization tactics: <br>- Batch requests for GPU efficiency. <br>- Use mixed precision or quantization for faster inference. <br>- Horizontally scale with microservices. <br>Example: Convert model weights to 8-bit quantization to reduce memory and inference time.                                                                                                                           </td></tr><tr><td data-label="Summary"> <strong>User Experience and Feedback Loops</strong>      </td><td data-label="Notes"> UX-focused practices: <br>- Log user interactions to fine-tune prompts/models. <br>- Provide fallback responses for errors. <br>- Iterate UX based on analytics. <br>Example: Retrain on misunderstood queries logged by chatbot to improve quality.                                                                                                                                                </td></tr><tr><td data-label="Summary"> <strong>Key Takeaways / Summary</strong>                 </td><td data-label="Notes"> Building AI apps combines software engineering, ML, and user-centered design. Best practices: <br>- Clear frontend/backend/model/data architecture. <br>- Proper preprocessing, model selection, and integration. <br>- Use vector DBs, agents, and orchestration for intelligence. <br>- Emphasize testing, deployment, monitoring, optimization, and compliance for robust, maintainable systems. </td></tr></tbody></table></div><div class="row-count">Rows: 15</div></div><div class="table-caption" id="Table10" data-table="Book_0002_10" style="margin-top:2mm;margin-left:3mm;"><strong>Chapter 10 • Building AI-Powered Applications</strong></div>
<div class="table-wrapper" data-table-id="table-10"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by **Summary**"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>Summary</strong></div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th><th class="tv-col" role="button" aria-label="Sort by **Notes**"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>Notes</strong></div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Summary"> <strong>Introduction to AI-Powered Applications</strong> </td><td data-label="Notes"> AI-powered applications integrate machine learning, natural language processing, computer vision, or other AI capabilities into software solutions. They can automate tasks, provide intelligent insights, or enhance user experiences.<br>Examples include chatbots, recommendation engines, image recognition tools, and autonomous agents.                                                                                                                                                                                                                                                                                                                                </td></tr><tr><td data-label="Summary"> <strong>Architecture Overview</strong>                   </td><td data-label="Notes"> Building AI applications requires a clear architecture:<br>- Frontend: interface for user interaction (web, mobile, desktop).<br>- Backend: manages data processing, model inference, APIs.<br>- Model Layer: hosts AI models, either locally or via cloud services.<br>- Data Layer: handles storage, retrieval, and preprocessing of training/inference data.<br>Example: A chatbot application may have a React frontend, FastAPI backend, a GPT model hosted on a server, and a database for conversation history.                                                                                                                                                       </td></tr><tr><td data-label="Summary"> <strong>Choosing AI Models</strong>                      </td><td data-label="Notes"> Selecting the right AI model depends on the application:<br>- Text Generation: GPT, LLaMA, or OpenAI API.<br>- Image Generation: Stable Diffusion, Midjourney API.<br>- Speech Recognition: Whisper, DeepSpeech.<br>- Recommendation Engines: Collaborative filtering, embeddings.<br>Consider trade-offs in accuracy, latency, cost, and scalability.                                                                                                                                                                                                                                                                                                                       </td></tr><tr><td data-label="Summary"> <strong>Data Preparation and Preprocessing</strong>      </td><td data-label="Notes"> Proper data handling is crucial:<br>- Cleaning: remove duplicates, fix formatting, handle missing values.<br>- Normalization / Tokenization: for text or numeric inputs.<br>- Augmentation: generate additional training examples for robustness.<br>- Splitting: training, validation, and test sets to avoid overfitting.<br>Example: For a text classifier, tokenize sentences, remove stop words, and create embeddings before feeding into a model.                                                                                                                                                                                                                     </td></tr><tr><td data-label="Summary"> <strong>Integration of AI Models</strong>                </td><td data-label="Notes"> AI models can be integrated via APIs, SDKs, or direct embedding:<br>- Local Inference: run models on your own servers.<br>- Cloud APIs: OpenAI, Hugging Face, Stability.ai for managed services.<br>- SDKs and Libraries: LangChain, PyTorch, TensorFlow, or ONNX Runtime.<br>Example: Using FastAPI, expose GPT text generation as an endpoint <code>/generate</code> for frontend consumption.                                                                                                                                                                                                                                                                                        </td></tr><tr><td data-label="Summary"> <strong>Prompt and Workflow Management</strong>          </td><td data-label="Notes"> Effective prompts and workflows are key for LLM-based applications:<br>- Use structured prompts to elicit consistent responses.<br>- Implement conversation state, memory, or context windows.<br>- Automate pipelines with LangChain or similar orchestration tools.<br>Example: In a customer support bot, maintain conversation history and guide GPT to answer only within domain-specific knowledge.                                                                                                                                                                                                                                                                    </td></tr><tr><td data-label="Summary"> <strong>Vector Databases and Semantic Search</strong>    </td><td data-label="Notes"> Many AI apps rely on embeddings for semantic search or retrieval:<br>- Generate vector representations of text or images.<br>- Store in FAISS, Pinecone, or Milvus.<br>- Retrieve nearest neighbors for recommendations, Q\&A, or image search.<br>Example: User query → embed → search vector DB → return relevant documents → feed to LLM for answer generation.                                                                                                                                                                                                                                                                                                           </td></tr><tr><td data-label="Summary"> <strong>Autonomous Agents and Tool Integration</strong>  </td><td data-label="Notes"> Advanced AI applications include agents with memory and tools:<br>- Agents can call APIs, perform tasks, and maintain state.<br>- Integration with calculators, search engines, or external databases enhances capabilities.<br>Example: An AI travel planner agent can retrieve flight data, book hotels, and summarize recommendations for the user.                                                                                                                                                                                                                                                                                                                       </td></tr><tr><td data-label="Summary"> <strong>Testing and Evaluation</strong>                  </td><td data-label="Notes"> Rigorous testing ensures reliability and accuracy:<br>- Unit Testing: verify components function correctly.<br>- Integration Testing: ensure smooth communication between modules.<br>- Model Evaluation: accuracy, BLEU, ROUGE, FID for image/text tasks.<br>Example: Test chatbot with a set of sample queries and evaluate response relevance and correctness.                                                                                                                                                                                                                                                                                                            </td></tr><tr><td data-label="Summary"> <strong>Deployment Strategies</strong>                   </td><td data-label="Notes"> Deployment depends on scale and requirements:<br>- Containerization: Docker or Kubernetes for scalable deployment.<br>- Serverless: AWS Lambda, Azure Functions for lightweight services.<br>- Cloud Hosting: AWS, GCP, or Azure with GPU support for inference.<br>Example: Deploy an AI image generation service on GCP with GPU instance → expose REST API → integrate with frontend.                                                                                                                                                                                                                                                                                     </td></tr><tr><td data-label="Summary"> <strong>Security, Privacy, and Compliance</strong>       </td><td data-label="Notes"> Consider legal and ethical implications:<br>- Data privacy: handle PII carefully.<br>- Model outputs: filter unsafe or biased content.<br>- Compliance: GDPR, HIPAA, or other regional regulations.<br>Example: Mask sensitive data before sending to third-party LLM APIs; log user consent.                                                                                                                                                                                                                                                                                                                                                                                </td></tr><tr><td data-label="Summary"> <strong>Monitoring and Maintenance</strong>              </td><td data-label="Notes"> Continuous monitoring ensures stability:<br>- Track usage, errors, response times.<br>- Monitor model drift or degraded performance over time.<br>- Retrain or update models as necessary.<br>Example: Use Prometheus + Grafana to monitor API response times and model latency; trigger retraining when performance drops below thresholds.                                                                                                                                                                                                                                                                                                                                 </td></tr><tr><td data-label="Summary"> <strong>Scalability and Optimization</strong>            </td><td data-label="Notes"> Optimize for cost and performance:<br>- Batch requests for GPU efficiency.<br>- Use mixed precision or quantization for faster inference.<br>- Horizontal scaling with microservices.<br>Example: Convert LLM weights to 8-bit quantization → reduce memory usage and inference time without major accuracy loss.                                                                                                                                                                                                                                                                                                                                                            </td></tr><tr><td data-label="Summary"> <strong>User Experience and Feedback Loops</strong>      </td><td data-label="Notes"> A well-designed AI app requires iterative feedback:<br>- Collect user interactions → fine-tune prompts or models.<br>- Implement fallback responses for errors or unexpected outputs.<br>- Continuously improve UX based on analytics.<br>Example: Chatbot logs misunderstood queries → retrain model with additional examples → improve response quality.                                                                                                                                                                                                                                                                                                                   </td></tr><tr><td data-label="Summary"> <strong>Key Takeaways / Summary</strong>                 </td><td data-label="Notes"> Building AI-powered applications combines software engineering, ML modeling, and user-centric design.<br>Best practices include:<br>- Clear architecture with frontend, backend, model, and data layers.<br>- Proper data preprocessing, model selection, and integration.<br>- Use vector databases, autonomous agents, and workflows for intelligent applications.<br>- Testing, deployment, monitoring, and optimization ensure robustness.<br>- Ethical, privacy, and compliance considerations guide responsible AI deployment.<br>Following these principles allows creation of scalable, maintainable, and intelligent AI applications suitable for real-world usage. </td></tr></tbody></table></div><div class="row-count">Rows: 15</div></div><script src="assets/xlsx.full.min.js?v=1758605028" defer></script>
<script src="assets/script.js?v=1759748863" defer></script>
<script src="assets/worker.js?v=1758331710" defer></script>
<script>
(function(){
  const template = "{table}_{date}";
  const userVal = "";
  const hrefPrefix = "assets";
  function formatName(tableName) {
    const date = (new Date()).toISOString().slice(0,10);
    return template.replace('{table}', tableName).replace('{date}', date).replace('{user}', userVal);
  }
  const btn = document.getElementById('exportBtn');
  if(btn) {
    btn.addEventListener('click', async function() {
      try {
        const html = document.documentElement.outerHTML;
        if(html.length > 2000000) { alert('Export refused: html too large'); return; }
        if(window.Worker) {
          const workerUrl = (function(){ try{ return hrefPrefix + '/worker.js'; }catch(e){ return null; } })();
          if(workerUrl) {
            try {
              const worker = new Worker(workerUrl);
              worker.postMessage({html: html, format: 'pdf'});
              worker.onmessage = function(e) { console.log('worker:', e.data); alert('Worker replied: '+(e.data.msg||e.data.status)); };
            } catch(err) { console.warn('Worker create failed', err); alert('Worker not available'); }
          } else { alert('Worker not available'); }
        } else { alert('Export worker not supported in this environment.'); }
      } catch(err) { console.warn('Export failed', err); alert('Export worker not available. See console for details.'); }
    });
  }
})();
window.addEventListener('load', function(){ try{ document.querySelectorAll('.table-wrapper').forEach(function(e){ e.style.opacity='1'; }); }catch(e){} });
</script>
</div>
</body>
</html>