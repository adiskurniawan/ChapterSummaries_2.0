<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='utf-8'><meta name='viewport' content='width=device-width,initial-scale=1'>
<title>Tables Viewer v2.1</title>
<style>:root{--main-header-height:56px;} .table-wrapper{opacity:0;min-height:24px;transition:opacity .12s linear}</style>
<link rel="stylesheet" href="assets/style.css?v=1759925496">
<link rel="stylesheet" href="assets/overrides.css?v=1768668413">
</head><body>
<div id="tables-viewer" role="region" aria-label="Tables viewer">
<div id="stickyMainHeader">
<div id="tv-header">
<div><h1>Tables Viewer v2.1</h1></div>
<div style="display:flex;gap:8px;align-items:center;">
<input id="searchBox" class="tv-search" type="search" placeholder="Search" aria-label="Search tables" />
<button id="modeBtn" type="button" onclick="toggleMode()" aria-label="Toggle theme">Theme</button>
<button id="toggleAllBtn" type="button" aria-label="Toggle all" onclick="toggleAllTables()">Collapse All Tables</button>
<button id="copyAllPlainBtn" class="copy-btn" type="button" onclick="copyAllTablesPlain()" aria-label="Copy all tables as plain text">Copy All Tables (Plain Text)</button>
<button id="copyAllTablesBtn" class="copy-all-btn" type="button" onclick="copyAllTablesMarkdown()" aria-label="Copy All Tables (Markdown)">Copy All Tables (Markdown)</button>
<button id="copyAllMdBtn" style="display:none" aria-hidden="true"></button>
<button id="resetAllBtn" type="button" onclick="resetAllTables()" aria-label="Reset All Tables">Reset All Tables</button>
</div></div>
<script>
(function(){
  function ensureDelegation(){
    try {
      var vis = document.getElementById('copyAllTablesBtn');
      var alias = document.getElementById('copyAllMdBtn');
      if(!vis || !alias) return;
      alias.addEventListener = function(type, listener, options){
        vis.addEventListener(type, listener, options);
      };
      alias.removeEventListener = function(type, listener, options){
        vis.removeEventListener(type, listener, options);
      };
      Object.defineProperty(alias, 'onclick', {
        set: function(fn){ vis.onclick = fn; },
        get: function(){ return vis.onclick; },
        configurable: true
      });
      alias.focus = function(){ vis.focus(); };
      alias.blur = function(){ vis.blur(); };
    } catch(e) {
      try{ console && console.warn && console.warn('alias delegation failed', e); }catch(_){} 
    }
  }
  if(document.readyState === 'loading'){
    document.addEventListener('DOMContentLoaded', ensureDelegation);
  } else {
    ensureDelegation();
  }
})();
</script>
<noscript><div style='color:#b91c1c'>JavaScript is disabled. Tables will be shown statically. For large tables enable JS for virtualization.</div></noscript>
<div id="tocBar" role="navigation" aria-label="Table of contents"><ul><li class="toc-item"><a class="toc-link" href="#Table1">Table 1</a></li>
<li class="toc-item"><a class="toc-link" href="#Table2">Table 2</a></li>
<li class="toc-item"><a class="toc-link" href="#Table3">Table 3</a></li>
<li class="toc-item"><a class="toc-link" href="#Table4">Table 4</a></li></ul></div></div>
<div class="table-caption" id="Table1" data-table="Docu_0180_01" style="margin-top:2mm;margin-left:3mm;"><strong>Table 1</strong></div>
<div class="table-wrapper" data-table-id="table-1"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by PQ_Injector — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">PQ_Injector — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="PQ_Injector — Per-function Expert Technical Breakdown"> <strong>Module-level metadata (contract & overview):</strong><br><strong>Owner:</strong> TEAM_PQ_INJECTOR (listed in OWNERS.md and referenced in release manifests).<br><strong>Public API (surface):</strong> Add_Query_From_M, Add_Query_From_M_With_Parameters, Parameterize_Template, Preview_M, Compute_mChecksum, Validate_M_Syntax, EnsureQueryNameUnique, Create_Connection, Create_Connection_Model, Add_Query_As_ConnectionOnly, Update_Query_Formula, Remove_Query, Export_M_Artifact, Persist_M_Artifact (AtomicWrite wrapper), Inspect_Queries, Get_Query_Diagnostics, SafeCreateQueryName, Set_Connection_Credentials_Safely, Snapshot_Queries, Rollback_Query_Insert, AuditEmit_PQAction, Register_InjectionHook. <br><strong>Audits emitted:</strong> pq_inject.attempt, pq_inject.completed, pq_inject.failure, pq_preview.start, pq_preview.complete, pq_mchecksum.computed, pq_connection.created, pq_connection.updated, pq_connection.failure, pq_export.attempt, pq_export.completed, pq_export.failure, pq_diagnostics.collected, pq_query.removed, pq_query.updated. Every audit row contains correlationId, module=PQ_Injector, procedure, paramsHash, mChecksum when applicable, artifactChecksum when applicable, and timestamp. <br><strong>Purpose and intended use:</strong> Provide deterministic, auditable, safe APIs for injecting Power Query (M) artifacts into Excel workbooks and Power Query runtimes. Ensure injected queries are validated, parameterized, persisted as auditable artifacts, optionally created as connections only or model-backed connections, and that all side-effects are crash-safe and reversible. Avoid performing blocking long IO on UI thread; defer to worker processes for heavy persistence and network exports. <br><strong>Non-goals / constraints:</strong> Not a general-purpose M authoring tool; not responsible for authoring template repositories; not intended to perform data refresh operations itself (use PQ_Refresh / PQ_Diagnostics). Avoid embedding secrets in top-level audit rows; credentials are passed to credential managers or external secure stores. No uncontrolled workbook DOM modifications; always follow host-approved APIs and naming policies. </td></tr><tr><td data-label="PQ_Injector — Per-function Expert Technical Breakdown"> <strong>Operational guarantees & invariants (module-level):</strong><br>1. Determinism: Given identical inputs (template text, parameters, correlationId, config.hash) and same runtime version, the computed <code>mChecksum</code> and persisted artifact bytes must be reproducible across runs and platforms. <br>2. Audit anchoring: Any mutation to workbook queries, connections, or persisted artifacts must be accompanied by at least one audit row referencing correlationId and essential provenance fields. <br>3. Crash-safety: Persistence of M artifacts uses atomic-write semantics (via CORE_Utilities.AtomicWrite or PQ_Utilities.AtomicWrite wrapper) to ensure final-on-success semantics; either old query state remains or new query fully replaces it. <br>4. UI-safety: Avoid long-running IO or blocking actions on the UI thread. Heavy persistence, remote exports, and checksum verification run on background workers; UI handlers only coordinate and emit UserAction audits. <br>5. Name safety: Ensure generated or requested query names do not collide with existing workbook objects; automatically resolve and document name normalization decisions in audits. <br>6. Template governance: For regulated templates, require template owner approval and manifest-signed verification before injection. <br><strong>Performance SLOs:</strong> Inject (local in-memory add) latency for small M payloads <200ms on average in UI helper; persisted artifact atomic-write round-trip median <300ms on local SSD; mChecksum computation for typical template <50KB within 50ms. <br><strong>CI / acceptance gates:</strong> mChecksum parity vectors across supported implementations; cross-platform injection acceptance tests (Excel Windows, Excel Mac where supported, Power BI Desktop if applicable); audit emission verification tests; static checks preventing workbook DOM calls on UI OnLoad handlers. </td></tr><tr><td data-label="PQ_Injector — Per-function Expert Technical Breakdown"> <strong>Design overview & high-level flow (conceptual):</strong><br>1. Operator triggers injection from PQ_Ribbon -> PQ_Injector receives <code>InjectTemplateRequest</code> containing templateId, parameterValues (optional), targetQueryName (optional), injectionMode (<code>InsertInline | ConnectionOnly | ModelConnection</code>), correlationId. <br>2. PQ_Injector computes canonical parameters: validate parameter schema, fill defaults, canonicalize types, compute <code>paramsHash</code>. <br>3. Parameterize_Template: applies parameter substitutions to canonical M template to produce final M formula for injection; compute <code>mChecksum</code> of produced M text. <br>4. Validate_M_Syntax: lightweight client-side syntax checks where available; for heavy validation or host-dependent parsing, schedule worker validation or preview execution in isolated host with redacted inputs. <br>5. EnsureQueryNameUnique: compute safe query name with normalization rules and conflict resolution policy; if renamed, emit audit with name mapping. <br>6. Persist_M_Artifact: create durable artifact (artifact JSON containing name, mText, params, mChecksum, templateVersion, manifest references) and persist via AtomicWrite; emit pq_export and pq_mchecksum audits. <br>7. Add_Query_From_M: call workbook.Queries.Add (or host API) from UI helper using the persisted artifact as source to ensure the in-memory query matches persisted artifact; optionally create a connection object and/or model connection according to injectionMode. <br>8. Post-inject tasks: optionally register query in PQ_Library index if template was saved there, collect diagnostics, update <code>pq_inject.completed</code> audit row with artifactChecksum and connection metadata. </td></tr><tr><td data-label="PQ_Injector — Per-function Expert Technical Breakdown"> <strong>Cross-cutting invariants & security rules (must/shall):</strong><br>1. Do not execute M queries against live external connectors during injection unless explicitly requested and permissioned by operator; injection should be syntactic and structural by default. <br>2. Never store credentials inside M artifacts or top-level audit rows. Credential references may be stored as secure handles or connection IDs. <br>3. For regulated templates, require manifest signature verification and owner approval before injection and record these validations in the audit row. <br>4. Respect workbook privacy: do not upload workbook contents or list of queries to remote telemetry without operator consent. <br>5. All persisted artifacts must include <code>correlationId</code>, <code>producerVersion</code>, and <code>producerFingerprint</code> for later forensic replay. </td></tr><tr><td data-label="PQ_Injector — Per-function Expert Technical Breakdown"> <strong><code>Add_Query_From_M(name, formula, injectionMode=&quot;inline&quot;, connectionOptions=null, operatorId=null)</code></strong><br><strong>Purpose & contract:</strong> inject a Power Query (M) formula into the active workbook in a deterministic, auditable, and reversible fashion. <code>injectionMode</code> controls whether the query is added as inline (worksheet query), connection-only, or model-backed. The function returns structured result <code>{success, queryName, queryId, connectionId?, artifactChecksum, mChecksum, attempts, diagnosticsRef}</code> and emits audit rows on attempt and completion. <br><strong>Parameters & constraints:</strong> <code>name</code> (string — may be null to request auto-name), <code>formula</code> (string — final M text produced by Parameterize_Template), <code>injectionMode</code> ∈ {inline, connectionOnly, modelConnection}, <code>connectionOptions</code> optional dict (e.g., {enableRefresh: bool, loadToModel: bool, credentialHandle: id}). <code>operatorId</code> optional for audit. The function must not perform long-running file IO on UI thread; if persistence or export is required, delegate to Persist_M_Artifact and use a background worker. <br><strong>Must/shall behaviors:</strong><br>1. Call EnsureQueryNameUnique(name) to compute final <code>queryName</code> and document name mapping in audit if mutated. <br>2. Compute <code>mChecksum</code> using canonical normalized bytes (UTF-8 normalized, strip CRLF differences, stable whitespace normalization rules) and emit pq_mchecksum.computed audit with paramsHash and inputHash. <br>3. Persist artifact via Persist_M_Artifact(artifactPayload) before calling workbook API unless caller explicitly requests ephemeral inject (rare, must be approved and audited). <br>4. Use host workbook API to Add Query, catch host-specific exceptions, map them to PQ error codes, and retry only for transient host errors using Retry wrapper with idempotent_assert=true. <br>5. On success, optionally create connection object per connectionOptions. <br>6. Emit pq_inject.completed audit with artifactChecksum, queryName, queryId, connection metadata, and mChecksum. <br><strong>Edge cases & error handling:</strong><br>1. Name collision: if final query name collides with an existing query that is user-modified, do not overwrite without explicit operator confirmation; use auto-suffix scheme <code>name (1)</code>, emit pq_inject.resolved_name audit and provide map in result. <br>2. Host API limitations: if workbook host prevents programmatic addition due to trust settings or macro policies, record user-facing error UTIL_PQ_HOST_POLICY_BLOCK and emit pq_inject.failure with remediation guidance. <br>3. Large formula (>configured limit): if formula exceeds safe buffer size for host API, persist artifact and call a helper (signed XLAM or worker) to split or write via supported host channels; emit pq_inject.degraded and include reason. <br>4. Partial failure after workbook change: if workbook query is added but post-processing (e.g., connection creation) fails, produce reversible plan and persist rollback artifact via Rollback_Query_Insert; emit pq_inject.partial_failure with beforeChecksum and afterChecksum. <br><strong>Observability & audits:</strong> pq_inject.attempt(correlationId, operatorId, templateId?, paramsHash, requestedName) pq_inject.completed(correlationId, operatorId, queryName, queryId, artifactChecksum, mChecksum, durationMs) pq_inject.failure(correlationId, operatorId, errorCode, diagnosticsRef). <br><strong>Examples & narratives:</strong><br>1. Admin injects <code>Sales_Cleansed</code> template with parameter region=EMEA into workbook; Add_Query_From_M computes safe name <code>Sales_Cleansed</code> (no collision), persists artifact, adds query inline, creates modelConnection as requested, and returns <code>{success:true, queryName:&quot;Sales_Cleansed&quot;, connectionId:&quot;conn-123&quot;, artifactChecksum:&quot;sha256:...&quot;}.</code> <br>2. User attempts to inject <code>Query1</code> but host macro policy blocks programmatic workbook modifications; function returns failure with guidance and a persisted artifact the operator can import manually. <br><strong>Tests & CI vectors:</strong> unit test for name collision resolution, simulated host API exceptions tests, golden mChecksum vectors for parameterization parity, integration tests verifying persisted artifact equals injected query content. </td></tr><tr><td data-label="PQ_Injector — Per-function Expert Technical Breakdown"> <strong><code>Parameterize_Template(templateText, params, paramSchema=null, correlationId=null)</code></strong><br><strong>Purpose & contract:</strong> apply a template's parameter map to canonical M template text to produce final M formula. Pure function when used with in-memory strings; emits audit pq_preview.start/complete when used in preview/injection flows. Returns <code>{mText, paramsValidated, paramsHash, paramizationLog}</code>. <br><strong>Behavior & steps:</strong><br>1. Load template parameter descriptors (type, default, allowedValues, secureFlag). <br>2. Validate <code>params</code> against <code>paramSchema</code>; coerce types where safe (e.g., numeric strings -> number), record coercions in paramizationLog. Reject mismatches with ErrorCode=PQ_PARAM_VALIDATE_FAIL and include field-level diagnostics. <br>3. For secure parameters (secureFlag=true), redact values in audit and store only parameterFingerprint in evidence store; the produced <code>mText</code> should contain placeholders referencing external secure handles rather than literal secrets. <br>4. Parameter substitution approaches supported: simple token substitution, M-aware substitution (inserting typed literal expressions), and function-parameter injection (wrapping template in let-block with parameter assignments). Choose M-aware substitution by default to preserve typing and quoting correctness. <br>5. Normalize line endings and canonical whitespace; compute <code>paramsHash</code> and <code>mChecksum</code> over normalized bytes. <br>6. Return final <code>mText</code> and diagnostics including any defaulted parameters and coercions. <br><strong>Tie-breakers & determinism:</strong> when templates include nondeterministic placeholders (e.g., <code>#RANDOM</code> for previews), Parameterize_Template must refuse such placeholders unless a DeterministicRNG seed is provided; when seed is provided, replace using DeterministicRNG to ensure reproducibility. <br><strong>Edge cases:</strong><br>1. Parameter-dependent template branches: templates that include conditional sections must be evaluated under a safe preview interpreter or validated syntactically to ensure substitution yields valid M. If safety cannot be proven syntactically, schedule worker-side preview execution with redacted external calls. <br>2. Large binary parameter values: refuse embedding and use external artifact references instead. <br><strong>Observability:</strong> pq_preview.start(correlationId, templateId, paramsHash), pq_preview.complete(correlationId, mChecksum, durationMs). <br><strong>Examples:</strong> substituting <code>dateFrom</code> and <code>dateTo</code> into a template using typed M expressions so that <code>#date(2023,1,1)</code> appears rather than a string literal. </td></tr><tr><td data-label="PQ_Injector — Per-function Expert Technical Breakdown"> <strong><code>Compute_mChecksum(mText, algorithm=&quot;sha256&quot;, normalizationRules=null)</code></strong><br><strong>Purpose & contract:</strong> compute deterministic checksum for a normalized M formula to enable artifact parity checks and audit linkage. Must be stable across supported hosts and languages. Returns <code>{mChecksum, normalizedBytes, hashAlgorithm, durationMs}</code>. <br><strong>Normalization rules (must/shall):</strong><br>1. Normalize line endings to LF (\\n). <br>2. Trim trailing whitespace on lines. <br>3. Collapse multiple consecutive blank lines to a single blank line unless template explicitly marks significant blank lines via canonical marker. <br>4. Preserve M token semantics; do not strip insignificant parentheses or alter token case in identifiers unless template metadata requires canonical casing. <br>5. UTF-8 encode with NFC normalization. <br>6. Optionally canonicalize comments: either preserve comments in normalized bytes for templates where comments are part of legal audit trail, or strip them if configured in template manifest. <br><strong>Edge behaviors:</strong> Crypto algorithm choices limited to supported set; default sha256. When algorithm changes, compute and persist migration manifest and ensure CI golden vectors are updated. <br><strong>Audits & observability:</strong> pq_mchecksum.computed(correlationId, templateId?, mChecksum, normalizedSize). </td></tr><tr><td data-label="PQ_Injector — Per-function Expert Technical Breakdown"> <strong><code>Persist_M_Artifact(artifactPayload, targetPath, atomicOptions={}, maxAttempts=3)</code></strong><br><strong>Purpose & contract:</strong> persist canonical JSON artifact describing the M query into durable storage using atomic primitives. Artifact contains metadata: name, mText, mChecksum, paramsHash, templateVersion, producerVersion, correlationId, timestamp. Return <code>{success, artifactPath, artifactChecksum, attempts, durationMs}</code>. <br><strong>Behavior & steps:</strong><br>1. Validate artifact schema. <br>2. Serialize artifact to canonical JSON with stable key order and UTF-8 NFC encoding. <br>3. Compute payload checksum (sha256). <br>4. Write using AtomicWrite with fsyncFile and fsyncParent semantics where available. <br>5. On transient errors use Retry wrapper; on ENOSPC or EPERM emit actionable audits and recovery suggestions. <br>6. Post-write verification: reopen and verify checksum equals computed payload checksum; on mismatch, attempt repair with AtomicWriteRepair or escalate. <br><strong>Cross-platform fallbacks:</strong> on host FS with weak rename semantics, record util.atomic_write.degraded audit and follow staged fallback (write artifact to local staging area and emit pq_export.degraded). <br><strong>Observability & audits:</strong> pq_export.attempt(correlationId, targetPath, payloadHash) pq_export.completed(correlationId, targetPath, artifactChecksum, durationMs). </td></tr><tr><td data-label="PQ_Injector — Per-function Expert Technical Breakdown"> <strong><code>Create_Connection(connectionName, connectionType, connectionOptions, credentialHandle=null, operatorId=null)</code></strong><br><strong>Purpose & contract:</strong> create a host connection object corresponding to an injected query with safe handling of credentials, refresh options, privacy levels, and optionally model mapping. Returns <code>{success, connectionId, artifactChecksum, diagnosticsRef}</code>. <br><strong>Parameters & constraints:</strong> <code>connectionType</code> ∈ {OLEDB, ODBC, Web, File, DataModel}; <code>connectionOptions</code> include loadBehavior (connectionOnly / loadToSheet / loadToModel), privacy settings, refreshPolicy, isHidden, enableBackgroundRefresh. <code>credentialHandle</code> is a secure reference. <br><strong>Behavior & steps:</strong><br>1. Validate that the host supports requested connectionType and options; if not supported, return PQ_CONN_UNSUPPORTED and provide remediation. <br>2. Create connection object via host APIs, pass credentialHandle rather than raw secrets. For model connections, ensure the semantic model is accessible and write mapping metadata into the artifact. <br>3. Persist connection metadata via Persist_M_Artifact or jobDescriptor store with correlationId to ensure reproducibility. <br>4. If creation fails after artifact persisted, record reversible plan and roll back if operator requests. <br><strong>Security & credential handling:</strong> do not log raw credentials; store only credentialHandle or masked fingerprint in audit rows. Credentials lifecycle must follow host credential manager APIs. <br><strong>Audits:</strong> pq_connection.created(correlationId, connectionId, connectionType, connectionOptionsFingerprint) pq_connection.failure(correlationId, errorCode, diagnosticsRef). </td></tr><tr><td data-label="PQ_Injector — Per-function Expert Technical Breakdown"> <strong><code>EnsureQueryNameUnique(requestedName, workbookContext, conflictPolicy=&quot;auto-suffix&quot;)</code></strong><br><strong>Purpose & contract:</strong> compute safe canonical query name under workbook constraints and conflict policy. Return <code>{finalName, changed, originalName, collisionResolutionLog}</code>. <br><strong>Behavior & rules:</strong><br>1. Normalize requestedName using canonical rules: strip control characters, trim, collapse repeated whitespace, limit to host-supported length, strip unsupported characters depending on host. <br>2. If normalized name collides with workbook query or table names, apply <code>conflictPolicy</code>:<br>- <code>fail</code> -> return error PQ_NAME_COLLISION.<br>- <code>auto-suffix</code> -> append <code> (n)</code> deterministic suffix where n is smallest integer producing no collision; suffix calculation must be deterministic and stable across runs; use deterministic ordering from existing names list sorted case-insensitively. <br>- <code>overwrite</code> -> only allowed if operator explicitly authorized and existing query is unmodified or matches artifactChecksum; emit audit pq_inject.overwrite_authorized. <br>3. Document mapping in audit. <br><strong>Edge cases:</strong> workbook with case-insensitive names but case-preserving host; ensure detection uses host semantics. </td></tr><tr><td data-label="PQ_Injector — Per-function Expert Technical Breakdown"> <strong><code>Validate_M_Syntax(mText, validationMode=&quot;lightweight&quot;, correlationId=null)</code></strong><br><strong>Purpose & contract:</strong> provide host-aware, deterministic syntactic validation for M text. Modes:<br>- <code>lightweight</code>: static token-level validation without executing connectors.<br>- <code>host-parse</code>: call host parser where available (non-executing) to validate parse tree.<br>- <code>preview-exec</code>: run in sandboxed preview with redacted connectors (requires worker or trusted host). <br><strong>Behavior & safeguards:</strong><br>1. For lightweight mode, run tokenizer-based checks (balanced brackets, basic token validity, prohibited host-specific extensions). <br>2. For host-parse, call host parser API where available and capture parse tree and minor warnings. Host parse must not resolve external connectors. <br>3. For preview-exec, run M in isolated host with timeouts and resource caps; redact external credentials and substitute deterministic test connectors; record diagnostics and truncated results for preview. <br>4. On syntax errors, return structured diagnostics pointing to line/column and include suggested fixes where unambiguous. <br><strong>Audits & telemetry:</strong> pq_validation.start/complete with validationMode and diagnosticsRef. </td></tr><tr><td data-label="PQ_Injector — Per-function Expert Technical Breakdown"> <strong><code>Preview_M(templateId, params, sampleLimit=50, deterministicSeed=null, correlationId=null)</code></strong><br><strong>Purpose & contract:</strong> produce a safe, deterministic preview of a parameterized template suitable for operator review. Returns <code>{previewRows, previewSchema, sampleFingerprint, mChecksum, diagnosticsRef}</code>. <br><strong>Behavior & steps:</strong><br>1. Parameterize_Template to produce mText; compute mChecksum. <br>2. If deterministicSeed provided, use DeterministicRNG to select sample rows for any sampling operations inside the template; otherwise derive seed from correlationId to enable reproducibility. <br>3. Execute the parameterized M in a sandboxed preview runtime with connectors disabled or stubbed unless operator explicitly requests live connector preview; ensure execution timeouts and memory caps. <br>4. Redact PII in previewRows unless operator explicitly authorizes non-redacted preview and that action is auditable. <br>5. Return sample up to sampleLimit rows, previewSchema, and sampleFingerprint computed from DeterministicRNG state and preview data checksum. <br><strong>Edge behaviors & governance:</strong> live connector preview requires explicit operator consent and must be recorded with additional audits and possibly two-person approval if regulated data may be exposed. <br><strong>Audits:</strong> pq_preview.start/complete with sampleFingerprint and evidenceRef for full preview payload. </td></tr><tr><td data-label="PQ_Injector — Per-function Expert Technical Breakdown"> <strong><code>Get_Query_Diagnostics(queryNameOrId, correlationId=null)</code></strong><br><strong>Purpose & contract:</strong> collect diagnostics for a given query: last refresh time, last refresh duration, last error trace, provider chain, dependency graph, and mChecksum recorded at injection time. Returns diagnostics object and emits pq_diagnostics.collected audit. <br><strong>Behavior & steps:</strong><br>1. Query host runtime for refresh history and last error traces; normalize timestamps to UTC and redact sensitive paths. <br>2. Compute dependency graph by analyzing query expression and referencing host's query/link metadata. <br>3. If query formula differs from persisted artifact <code>mChecksum</code>, mark as <code>divergent</code> and include both checksums in diagnostics. <br>4. Provide actionable remediation hints (e.g., refresh provider credentials, re-inject missing referenced query). <br><strong>Audits:</strong> pq_diagnostics.collected(correlationId, queryId, mChecksum, divergentFlag). </td></tr><tr><td data-label="PQ_Injector — Per-function Expert Technical Breakdown"> <strong><code>Export_M_Artifact(queryNameOrId, destination, atomicExportOptions={}, includeDiagnostics=true)</code></strong><br><strong>Purpose & contract:</strong> export a persisted M artifact and associated diagnostics to a destination (local path, network share, artifact store). Uses atomic export policy to avoid partial writes. Returns <code>{success, destinationUri, artifactChecksum, attempts}</code>. <br><strong>Behavior & steps:</strong><br>1. Resolve artifact from persisted local artifacts store using queryId and artifact checksum. <br>2. Compose export payload including artifact JSON, optional diagnostics, export manifest with correlationId and timestamp. <br>3. Use Persist_M_Artifact / AtomicWrite to write to destination; on network paths, follow staged fallback semantics and emit pq_export.degraded if atomic rename cannot be guaranteed. <br>4. Emit pq_export.completed with artifact checksum and destinationUri. <br><strong>Edge cases & recovery:</strong> handle ENOSPC, permission errors, partial remote writes; persist forensic_manifest and emit pq_export.failure with remediation runbook. </td></tr><tr><td data-label="PQ_Injector — Per-function Expert Technical Breakdown"> <strong><code>Rollback_Query_Insert(queryName, queryId, artifactPath, operatorId=null)</code></strong><br><strong>Purpose & contract:</strong> safely revert a previously applied injection when operator requests rollback or when partial-failure requires revert. Return <code>{success, reverted, reason, durationMs, diagnosticsRef}</code>. <br><strong>Behavior & steps:</strong><br>1. Validate that artifactPath or persisted rollback plan exists. <br>2. If original query existed before injection, restore previous formula and metadata from persisted beforeChecksum snapshot. <br>3. If injection created new objects, remove them in a reversible manner and persist rollback artifact via AtomicWrite. <br>4. Emit pq_query.removed or pq_query.updated audit rows with beforeChecksum and afterChecksum. <br><strong>Safeguards:</strong> require operator confirmation for destructive rollback; for regulated workflows, enforce two-person approval on rollbacks affecting regulated datasets. </td></tr><tr><td data-label="PQ_Injector — Per-function Expert Technical Breakdown"> <strong><code>Inspect_Queries(workbookContext, filter=null)</code></strong><br><strong>Purpose & contract:</strong> enumerate query metadata in workbook context, compute per-query fingerprints, and reconcile with persisted artifacts. Return list <code>[ {queryName, queryId, mChecksum, artifactMatch, lastModified, modifiedBy} ... ]</code>. <br><strong>Behavior & steps:</strong><br>1. Pull host query list and map to persisted artifact indexes by name and mChecksum. <br>2. Detect divergences (query formula different from artifact mChecksum) and mark <code>artifactMatch=false</code> with diagnostics. <br>3. Provide summary metrics: count_injected, count_divergent, count_unpersisted. <br>4. Emit pq_library.access or pq_inspect.completed audit. </td></tr><tr><td data-label="PQ_Injector — Per-function Expert Technical Breakdown"> <strong>Observability, Telemetry & Error Catalog (concepts & mapping)</strong><br><strong>Audit schema for PQ_Injector:</strong> timestamp, correlationId, operatorId (optional), module=PQ_Injector, procedure, paramsHash, mChecksum (optional), artifactChecksum (optional), resultStatus, evidenceRef (optional), durationMs, hostPlatformVersion, hostPolicyFlags. Evidence storage encrypted and access-controlled. <br><strong>Representative ErrorCodes:</strong> PQ_INJECT_BLOCKED_BY_POLICY, PQ_NAME_COLLISION, PQ_PARAM_VALIDATE_FAIL, PQ_HOST_API_ERROR, PQ_CONN_UNSUPPORTED, PQ_EXPORT_ENOSPC, PQ_MCHECKSUM_MISMATCH, PQ_PREVIEW_FORBIDDEN, PQ_EXPORT_DEGRADED, PQ_INVALID_CREDENTIAL_HANDLE. Each error code maps to operator remediation suggestions via SafeErrorToUser mapping. <br><strong>Key metrics (local buffered):</strong> pq_inject.latency_ms, pq_inject.success_rate, pq_preview.latency_ms, pq_export.latency_ms, pq_mchecksum.count, pq_diagnostics.collection_count. Metrics are locally buffered and uploaded by CORE_Telemetry in audited batches. </td></tr><tr><td data-label="PQ_Injector — Per-function Expert Technical Breakdown"> <strong>Testing matrix, property tests, and cross-platform golden governance</strong><br><strong>Unit tests (must include):</strong><br>1. Parameterize_Template unit vectors for various types (strings, numbers, dates, lists, records) ensuring typed M insertion and escaping correctness. <br>2. Compute_mChecksum goldens: same M with different line ending and comment variants produce expected normalized checksum. <br>3. Add_Query_From_M host API mock tests covering success, transient host exceptions (retry), hard failures (policy blocks), and partial-failure recovery. <br>4. EnsureQueryNameUnique tests with case-insensitive vs case-preserving host semantics. <br><strong>Integration tests:</strong><br>1. End-to-end inject: parameterize -> persist artifact -> host add query -> create connection -> export artifact -> diagnostics verify artifact parity. <br>2. Preview sandbox tests with deterministic seeding (DeterministicRNG) to reproduce preview rows; verify serialization of RNG state. <br>3. Export degraded path tests on simulated SMB/NFS filesystem with weak rename semantics. <br><strong>Property tests:</strong><br>1. Idempotency property for Add_Query_From_M under retries; ensure no duplicate queries created when retries occur. <br>2. Parameterization property: Parameterize_Template(paramsA) + Parameterize_Template(paramsB) where paramsA==paramsB must produce identical mChecksum. <br><strong>CI golden gating:</strong> golden vectors for mChecksum and parameterization parity across supported host platforms and languages (VBA/COM helper, .NET add-in, Python worker). Static analyzer to flag direct workbook writes in OnLoad. </td></tr><tr><td data-label="PQ_Injector — Per-function Expert Technical Breakdown"> <strong>Developer guidance, allowed & forbidden patterns (explicit)</strong><br><strong>Required usage patterns:</strong><br>1. Always persist artifact via Persist_M_Artifact before final Add_Query_From_M unless operator explicitly requests ephemeral injection and the action is recorded with explicit audit. <br>2. Always compute and record mChecksum and paramsHash for every injection action. <br>3. Use EnsureQueryNameUnique to normalize and avoid accidental overwrites. <br>4. For regulated templates, require manifest signature verification prior to injection. <br>5. When performing preview or sampling, seed DeterministicRNG from correlationId for reproducibility and persist RNG state for forensic replay. <br><strong>Forbidden practices:</strong><br>1. Do not embed clear-text credentials into M templates or persisted artifacts. <br>2. Do not perform long-running writes or network exports on UI thread. <br>3. Do not bypass audit emission for injection or export flows. <br>4. Do not assume host rename semantics on network filesystems—use atomic write fallbacks provided by CORE_Utilities. </td></tr><tr><td data-label="PQ_Injector — Per-function Expert Technical Breakdown"> <strong>Operational runbooks & incident playbooks (executable steps)</strong><br><strong>Injection blocked by host policy (operator cannot programmatically add query):</strong><br>1. pq_inject.failure with PQ_INJECT_BLOCKED_BY_POLICY emitted containing policyFlags and hostPolicyName. <br>2. Persist artifact via Persist_M_Artifact to a user-visible staging area and emit pq_export.completed with staging URI. <br>3. Provide operator with manual import steps and attach artifactChecksum for verification. <br>4. Log host diagnostic info and open ticket if automatic injection essential. <br><strong>mChecksum mismatch detected after injection (divergent query):</strong><br>1. Inspect pq_mchecksum and pq_inject audits; fetch persisted artifact. <br>2. Use Inspect_Queries to compute divergence details. <br>3. If divergence is unintended, run Rollback_Query_Insert using persisted beforeSnapshot and re-inject artifact; record all steps in audit. <br><strong>Export ENOSPC runbook:</strong><br>1. Collect pq_export.failure with details; check filesystem free space. <br>2. Use atomic export staging option to write to local volume on same mount; recompute artifactChecksum and compare. <br>3. If needed, escalate to infra with forensic_manifest and audit_tail rows. </td></tr><tr><td data-label="PQ_Injector — Per-function Expert Technical Breakdown"> <strong>Conceptual Power Query (M) patterns — how PQ_Injector principles map to templates and runtimes</strong><br><strong>Context:</strong> M is declarative and host-dependent. PQ_Injector cannot and should not attempt to control all runtime behaviors; instead it orchestrates injection, validation, artifact persistence, and host connection creation to produce auditable and reproducible artifacts. <br><strong>Patterns & recommendations:</strong><br>1. <strong>Parameterize using typed M expressions:</strong> always insert parameters as typed M expressions (e.g., <code>#date(...)</code>, <code>#duration(...)</code>) rather than interpolated strings to avoid locale and parsing differences. <br>2. <strong>Preview via sandboxed execution:</strong> do not run template previews against live production connectors without explicit consent. Use deterministic seeds and persist RNG state for reproducible previews. <br>3. <strong>Persist authoritative artifact:</strong> for any template used in production or shared, persist canonical artifact and use <code>mChecksum</code> as the single source of truth for audits and rollbacks. <br>4. <strong>Avoid embedding secrets:</strong> use credential handles or connection references passed in via connectionOptions rather than literal secrets in M text. <br>5. <strong>Model-backed loads:</strong> for reports requiring semantic model integration, prefer Create_Connection_Model with explicit mapping metadata persisted alongside artifact. </td></tr><tr><td data-label="PQ_Injector — Per-function Expert Technical Breakdown"> <strong>Conceptual DAX patterns — interactions between PQ_Injector and semantic models</strong><br><strong>Context:</strong> DAX is read-time and cannot perform side-effects. PQ_Injector must therefore perform deterministic allocations and rounding in ETL and persist results consumable by DAX. <br><strong>Patterns & recommendations:</strong><br>1. <strong>Push calculated columns requiring authoritative rounding to ETL:</strong> use SafeRound and SafeRoundResiduals during M processing or worker-side transforms and persist integer cents when necessary so DAX measures only aggregate. <br>2. <strong>Persist RunMetadata table:</strong> PQ_Injector must optionally write a RunMetadata artifact (correlationId, artifactChecksum, mChecksum, templateVersion, runTs) into the data model to allow DAX and reports to surface provenance. <br>3. <strong>Deterministic sampling for model-level testing:</strong> compute deterministic sample keys in ETL (e.g., hash of primary key + correlationSalt) and persist seed metadata for replay. <br>4. <strong>Model checksums & reconciliation:</strong> after injection and ETL, PQ_Injector should produce a reconciliation artifact (datasetChecksum) that DAX measures can compare to detect drift; DAX measures surface dataset health but do not perform reconciliation. </td></tr><tr><td data-label="PQ_Injector — Per-function Expert Technical Breakdown"> <strong>Forensic artifacts, evidence paths & recommended retention (concise):</strong><br><strong>Minimum artifacts to retain per injection:</strong> artifact JSON with <code>mText</code>, <code>mChecksum</code>, <code>paramsHash</code>, templateVersion, persisted into evidence store; serialized RNG state for previews and sampling; audit rows (<code>pq_inject.*</code>, <code>pq_preview.*</code>, <code>pq_export.*</code>); host diagnostics and query diagnostics; rollback plans when created. <br><strong>Retention policy:</strong> hot evidence store 30 days, warm archive 7 years for regulated datasets, cold per regulation. Evidence refs in audits must point to encrypted artifacts; PII must never appear in top-level audit rows. </td></tr><tr><td data-label="PQ_Injector — Per-function Expert Technical Breakdown"> <strong>Acceptance checklist before module release (detailed):</strong><br>1. Owners recorded in OWNERS.md; approvers designated for template governance. <br>2. Public API documented; backward-compatible versioning. <br>3. Golden vectors for mChecksum and Parameterize_Template parity across host implementations. <br>4. AtomicWrite integration tests and degraded-path tests for network filesystems. <br>5. Static checks preventing UI-thread blocking writes. <br>6. Audit hook coverage validated by test harness emitting expected audit rows. <br>7. Release manifest signed and PQ_Injector artifacts included in modAudit rotation. <br><strong>Blocking conditions:</strong> missing audit emissions on inject/export flows, golden vector failures, or forbidden-API static check failures. </td></tr><tr><td data-label="PQ_Injector — Per-function Expert Technical Breakdown"> <strong>Test plan highlights & representative tests (explicit)</strong><br><strong>Unit tests:</strong><br>1. Parameterization: vector tests for typed insertion, escaping, defaulting, and secure parameter handling. <br>2. mChecksum: parity tests for varying whitespace and comment policies. <br>3. Name resolution: collision resolution under case-insensitive and case-preserving hosts. <br>4. Host API error mapping and Retry correctness. <br><strong>Integration tests:</strong><br>1. End-to-end inject -> persist -> host add -> create connection -> export -> diagnostics verification. <br>2. Preview reproducibility using DeterministicRNG and persisted RNG state. <br><strong>Property tests:</strong><br>1. Idempotency of Add_Query_From_M under repeated identical requests. <br>2. Parameterization mapping stability across template versions where compatible. </td></tr><tr><td data-label="PQ_Injector — Per-function Expert Technical Breakdown"> <strong>Operator quick commands & examples (prescriptive):</strong><br>1. <code>pq inject --template &lt;id&gt; --params &#x27;{&quot;dateFrom&quot;:&quot;2025-01-01&quot;}&#x27; --mode connectionOnly --correlation r-YYYYMMDD-xxx</code> — persist artifact, create connection only, emit pq_inject.completed with artifactChecksum. <br>2. <code>pq preview --template &lt;id&gt; --params ... --sample 50 --seed &lt;seed&gt;</code> — create deterministic preview and store RNG state. <br>3. <code>pq export --query &lt;name&gt; --dest \\share\exports\</code> — atomic export; on ENOSPC follow staging fallback. <br>4. <code>pq rollback --query &lt;name&gt; --correlation &lt;id&gt;</code> — revert an injection using persisted snapshot (requires operator confirmation). </td></tr><tr><td data-label="PQ_Injector — Per-function Expert Technical Breakdown"> <strong>Common failure modes & mitigations (expanded)</strong><br><strong>Failure mode: host policy blocks injection</strong><br>1. Cause: macro or trust settings prevent programmatic modifications. <br>2. Mitigation: persist artifact and provide operator manual import steps; request infra to sign XLAM or update host policy when enterprise-managed. <br><strong>Failure mode: injected query diverges from persisted artifact</strong><br>1. Cause: operator edited query manually after injection or host add path mutated formula. <br>2. Mitigation: Inspect_Queries to detect divergence; if unintended, Rollback_Query_Insert and re-inject persisted artifact; update user guidance to avoid manual edits on injected artifacts. <br><strong>Failure mode: mChecksum mismatch between persisted artifact and injected formula</strong><br>1. Cause: normalization differences or host API modified whitespace/comments. <br>2. Mitigation: ensure Compute_mChecksum uses host-compatible normalization rules; preserve comments where important; update golden vectors. </td></tr><tr><td data-label="PQ_Injector — Per-function Expert Technical Breakdown"> <strong>Governance checklists & PR requirements (explicit)</strong><br>1. PR must include unit tests for new behavior, golden vectors for mChecksum changes, and audit emission validation. <br>2. Changes to parameterization semantics or mChecksum normalization require owner approval and migration manifest. <br>3. Any change to persistence semantics or AtomicWrite usage must include cross-platform tests and SRE sign-off. <br>4. Release manifest update required for production changes affecting injection semantics or regulated templates. </td></tr><tr><td data-label="PQ_Injector — Per-function Expert Technical Breakdown"> <strong>Extremely detailed narratives & examples (selected)</strong><br><strong>Scenario 1 — Operator injects regulated template requiring high numeric fidelity:</strong><br>1. Operator selects <code>Revenue_Schedule</code> template, marked <code>requiresHighPrecision=true</code>. PQ_Ribbon composes request with correlationId r-20260117-abc and chosen parameters; PQ_Injector emits pq_preview.start and computes deterministic preview using seeded DeterministicRNG. <br>2. Parameterize_Template applies parameters as typed M expressions and computes mChecksum. Because template requires high precision, PQ_Injector schedules worker-side SafeRound aggregation to produce authoritative numeric columns and persists the resulting artifact via Persist_M_Artifact. <br>3. Add_Query_From_M is invoked to add query into workbook; because persistence already happened, the injected in-memory formula is guaranteed to match persisted artifact. PQ_Injector creates model connection mapping columns as integer cents where required; pq_inject.completed includes artifactChecksum and model mapping metadata. <br>4. Forensic replay: evidenceRef points to persisted RNG state and artifact JSON enabling exact reproduction of the injected query and numeric transforms. <br><strong>Scenario 2 — Template injection in a locked-down tenant where programmatic injection is blocked:</strong><br>1. PQ_Injector attempts Add_Query_From_M but host returns macro-policy error. PQ_Injector persists artifact to staging path and emits pq_inject.failure with PQ_INJECT_BLOCKED_BY_POLICY. <br>2. Operator is shown step-by-step manual import instructions with artifact checksum to verify integrity. Admin later updates host trust policy and automation proceeds; audit chain links the staging artifact and final injection. <br><strong>Scenario 3 — Deterministic preview for sampling-sensitive template:</strong><br>1. Template includes a sample function with seed parameter. Operator runs preview with correlationId r-...; PQ_Injector seeds DeterministicRNG using correlationId and <code>preview-v1</code> salt and records serialized RNG state in evidence store. <br>2. Preview rows are produced and stored; operator reviews and accepts injection. Later complaint about nondeterministic preview is resolved by replaying preview using the serialized RNG state and producing identical sample rows. </td></tr><tr><td data-label="PQ_Injector — Per-function Expert Technical Breakdown"> <strong>Appendices: forensic artifacts, evidence paths & recommended retention (expanded)</strong><br><strong>Minimum forensic artifacts per injection:</strong><br>1. persisted artifact JSON with <code>mText</code>, <code>mChecksum</code>, <code>paramsHash</code>, <code>templateVersion</code>, <code>producerVersion</code>, <code>correlationId</code>. <br>2. audit_tail rows for correlationId. <br>3. serialized RNG state for preview runs. <br>4. host diagnostics & query diagnostics (last refresh history). <br>5. rollback plans if created. <br><strong>Evidence storage & access:</strong> hot evidence store for 30 days; warm archive 7 years for regulated artifacts; evidenceRef in audits must be an encrypted pointer with ACLs. </td></tr><tr><td data-label="PQ_Injector — Per-function Expert Technical Breakdown"> <strong>Appendix A — Example audit row schema (descriptive):</strong><br><strong>Fields required for PQ_Injector audits:</strong> timestamp, correlationId, module=PQ_Injector, procedure, operatorId (optional), paramsHash, mChecksum (optional), artifactChecksum (optional), evidenceRef (optional), hostPlatformVersion, metadata object with durationMs, attempts, queryId, connectionId. <br><strong>Policy note:</strong> do not include raw PII or credentials in top-level audit rows; store sanitized parameters in evidence store. </td></tr><tr><td data-label="PQ_Injector — Per-function Expert Technical Breakdown"> <strong>Appendix B — Common failure modes & mitigations (expanded)</strong><br><strong>Failure: partial injection due to host crash</strong><br>1. Cause: host crash after query persisted but before connection created. <br>2. Mitigation: detect via pq_inject.partial_failure, persist rollback plan and allow operator to resume insertion; worker reconciles persisted artifact and host state on next run. <br><strong>Failure: parameter coercion differences across platforms</strong><br>1. Cause: different language runtime coercion (e.g., decimal vs double) producing different serialized M expressions. <br>2. Mitigation: Parameterize_Template must use explicit typed M expressions and cross-platform golden tests; flag templates with potential platform-sensitive coercion. </td></tr><tr><td data-label="PQ_Injector — Per-function Expert Technical Breakdown"> <strong>Appendix C — PQ & DAX short checklists for template authors and report builders</strong><br><strong>PQ Template author checklist:</strong><br>1. Provide parameter schema with types and default values. <br>2. Mark templates requiring high numeric fidelity as <code>requiresHighPrecision</code>. <br>3. Avoid embedding secrets; use secure parameter handles. <br>4. Include <code>mChecksum</code> and manifest with signature for regulated templates. <br><strong>DAX/report builder checklist:</strong><br>1. Consume RunMetadata table for provenance and artifactChecksum. <br>2. Avoid performing allocations or residual rounding in DAX; perform in ETL and persist final integers. <br>3. Use persisted hash keys for deterministic sampling if necessary. </td></tr><tr><td data-label="PQ_Injector — Per-function Expert Technical Breakdown"> <strong>Final mandatory constraints (firm):</strong><br>1. Always persist authoritative artifacts for injected queries that will be consumed by other processes. <br>2. Always compute and record mChecksum and paramsHash and link them in audits. <br>3. Do not embed credentials in persisted artifacts; use secure handles. <br>4. For regulated or PII-touching workflows, enforce two-person approval for automatic injections that modify production models. <br>5. All injections and exports must emit audit rows and include evidenceRef when full payloads are necessary for forensics. </td></tr><tr><td data-label="PQ_Injector — Per-function Expert Technical Breakdown"> <strong>Checked:</strong> Ten-pass internal review for internal consistency, audit coverage, deterministic chain from operator action -> parameterization -> mChecksum -> persisted artifact -> workbook injection -> connection creation -> export, and cross-mapping to PQ preview and DAX model patterns. </td></tr></tbody></table></div><div class="row-count">Rows: 35</div></div><div class="table-caption" id="Table2" data-table="Docu_0180_02" style="margin-top:2mm;margin-left:3mm;"><strong>Table 2</strong></div>
<div class="table-wrapper" data-table-id="table-2"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by PQ_Diagnostics — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">PQ_Diagnostics — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="PQ_Diagnostics — Per-function Expert Technical Breakdown"> <strong>Module-level metadata (contract & overview):</strong><br><strong>Owner:</strong> TEAM_PQ_DIAGNOSTICS (primary, secondary, and on-call rotation recorded in OWNERS.md and release manifests).<br><strong>Public API (surface):</strong> <code>CollectQueryDiagnostics</code>, <code>CaptureRefreshTrace</code>, <code>CollectQueryPlan</code>, <code>CaptureProviderDiagnostics</code>, <code>CaptureConnectionFingerprint</code>, <code>ParseErrorTrace</code>, <code>AggregateRefreshPath</code>, <code>BuildDiagnosticReport</code>, <code>ExportDiagnostics</code>, <code>ReplayDiagnostics</code>, <code>CorrelateWithAudit</code>, <code>HealthCheck</code>, <code>SampleDiagnostics</code>, <code>PruneDiagnosticsRetention</code>, <code>InspectTempArtifacts</code>, <code>ScheduleDiagnosticsCapture</code>, <code>GetDiagnosticSummaries</code>, <code>ListEvidenceRefs</code>, <code>FetchEvidenceBundle</code>, <code>ResolvePlanHash</code>, <code>CompareReplayArtifacts</code>, <code>SignDiagnosticManifest</code>.<br><strong>Audits emitted:</strong> <code>pq_diagnostics.requested</code>, <code>pq_diagnostics.started</code>, <code>pq_diagnostics.step:&lt;stepName&gt;</code>, <code>pq_diagnostics.complete</code>, <code>pq_diagnostics.failure</code>, <code>pq_diagnostics.export.attempt</code>, <code>pq_diagnostics.export.completed</code>, <code>pq_diagnostics.replay.started</code>, <code>pq_diagnostics.replay.completed</code>, <code>pq_diagnostics.prune.completed</code>, <code>pq_diagnostics.health.ok</code>, <code>pq_diagnostics.health.failed</code>. Every audit row includes <code>correlationId</code>, <code>module=PQ_Diagnostics</code>, <code>procedure</code>, <code>paramsHash</code>, and <code>resultHash</code> when applicable; large payloads are referenced by <code>evidenceRef</code> and never embedded directly in audit rows.<br><strong>Purpose and intended use:</strong> provide deterministic, auditable, minimal-invasive diagnostics and replayable captures for Power Query (M) templates, preview flows, refresh runs, provider interactions, and injected queries. Serve support engineers, SRE, template authors, compliance teams, and incident responders by producing reproducible artifacts (plans, traces, provider diagnostics, sample data, error fingerprints), reductionist summaries for rapid triage, and signed manifests for regulatory evidence. The module is intentionally read-only in default flows; any action that would mutate workbook state requires explicit audited operator confirmation and a separate remediation flow.<br><strong>Non-goals / constraints:</strong> do not persist raw credentials or secrets in audit rows, do not attempt remediation within collect functions, do not perform provider-side debugging without recorded operator consent, do not depend on external telemetry during UI-thread operations, and avoid platform-specific behaviors in canonical artifacts (plan canonicalization compensates for host differences). </td></tr><tr><td data-label="PQ_Diagnostics — Per-function Expert Technical Breakdown"> <strong>Operational guarantees (invariants & SLOs):</strong><br>1. Deterministic reproduction: given a diagnostics evidence bundle and serialized deterministic RNG (where used), replays produce byte-for-byte equivalent artifacts where external provider responses were captured or stubbed. <br>2. Audit-anchored persistence: every persisted diagnostic artifact is referenced by at least one audit row containing <code>correlationId</code> and <code>artifactChecksum</code>. <br>3. Non-invasiveness: diagnostics collections are read-only and have no side-effects on workbook queries or cells; only explicit remediation flows may apply changes and must emit their own audits. <br>4. UI-safety: heavy captures (full plan, full trace, large sample exports) are scheduled to worker processes; UI-base calls execute lightweight sampling and metadata collection. <br>5. PII minimization and redaction: top-level audits include parameter hashes only; full diagnostic payloads are sanitized, encrypted at rest, and accessible only via controlled evidenceRef with access policies. <br>6. Atomic durability: persisted evidence and manifests use <code>AtomicWrite</code> semantics to prevent partial artifacts; in environments without atomic guarantees a staged manifest approach is used and <code>pq_diagnostics.export.degraded</code> is emitted. <br>7. Observability: all long-running steps emit start/complete audits and include duration metrics; local metric buffers exist for bulk uplink by CORE_Telemetry. <br><strong>Performance SLOs:</strong> median light-mode <code>CollectQueryDiagnostics</code> < 250ms; worker-mode full capture median < 8s (plus provider latency); replay default runtime target < 10 minutes; evidence export median < 500ms for local SSD, subject to target characteristics. <br><strong>CI / acceptance gates:</strong> deterministic sampling golden vectors, plan canonicalization parity tests across supported hosts, redaction enforcement tests, evidence encryption verification, atomic persistence tests, static analyzer checks preventing direct logging of credentials. </td></tr><tr><td data-label="PQ_Diagnostics — Per-function Expert Technical Breakdown"> <strong><code>CollectQueryDiagnostics(connectionId, queryId, includePlan=false, includeTrace=false, samplePercent=100, rng=null, timeoutMs=30000, mode=&quot;auto&quot;, retentionTag=&quot;hot&quot;)</code> — exhaustive technical breakdown</strong><br><strong>Purpose & contract:</strong> gather a curated diagnostics package for a query execution context suitable for support triage, forensic replay, and performance profiling. Must not mutate query state. Returns <code>{success, packageId, packageMeta, evidenceRef?}</code> where <code>evidenceRef</code> points to encrypted evidence bundle when payloads exceed audit size thresholds. <br><strong>Detailed parameters & semantics:</strong><br>1. <code>connectionId</code> — stable id referencing persisted connection metadata; used to compute <code>connectionFingerprint</code>. <br>2. <code>queryId</code> — canonical query identifier (workbook query name or job descriptor id); if not found return <code>PQ_DIAG_QUERY_NOT_FOUND</code>. <br>3. <code>includePlan</code> — request provider explain/plan export; may be <code>false</code> for providers that deny plan access. <br>4. <code>includeTrace</code> — request event-level mashup+provider trace; worker-mode capture recommended. <br>5. <code>samplePercent</code> — 0–100 int; when <100, deterministic sampling performed. <br>6. <code>rng</code> — optional <code>DeterministicRNG</code> instance; if omitted and deterministic behavior required, derive seed from <code>correlationId|queryId</code>. <br>7. <code>timeoutMs</code> — overall capture timeout; partial captures are returned if timeout reached. <br>8. <code>mode</code> — <code>&quot;ui&quot;|&quot;worker&quot;|&quot;auto&quot;</code>; <code>auto</code> escalates to worker when heavy captures requested and UI thread is unsuitable. <br>9. <code>retentionTag</code> — evidence retention classification (hot/warm/cold) controlling storage lifecycle. <br><strong>Preconditions & validations:</strong> verify <code>connectionId</code> exists and that the calling context has permission to collect (operatorId matches or delegated support role). For <code>includePlan</code>/<code>includeTrace</code> from UI <code>mode=&quot;auto&quot;</code> attempts to schedule worker capture; if scheduling unavailable return <code>UI_THREAD_FORBIDDEN</code>. Emit initial audit <code>pq_diagnostics.started</code> containing <code>paramsHash</code> to anchor the action. <br><strong>Collection pipeline (high-level):</strong><br>1. Resolve and canonicalize query descriptor: <code>{name, mChecksum, templateVersion, parameterHash, lastPreviewTs}</code>. <br>2. Build capture context: hostFlavor, PQEngineVersion, correlationId, operatorId (if present), jobDescriptor ref. <br>3. Determine sampling seed: use provided <code>rng.serialize()</code> or derive <code>DeterministicRNG(seed_source=correlationId|queryId|packageSalt)</code>; record <code>seedFingerprint</code>. <br>4. Collect light metadata immediately: timings for last preview/refresh, provider name, connectionFingerprint via <code>CaptureConnectionFingerprint</code>, memory/CPU host snapshot. <br>5. If <code>samplePercent</code> < 100 perform <code>SampleDiagnostics</code> deterministically selecting rows/pages/events. <br>6. If <code>includePlan</code> request <code>CollectQueryPlan</code> in worker mode if required; attach <code>planHash</code> or <code>planAvailable=false</code>. <br>7. If <code>includeTrace</code> request <code>CaptureRefreshTrace</code>, stream to evidence store chunked and compressed. <br>8. Parse any immediate errors via <code>ParseErrorTrace</code> and compute <code>errorFingerprints</code>. <br>9. Build <code>packageMeta</code> including artifact list, checksums, summary metrics, critical path hints, and reproduction steps; persist manifest via <code>AtomicWrite</code>. <br>10. Emit <code>pq_diagnostics.complete</code> with <code>packageId</code> and <code>artifactChecksum</code>. <br><strong>Output packageMeta schema (canonical):</strong> <code>{packageId, connectionFingerprint, mChecksum, planHash?, traceId?, sampleSeedFingerprint, artifacts:[{type, evidenceRef, checksum, sizeBytes, retentionTag}], summary:{timings:{totalMs, criticalPathMs}, providerErrors:[{code, count, sampleErrorFingerprint}], samplePreview:[rows], reproduction:{seed, correlationId, commands}}}</code>. <br><strong>Edge cases & failure modes:</strong><br>1. Unknown <code>queryId</code> -> <code>PQ_DIAG_QUERY_NOT_FOUND</code> audit and error result. <br>2. Provider denies <code>EXPLAIN</code> -> <code>planAvailable=false</code> with <code>providerReason</code> included. <br>3. Capture truncated by <code>maxEvents</code> or <code>timeout</code> -> <code>partial=true</code> with <code>partialReason</code>. <br>4. Evidence store quota exceeded -> emit <code>pq_diagnostics.step:evidence_store.ENOSPC</code> and persist partial metadata. <br><strong>Audits generated:</strong> <code>pq_diagnostics.started(correlationId, paramsHash)</code> <code>pq_diagnostics.step:sample_selected(correlationId, sampleSeedFingerprint)</code> <code>pq_diagnostics.step:plan_collected(correlationId, planHash|planAvailable=false)</code> <code>pq_diagnostics.step:trace_captured(correlationId, traceId, eventsCount, evidenceRef)</code> <code>pq_diagnostics.complete(correlationId, packageId, artifactChecksum, durationMs)</code>. <br><strong>Examples & narratives:</strong><br>1. Quick triage: support calls <code>CollectQueryDiagnostics(connSales, qOrdersPreview, includePlan=false, includeTrace=false, samplePercent=10)</code> which returns small deterministic sample and timing summary enabling immediate reproduction on a developer workstation. <br>2. Forensic path: after a nightly refresh failure, <code>CollectQueryDiagnostics(... includePlan=true, includeTrace=true, mode=&quot;worker&quot;)</code> creates a full evidence bundle; SRE runs <code>ReplayDiagnostics</code> in sandbox using captured provider responses and serialized RNG to prove root cause. </td></tr><tr><td data-label="PQ_Diagnostics — Per-function Expert Technical Breakdown"> <strong><code>CaptureRefreshTrace(connectionId, runId, startTs=null, endTs=null, eventFilter=null, maxEvents=100000, streamToEvidence=true, compress=true, redactPatterns=[...])</code> — exhaustive technical breakdown</strong><br><strong>Purpose & contract:</strong> ingest and persist a time-ordered stream of mashup engine and provider events for a single refresh run. The trace artifact is intended to support fine-grained performance analysis, failure reconstruction, and retry behavior analysis. <br><strong>Event taxonomy:</strong> captured events include <code>ProviderCallStart</code>, <code>ProviderCallEnd</code>, <code>ProviderResponse</code>, <code>CacheLookup</code>, <code>CacheHit</code>, <code>MashupStepStart</code>, <code>MashupStepEnd</code>, <code>RowPage</code>, <code>DataBuffer</code>, <code>ErrorEvent</code>, <code>RetryAttempt</code>, <code>NetworkTimeout</code>, <code>GCEvent</code>, <code>ResourceThrottle</code>. Each event stored includes <code>timestamp</code> (UTC ISO8601 ms), <code>sequenceNumber</code> (monotonic), <code>nodeId</code> (queryId/stepId), <code>eventType</code>, <code>durationMs</code> (when applicable), and <code>payloadFingerprint</code>. <br><strong>Capture semantics:</strong><br>1. Subscribe to engine event bus or provider diagnostics hooks; if direct subscription impossible, reconstruct events using available logs and provider call records. <br>2. Apply <code>eventFilter</code> (allowlist or blocklist) to keep trace focused. <br>3. Enforce <code>maxEvents</code> to bound size; when truncated produce <code>traceSummary</code> and <code>partial=true</code> with <code>droppedEventCount</code>. <br>4. Redact sensitive payload fields per <code>redactPatterns</code> and established redaction policy before any persistence. Redaction replaces values with deterministic fingerprints (e.g., <code>sha256(value)</code> masked) stored in evidence manifest. <br>5. Stream to evidence store chunked; if <code>compress=true</code> apply streaming compression (GZIP/DEFLATE) with chunk manifest containing chunk checksums for piecewise verification. <br>6. Compute derived aggregates on ingest: event counts by type, average latencies, retry histograms, top slow nodes, concurrency peaks. Include <code>criticalSegment</code> summary listing the top N slow nodes by exclusive/inclusive time. <br><strong>Trace summary fields:</strong> <code>traceId</code>, <code>eventsCaptured</code>, <code>durationMs</code>, <code>errorCount</code>, <code>retryCount</code>, <code>avgProviderLatencyMs</code>, <code>criticalSegment</code> (array), <code>droppedEventCount</code>, <code>evidenceRef</code> (if persisted). <br><strong>Edge conditions & provider behavior:</strong><br>1. Provider streaming responses (very high volume) -> capture first N pages and compute aggregate statistics; advise operator to increase <code>maxEvents</code> or use stratified sampling for deep analysis. <br>2. Provider denies capture hooks -> record <code>PROVIDER_TRACE_DENIED</code> and include provider capabilities metadata. <br><strong>Audit events:</strong> <code>pq_diagnostics.step:trace_captured(correlationId, traceId, eventsCaptured, evidenceRef)</code>. <br><strong>Narratives & examples:</strong><br>1. A trace reveals a repeating pattern where a provider returns 429 Too Many Requests followed by a series of retries that overlap with a heavy join step in the mashup engine; the <code>criticalSegment</code> points to the join as the concurrency bottleneck and the trace shows the exact timestamps of each retry enabling precise throttling diagnosis. <br>2. Compression and chunking allow the team to store a 1.5GB raw trace as 12 compressed chunks with manifest checksums and to replay only the relevant chunks for a focused investigation. </td></tr><tr><td data-label="PQ_Diagnostics — Per-function Expert Technical Breakdown"> <strong><code>CollectQueryPlan(queryText, parameters, provider, timeoutMs=15000, requireCanonical=true, fallbackInstrumentation=true)</code> — exhaustive technical breakdown</strong><br><strong>Purpose & contract:</strong> obtain an execution plan from the provider or reconstruct a canonical plan representing the operator ordering and cost hints. Output must be canonicalized for checksumming and cross-host parity. <br><strong>Plan acquisition strategies:</strong><br>1. Provider-native <code>EXPLAIN</code>/<code>PLAN</code> call for SQL-like providers. <br>2. Provider-specific plan APIs (some NoSQL or connector providers supply explain endpoints). <br>3. If provider lacks plan APIs and <code>fallbackInstrumentation=true</code>, run an instrumented execution in worker-mode capturing step operator sequence and cost estimates; produce heuristic plan. <br><strong>Canonicalization rules:</strong><br>1. Remove ephemeral data: object ids, timestamps, node handles, memory addresses. <br>2. Normalize numeric cost estimates to ranges to avoid instability across runs (e.g., cost ~ [100-120]). <br>3. Sort commutative children in deterministic order (by hash of subtree canonicalization). <br>4. Remove or normalize host-specific dialect differences in operator names via a provider mapping layer. <br>5. Output canonical plan with stable whitespace and param ordering to enable deterministic checksumming. <br><strong>Plan artifact:</strong> <code>canonicalPlanText</code>, <code>planHash</code> = SHA256(canonicalPlanText), <code>planSummary</code> = {operatorCounts, estimatedCardinalities, topOperators}. <br><strong>Provider capability mapping:</strong> maintain <code>providerCapabilities</code> directory that lists whether a provider supports plan export, plan level detail, and any special handling required. <br><strong>Failure modes & fallbacks:</strong> if plan export is refused, return <code>planAvailable=false</code> and <code>fallbackInstrumentation</code> output when permitted. If instrumentation prohibited by provider policy or operator setting, produce <code>planUnavailableReason</code>. <br><strong>Audit:</strong> <code>pq_diagnostics.step:plan_collected(correlationId, provider, planHash, planAvailable)</code>. <br><strong>Narrative example:</strong> a pre-release test identifies that the MPP provider returns different plan representations across versions; canonicalization collapses these differences into the same <code>planHash</code> for golden comparison and triggers a compatibility check in CI. </td></tr><tr><td data-label="PQ_Diagnostics — Per-function Expert Technical Breakdown"> <strong><code>CaptureProviderDiagnostics(providerHandle, requestContext, headersFingerprint=true, redactPatterns=[...], bodyPreviewKb=128, captureTlsInfo=true)</code> — exhaustive technical breakdown</strong><br><strong>Purpose & contract:</strong> capture provider-level diagnostics (request/response metadata, status, headers fingerprint, truncated body preview, TLS handshake details) while ensuring no sensitive headers or secrets are persisted. <br><strong>Data collected:</strong> method, endpointFingerprint (hash of scheme+host+port), providerVersion, TLS cipher suite (if allowed), responseStatus, responseLatencyMs, <code>headersFingerprint</code> (hash of header names/structure), <code>bodyPreview</code> truncated and redacted up to <code>bodyPreviewKb</code>, response body hash, and <code>requestFingerprint</code> for correlation. <br><strong>Redaction rules & enforcement:</strong> automatically redact fields matching <code>Authorization</code>, <code>Cookie</code>, <code>Set-Cookie</code>, <code>Authentication</code>, and configured <code>redactPatterns</code>; replace with deterministic fingerprints (e.g., <code>REDACTED_SHA256:&lt;hash&gt;</code>). Log a <code>redactionSummary</code> in the artifact meta listing redacted fields and counts. <br><strong>Constraints & policies:</strong> do not invoke provider debug endpoints or request server-side logs without explicit operator consent and recorded audit approval. Respect provider rate limits and terms of service. <br><strong>Audit:</strong> <code>pq_diagnostics.step:provider_diagnostics(correlationId, providerFingerprint, responseStatus, evidenceRef?)</code>. <br><strong>Example use-case:</strong> an HTTP provider returns a transient 500 with HTML body; <code>CaptureProviderDiagnostics</code> records the response status and body preview (redacted) and includes a TLS fingerprint showing client-server negotiation parameters to assist SRE in TLS mismatch diagnostics. </td></tr><tr><td data-label="PQ_Diagnostics — Per-function Expert Technical Breakdown"> <strong><code>CaptureConnectionFingerprint(connectionConfig, includeDriver=false, salt=&quot;&quot;)</code> — exhaustive technical breakdown</strong><br><strong>Purpose & contract:</strong> compute a stable, non-sensitive fingerprint to identify a connection across runs without storing credentials or PII. Intended to group failures and identify persistent endpoints. <br><strong>Fingerprint composition (non-sensitive):</strong> providerType, hostFingerprint (sha256 of host:port), connectionMode (direct/proxy), authMethodType (OAuth/Basic/Token/Integrated), driverName+version (if includeDriver true), sanitizedOptionsHash (hash of non-PII options), and optional salt to namespace. <br><strong>Constraints:</strong> do not include username, password, tokens, or any header values; store only structural and hashed metadata. <br><strong>Operational use:</strong> group <code>pq_diagnostics</code> packages by <code>connectionFingerprint</code> to detect repeated upstream issues across different operator credentials. </td></tr><tr><td data-label="PQ_Diagnostics — Per-function Expert Technical Breakdown"> <strong><code>ParseErrorTrace(errorObject, mashupContext, sourceArtifact=null, redactionPolicy=&quot;standard&quot;, maxFrames=50)</code> — exhaustive technical breakdown</strong><br><strong>Purpose & contract:</strong> parse raw engine/provider exception objects into a canonical structured error descriptor for aggregation and remediation guidance. This function produces <code>errorFingerprint</code> to deduplicate identical errors across runs. <br><strong>Parsing pipeline:</strong><br>1. Normalize raw message: strip host-specific tokens, IPs, PIDs, memory addresses. <br>2. Unwrap nested exceptions to identify root cause vs surface symptom. <br>3. Extract <code>stackFrames</code> with function names, step ids, and mashup step contexts; prune vendor internal frames unless <code>debugVerbose</code> enabled. <br>4. Map to standardized error codes <code>PQ_ERR_*</code> using provider and engine mapping tables with confidence scores. <br>5. Supply <code>remediationHints</code> and <code>reproSteps</code> derived from cause mapping (e.g., <code>refresh with smaller batch</code>, <code>increase timeout</code>, <code>re-authenticate</code>). <br><strong>Output schema:</strong> <code>{parsedError:{code, shortMessage, longMessage, stackFrames[]}, errorFingerprint, remediationHints[], reproducibilityHints, evidenceRef?}</code>. <br><strong>Edge cases:</strong> errors without stack traces still produce a fingerprint via canonicalized message and context; extremely verbose logs truncated and referenced via <code>evidenceRef</code>. <br><strong>Audit:</strong> <code>pq_diagnostics.step:error_parsed(correlationId, errorFingerprint, code)</code>. <br><strong>Example:</strong> HTTP 401 mapping to <code>PQ_ERR_PROVIDER_AUTH</code> yields remediation hint <code>Verify provider credentials and refresh tokens using secure reauth flow</code> and a <code>reproSteps</code> mapping to <code>CaptureProviderDiagnostics</code> for header fingerprints. </td></tr><tr><td data-label="PQ_Diagnostics — Per-function Expert Technical Breakdown"> <strong><code>AggregateRefreshPath(mashupSequence[], providerCallRecords[], includeConcurrency=true, lockHints=true)</code> — exhaustive technical breakdown</strong><br><strong>Purpose & contract:</strong> generate a canonical directed graph representing query dependencies, provider edges, and compute critical path timings and concurrency hotspots for performance and correctness analysis. <br><strong>Algorithmic steps:</strong><br>1. Canonicalize nodes to <code>{queryId, stepId, mChecksum}</code> and edges according to dependencies. <br>2. Annotate nodes with provider calls and resource usage fingerprints. <br>3. Use topological traversal to compute earliest start, latest finish, exclusive and inclusive durations, and identify the critical path using longest-path in DAG heuristics adjusted for resource locks. <br>4. Provide <code>concurrencyMap</code> indicating overlapping node windows and <code>contentionPoints</code> where resource locks or provider throttles serialized execution. <br><strong>Output:</strong> <code>{dependencyGraph, criticalPathNodes[], aggregateTimings, concurrencyMap, contentionPoints}</code>. <br><strong>Use-case:</strong> explains performance regressions where a provider call created a serialization barrier causing downstream queueing—critical for SRE tuning and query rewrite suggestions. <br><strong>Audit:</strong> <code>pq_diagnostics.step:refresh_graph(correlationId, nodeCount, criticalPathMs)</code>. </td></tr><tr><td data-label="PQ_Diagnostics — Per-function Expert Technical Breakdown"> <strong><code>BuildDiagnosticReport(packageMeta, format=&quot;json&quot;, includeArtifacts=false, redactPolicy=&quot;standard&quot;, linkEvidence=true, includeReproSteps=true, signManifest=true)</code> — exhaustive technical breakdown</strong><br><strong>Purpose & contract:</strong> assemble a signed, auditable diagnostic report combining summary, detailed artifacts, reproduction instructions, and a manifest of evidenceRefs and checksums. Intended for support tickets, vendor sharing (sanitized), and compliance submissions. <br><strong>Report components:</strong><br>1. Header: <code>correlationId</code>, <code>packageId</code>, <code>operatorId</code> (if available), <code>runTs</code>, <code>reportId</code>. <br>2. Top-line summary: short reproducible one-liner describing core symptom, top contributing factor, and immediate action. <br>3. Run metadata and environment snapshot: hostParityVector, PQEngineVersion, providerVersion, connectionFingerprint. <br>4. Diagnostics summary: timings, sample preview, planHash, trace summary, error fingerprints. <br>5. Evidence index: list of artifacts with <code>evidenceRef</code>, <code>artifactChecksum</code>, size, retention tag, and access policy notes. <br>6. Reproduction steps: explicit commands for <code>replay.run</code> including seed fingerprints and sandbox options; note <code>allowNetwork=false</code> by default. <br>7. Remediation suggestions: prioritized, pragmatic fixes with <code>confidenceScore</code>. <br>8. Signed manifest: <code>reportChecksum</code>, <code>signature</code> using release manifest signing key. <br><strong>Formatting & output:</strong> support <code>json</code>, <code>html</code>, <code>markdown</code> with embedded manifest in <code>html</code> and <code>json</code> formats. If <code>includeArtifacts=true</code> embed small sanitized artifacts (< 64KB) inline; otherwise reference via <code>evidenceRef</code>. <br><strong>Security & redaction:</strong> enforce redaction policy—no PII in top-level summary; evidence storage accessible under ACLs only. <br><strong>Audits:</strong> <code>pq_diagnostics.step:report_built(correlationId, reportId, format, reportChecksum)</code>. <br><strong>Example narrative:</strong> vendor support requires planHash and traceId; <code>BuildDiagnosticReport</code> produces signed HTML with manifest and a <code>reproduction</code> section that allows vendor to run a local reproduction using stubs. </td></tr><tr><td data-label="PQ_Diagnostics — Per-function Expert Technical Breakdown"> <strong><code>ExportDiagnostics(reportBlobRef, targetUri, atomic=true, maxAttempts=3, retentionTag=&quot;hot&quot;, metadata={})</code> — exhaustive technical breakdown</strong><br><strong>Purpose & contract:</strong> export diagnostic reports and evidence manifests to external destinations robustly and audibly; use atomic semantics where possible and provide staged fallback for non-atomic targets. <br><strong>Detailed behavior:</strong><br>1. For file systems supporting atomic replace, write to temp path and execute <code>AtomicWrite</code> semantics (<code>os.replace</code>, platform-specific ReplaceFile) ensuring atomic deliver semantics. <br>2. For object stores or network targets without atomic replace, use staged upload: upload a <code>reportId.tmp</code>, then <code>reportId.manifest</code> with checksum and finalization marker <code>reportId.ready</code>; emit <code>pq_diagnostics.export.degraded</code>. <br>3. On transient errors apply <code>Retry</code> wrapper with idempotency assertion and deterministic_jitter for CI. <br>4. Ensure exported metadata contains <code>reportChecksum</code>, <code>correlationId</code>, <code>evidenceRefs</code>, <code>retentionTag</code>, and <code>signedManifestRef</code>. <br><strong>Error handling & runbook:</strong> on <code>ENOSPC</code> produce <code>pq_diagnostics.export.failure</code> including <code>mountPath</code>, <code>freeBytes</code>, and suggested operator actions (<code>stage-local</code>, change retention). If permission errors occur, provide ACL snapshot and recommended commands. <br><strong>Audits:</strong> <code>pq_diagnostics.export.attempt(correlationId, reportId, destinationUri, paramsHash)</code> <code>pq_diagnostics.export.completed(correlationId, reportId, destinationUri, artifactChecksum)</code>. </td></tr><tr><td data-label="PQ_Diagnostics — Per-function Expert Technical Breakdown"> <strong><code>ReplayDiagnostics(evidenceRef, sandboxOptions={allowNetwork:false, maxRuntimeMs:600000, memoryMb:2048}, restoreRngState=true, dryRun=true, replayMode=&quot;strict&quot;)</code> — exhaustive technical breakdown</strong><br><strong>Purpose & contract:</strong> run a deterministic replay using captured trace and artifact bundle within an isolated sandbox to reproduce observed behavior. Replays are read-only against production systems unless explicit operator approval sets <code>allowNetwork=true</code> and supplies controlled tokens for provider access; such approval must be audited. <br><strong>Replay safety & constraints:</strong><br>1. Default <code>allowNetwork=false</code> ensures no external calls; captured provider responses are used to feed the replay. <br>2. Replay uses a headless mashup runner that faithfully implements the same operator semantics as host engine; parity vectors for engine implementation are used to detect host-induced divergence. <br>3. <code>restoreRngState</code> restores <code>DeterministicRNG</code> state when persisted; otherwise seed derivation is used and differences recorded. <br><strong>Replay procedure:</strong><br>1. Fetch evidence bundle and validate checksums. <br>2. Recreate sanitized environment and restore serialized RNG state if present. <br>3. Rehydrate provider responses from snapshots or stubs; if <code>allowNetwork=true</code> with operator approval use safe tokens or proxy stubs. <br>4. Execute step-by-step mashup operations in the headless runner with instrumentation; write replay artifacts to ephemeral storage. <br>5. Compute <code>replayArtifactChecksums</code> and a <code>deltaReport</code> comparing them to original artifact checksums. <br>6. Persist <code>replayLogRef</code> and produce <code>replayOutcome</code> with <code>match:true/false</code> and diagnostics for mismatches. <br><strong>Audit:</strong> <code>pq_diagnostics.replay.started(correlationId, replayId, evidenceRef)</code> <code>pq_diagnostics.replay.completed(correlationId, replayId, outcome, artifactChecksums, deltaReportRef)</code>. <br><strong>Example narrative:</strong> compliance requests replay proof for a sensitive run; replay executed in a fully sandboxed environment reproduces the artifactChecksum for the report, enabling an auditable compliance submission. </td></tr><tr><td data-label="PQ_Diagnostics — Per-function Expert Technical Breakdown"> <strong><code>CorrelateWithAudit(correlationId, diagnosticsPackageMeta, auditTailWindow=1000)</code> — exhaustive technical breakdown</strong><br><strong>Purpose & contract:</strong> anchor diagnostics packages to the append-only audit chain and produce a cryptographically signed <code>forensic_manifest</code> for chain-of-custody. <br><strong>Procedure:</strong><br>1. Query audit store for rows matching <code>correlationId</code> and within the configured <code>auditTailWindow</code> to capture pre/post related actions. <br>2. Compute <code>auditRowHashes[]</code> and an <code>auditChainHash</code> by chaining consecutive record hashes. <br>3. Build <code>forensic_manifest</code> listing artifacts, <code>evidenceRefs</code>, <code>artifactChecksums</code>, <code>auditRowHashes</code>, <code>operatorApprovals</code>, and <code>configHash</code>. <br>4. Sign manifest with release manifest key and persist via <code>AtomicWrite</code> to evidence store; return <code>manifestRef</code>. <br><strong>Output:</strong> <code>{manifestRef, linkedAuditRowsCount, auditChainHash}</code>. <br><strong>Audit:</strong> <code>pq_diagnostics.step:correlated_with_audit(correlationId, manifestRef, linkedCount)</code>. <br><strong>Use-case:</strong> required for regulatory packages where a signed manifest and audit chain are necessary to demonstrate reproducibility and unbroken evidence chain. </td></tr><tr><td data-label="PQ_Diagnostics — Per-function Expert Technical Breakdown"> <strong><code>HealthCheck()</code> — exhaustive technical breakdown</strong><br><strong>Purpose & contract:</strong> quick diagnostic check validating critical module dependencies: evidence store connectivity, <code>AtomicWrite</code> helper health, available temp disk quota, worker queue availability, service account permissions for export destinations, and redaction policy enforcement. Returns <code>{ok, checks:{evidenceStore, atomicWrite, diskQuota, workerQueue, retentionPolicy, secretsAccess}, timestamp}</code> and emits <code>pq_diagnostics.health.ok</code> or <code>pq_diagnostics.health.failed</code>. <br><strong>Operational usage:</strong> invoked at add-in load, periodically by scheduler, and integrated in CI gating. Health check must be lightweight and not perform heavy IO or exports in the UI thread. </td></tr><tr><td data-label="PQ_Diagnostics — Per-function Expert Technical Breakdown"> <strong><code>SampleDiagnostics(populationSize, samplePercent, correlationId, rng=null, strata=null, deterministic=true, minSample=10)</code> — exhaustive technical breakdown</strong><br><strong>Purpose & contract:</strong> deterministic sampling helper used by the diagnostics module to select rows, pages, or events to include in packages while maintaining reproducibility and statistical representativeness. <br><strong>Algorithmic choices & behavior:</strong><br>1. If <code>rng</code> null and <code>deterministic=true</code> derive <code>rng</code> from <code>DeterministicRNG(seed_source=correlationId|populationFingerprint)</code> to guarantee repeatability. <br>2. If <code>strata</code> present perform proportional allocation across strata and then deterministic selection within each stratum. <br>3. For very large or streaming populations use deterministic reservoir sampling seeded by <code>rng</code>. <br>4. Ensure <code>minSample</code> lower bound to avoid empty samples in small populations. <br><strong>Output:</strong> <code>{indices[], sampleSeedFingerprint, methodUsed, deterministic=true}</code> with <code>indices</code> sorted for stable presentation. <br><strong>Audit:</strong> <code>pq_diagnostics.step:sampling(correlationId, populationSize, samplePercent, sampleSeedFingerprint)</code>. <br><strong>Examples:</strong> selecting 5% of rows from a 1M row table using stratified by country ensures distributed representation; sampling seed serialized into evidence bundle for exact replay. </td></tr><tr><td data-label="PQ_Diagnostics — Per-function Expert Technical Breakdown"> <strong>Cross-cutting Observability, Telemetry & Error catalog (concepts & mapping)</strong><br><strong>Audit schema (required fields):</strong> <code>timestamp</code>, <code>correlationId</code>, <code>module=PQ_Diagnostics</code>, <code>procedure</code>, <code>operatorId</code> (optional), <code>paramsHash</code>, <code>resultHash</code> (optional), <code>evidenceRef</code> (optional), <code>durationMs</code>, <code>metadata</code> including <code>artifactChecksum</code>, <code>reportId</code>, <code>tempChunkList</code>. All major steps must emit start/complete events. <br><strong>Top-level diagnostic events:</strong> <code>pq_diagnostics.requested</code>, <code>pq_diagnostics.started</code>, <code>pq_diagnostics.step:sample_selected</code>, <code>pq_diagnostics.step:plan_collected</code>, <code>pq_diagnostics.step:trace_captured</code>, <code>pq_diagnostics.step:error_parsed</code>, <code>pq_diagnostics.export.attempt/completed</code>, <code>pq_diagnostics.replay.started/completed</code>, <code>pq_diagnostics.prune.completed</code>. <br><strong>Representative ErrorCodes and operator guidance mapping:</strong><br>1. <code>PQ_DIAG_QUERY_NOT_FOUND</code> — verify queryId and presence in workbook or job descriptor. <br>2. <code>PQ_DIAG_PLAN_UNAVAILABLE</code> — provider denies plan export; recommend worker-mode instrumentation or vendor assistance. <br>3. <code>PQ_DIAG_TRACE_CAPTURE_FAILED</code> — evidence store or hook failure; inspect temp artifacts. <br>4. <code>PQ_DIAG_EXPORT_ENOSPC</code> — destination out-of-space; use stage-local fallback and contact infra. <br>5. <code>PQ_DIAG_REPLAY_NETWORK_FORBIDDEN</code> — replay attempted with network disabled; obtain audited approval to enable network if necessary. <br>6. <code>PQ_DIAG_PARTIAL_CAPTURE</code> — timeout/truncation; optionally re-run with increased budgets. <br><strong>Metrics:</strong> <code>pq_diagnostics.packages_per_hour</code>, <code>pq_diagnostics.full_trace_size_bytes</code>, <code>pq_diagnostics.replay_success_rate</code>, <code>pq_diagnostics.export.latency_ms</code>, <code>pq_diagnostics.partial_capture_rate</code>. Metrics buffered locally and uploaded by CORE_Telemetry; module must not perform remote uploads on the UI fast path. </td></tr><tr><td data-label="PQ_Diagnostics — Per-function Expert Technical Breakdown"> <strong>Testing matrix, property tests, and cross-host parity governance (comprehensive)</strong><br><strong>Unit tests (required):</strong><br>1. Deterministic sampling parity: given identical seed and population, sample indices match across host implementations. <br>2. Plan canonicalization idempotence: transient fields removed produce stable <code>planHash</code>. <br>3. Trace capture truncation produces <code>partial=true</code> and correct <code>droppedEventCount</code>. <br>4. ParseErrorTrace canonicalizes nested exceptions to identical <code>errorFingerprint</code> when equivalent. <br>5. BuildDiagnosticReport redaction verification: no PII in top-level summary and evidenceRef points to encrypted artifact. <br><strong>Integration tests:</strong><br>1. Full collect->report->export pipeline with mocked providers and object storage; verify <code>artifactChecksum</code> and audit emissions. <br>2. ReplayDiagnostics sandbox test ensuring <code>allowNetwork=false</code> prevents outbound connections and replay artifacts match expected checksums. <br>3. CorrelateWithAudit end-to-end manifest signing and audit chain verification. <br><strong>Property tests & fuzzing:</strong><br>1. Sampling coverage and representation properties under random seeds and populations. <br>2. Trace chunking/reassembly property tests ensure chunk checksums combine to original trace summary. <br><strong>Cross-host golden gating:</strong> produce golden vectors for sampling outputs, canonicalized plans, error fingerprints, and replay artifacts; CI requires parity across supported host environments (Windows Excel, Excel for Mac, headless worker dockers) and across language SDKs used by orchestration (Python/JS/VBA) before changes merge. <br><strong>Security and privacy tests:</strong> automated redaction tests for headers and bodies, evidence encryption verification, and ACL test harness to ensure evidenceRefs enforce access permissions. </td></tr><tr><td data-label="PQ_Diagnostics — Per-function Expert Technical Breakdown"> <strong>Developer guidance, allowed & forbidden patterns (explicit)</strong><br><strong>Required usage patterns:</strong><br>1. Always use <code>CollectQueryDiagnostics</code> for support capture requests; prefer <code>mode=&quot;worker&quot;</code> for heavy captures. <br>2. Persist <code>correlationId</code> and seed values in job descriptors for reproducibility. <br>3. Use <code>AtomicWrite</code> for all persisted evidence and manifests; static analyzers should fail builds that write direct final artifact paths from UI code. <br>4. Seed deterministic sampling with <code>DeterministicRNG</code> and persist serialized RNG state when exact replay is required. <br>5. Emit audit rows for every capture, export, and replay; audits must include <code>paramsHash</code> and <code>artifactChecksum</code> when available. <br><strong>Forbidden patterns:</strong><br>1. Do not log raw credentials, tokens, or PII in audits or diagnostics payloads. <br>2. Do not enable network during replay without explicit audited operator approval. <br>3. Do not perform in-place query rewrites or destructive operations during capture flows. <br>4. Do not bypass redaction policy for convenience. <br><strong>Code-review checklist:</strong> verify audit emissions, evidenceRef usage, <code>AtomicWrite</code> usage for persisted artifacts, deterministic sampling seed propagation, UI-thread safety for heavy flows, redaction enforcement, and presence of cross-host parity tests for any canonicalization logic changes. </td></tr><tr><td data-label="PQ_Diagnostics — Per-function Expert Technical Breakdown"> <strong>Operational runbook & incident playbooks (executable steps)</strong><br><strong>Trace capture ENOSPC runbook:</strong><br>1. Inspect <code>pq_diagnostics.export.failure</code> audit row for <code>correlationId</code> and <code>destinationUri</code>. <br>2. Run <code>diagnostics inspect-temp --correlation &lt;id&gt;</code> to enumerate temp chunk files and compute their checksums. <br>3. If local staging feasible, re-run <code>diagnostics export --report &lt;id&gt; --target --stage-local</code> to move artifacts onto same volume then re-export. <br>4. If blocked by quota or permissions escalate with <code>forensic_manifest</code> attached to infra ticket; include <code>pq_diagnostics</code> export failure audit. <br><strong>Non-deterministic replay triage:</strong><br>1. Retrieve <code>pq_diagnostics.replay.started</code> and <code>pq_diagnostics.replay.completed</code> audits and the <code>evidenceRef</code> for RNG state. <br>2. Run <code>replay.run --evidence &lt;ref&gt; --dry-run</code> and compare <code>replayArtifactChecksums</code> to original artifacts. <br>3. If mismatch persists, collect host parity vectors (engine versions, OS, PQ engine flavor) and escalate to parity/golden team with delta artifacts. <br><strong>Provider auth failure remediation:</strong><br>1. Locate <code>ParseErrorTrace</code> mapping to <code>PQ_ERR_PROVIDER_AUTH</code> and capture <code>providerFingerprint</code>. <br>2. Request operator reauthorization via secure flow; after reauth re-run <code>CollectQueryDiagnostics</code> to confirm resolution; persist new <code>connectionFingerprint</code> in config store. </td></tr><tr><td data-label="PQ_Diagnostics — Per-function Expert Technical Breakdown"> <strong>Extremely detailed long-form narratives & examples (multiple scenarios)</strong><br><strong>Scenario 1 — Intermittent scheduled refresh failure (end-to-end forensic trace):</strong><br>1. Monitoring alert links to correlationId <code>r-20260102-b78</code>. SRE triggers <code>CollectQueryDiagnostics(connReporting, qDailyAggregate, includePlan=true, includeTrace=true, mode=&quot;worker&quot;)</code>. {audit: <code>pq_diagnostics.requested</code>} <br>2. <code>CaptureRefreshTrace</code> captures event stream across the 00:00-00:08 window including repeated provider 429 responses timestamped precisely with sequence numbers. The traceSummary reveals <code>retryCount=5</code> with backoff pattern correlating to a scheduled spike of traffic. {audit: <code>pq_diagnostics.step:trace_captured</code>} <br>3. <code>CollectQueryPlan</code> returns <code>planHash</code> where <code>HashAggregate</code> is performing heavy grouping on a high-cardinality key; canonical plan maps to historical known pattern flagged in plan registry. {audit: <code>pq_diagnostics.step:plan_collected</code>} <br>4. <code>AggregateRefreshPath</code> indicates a join re-ordering caused a late materialization step that overlapped with provider retries, producing cascading latency. <br>5. <code>BuildDiagnosticReport</code> provides reproduction steps for vendor: <code>replay.run --evidence &lt;traceRef&gt; --restoreRngState=true --dry-run</code>. Report includes <code>planHash</code>, <code>traceId</code>, and <code>criticalSegment</code> with exact timestamps. {audit: <code>pq_diagnostics.step:report_built</code>} <br>6. Vendor reproduces with provided plan and identifies a throttle rule; SRE coordinates scheduling change and introduces a migration to pre-aggregate upstream keys to reduce join cardinality. Post-fix re-run <code>CollectQueryDiagnostics</code> validates fix—replay artifactChecksum matches new artifact and scheduled run completes within SLO. <br><strong>Scenario takeaways:</strong> full trace capture with plan canonicalization and deterministic replay allowed cross-team vendor collaboration without exposing PII or credentials and enabled CI-gated remediation. <br><strong>Scenario 2 — Numeric fidelity drift after PQ template injection (deep example):</strong><br>1. Production model exhibits cent-level drift post-injection of a new PQ template. Template author provides <code>mChecksum</code>. Ops runs <code>CollectQueryDiagnostics(connFin, qLedger, includePlan=true, includeTrace=false, samplePercent=100)</code> and captures sample rows and provider plan. {audit: <code>pq_diagnostics.started</code>} <br>2. <code>CollectQueryPlan</code> shows provider casts numeric columns to double before aggregation; <code>ParseErrorTrace</code> notifies no error but <code>planSummary</code> flags the <code>CAST</code> operator. <code>BuildDiagnosticReport</code> recommends offloading final aggregation to worker-safe <code>SafeRound</code> pipeline and persisting final cents via <code>AtomicWrite</code>. <br>3. Ops implements worker-side aggregation with <code>SafeRoundResiduals</code> and persists results; <code>CollectQueryDiagnostics</code> verified final artifactChecksum matches expected golden vector and <code>RunMetadata</code> updated. CI includes golden tests ensuring cross-host parity. <br><strong>Takeaways:</strong> diagnostic pipeline identifies provider-side implicit casting and prescribes worker-side atomicized rounding to ensure regulatory compliance. <br><strong>Scenario 3 — MatchMerge tie-breaking non-determinism (detailed reconstruction):</strong><br>1. Different merge proposals observed across runs for identical input dataset. Operator provides <code>correlationId</code>. <code>CollectQueryDiagnostics</code> captures <code>SampleDiagnostics</code> seeds and <code>ParseErrorTrace</code> indicates RNG state not serialized at proposal persist stage. <br>2. Replay without RNG state reproduces different proposals; with restored RNG state (provided by operator from earlier evidence), replay matches original outputs. <code>BuildDiagnosticReport</code> documents missing RNG serialization bug and prescribes mandatory RNG state persistence for MatchMerge. <br>3. CI adds golden vector test ensuring RNG serialization round-trip parity; deployment includes migration to update existing proposals with serialized RNG metadata. <br><strong>Takeaway:</strong> enforcing RNG state persistence prevents ambiguous merges and ensures reproducibility. </td></tr><tr><td data-label="PQ_Diagnostics — Per-function Expert Technical Breakdown"> <strong>Conceptual Power Query (M) patterns — mapping PQ_Diagnostics to PQ workflows (detailed mapping & best practices)</strong><br><strong>Context:</strong> Power Query (M) runs in-process in Excel or headless engines with limited introspection. <code>PQ_Diagnostics</code> cannot instrument the internal host engine directly from M; orchestration layer (add-in or worker) performs diagnostics collection and interacts with M where safe. <br><strong>Mapping patterns & recommendations:</strong><br>1. <strong>Light-mode preview capture:</strong> UI preview flows should capture parameterHash, <code>mChecksum</code>, deterministic seed for preview sampling, small sample rows, and timing metadata. Use <code>mode=&quot;auto&quot;</code> to escalate heavy capture requests to worker and record <code>preview.audit</code> linking. <br>2. <strong>Worker-mode high-fidelity capture:</strong> heavy plan and trace capture should run in worker processes with higher timeouts and ephemeral staging; avoid performing heavy IO from the UI thread. <br>3. <strong>Template flags and contract:</strong> templates should include metadata: <code>requiresHighPrecision</code>, <code>planCaptureHint</code>, <code>diagnosticsHint</code>, and <code>evidenceRetention</code>. Orchestrator respects these flags to determine whether to offload numeric-critical transforms to <code>SafeRound</code> worker paths. <br>4. <strong>Seeding & reproducibility:</strong> preview flows must accept <code>seed</code> parameter derived from <code>correlationId</code> and persist to preview audit; <code>CollectQueryDiagnostics</code> should include that seedFingerprint for exact preview reproduction. <br>5. <strong>Atomic artifact injection:</strong> authoritative M query artifacts intended for team consumption or audits should be written to evidence store using <code>AtomicWrite</code> and then injected via workbook API referencing the same artifactChecksum; <code>pq_inject</code> audit should include artifactChecksum and mChecksum to validate parity. <br>6. <strong>Provider plan capture:</strong> because M lacks standardized plan introspection, orchestrator should call <code>CollectQueryPlan</code> post-parameterization and before injection if <code>requiresHighPrecision</code> or <code>planCaptureHint</code> present. <br>7. <strong>Retry & idempotency for PQ refresh orchestration:</strong> orchestrator uses <code>Retry</code> wrapper (with idempotency tokens persisted) for job persistence and export; M code stays declarative while orchestration manages resilience. <br><strong>Operator flow example (preview->inject->diagnose):</strong><br>1. Operator previews template; <code>PQ_Ribbon</code> computes <code>seed=SeedFromCorrelation(correlationId, templateId)</code> and passes it to preview parameters; a <code>pq_preview</code> audit is recorded. <br>2. On inject, orchestrator writes final M artifact via <code>AtomicWrite</code> and then calls <code>workbook.Queries.Add</code> using that artifact. <code>pq_inject</code> audit references artifactChecksum and <code>mChecksum</code>. <br>3. If a downstream refresh fails, operator runs <code>CollectQueryDiagnostics</code> for the injected query and receives an evidenceRef and <code>BuildDiagnosticReport</code> with <code>replay</code> instructions. </td></tr><tr><td data-label="PQ_Diagnostics — Per-function Expert Technical Breakdown"> <strong>Conceptual DAX patterns — mapping PQ_Diagnostics to semantic models & DAX troubleshooting (detailed mapping & governance)</strong><br><strong>Context:</strong> DAX operates at query time against persisted model metadata; it cannot produce side-effects or persist artifacts. Diagnostics for DAX performance and correctness are produced by ETL and model engine (Analysis Services) and surfaced via DAX measures referencing persisted <code>RunMetadata</code>. <br><strong>Patterns & recommendations:</strong><br>1. <strong>Persist RunMetadata table in ETL:</strong> ETL should write <code>RunMetadata</code> atomically containing <code>correlationId</code>, <code>artifactChecksum</code>, <code>packageId</code>, <code>runTs</code>, <code>diagnosticsReportRef</code>, and <code>engineParityVector</code>. DAX measures and reports consult this <code>RunMetadata</code> for provenance indicators. <br>2. <strong>DMV snapshot captures:</strong> <code>CollectQueryDiagnostics</code> should capture Analysis Services DMVs at ETL completion (query caches, plan caches, table statistics) and store as evidence to correlate slow DAX measures to model statistics. <br>3. <strong>HashKey deterministic sampling for DAX:</strong> in ETL compute <code>HashKey = HASH(PrimaryKey | correlationSalt)</code> and persist <code>correlationSalt</code> in <code>RunMetadata</code>. DAX filters like <code>MOD(HashKey, 100) &lt; k</code> allow deterministic sample views inside reports; <code>SampleDiagnostics</code> uses the same seed for parity. <br>4. <strong>Reconciled flags via artifactChecksum:</strong> ETL writes reconciled artifact and <code>RunMetadata</code>; DAX measure compares model table checksum to expected <code>artifactChecksum</code> and surfaces <code>ReconciledFlag</code> for report consumers. <br>5. <strong>Avoid heavy fixes in DAX:</strong> all transformations requiring rounding, residual distribution, or deterministic tie-breakers should be resolved in ETL using <code>SafeRound</code> primitives and persisted; DAX should only read persisted, reconciled integers or decimals. <br><strong>Narrative example:</strong> a slow DAX measure is traced to a high cardinality materialization; DMV snapshot captured by <code>CollectQueryDiagnostics</code> shows missing statistics and skew in cardinality; ETL is updated to pre-aggregate keys and run <code>CollectQueryDiagnostics</code> captures the post-fix artifactChecksum that DAX surfaces as <code>ReconciledFlag=1</code>. </td></tr><tr><td data-label="PQ_Diagnostics — Per-function Expert Technical Breakdown"> <strong>Appendices: forensic artifacts, evidence paths & recommended retention (expanded and prescriptive)</strong><br><strong>Minimum forensic artifacts for an incident:</strong><br>1. <code>ribbon-map.json</code> and release manifest with signatures. <br>2. <code>jobDescriptor.json</code> persisted via <code>AtomicWrite</code> including <code>jobId</code>, <code>paramsHash</code>, <code>configHash</code>, and <code>correlationId</code>. <br>3. <code>audit_tail.csv</code> covering pre/post windows with <code>pq_diagnostics.*</code> rows. <br>4. artifact files with <code>SHA256</code> checksums and <code>artifact.metadata.json</code>. <br>5. serialized RNG state (<code>rng_state.blob</code>) for sampling/tie-breaker reproduction. <br>6. <code>SafeRound</code> inputs and canonicalized decimal snapshots for numeric forensic. <br>7. trace artifacts and chunk manifests from <code>CaptureRefreshTrace</code>. <br>8. temp artifact listing from <code>InspectTempArtifacts</code> and recovery scripts. <br><strong>Evidence Store & retention:</strong><br>1. Hot evidence store path: <code>evidence/hot/pq_diagnostics/&lt;correlationId&gt;/</code> retained 30 days for rapid access. <br>2. Warm archive: secure archive for 7 years for regulatory retention; access strictly controlled and logged; manifests include chain-of-custody attributes and signatures. <br>3. Cold archive: cold storage per policy with retrieval windows. <br>4. <code>forensic_manifest.json</code> enumerates URIs, checksums, retentionTag, and access policy for investigators. <br><strong>Retention verification cadence:</strong> monthly retention verification job emits <code>pq_diagnostics.prune.completed</code> with counts of pruned artifacts; audits include proof-of-delete records. </td></tr><tr><td data-label="PQ_Diagnostics — Per-function Expert Technical Breakdown"> <strong>Acceptance checklist before module release (detailed & blocking conditions):</strong><br>1. Owners listed in OWNERS.md with on-call contacts. <br>2. Public API stable and documented with semantic versioning. <br>3. Evidence encryption and key-management tested; key rotation documented. <br>4. Deterministic sampling goldens validated and parity tests across hosts passing. <br>5. Plan canonicalization golden vectors validated for transformed provider output. <br>6. Redaction tests verify absence of PII in top-level audit rows and presence in encrypted evidence only. <br>7. CI gates include static analyzer checks forbidding credential logging and UI-thread heavy IO. <br>8. Integration tests for <code>CollectQueryDiagnostics</code>-><code>BuildDiagnosticReport</code>-><code>ExportDiagnostics</code>-><code>CorrelateWithAudit</code> pipeline pass. <br><strong>Blocking conditions:</strong> missing audit emissions, redaction failures, golden regressions, or absence of AtomicWrite usage for persisted artifacts. </td></tr><tr><td data-label="PQ_Diagnostics — Per-function Expert Technical Breakdown"> <strong>Extremely detailed test plan highlights & scripts (explicit, conceptual)</strong><br><strong>Unit tests (explicit):</strong><br>1. Sampling parity and deterministic RNG seeding across seeds. <br>2. Plan canonicalization idempotence across ephemeral provider fields. <br>3. Trace truncation and chunk reassembly tests. <br>4. ParseErrorTrace canonicalization for nested exceptions. <br>5. BuildDiagnosticReport redaction and manifest signing tests. <br><strong>Integration tests:</strong><br>1. collect->report->export: simulate provider interactions and verify <code>artifactChecksum</code> and audit emissions. <br>2. replay sandbox tests: ensure <code>allowNetwork=false</code> prevents outbound connections and that replay artifacts match golden checksums. <br>3. CorrelateWithAudit test ensures manifest contains auditRowHashes with valid signature. <br><strong>Performance tests:</strong><br>1. trace capture throughput under synthetic high event rates, verifying chunk manifest integrity. <br>2. report build latency for large packages (>100MB) and stress test AtomicWrite on various filesystem semantics. <br><strong>Security tests:</strong> PII redaction fuzzing, evidence encryption key rotation, ACL enforcement for evidenceRefs. <br><strong>CI gating:</strong> golden vectors, static checks, and integration test suites must pass; performance regressions cause gate failure requiring owner approval. </td></tr><tr><td data-label="PQ_Diagnostics — Per-function Expert Technical Breakdown"> <strong>Operator runbook quick commands & examples (concise & prescriptive)</strong><br>1. <code>diagnostics collect --correlation r-YYYYMMDD-abc --query qName --include-plan</code> — collect diagnostics with plan and stage to evidence store. <br>2. <code>diagnostics export --report &lt;id&gt; --target sftp://host/path</code> — export a signed report using atomic semantics where supported. <br>3. <code>diagnostics replay --evidence &lt;ref&gt; --dry-run</code> — deterministic sandbox replay without network. <br>4. <code>diagnostics inspect-temp --correlation &lt;id&gt;</code> — list temporary chunks from failed captures and compute checksums for manual recovery. <br>5. <code>diagnostics sign-manifest --manifest &lt;id&gt;</code> — re-sign manifest with release manifest key if necessary during regulatory packaging. <br><strong>When to call SRE:</strong> after two <code>pq_diagnostics.export</code> ENOSPC failures for critical artifacts, repeated replay mismatches suggesting host parity failure, or evidence store permission anomalies; include <code>forensic_manifest</code> with ticket. </td></tr><tr><td data-label="PQ_Diagnostics — Per-function Expert Technical Breakdown"> <strong>Final notes, governance & mandatory constraints (firm & non-negotiable):</strong><br>1. Never persist raw credentials or unredacted PII in audit rows or reports; evidence store only with explicit governance and encrypted at rest. <br>2. All persisted artifacts must use <code>AtomicWrite</code> semantics or the staged manifest fallback and emit <code>pq_diagnostics.export.degraded</code> when atomic replace cannot be guaranteed. <br>3. Deterministic RNG seeds must be derived from <code>correlationId</code> for operator-visible sampling; if exact replay required serialize RNG state and store encrypted in evidence store. <br>4. Replays default to sandboxed, network-disabled execution; enabling network must be explicitly approved and audited. <br>5. Exported diagnostic packages must include <code>reportChecksum</code>, <code>correlationId</code>, signed manifestRef, and evidenceRef links. <br>6. All heavy IO or potentially blocking operations are forbidden on the UI thread and must be scheduled to worker processes. <br><strong>Checked:</strong> content underwent multi-pass verification for internal consistency, audit linkage, deterministic sampling, redaction policy, plan canonicalization, replay safeguards, and cross-module dependencies (tenfold review). </td></tr><tr><td data-label="PQ_Diagnostics — Per-function Expert Technical Breakdown"> <strong>Appendix A — Example audit row schema (descriptive):</strong><br><strong>Fields required for PQ_Diagnostics audits:</strong> <code>timestamp</code>, <code>correlationId</code>, <code>module</code>, <code>procedure</code>, <code>operatorId</code> (optional), <code>paramsHash</code>, <code>resultHash</code> (optional), <code>evidenceRef</code> (optional), <code>prevHash</code> (optional), <code>configHash</code>, <code>metadata</code> including <code>durationMs</code>, <code>attempts</code>, <code>artifactChecksum</code>, <code>reportId</code>, <code>tempChunkList</code>. <br><strong>Policy note:</strong> top-level audit rows must not contain PII; sanitized parameters and large payloads stored in the encrypted evidence store and referenced by <code>evidenceRef</code>. </td></tr><tr><td data-label="PQ_Diagnostics — Per-function Expert Technical Breakdown"> <strong>Appendix B — Common failure modes & mitigations (expanded):</strong><br><strong>Failure mode: oversized trace artifacts causing ENOSPC</strong><br>1. Cause: <code>includeTrace=true</code> on a high-volume provider without <code>maxEvents</code> or compression. <br>2. Mitigation: enforce <code>maxEvents</code>, apply streaming compression and chunking, use stratified sampling, produce <code>pq_diagnostics.step:truncated</code> and <code>forensic_manifest</code> with chunk checksums; operator may request selective time windows for re-capture. <br><strong>Failure mode: replay mismatch despite captured evidence</strong><br>1. Cause: missing RNG state, host engine parity differences, truncated provider snapshots, or dependency on ephemeral external state. <br>2. Mitigation: mandate RNG state serialization where reproducibility matters, persist host parity vectors in manifest, capture provider snapshots, and add parity diagnostics in CI to reduce heterogeneity. <br><strong>Failure mode: plan unavailable from provider</strong><br>1. Cause: provider blocks <code>EXPLAIN</code> or lacks capability. <br>2. Mitigation: fallback to worker-mode instrumentation to reconstruct plan heuristically, or request vendor plan capture with signed manifest and audit. </td></tr><tr><td data-label="PQ_Diagnostics — Per-function Expert Technical Breakdown"> <strong>Appendix C — Governance checklists & PR requirements (explicit):</strong><br>1. PR must include unit tests for deterministic behaviors, golden vectors for plan canonicalization, and redaction tests. <br>2. Changes to redaction or retention policies require owner and compliance approval and release manifest updates. <br>3. Changes to export semantics or <code>AtomicWrite</code> usage must include cross-platform regression tests and SRE sign-off. <br>4. Any change to <code>DeterministicRNG</code> serialization format requires migration instructions and golden parity updates. </td></tr><tr><td data-label="PQ_Diagnostics — Per-function Expert Technical Breakdown"> <strong>Appendix D — Incident reconstruction example (ordered & executable):</strong><br><strong>Incident synopsis:</strong> operator reports "Allocation mismatch for run r-20260112-455" — artifact sums different from ledger. <br><strong>Forensic reconstruction steps (ordered):</strong><br>1. Retrieve <code>pq_diagnostics.*</code> and <code>UserAction</code> audit rows for the <code>correlationId</code>. <br>2. Pull <code>artifactChecksum</code> from <code>pq_diagnostics.complete</code> and evidenceRef pointing to <code>SafeRound</code> input snapshot. <br>3. Download <code>rng_state.blob</code> and verify integrity via its checksum; ensure evidence store ACL allows access to investigators. <br>4. Run <code>replay.run --evidence &lt;ref&gt; --dry-run</code> restoring RNG; execute <code>SafeRoundResiduals</code> using persisted canonical decimals. <br>5. Compare produced artifact checksums to original; if identical, pipeline determinism confirmed; if mismatch, inspect <code>atomic_write.verification_failed</code> and temp artifact lists. <br>6. Compile <code>forensic_manifest.json</code> including audit rows, evidenceRefs, and replay delta; escalate to compliance if regulatory impact identified. <br><strong>Outcome:</strong> reproducible replay yields identical artifact showing pipeline correctness; incident closed with runbook updates on tie-break expectations and RNG state persistence. </td></tr><tr><td data-label="PQ_Diagnostics — Per-function Expert Technical Breakdown"> <strong>Appendix E — PQ & DAX short checklists for template authors and report builders:</strong><br><strong>PQ Template author checklist:</strong><br>1. Include <code>mChecksum</code> and <code>templateVersion</code> in template metadata. <br>2. Mark <code>requiresHighPrecision</code> and <code>planCaptureHint</code> for templates performing critical numeric transforms. <br>3. Parameterize a <code>seed</code> for preview flows and persist seed in preview audit. <br>4. Offload final numeric aggregation to worker <code>SafeRound</code> flows for regulated outputs; persist final artifacts with <code>AtomicWrite</code>. <br>5. Provide <code>diagnosticsHint</code> specifying recommended <code>includePlan</code>/<code>includeTrace</code> flags for support. <br><strong>DAX/report builder checklist:</strong><br>1. Consume <code>RunMetadata</code> table for provenance and <code>artifactChecksum</code> verification. <br>2. Avoid performing allocation/residual rounding in DAX; convert to ETL-managed persisted columns. <br>3. Use hashed stable keys computed during ETL to enable deterministic sampling and debug views in reports. </td></tr><tr><td data-label="PQ_Diagnostics — Per-function Expert Technical Breakdown"> <strong>Appendix F — Long-form operator scenario: forensic reconstruction and regulatory package (procedural):</strong><br><strong>Scenario synopsis:</strong> regulator requests audit package for a sensitive run produced two weeks earlier; operator must provide signed manifest, reproducible evidence, and replay instructions. <br><strong>Procedural steps:</strong><br>1. Locate <code>pq_diagnostics</code> package by <code>correlationId</code> and verify <code>pq_diagnostics.complete</code> artifactChecksum. <br>2. Run <code>CorrelateWithAudit(correlationId, packageMeta)</code> to build <code>forensic_manifest</code> linking audit rows, evidenceRefs, and artifact checksums; sign manifest with release manifest key. <br>3. Execute <code>ReplayDiagnostics</code> in sandbox with <code>restoreRngState=true</code> and <code>allowNetwork=false</code> to generate <code>replayOut</code> demonstrating artifact parity. <br>4. Build final regulatory bundle: <code>report.html</code> (signed), <code>forensic_manifest.json</code> (signed), evidenceChunks (encrypted), <code>audit_tail.csv</code>, and <code>release_manifest</code> signature; persist bundled package via <code>AtomicWrite</code> to regulated archive with appropriate retention tags. <br>5. Deliver bundle to regulator endpoint according to policy; record <code>submission.audit</code> and preserve chain-of-custody logs. <br><strong>Compliance takeaway:</strong> reproducible evidence bundles and signed manifests are necessary to satisfy audit and regulatory requests while preserving PII constraints. </td></tr><tr><td data-label="PQ_Diagnostics — Per-function Expert Technical Breakdown"> <strong>Appendix G — Migration guidance & backward compatibility (detailed):</strong><br><strong>When RNG serialization format changes:</strong><br>1. Provide a compatibility layer that can read both legacy and new serialized RNG state formats. <br>2. Provide migration tool <code>rng_migrate --evidence &lt;ref&gt;</code> that updates stored RNG blobs and re-computes seedFingerprints. <br>3. Update CI golden vectors and require parity approval from RNG owners. <br><strong>When plan canonicalization rules change:</strong><br>1. Ship canonicalizer in major version and maintain <code>planHashV1</code> and <code>planHashV2</code> columns in registry for migration. <br>2. Provide <code>ResolvePlanHash(planText)</code> utility to compute both v1 and v2 hashes for transition period. <br><strong>When export semantics change:</strong><br>1. Ensure <code>AtomicWrite</code> semantics remain the default; if fallback staging changes, document in release manifest and migrate existing manifests via <code>sign-manifest</code> flow. </td></tr><tr><td data-label="PQ_Diagnostics — Per-function Expert Technical Breakdown"> <strong>Appendix H — Example diagnostic report sections (template outline for authors & support):</strong><br><strong>Report header:</strong> <code>reportId</code>, <code>correlationId</code>, <code>operatorId</code>, <code>runTs</code>, <code>reportChecksum</code>. <br><strong>Executive summary:</strong> short reproducible one-liner. <br><strong>Run metadata:</strong> hostParityVector, PQEngineVersion, connectionFingerprint, mChecksum. <br><strong>Top findings:</strong> key errors, planHash, critical path nodes. <br><strong>Detailed artifacts index:</strong> list of evidenceRefs with checksums and sizes. <br><strong>Repro steps:</strong> exact <code>replay.run</code> invocation with seedFingerprint and sandbox options. <br><strong>Remediation actions:</strong> prioritized suggestions, estimated effort, owner. <br><strong>Signed manifest & chain-of-custody:</strong> signature block and rail of auditRow hashes. </td></tr><tr><td data-label="PQ_Diagnostics — Per-function Expert Technical Breakdown"> <strong>Appendix I — Cross-team responsibilities (concise mapping):</strong><br>1. TEAM_PQ_DIAGNOSTICS — owns capture/replay code, evidence store interface, plan canonicalization, and analytics summaries. <br>2. TEAM_UTILITIES — provides <code>AtomicWrite</code>, <code>DeterministicRNG</code>, <code>Retry</code>, and <code>SafeRound</code> primitives used by diagnostics. <br>3. TEAM_SRE — owns evidence store availability, export endpoints, and infrastructure support for large artifact handling. <br>4. TEAM_COMPLIANCE — verifies signed manifests, retention policies, and regulatory packaging. <br>5. TEAM_TEMPLATE_OWNERS — declare <code>diagnosticsHint</code> and <code>requiresHighPrecision</code> flags in template manifests. </td></tr><tr><td data-label="PQ_Diagnostics — Per-function Expert Technical Breakdown"> <strong>Appendix J — Example CSV of minimal diagnostic manifest (illustrative fields for tooling integration):</strong><br><code>packageId,correlationId,reportId,artifactChecksum,evidenceRef,traceId,planHash,connectionFingerprint,operatorId,runTs</code> <br><strong>Usage:</strong> tools import this CSV to attach diagnostics to support tickets and compliance trackers. </td></tr><tr><td data-label="PQ_Diagnostics — Per-function Expert Technical Breakdown"> <strong>Closing operational constraint (must not be bypassed):</strong><br>All processes that produce artifacts consumed by other modules must: persist job descriptors, seed deterministic RNGs from <code>correlationId</code> for operator-visible sampling, use <code>AtomicWrite</code> for final artifacts, and emit required audit rows. This constraint is mandatory for regulated or PII-touching workflows and enforced by CI static checks and production monitors. </td></tr></tbody></table></div><div class="row-count">Rows: 38</div></div><div class="table-caption" id="Table3" data-table="Docu_0180_03" style="margin-top:2mm;margin-left:3mm;"><strong>Table 3</strong></div>
<div class="table-wrapper" data-table-id="table-3"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by PQ_Export — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">PQ_Export — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="PQ_Export — Per-function Expert Technical Breakdown"> <strong>Module-level metadata (contract & overview):</strong><br><strong>Owner:</strong> TEAM_PQ_EXPORT — documented in OWNERS.md with on-call rotation, approvers, and escalation contacts listed in release manifests. <br><strong>Public API (complete surface):</strong> PrepareExportPayload, ComputeMChecksum, CollectPQDiagnostics, PackageExportArtifact, AtomicExport, VerifyExport, StageLocalFallback, RetryExport, EmitExportAuditRow, WriteExportManifest, ValidateExportDestination, EncryptAndSignArtifact, InspectTempArtifacts, RepairExportArtifact, RotateExportRetention, ExportConnectionsList, ExportQueryList, ExportTemplateArchive, ExportReports, ExportIdempotencyKey, ExportSigningKeyRotate, ExportEvidencePublish. <br><strong>Audits emitted:</strong> pq_export.attempt, pq_export.completed, pq_export.failure, pq_export.verification_failed, pq_export.degraded, pq_export.stage_local, pq_export.manifest_written, pq_export.temp_artifact_found, pq_export.repair, pq_export.rotate_retention, pq_export.prepare.start/complete/failure, pq_export.package.attempt/completed, pq_export.atomic.attempt/completed/failure, pq_export.verify.attempt/result, pq_export.diagnostics.collected. Every audit row includes: correlationId, module=PQ_Export, procedure, paramsHash, artifactChecksum (where applicable), destinationUri (sanitized), timestamp, operatorId (if interactive), evidenceRef when full payload or diagnostics are stored externally. <br><strong>Purpose & intended use:</strong> orchestrate deterministic, auditable, crash-safe export of Power Query artifacts (M queries, parameter manifests, connection descriptors) and diagnostics. Provide canonical packaging, compute stable fingerprints (mChecksum and artifactChecksum), write artifacts atomically to durable storage (with staged fallback), verify integrity, sign/encrypt when required by policy, and emit audits and manifests for downstream reconciliation and compliance. Intended to run in worker/agent contexts; not on the ribbon UI thread. <br><strong>Non-goals / constraints:</strong> not responsible for credential lifecycle management (use CORE_CredentialVault); not a general-purpose long-term archive engine (use CORE_Storage for lifecycle beyond export retention); not performing heavy interactive UI operations; will not attempt to recover corrupted remote stores automatically beyond safe staged retransfer patterns; must not include raw PII in top-level audit fields (use evidenceRef for detailed payloads). </td></tr><tr><td data-label="PQ_Export — Per-function Expert Technical Breakdown"> <strong>Operational guarantees (module-level invariants & SLOs):</strong><br>1. Determinism: identical canonical inputs, correlationId, and configuration produce identical artifact bytes and checksums across supported hosts. <br>2. Atomicity: consumers observe either the previous artifact or the new artifact — never a truncated file. <br>3. Audit-anchored: each export attempt emits at least one audit row anchored to correlationId and paramsHash. <br>4. Verification: artifacts are checksummed and verified post-write; any mismatch triggers controlled rollback and an explicit verification failure audit. <br>5. Degraded path: when atomic replace semantics unavailable (e.g., some NFS/SMB deployments), module falls back to documented staged-local flows and emits pq_export.degraded with rationale. <br>6. UI safety: no long blocking IO on UI thread; export orchestration must occur in background worker processes or scheduled jobs. <br><strong>SLOs:</strong> median atomic export for 1MB artifact on local SSD <200ms; verify pass rate >99.9% in healthy infra; stage-local fallback success >99% when remote is degraded. <br><strong>CI / acceptance gates:</strong> cross-platform golden artifact parity, manifest write atomicity tests, degraded-path tests (simulated NFS failure), signature verification tests, audit emission verification, forbidden-API static checks. </td></tr><tr><td data-label="PQ_Export — Per-function Expert Technical Breakdown"> <strong>High-level workflow & invariants (conceptual runbook):</strong><br>1. Prepare & canonicalize artifact inputs (PrepareExportPayload). <br>2. Compute stable mChecksum for template + params (ComputeMChecksum). <br>3. Collect host/query diagnostics (CollectPQDiagnostics). <br>4. Package canonical contents into deterministic archive (PackageExportArtifact). <br>5. Validate destination (ValidateExportDestination). <br>6. Use AtomicExport to write artifact atomically; on failure, use RetryExport orchestration. <br>7. Verify exported artifact (VerifyExport). <br>8. Persist manifest atomically (WriteExportManifest). <br>9. Emit export audit rows linking artifactChecksum, mChecksum, manifestChecksum. <br>10. If primary path unavailable, StageLocalFallback and schedule retransfer job using ExportIdempotencyKey. <br><strong>Cross-cutting invariants:</strong> deterministic RNG seeded from correlationId for sampling; any numeric-critical transforms must be offloaded to SafeRound flows; all long-lived state writes accompanied by audit rows and optional evidenceRef to encrypted evidence storage. </td></tr><tr><td data-label="PQ_Export — Per-function Expert Technical Breakdown"> <strong><code>PrepareExportPayload(inputs, templateMeta, diagnostics=null, requireHighPrecision=false)</code></strong><br><strong>Purpose & contract:</strong> produce a canonical, deterministic payload from M queries and parameter metadata suitable for packaging. Must be pure and side-effect free. Ensures stable normalization (syntax/whitespace/Unicode), deterministic parameter ordering, and optional decimal normalization for high-precision templates. Returns {canonicalMMap, paramsManifest, diagnosticsBlob, payloadFingerprint}. <br><strong>Parameters & return:</strong> <br>• <code>inputs</code>: list of query objects: {name, originalM, parameters}. <br>• <code>templateMeta</code>: {templateId, templateVersion, ownerId, packagingPolicy}. <br>• <code>diagnostics</code>: optional diagnostics snapshot collected by host (may be null). <br>• <code>requireHighPrecision</code>: boolean. <br>Return: payload object with canonicalized M texts keyed by query name, paramsManifest JSON string with stable ordering, optional diagnosticsBlob reference, and payloadFingerprint = sha256hex(canonicalM + paramsManifest + diagnosticsBlob) computed deterministically. <br><strong>Primary invariants (must/shall):</strong><br>1. Deterministic canonicalization: same inputs -> same canonical outputs. <br>2. No time-variant metadata in canonicalized blobs (strip mtimes, host-specific IDs). <br>3. PII handling: sanitize sensitive param values and produce evidenceRef for encrypted full params. <br><strong>Algorithm & implementation notes:</strong><br>1. Parse M via a parser-aware normalization step to avoid accidental semantic changes; do not use naive regexes. <br>2. Normalize Unicode to NFC and line endings to LF. <br>3. Remove comment blocks that are explicitly flagged as non-authoritative or developer comment-only regions, but preserve comment markers for audit traceability. <br>4. Normalize numeric literals: when requireHighPrecision true annotate decimals with explicit metadata (scale) and include decimal normalization map in diagnosticsBlob. <br>5. Serialize paramsManifest to compact, canonical JSON with stable key ordering and deterministic number formatting (e.g., fixed exponent notation rules). <br>6. Compute payloadFingerprint as streaming SHA256 to support large inputs. <br><strong>Edge cases & invalid inputs:</strong><br>1. Unparseable M text: return EX_PQ_PREP_PARSE_FAIL with error position and emit pq_export.prepare.failure. <br>2. Excessively large artifacts (> configured cap): return EX_PQ_PREP_TOO_LARGE and suggest chunking. <br>3. Mixed encodings: coerce to UTF-8 or error EX_PQ_PREP_ENCODING_FAIL. <br><strong>Observability & audit fields:</strong> pq_export.prepare.start(correlationId, templateId, payloadCandidateHash) and pq_export.prepare.complete(correlationId, payloadFingerprint, durationMs). On error emit pq_export.prepare.failure with evidenceRef. <br><strong>Examples & narratives:</strong><br>1. Template preview edited by operator: PrepareExportPayload canonicalizes edited M ensuring that visual whitespace changes do not alter canonical payloadFingerprint used to prove that injected query equals the exported artifact. <br>2. High-precision finance template: requireHighPrecision true causes decimal map to be included and recorded so rounding decisions can be replayed for compliance. <br><strong>Tests & CI vectors:</strong><br>1. Unicode normalization golden vectors. <br>2. Parser re-serialization round-trip test: parse canonicalM -> reserialize -> identical string. <br>3. Fuzzed invalid M inputs verifying safe error reporting. </td></tr><tr><td data-label="PQ_Export — Per-function Expert Technical Breakdown"> <strong><code>ComputeMChecksum(canonicalMMap, paramsManifest)</code></strong><br><strong>Purpose & contract:</strong> compute a stable <code>mChecksum</code> fingerprint for a template package formed by canonicalMMap and paramsManifest. <code>mChecksum</code> is the canonical fingerprint used to track template versions and map to provenance. Implementation must produce identical results across supported languages and OSes. <br><strong>Parameters & return:</strong> canonicalMMap: map of queryName->canonicalM; paramsManifest: canonical JSON string. Returns {mChecksum: sha256hex, mShort: first12chars}. <br><strong>Primary invariants:</strong><br>1. Input must be canonicalized via PrepareExportPayload. <br>2. Hash computation uses canonical concatenation order: alphabetically ordered query names; each canonicalM is prefixed with a stable header (e.g., "===query:<name>===\n"). <br><strong>Algorithm & implementation notes:</strong><br>1. Use streaming SHA256 to handle large query sets. <br>2. Combine inputs as: for each queryName in sorted(canonicalMMap): append header + canonicalM; after queries append paramsManifest with an explicit newline separator. <br>3. Return hex digest lowercase; compute short reference for UI caches. <br><strong>Edge cases:</strong><br>1. Empty template (no queries): defined mChecksum = SHA256("EMPTY_TEMPLATE_v1") to avoid nulls. <br>2. Non-UTF8 bytes: reject with EX_PQ_CHECKSUM_ENCODING_FAIL. <br><strong>Observability & audit fields:</strong> pq_export.mchecksum(correlationId, templateId, mChecksum). <br><strong>Tests:</strong> cross-language parity golden tests for known canonical inputs. </td></tr><tr><td data-label="PQ_Export — Per-function Expert Technical Breakdown"> <strong><code>CollectPQDiagnostics(connectionHandles, queries, refreshContext, samplePolicy)</code></strong><br><strong>Purpose & contract:</strong> collect structured diagnostics for a preview/export run including provider traces, query step timings, folding metadata, error stacks, and refresh path sequences. When volume of traces is large apply deterministic sampling seeded by correlationId. Diagnostics must be scrubbed to remove credentials and secrets; any sanitized portion must be stored separately encrypted and referenced by evidenceRef. <br><strong>Parameters & return:</strong> connectionHandles: list of connections; queries: list of query descriptors; refreshContext: {hostOs, powerQueryVersion, providerVersions, correlationId}; samplePolicy: {maxTraces, samplingSeed=null}. Returns diagnosticsBlob (possibly compressed) and diagnosticsFingerprint. <br><strong>Primary invariants (must/shall):</strong><br>1. Deterministic sampling: when sampling is needed, use DeterministicRNG seeded from correlationId so replay reproduces same sample. <br>2. No secrets: scrub credentials, tokens; replace with scrubbed placeholders and produce evidenceRef for full encrypted traces when required. <br><strong>Algorithm & implementation notes:</strong><br>1. Capture per-query metrics: loadTimeMs, foldHint, provider, step-level timings, lastError message (redacted), and provider trace snippets. <br>2. For provider-level network traces capture only metadata and error frames; avoid storing connection strings or raw tokens. <br>3. Compress large traces and store in evidence store with evidenceRef referenced in diagnosticsBlob when size exceeds threshold. <br><strong>Edge cases:</strong><br>1. Unknown provider: produce minimal stub and emit pq_export.diagnostics.partial. <br>2. Provider returns binary traces: compress and record evidenceRef. <br><strong>Observability & audit fields:</strong> pq_export.diagnostics.collected(correlationId, diagnosticsFingerprint, queriesCount, evidenceRefOptional). <br><strong>Examples:</strong> a PQ refresh failing on OLEDB provider yields diagnosticsBlob capturing step timings and provider error codes enabling SRE to triage underlying data source issues. <br><strong>Tests:</strong> scrub tests verifying no credentials in diagnostics; parity tests under deterministic sampling. </td></tr><tr><td data-label="PQ_Export — Per-function Expert Technical Breakdown"> <strong><code>PackageExportArtifact(payload, mChecksum, diagnosticsBlob=null, manifestFields={})</code></strong><br><strong>Purpose & contract:</strong> produce a deterministic archive (artifact) containing canonicalM files, paramsManifest, optional diagnostics, and manifest stub. Artifact bytes must be canonical across hosts (no variable mtimes or host-specific metadata) and include a packagingVersion to allow future format evolution. Returns artifactStream (seekable), artifactSizeBytes, artifactFingerprintCandidate (sha256hex). <br><strong>Packaging rules & invariants:</strong><br>1. Deterministic archive: file ordering fixed, metadata normalized (mtime=0, uid/gid stripped or normalized), compression deterministic (fixed compression level if used). <br>2. Manifest included as first entry: manifest stub contains correlationId, mChecksum, packagingVersion, templateVersion, ownerId, packagingPolicy. <br>3. Archive format policy: prefer deterministic ZIP with normalized headers; alternative tar+gz with deterministic headers permitted but must be signaled in packagingVersion. <br><strong>Algorithm & implementation notes:</strong><br>1. Build archive in streaming manner computing SHA256 while writing. <br>2. Files layout example: manifest.json, queries/<name>.m, params.json, diagnostics/<fingerprint>.json (if present), reports/. <br>3. Use canonical header for each file (no timestamps, deterministic perms). <br><strong>Edge cases & fallbacks:</strong><br>1. Host lacks deterministic ZIP implementation: implement deterministic tar writer or produce content manifest JSON + flat file set; emit pq_export.package.degraded and add rationale. <br>2. Artifact size > allowed thresholds: split into chunked package files and produce top-level reassembly manifest. <br><strong>Observability & audit fields:</strong> pq_export.package.attempt(correlationId, artifactCandidateFingerprint, artifactSizeBytes). pq_export.package.completed(correlationId, artifactFingerprintCandidate). <br><strong>Examples:</strong> packaging step ensures identical artifact bytes produced in CI golden run and on operator workstation — allowing byte-for-byte provenance checks. <br><strong>Tests:</strong> cross-platform byte-for-byte parity tests for representative template sets; reassembly tests for chunked artifacts. </td></tr><tr><td data-label="PQ_Export — Per-function Expert Technical Breakdown"> <strong><code>AtomicExport(artifactStream, artifactPath, tmpSuffix=&quot;.pqpart&quot;, fsyncFile=true, fsyncParent=true, perms=null, maxAttempts=3)</code></strong><br><strong>Purpose & contract:</strong> write artifact to <code>artifactPath</code> using crash-safe atomic semantics. After success, <code>artifactPath</code> must contain the new artifact with verified checksum, and no caller should observe a truncated (partial) artifact. Return {success, artifactPath, artifactChecksum, attempts, elapsedMs}. On failure emit pq_export.atomic.failure with diagnostics. <br><strong>Detailed step-by-step behavior (must/shall):</strong><br>1. Call ValidateExportDestination to ensure path canonical and acceptable. <br>2. Compute tempPath = artifactPath + tmpSuffix + "." + pid + "." + deterministicSuffix. <br>3. Open tempPath with exclusive create flags (O_EXCL); stream artifact bytes while computing SHA256; optionally write sidecar metadata file with payloadFingerprint, correlationId and packagingVersion. <br>4. If fsyncFile true ensure fdatasync/fsync called on file descriptor after write completes. <br>5. Use atomic replace semantics: os.replace on POSIX or ReplaceFile/MoveFileEx with appropriate flags on Windows. <br>6. If fsyncParent true, fsync parent directory to ensure rename persistence. <br>7. Re-open artifactPath read-only and compute SHA256 to verify artifactChecksum equals computed checksum. <br>8. On verification mismatch perform controlled rollback, attempt repair from tempPath if possible, and emit pq_export.verification_failed. <br>9. On transient errors (EINTR, EIO, ETIMEDOUT), apply RetryExport wrapper according to retry/backoff policy. <br><strong>Cross-platform concerns & documented fallbacks:</strong><br>1. Network filesystems with weak rename semantics: detect mountType via ValidateExportDestination; if not safe, return E_DEGRADED and recommend StageLocalFallback. <br>2. Windows locked-file semantics: handle SHARE_DENY and implement retry/backoff with escalating operator notification. <br>3. Cross-device renames: copy-and-rename-safe-sequence with atomic marker file approach and pq_export.degraded audit. <br><strong>Recovery & runbook:</strong><br>1. If temp artifacts persist on failure, call InspectTempArtifacts and present candidate temp copies for repair. <br>2. For ENOSPC: emit pq_export.atomic.ENOSPC with mount path and freeBytes; optionally trigger StageLocalFallback. <br>3. For permission denied: emit pq_export.atomic.EPERM with ACL snapshot evidenceRef. <br><strong>Observability & audit fields:</strong> pq_export.atomic.attempt(correlationId, artifactPath, tempPath, payloadFingerprint) pq_export.atomic.completed(correlationId, artifactPath, artifactChecksum, durationMs, attempts) pq_export.atomic.failure(correlationId, artifactPath, errorCode, attempts, diagnosticsRef). <br><strong>Tests & CI:</strong> simulate rename/fsync failures via filesystem mocks; concurrency tests with concurrent readers to verify there are no partial reads observed; cross-OS tests for replace semantics. </td></tr><tr><td data-label="PQ_Export — Per-function Expert Technical Breakdown"> <strong><code>VerifyExport(artifactPath, expectedChecksum, allowRepair=false)</code></strong><br><strong>Purpose & contract:</strong> recompute artifact checksum and compare to expected; if mismatch and allowRepair true attempt repair using available temp artifacts. Return {verified: bool, actualChecksum, repairAttempted: bool, repairResult}. <br><strong>Steps:</strong><br>1. Stream-read artifactPath computing SHA256; compare to expectedChecksum. <br>2. If matches, emit pq_export.verify.result verified. <br>3. If mismatch and allowRepair true: InspectTempArtifacts for candidate temp copies; for each candidate compute checksum and if matches expectedChecksum attempt atomic replace via RepairExportArtifact. <br>4. If repair fails, emit pq_export.verification_failed and escalate if artifact marked regulated. <br><strong>Edge cases:</strong><br>1. Corrupt reads due to partial writes: ensure AtomicExport invariants; if detected, escalate to SRE with forensic_manifest. <br><strong>Observability & audit fields:</strong> pq_export.verify.attempt(correlationId, artifactPath, expectedChecksum) pq_export.verify.result(correlationId, artifactPath, verified, actualChecksum, durationMs). <br><strong>Tests:</strong> inject corrupted bytes into artifact and assert verify detects mismatch and repair path works when temp copy available. </td></tr><tr><td data-label="PQ_Export — Per-function Expert Technical Breakdown"> <strong><code>StageLocalFallback(artifactStream, stagePath, policy={})</code></strong><br><strong>Purpose & contract:</strong> persist artifact locally when primary destination unavailable; persist using local AtomicWrite semantics and emit pq_export.stage_local with reason and stagedChecksum. Schedules retransfer job with ExportIdempotencyKey for eventual finalization. <br><strong>Behavior & steps:</strong><br>1. Choose stagePath preferably on same volume as retransfer worker to avoid cross-device copy. <br>2. Write using AtomicExport semantics on local FS; compute stagedChecksum. <br>3. Emit pq_export.stage_local(correlationId, stagePath, stagedChecksum, reason). <br>4. Persist a retransferDescriptor (AtomicWrite) containing stagePath, targetDestination, ExportIdempotencyKey, and schedule time. <br>5. For regulated artifacts require two-person approval audit row before stage-local permitted. <br><strong>Governance & security:</strong> stage directories must be access-controlled and encrypted-at-rest according to policy; stage retention limited and managed by RotateExportRetention. <br><strong>Observability & audit fields:</strong> pq_export.stage_local(correlationId, stagePath, stagedChecksum, reason). <br><strong>Tests:</strong> simulate remote store outages and verify stage-local write and scheduled retransfer artifacts. </td></tr><tr><td data-label="PQ_Export — Per-function Expert Technical Breakdown"> <strong><code>RetryExport(fn_export, retries=3, backoff={baseMs:200, factor:2}, jitter=true, deterministic_jitter=false, idempotencyKey=null, cancelToken=null)</code></strong><br><strong>Purpose & contract:</strong> robust retry orchestration wrapper to handle transient export failures while preserving idempotency semantics. Emit pq_export.retry.attempt and pq_export.retry.complete audits. <br><strong>Behavioral rules & safeguards:</strong><br>1. Only retry on exceptions classified as transient in ExportError taxonomy. <br>2. If idempotencyKey provided ensure fn_export is safe to re-run or external idempotency guard present. <br>3. If deterministic_jitter true use DeterministicRNG seeded from correlationId to compute jitter for CI repeatability. <br>4. Honor cancelToken to abort early and emit pq_export.retry.cancelled. <br><strong>Backoff semantics:</strong><br>1. backoffMs = baseMs * factor<strong>(attemptIndex-1) with jitter applied. <br>2. Limit total retry budget per-call to avoid worker starvation. <br></strong>Observability & audit fields:<strong> pq_export.retry.attempt(correlationId, target, attemptIndex, errorCode, backoffMs) pq_export.retry.complete(correlationId, target, attempts, outcome, elapsedMs). <br></strong>Tests & policy enforcement:** static analyzer rejects RetryExport usage from UI thread; CI uses deterministic_jitter for golden timing tests. </td></tr><tr><td data-label="PQ_Export — Per-function Expert Technical Breakdown"> <strong><code>EmitExportAuditRow(correlationId, operatorId, procedure, paramsHash, resultHash=null, artifactChecksum=null, destinationUri=null, evidenceRef=null, metadata={})</code></strong><br><strong>Purpose & contract:</strong> canonical method to write an audit row for PQ_Export with consistent schema and PII constraints. Writes to CORE_Audit append-only buffer and persists audit_tail locally for immediate forensic needs. <br><strong>Schema fields:</strong> timestamp, correlationId, module=PQ_Export, procedure, operatorId, paramsHash, resultHash, artifactChecksum, destinationUri(sanitized), evidenceRef, durationMs, errorCode (optional), metadata object. <br><strong>Constraints:</strong> do not include raw PII in top-level fields; when full parameters or raw traces are needed store them encrypted in evidence store and reference with evidenceRef. <br><strong>Behavior & notes:</strong> audit append must be low-latency and non-blocking; for critical failures perform synchronous flush to local durable buffer to ensure persisted trail even if downstream telemetry fails. <br><strong>Observability:</strong> audit write returns auditRowId and optional evidenceRef pointer for external evidence. <br><strong>Tests:</strong> audit schema conformance tests and retention verification tests in CI. </td></tr><tr><td data-label="PQ_Export — Per-function Expert Technical Breakdown"> <strong><code>WriteExportManifest(artifactPath, artifactChecksum, manifestPath, manifestFields)</code></strong><br><strong>Purpose & contract:</strong> atomically persist final manifest JSON mapping artifact to provenance metadata. Must be written atomically and include configHash and correlationId. Returns manifestChecksum. <br><strong>Required manifest fields:</strong> correlationId, artifactPath, artifactChecksum, mChecksum, packagingVersion, templateVersion, ownerId, operatorId (optional), configHash, signedBy (if signature applied), packagingPolicy, timestamp. <br><strong>Behavior & steps:</strong><br>1. Build manifest JSON with canonical key order and deterministic formatting. <br>2. Persist manifest via AtomicExport (AtomicWrite) to manifestPath to ensure either old or new manifest persists. <br>3. Optionally sign manifest using EncryptAndSignArtifact signing flow and include signatureRef in manifest. <br>4. Emit pq_export.manifest_written(correlationId, manifestPath, manifestChecksum). <br><strong>Edge cases:</strong> manifest write failure after artifact persisted must produce pq_export.manifest_missing audit and schedule compensating job to re-attempt manifest write. <br><strong>Tests:</strong> simulate manifest write failure and assert compensating behavior. </td></tr><tr><td data-label="PQ_Export — Per-function Expert Technical Breakdown"> <strong><code>ValidateExportDestination(destinationUri, requireExists=true, requireWrite=true, artifactSizeBytes=null)</code></strong><br><strong>Purpose & contract:</strong> pre-flight validation of export destination: path canonicalization, permission check, free-space check, and semantics detection (local, NFS, SMB, cloud bucket). Return validatedDestination object with canonicalPath, mountType, supportsAtomicReplace boolean, freeBytes. <br><strong>Checks performed:</strong><br>1. Path canonicalization (no traversal, normalized symlinks). <br>2. Existence check if required. <br>3. Write/replace permission check for effective user. <br>4. If artifactSizeBytes provided check free space and enforce ENOSPC policy. <br>5. Detect mount type and atomic replace support heuristics. <br><strong>Behavior on mismatch:</strong><br>1. If insufficient free space return EX_PQ_DEST_ENOSPC with mount path and freeBytes. <br>2. If mount lacks atomic replace for regulated artifacts return EX_PQ_DEST_NOT_SAFE and recommend StageLocalFallback. <br><strong>Observability:</strong> pq_export.destination.validated(correlationId, destinationUri, mountType, freeBytes) <br><strong>Tests:</strong> path traversal attack vectors, permission denial and insufficient space cases. </td></tr><tr><td data-label="PQ_Export — Per-function Expert Technical Breakdown"> <strong><code>EncryptAndSignArtifact(artifactStream, encryptionPolicy=null, signingKeyRef=null)</code></strong><br><strong>Purpose & contract:</strong> optional encryption/signing of artifact for confidentiality and non-repudiation per packagingPolicy. Encryption uses envelope encryption (ephemeral AES-GCM data key wrapped with KMS); signing uses detached signature persisted as signatureRef in manifest. Return {transformedArtifactStream, postTransformChecksum, signatureRef, evidenceRef}. <br><strong>Security constraints:</strong> private signing keys in HSM or CORE_KeyVault; signing performed in secure worker context; do not store signing keys in local disk. <br><strong>Behavior:</strong><br>1. If encryptionPolicy set: generate ephemeral data key, encrypt stream using AES-GCM while streaming, wrap data key via KMS and include key-wrap metadata in manifest. <br>2. Compute checksum of encrypted artifact as artifactChecksumPostEncryption. <br>3. If signingKeyRef provided: compute detached signature over final artifact bytes and persist signatureRef. <br><strong>Observability & audit fields:</strong> pq_export.encrypt.attempt, pq_export.sign.attempt, pq_export.encrypt.completed, pq_export.sign.completed with evidenceRef to key-wrap metadata. <br><strong>Edge cases & guidance:</strong><br>1. Ensure manifest includes key-wrap metadata and signatureRef. <br>2. For downstream consumers require public key or verification endpoint. <br><strong>Tests:</strong> round-trip encryption/decryption with mocked KMS, signature verification tests, negative tests for missing key permissions. </td></tr><tr><td data-label="PQ_Export — Per-function Expert Technical Breakdown"> <strong><code>InspectTempArtifacts(baseTempDir, correlationId=null, tmpSuffix=&quot;.pqpart&quot;)</code></strong><br><strong>Purpose & contract:</strong> scan temp directories for artifacts left by failed AtomicExport attempts, surface candidate temp copies for repair or forensic capture. Return listing with tempPaths, size, ownerPid, ageMs, candidatePayloadFingerprint computed where readable. <br><strong>Behavior:</strong><br>1. Enumerate temp files matching tmpSuffix pattern. <br>2. For each candidate attempt to compute payloadFingerprint (streaming hash) if readable. <br>3. Emit pq_export.temp_artifact_found(correlationId, tempPath, candidateFingerprint, ageMs). <br><strong>Repair guidance:</strong> only attempt automatic repair when ownership and permissions validated and within maintenance window. Otherwise persist forensic manifest and notify SRE. <br><strong>Tests:</strong> create temp artifacts of varying ages and verify detection. </td></tr><tr><td data-label="PQ_Export — Per-function Expert Technical Breakdown"> <strong><code>RepairExportArtifact(tempPath, targetPath, allowOverwrite=false, dryRun=true)</code></strong><br><strong>Purpose & contract:</strong> attempt to recover final artifact from a verified temp copy and atomically place it into <code>targetPath</code>. Must validate checksum and operator approvals before overwriting. Returns repairResult {repaired, reason, beforeChecksum?, afterChecksum?}. <br><strong>Safety rules:</strong><br>1. Require maintenance window or operator approval for cross-process overwrite in production. <br>2. Verify temp candidate checksum matches manifest expectation before replacing. <br>3. If dryRun true only report actions without performing replace. <br><strong>Behavior & steps:</strong><br>1. Read tempPath compute checksum. <br>2. Compare to expectedChecksum from manifest or artifact sidecar. <br>3. If matches and allowOverwrite true perform AtomicExport-style atomic replace (rename) with proper fsyncs. <br>4. Emit pq_export.repair(correlationId, tempPath, targetPath, result). <br><strong>Tests:</strong> repair success for valid temp copy and safe no-op when dryRun. </td></tr><tr><td data-label="PQ_Export — Per-function Expert Technical Breakdown"> <strong><code>RotateExportRetention(exportRoot, policy)</code></strong><br><strong>Purpose & contract:</strong> implement retention lifecycle for export artifacts and manifests. Delete or archive artifacts per retention buckets; preserve artifacts flagged as regulatory or protected. Emit pq_export.rotate_retention audit summarizing actions. <br><strong>Policy-driven steps:</strong><br>1. Enumerate artifacts and manifests under exportRoot. <br>2. Compute age and retention bucket per policy rules. <br>3. For deletions record forensic_manifest entries and perform secure delete or archive to warm/cold store; for archives write archive manifest and perform AtomicExport to archive store. <br>4. Emit pq_export.rotate_retention(correlationId, deletedCount, archivedCount, retainedProtectedCount). <br><strong>Edge cases:</strong><br>1. External references: if artifact referenced by other services mark retain and emit retention_conflict audit. <br><strong>Tests:</strong> simulation with mock artifacts and protected flags verifying no unintended deletions. </td></tr><tr><td data-label="PQ_Export — Per-function Expert Technical Breakdown"> <strong>Domain helper functions & convenience helpers:</strong><br><strong>ExportConnectionsList(connectionDescriptors, manifestFields)</strong> — produce canonical connections list with stable ordering, excluding raw credentials. <br><strong>ExportQueryList(queries, includeDependencies=false)</strong> — exports query names, mChecksum references, dependency graph, folding hints. <br><strong>ExportTemplateArchive(librarySelection)</strong> — packages multiple templates into a single export artifact with manifest mapping templateId -> mChecksum; supports per-template packagingVersion and per-template evidenceRefs. <br><strong>ExportReports(reportObjects)</strong> — generate human-readable HTML/PDF reports packaged alongside artifact; PDFs must be produced via canonical templating to ensure deterministic output where required. <br><strong>ExportIdempotencyKey(jobDescriptor)</strong> — compute deterministic idempotency key from correlationId + jobDescriptorHash enabling safe retries and duplicate-suppression by job scheduler. </td></tr><tr><td data-label="PQ_Export — Per-function Expert Technical Breakdown"> <strong>Cross-cutting Observability, Telemetry & Error catalog (concepts & mapping)</strong><br><strong>Audit schema for PQ_Export:</strong> each audit row includes: timestamp, correlationId, module=PQ_Export, procedure, operatorId (where interactive), paramsHash, resultHash (optional), artifactChecksum, destinationUri (sanitized), evidenceRef, durationMs, attempts, configHash, metadata. Full parameter dumps are stored encrypted in evidence store referenced by evidenceRef. <br><strong>Representative ErrorCodes & operator guidance:</strong><br>1. PQ_EXPORT_ENOSPC — run retention/space mitigation, StageLocalFallback. <br>2. PQ_EXPORT_EPERM — check ACLs, run permission remediation. <br>3. PQ_EXPORT_VERIFICATION_FAILED — InspectTempArtifacts, attempt RepairExportArtifact, re-run packaging if needed. <br>4. PQ_EXPORT_DEGRADED — treat artifact as staged-only until retransfer completed; schedule retransfer job. <br>5. PQ_EXPORT_ELOCKED — identify locking process, retry with backoff or escalate to SRE. <br>6. PQ_EXPORT_PARSE_FAIL — fix M text; emit prepare.failure and stop. <br><strong>Metrics:</strong> pq_export.latency_ms, pq_export.success_rate, pq_export.degraded_rate, pq_export.verify_fail_rate, pq_export.retry_count. Metrics buffered locally and exported by CORE_Telemetry in audited batches. <br><strong>Evidence policy:</strong> top-level audit rows store param hashes only; full sanitized parameters and raw diagnostics stored encrypted in evidence store (hot/warm archive) and referenced by evidenceRef. PII must never be present in top-level audit fields. </td></tr><tr><td data-label="PQ_Export — Per-function Expert Technical Breakdown"> <strong>Testing matrix, property tests, and cross-language golden governance (comprehensive)</strong><br><strong>Unit tests (must include):</strong><br>1. PrepareExportPayload canonicalization vectors for M constructs, Unicode, and param permutations. <br>2. ComputeMChecksum parity across supported languages and OSes for canonical inputs. <br>3. CollectPQDiagnostics scrub tests ensuring no credentials persist and deterministic sampling parity. <br>4. PackageExportArtifact deterministic archive bytes across OSes. <br>5. AtomicExport rename/fsync failure simulations using FS mocks. <br>6. VerifyExport mismatch and repair flows with temp artifacts. <br>7. StageLocalFallback behavior and retransfer descriptor persistence. <br><strong>Integration tests:</strong><br>1. End-to-end export: prepare -> checksum -> package -> validate -> atomic export -> verify -> manifest write -> audit emission. <br>2. Fault-injection tests: simulate ENOSPC, locked files, NFS rename anomalies; ensure degraded path and stage-local fallback exercised. <br>3. Signature & encryption round-trip with mocked KMS/HSM. <br><strong>Property tests:</strong><br>1. Deterministic artifact bytes under canonical input property test across random small variations. <br>2. Idempotency invariants: repeated runs with same ExportIdempotencyKey produce single authoritative artifact and idempotency suppression behaviour in job scheduler. <br><strong>Performance tests:</strong><br>1. Packaging throughput for artifacts at 1MB, 10MB, 100MB sizes. <br>2. AtomicExport latency p50/p95 under local SSD and network mount. <br><strong>CI golden gating:</strong><br>1. Golden artifact parity tests for regulated templates — any change to packaging must include migration manifest and owner approval. <br>2. Static analyzer forbids UI-thread IO operations. </td></tr><tr><td data-label="PQ_Export — Per-function Expert Technical Breakdown"> <strong>Developer guidance, allowed & forbidden patterns (explicit)</strong><br><strong>Required usage patterns:</strong><br>1. Always use PrepareExportPayload + ComputeMChecksum before packaging to ensure canonical fingerprints. <br>2. Always persist artifacts using AtomicExport semantics (not raw writes) when artifact will be consumed by other processes. <br>3. Always write manifest with WriteExportManifest and emit audit via EmitExportAuditRow. <br>4. Use ExportIdempotencyKey for retryable orchestrated exports; use deterministic_jitter in CI for repeatability. <br><strong>Forbidden patterns:</strong><br>1. Do not write raw credentials into diagnostics or manifests. <br>2. Do not perform long-running IO on UI thread; static analyzer must reject such PRs. <br>3. Do not rely on host-specific rename semantics without fallback; do not call os.rename on network mounts without ValidateExportDestination checks. <br><strong>Code-review checklist:</strong> ensure audit emission, AtomicExport usage, manifest write, evidenceRef usage for sensitive payloads, idempotency token presence for retriable flows, and signing fields present if packagingPolicy requires signatures. </td></tr><tr><td data-label="PQ_Export — Per-function Expert Technical Breakdown"> <strong>Operational runbooks & incident playbooks (executable steps)</strong><br><strong>AtomicExport ENOSPC runbook:</strong><br>1. Query pq_export.atomic.failure for correlationId and mount path; collect freeBytes snapshot from audit. <br>2. SSH to host and run df -h and iostat for mount; collect vmstat and kernel logs for timeframe. <br>3. If staging allowed, run exports stage-local --artifact <id> to move artifact to secure staging and re-run retransfer job. <br>4. If persistent, escalate to infra with forensic_manifest and audit_tail. <br><strong>Verification failure triage:</strong><br>1. Query pq_export.verification_failed audits for expectedChecksum and artifactPath. <br>2. Run VerifyExport in dry-run and InspectTempArtifacts for candidate temp copies. <br>3. If repair possible, schedule repair under maintenance window and re-verify; else re-run packaging and export. <br><strong>Stage-local handling for regulated artifacts:</strong><br>1. Require two-person approval before StageLocalFallback; record approval in audit rows. <br>2. Mark staged artifact as non-authoritative and schedule retransfer; notify downstream consumers. <br><strong>When to call SRE:</strong> repeated ENOSPC for critical export or persistent verification fail after repair attempts; include forensic_manifest and temp artifact listing in ticket. </td></tr><tr><td data-label="PQ_Export — Per-function Expert Technical Breakdown"> <strong>Extremely detailed long-form narratives & examples (multiple scenarios)</strong><br><strong>Scenario 1 — Operator exports regulated PQ template to shared cloud repo (complete trace):</strong><br>1. Operator clicks "Export" in PQ_Ribbon; ribbon handler creates correlationId r-20260117-xyz and emits UserAction audit. <br>2. Orchestrator calls PrepareExportPayload(requireHighPrecision=true) and ComputeMChecksum; payloadFingerprint and mChecksum produced and recorded. <br>3. CollectPQDiagnostics captures last-refresh traces scrubbed of credentials; diagnosticsFingerprint produced. <br>4. PackageExportArtifact builds deterministic ZIP archive and computes artifactCandidateFingerprint. <br>5. ValidateExportDestination confirms cloud bucket supports atomic object replace (per provider) and free space quotas. <br>6. AtomicExport streams artifact to final URI using provider-specific atomic replace APIs; fsync semantics satisfied where supported. <br>7. VerifyExport recomputes checksum and matches expected; WriteExportManifest atomically persists manifest with signatureRef after signing via HSM. <br>8. EmitExportAuditRow writes pq_export.completed linking correlationId, mChecksum, artifactChecksum and manifestChecksum. <br>9. CI golden-run compares artifactChecksum to golden for regulated templates and blocks release if mismatch. <br><strong>Traceability:</strong> chain of evidence from correlationId -> jobDescriptor -> mChecksum -> artifactChecksum -> manifestChecksum -> audit rows enables full reproducibility and compliance proofs. <br><strong>Scenario 2 — NFS rename anomaly and stage-local fallback:</strong><br>1. AtomicExport rename fails with EINVAL due to NFS idiosyncrasy; module emits pq_export.degraded and falls back to StageLocalFallback. <br>2. StageLocalFallback writes artifact to local staging area using AtomicExport semantics on local disk, produces stagedChecksum, and emits pq_export.stage_local. <br>3. A retransfer job created with ExportIdempotencyKey reattempts transfer when remote becomes available; job uses RetryExport with idempotencyKey to ensure safe re-attempts. <br>4. Until finalization, consumers treat staged artifact as non-authoritative; manifest indicates staging state and operator approval where needed. <br><strong>Governance:</strong> degrade gracefully, ensure operators are notified and downstream consumers blocked from using staged artifact as authoritative. <br><strong>Scenario 3 — Deterministic replay for compliance and forensic reproduction:</strong><br>1. Export run persisted evidenceRefs for canonicalM, diagnosticsBlob, and any RNG state used for sampling. <br>2. Compliance request arrives; forensic tool downloads evidence, re-runs PrepareExportPayload and PackageExportArtifact in reproduce mode producing byte-identical artifact and artifactChecksum matching original, verifying integrity of pipeline. <br>3. If mismatch, forensic manifest includes audit_tail and packaging logs allowing engineers to pinpoint divergence (e.g., packagingVersion change or canonicalization bug). <br><strong>Narrative takeaways:</strong> record evidence, make packaging deterministic, and store enough metadata to reproduce exact artifact for audits. </td></tr><tr><td data-label="PQ_Export — Per-function Expert Technical Breakdown"> <strong>Conceptual Power Query (M) patterns — mapping PQ_Export principles to PQ workflows (detailed conceptual mapping)</strong><br><strong>Context:</strong> M execution and PQ hosts differ; PQ_Export cannot change M runtime semantics but provides authoritative packaging and persistence around M artifacts. <br><strong>Mapping patterns & recommendations:</strong><br>1. <strong>Atomic persistence mapping:</strong> PQ cannot guarantee atomic file replace; PQ_Ribbon must call a signed helper or worker to run AtomicExport. PQ templates should rely on worker-side persistence for authoritative artifact creation. <br>2. <strong>mChecksum & injection governance:</strong> template metadata must include <code>mChecksum</code> computed by ComputeMChecksum; injector must verify mChecksum before injecting to prevent tampering. <br>3. <strong>Diagnostics capture:</strong> host collects provider traces and supplies to CollectPQDiagnostics; M code remains pure and does not store provider secrets. <br>4. <strong>High-precision numeric templates:</strong> mark templates <code>requiresHighPrecision</code>; offload numeric rounding/aggregation to worker SafeRound primitives and include decimal normalization map in artifact. <br>5. <strong>Idempotent refresh & export:</strong> orchestrator persist jobDescriptor with ExportIdempotencyKey before export; retries use idempotencyKey to avoid duplicates. <br><strong>Operator narrative:</strong> when exporting a template, prepare canonical payload, compute mChecksum, package and atomically persist artifact, and verify checksum; if staging occurs, operator must approve for regulated artifacts. </td></tr><tr><td data-label="PQ_Export — Per-function Expert Technical Breakdown"> <strong>Conceptual DAX patterns — mapping PQ_Export metadata into semantic models & reports (detailed conceptual mapping)</strong><br><strong>Context:</strong> DAX is read-time only; it consumes persisted artifacts and metadata. Exports must persist RunMetadata that DAX can query for provenance. <br><strong>Patterns & recommended practices:</strong><br>1. <strong>RunMetadata persistence:</strong> Export pipeline persist RunMetadata row (correlationId, artifactChecksum, exportTs, mChecksum, manifestChecksum) as table consumable by semantic model; DAX measures can surface verification status. <br>2. <strong>Checksum reconciliation:</strong> During refresh, ETL writes expected artifactChecksum into model table; DAX measures compare to current loaded checksum allowing UI flags for verification. <br>3. <strong>Deterministic sampling for dashboards:</strong> sampling seeds persisted with evidenceRef allow reproducible sample-driven diagnostics surfaced in DAX-driven reports. <br>4. <strong>Avoid allocation logic in DAX:</strong> rounding and allocation must be resolved at ETL/export time (SafeRoundResiduals) and persisted as resolved numeric fields; DAX should present final values and provenance. <br><strong>Narrative example:</strong> Worker exports reconciled sales table via PQ_Export and writes RunMetadata including artifactChecksum; DAX measure <code>IsVerified</code> returns 1 when artifactChecksum equals expected value in model metadata, driving a verification indicator in report. </td></tr><tr><td data-label="PQ_Export — Per-function Expert Technical Breakdown"> <strong>Forensic artifacts, evidence paths & recommended retention (expanded)</strong><br><strong>Minimum forensic artifacts to collect for an export incident:</strong><br>1. jobDescriptor.json persisted via AtomicWrite containing jobId, correlationId, paramsHash, configHash and idempotencyKey. <br>2. artifact file(s) with SHA256 checksums and manifest.json. <br>3. audit_tail.csv rows for the correlationId containing pq_export.* and UserAction rows. <br>4. diagnosticsBlob and evidenceRef to raw traces (encrypted). <br>5. serialized RNG state used for deterministic sampling. <br>6. temp artifact listings and InspectTempArtifacts output. <br><strong>Evidence storage & retention patterns:</strong><br>1. Hot evidence store: <code>/evidence/hot/pq_export/&lt;correlationId&gt;/</code> for 30 days; limited access. <br>2. Warm archive: secure, signed archive for regulatory retention (7 years or per regulation). <br>3. Forensic_manifest.json enumerating artifact URIs, checksums, evidenceRefs, and chain-of-custody metadata. <br><strong>Retention cadence & verification:</strong> monthly retention verification job emits housekeeping.audit and performs proof-of-delete for removed artifacts. </td></tr><tr><td data-label="PQ_Export — Per-function Expert Technical Breakdown"> <strong>Acceptance checklist before module release (detailed):</strong><br>1. Owners present in OWNERS.md and contactable. <br>2. Public API documented and versioned. <br>3. PrepareExportPayload and ComputeMChecksum golden vectors validated across OSes. <br>4. AtomicExport cross-platform tests and degraded-path tests pass. <br>5. Manifest write atomicity tests pass. <br>6. Audit emission tests validated in modAudit harness. <br>7. Security review completed for EncryptAndSignArtifact and HSM integration. <br>8. Static analyzer green for forbidden-API patterns. <br><strong>Blocking conditions:</strong> missing audit emissions for critical transitions, failing golden artifacts, manifest races, or unresolved security issues. </td></tr><tr><td data-label="PQ_Export — Per-function Expert Technical Breakdown"> <strong>Extremely detailed test plan highlights & example scripts (conceptual)</strong><br><strong>Unit test highlights:</strong><br>1. <code>test_prepare_canonicalization</code> — inputs include varied M syntax, Unicode, comments; assert canonicalM equals golden. <br>2. <code>test_mchecksum_parity</code> — compute checksum in multiple host implementations and assert equality. <br>3. <code>test_package_determinism</code> — build artifact twice and assert byte-for-byte equality. <br>4. <code>test_atomic_rename_failure</code> — simulate rename error and assert fallback stage-local path created and pq_export.degraded emitted. <br><strong>Integration tests:</strong><br>1. End-to-end export pipeline with mock destination implementing atomic replace API. <br>2. Fault injection scenario with ENOSPC and revision via StageLocalFallback and retransfer job. <br><strong>Performance & load:</strong><br>1. Packaging 1000 small templates concurrently to exercise streaming hash and archive concurrency. <br>2. AtomicExport latency under heavy I/O load. <br><strong>CI gates:</strong> golden parity, audit emission checks, security scanning for PII leaks. </td></tr><tr><td data-label="PQ_Export — Per-function Expert Technical Breakdown"> <strong>Operator quick commands & examples (prescriptive)</strong><br>1. <code>diagnostics collect --correlation r-YYYYMMDD-abc</code> — collects audit_tail.csv, diagnosticsBlob, serialized RNG state, artifacts, and forensic_manifest.json for run. <br>2. <code>exports repair --temp &lt;tempPath&gt; --target &lt;artifactPath&gt;</code> — attempts repair under maintenance; dry-run flag available. <br>3. <code>exports replay --correlation r-... --evidenceRef &lt;evidence&gt;</code> — deterministic replay using persisted canonicalM and diagnostics; dry-run option. <br>4. <code>exports retransfer --stage &lt;stagePath&gt; --dest &lt;destinationUri&gt;</code> — idempotent retransfer job to finalize staged artifacts. <br><strong>When to call SRE:</strong> after two ENOSPC attempts for critical job descriptors, or after repeated RetryExport exhaustion producing PQ_EXPORT_VERIFICATION_FAILED; include forensic_manifest and audit_tail. </td></tr><tr><td data-label="PQ_Export — Per-function Expert Technical Breakdown"> <strong>Common failure modes & mitigations (expanded)</strong><br><strong>Failure mode: verification mismatch after write</strong><br>1. Likely cause: non-deterministic packaging metadata or corrupted write. <br>2. Mitigation: run VerifyExport, InspectTempArtifacts, attempt RepairExportArtifact, re-run packaging under controlled environment; if repeated, open incident with forensic_manifest. <br><strong>Failure mode: partial artifact observed by consumer</strong><br>1. Likely cause: direct write to final path bypassing AtomicExport or rename semantics not atomic on remote FS. <br>2. Mitigation: enforce AtomicExport usage by policy and static analyzer; block consumer usage until manifest verification passes. <br><strong>Failure mode: non-deterministic mChecksum parity</strong><br>1. Likely cause: inconsistent canonicalization rules across hosts. <br>2. Mitigation: update canonicalization to use parser-aware normalization, add golden vectors, and perform cross-language parity runs. </td></tr><tr><td data-label="PQ_Export — Per-function Expert Technical Breakdown"> <strong>Governance checklists & PR requirements (explicit)</strong><br>1. PR must include unit tests for new behavior and golden vectors if packaging or checksum logic changed. <br>2. Changes to packaging format or checksum algorithm require migration manifest and approver sign-off from OWNERS. <br>3. Any change to AtomicExport semantics requires SRE sign-off and cross-OS regression tests. <br>4. Release manifest update and signing required for production change affecting template injection. </td></tr><tr><td data-label="PQ_Export — Per-function Expert Technical Breakdown"> <strong>Appendix — long-form incident reconstruction example (ordered steps)</strong><br><strong>Incident:</strong> "Export verification mismatch for run r-20260112-455" <br><strong>Reconstruction steps:</strong><br>1. Retrieve pq_export.* and UserAction audit rows for correlationId. <br>2. Pull artifactPath and manifest from artifacts store using manifestChecksum recorded in audit. <br>3. Compute local artifact checksum and compare to manifest expectedChecksum; record mismatch diagnostics. <br>4. InspectTempArtifacts for candidate temp copies; compare temp checksums to expectedChecksum. <br>5. Restore serialized RNG state from evidenceRef and re-run PrepareExportPayload and PackageExportArtifact in reproduce mode to check packaging parity. <br>6. If reproduction matches original artifactChecksum, suspect transport corruption; if reproduction differs, investigate canonicalization or packagingVersion changes. <br>7. Package forensic_manifest with artifact, temp artifacts, diagnostics, and audits; escalate to compliance if regulated. </td></tr><tr><td data-label="PQ_Export — Per-function Expert Technical Breakdown"> <strong>Appendix — PQ & DAX short checklists for template authors and report builders</strong><br><strong>PQ Template author checklist:</strong><br>1. Include <code>mChecksum</code> in template metadata. <br>2. Mark templates requiring strict numeric fidelity as <code>requiresHighPrecision</code>. <br>3. Parameterize preview seeds and emit preview audit. <br>4. Avoid embedding credentials in templates; reference connection descriptors only. <br><strong>DAX/report builder checklist:</strong><br>1. Consume <code>RunMetadata</code> table for run provenance and artifactChecksum. <br>2. Avoid performing allocation or rounding residuals in DAX; use ETL SafeRound flows. <br>3. Use deterministic hashed keys for sampling filters in model. </td></tr><tr><td data-label="PQ_Export — Per-function Expert Technical Breakdown"> <strong>Final non-negotiable constraints (firm):</strong><br>1. All artifacts consumed by other processes must be persisted using AtomicExport semantics; static analysis enforces. <br>2. Sensitive parameters must be stored encrypted in evidence store and referenced by evidenceRef; do not place raw PII in top-level audits. <br>3. DeterministicRNG seeded from correlationId must be used for any sampling or tie-breakers in export flows; serialize state when exact replay required. <br>4. Numeric-sensitive transforms must be performed by worker SafeRound flows and recorded in artifact diagnostics; do not rely on host M numeric semantics for regulated outputs. <br>5. Every export attempt emits at least one audit row anchored to correlationId and paramsHash. </td></tr></tbody></table></div><div class="row-count">Rows: 35</div></div><div class="table-caption" id="Table4" data-table="Docu_0180_04" style="margin-top:2mm;margin-left:3mm;"><strong>Table 4</strong></div>
<div class="table-wrapper" data-table-id="table-4"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by PQ_Audit — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">PQ_Audit — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>Module-level metadata (contract & overview):</strong><br><strong>Owner:</strong> TEAM_PQ_AUDIT recorded in OWNERS.md, release manifests, and deployment notes. <br><strong>Public API:</strong> AppendAuditRow, AppendChainedAudit, RotateAndSignAuditTail, VerifyAuditChain, QueryAuditRows, ExportAuditTail, CompactAuditTail, AuditReplay, AuditTailReader, AuditTailStreamer, AuditFlush, AuditRetentionSweep, ArchiveAuditSegment, AuditEvidenceStoreWrite, AuditEvidenceStoreRead, AuditEmitUserAction, AuditEmitSystemEvent, AuditMetricsSnapshot, AuditRepairTempRecords, AuditTailIndexRepair, AuditTailPrune, AuditTailSnapshot, AuditTailDiff. <br><strong>Audits emitted:</strong> pq.audit.append.start, pq.audit.append.complete, pq.audit.append.failure, pq.audit.chained.append.start, pq.audit.chained.append.complete, pq.audit.rotate.start, pq.audit.rotate.complete, pq.audit.verify.start, pq.audit.verify.result, pq.audit.export.attempt, pq.audit.export.complete, pq.audit.compact.start, pq.audit.compact.complete, pq.audit.replay.start, pq.audit.replay.complete, pq.audit.retention.sweep, pq.audit.evidence.written, pq.audit.evidence.read, pq.audit.stream.connect, pq.audit.stream.disconnect, pq.audit.flush.attempt, pq.audit.flush.complete, pq.audit.atomic.repair. Every audit row includes correlationId, module=PQ_Audit, procedure, paramsHash, and resultHash when applicable. <br><strong>Purpose and intended use:</strong> authoritative, append-only audit store for Power Query lifecycle events: preview, inject, refresh, diagnostics, template management, export, and governance operations. Provide cryptographic chaining, rotation signing, atomic export, compact archival, deterministic replayability, and fine-grained streaming for UI and operators. Fast paths are lightweight (local buffer append) and avoid network I/O; heavy operations (rotation signing, export with evidence) run in background workers. <br><strong>Non-goals / constraints:</strong> avoid embedding raw PII into top-level audit rows; store full payloads and sensitive parameters in the evidence store. Do not attempt distributed two-phase commit across unrelated artifact stores; instead rely on higher-level orchestrators for multi-artifact transaction semantics. Avoid requiring HSM keys on every client host; provide clear fallback patterns for offline signing. Do not attempt to replicate primary data persistence responsibilities—PQ_Audit records metadata, checksums, and evidence pointers only. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>Operational guarantees (module-level invariants & SLOs):</strong><br>1. Append-only invariant: once AppendChainedAudit returns success, that row is durable and part of canonical tail unless explicit retention/revocation operations occur with audit records. <br>2. Cryptographic chain: rotation manifests include prevRotationHash and rotationSequenceNumber producing an ordered, tamper-evident chain. <br>3. Replayability: audit rows plus evidenceRef entries provide sufficient inputs (parameters, seeds, canonicalized decimals, RNG state) to deterministically reproduce operator-visible artifacts. <br>4. UI thread safety: ribbon calls must use low-latency append path that returns quickly; long durability work deferred to worker processes. <br>5. Retention compliance: hot/warm/cold retention boundaries enforced and verified monthly; proofs-of-delete retained as audit artifacts when deletion occurs. <br>6. Observability: every long-lived or critical operation must emit start/complete/failure audit rows and buffered metrics for CORE_Telemetry. <br><strong>Performance SLOs:</strong> median buffered AppendAuditRow latency <15ms; synchronous durable append latency <200ms on local SSD; rotation signing for 10k rows <2s end-to-end; export throughput ≥500MB/min compressed. <br><strong>CI / acceptance gates:</strong> chain verification unit tests, rotation signature goldens across languages, retention simulation, atomic export failure injection, static analyzer checks for UI-thread blocking. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>AppendAuditRow(correlationId, rowPayload, evidenceRef=null, synchronous=false, maxAttempts=3)</strong> — exhaustive technical breakdown<br><strong>Purpose & contract:</strong> primary low-latency append API for UI and background tasks to record audit rows. Must be safe to call from ribbon handlers with synchronous=false. For regulated operations or when immediate durability is required, synchronous=true can be used but must be scheduled from a worker to avoid UI blocking. <br><strong>Parameters & returns:</strong> parameters: correlationId (string), rowPayload (structured object sanitized to remove PII), evidenceRef (opaque pointer), synchronous (bool), maxAttempts (int). Returns {success:boolean, appendPosition:<code>int|null</code>, rowHash:<code>string|null</code>, paramsHash:string, status:<code>&quot;buffered&quot;|&quot;durable&quot;|&quot;duplicate&quot;|&quot;failed&quot;</code>, errorCode:<code>null|string</code>}. <br><strong>Primary invariants (must/shall):</strong><br>1. Idempotency: repeated calls with identical correlationId and paramsHash must be deduplicated; AppendAuditRow must return existing appendPosition and status "duplicate" for such calls. <br>2. PII protection: if rowPayload contains PII per policy, function must reject or require evidenceRef linking to encrypted evidence. <br>3. Local durability for synchronous=true: success indicates data is persisted to local storage and will be included in the next rotation. <br><strong>Algorithm & implementation notes:</strong><br>1. Canonicalize rowPayload to deterministic JSON: stable key ordering, normalized numeric formats, UTC timestamps, and deterministic nonce generation derived from correlationId if necessary. <br>2. Compute paramsHash = SHA256(canonicalPayload). Compute tentative row envelope containing timestamp, correlationId, module, procedure, paramsHash, evidenceRef. <br>3. Duplicate detection: check in-memory LRU index of recent paramsHash->appendPosition. If present, return duplicate with existing appendPosition. <br>4. Buffering: if synchronous=false, append to in-memory circular buffer and mark as buffered. Background flusher will persist using AtomicWrite semantics to a tail segment file. <br>5. Synchronous persist: if synchronous=true, write to temp file and fsync (where supported), append to tail using atomic replace semantics, fsync parent dir, compute rowHash from prevRowHash and paramsHash, and return durable appendPosition. <br>6. Error handling & retry: on transient FS errors, apply Retry wrapper with deterministic_jitter option in CI to ensure reproducible timing in tests. On persistent final failure emit pq.audit.append.failure with diagnostics and a deterministic errorCode. <br><strong>Edge cases & invalid inputs:</strong><br>1. Missing or empty correlationId: return PQ_AUDIT_MISSING_CORRELATION with remediation instructions. <br>2. Oversized payload exceeding configured evidence threshold: return PQ_AUDIT_PAYLOAD_TOO_LARGE and instruct caller to write to evidence store first. <br>3. Non-serializable types: PQ_AUDIT_SERIALIZATION_FAIL and provide parameter schema hints. <br><strong>Observability & audit fields emitted:</strong> pq.audit.append.start(correlationId, paramsHash, evidenceRefPresent) pq.audit.append.complete(correlationId, paramsHash, rowHash, appendPosition, durationMs) pq.audit.append.duplicate(correlationId, paramsHash, existingPosition) pq.audit.append.failure(correlationId, paramsHash, errorCode). <br><strong>Tests & CI vectors:</strong> canonicalization golden vectors for varied payloads; duplicate append tests under concurrency; buffer flush crash-and-recover tests; large payload rejection; ENOSPC and EPERM simulations. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>AppendChainedAudit(rowPayload, evidenceRef=null, signerKeyId=null, synchronous=false)</strong> — chaining primitive for cryptographic continuity<br><strong>Purpose & contract:</strong> append an audit row that includes prevHash to form an unbroken chained ledger. Required for critical flows where tamper-evidence is needed (template inject, export, refresh completion). AppendChainedAudit may accept signerKeyId to mark rows for immediate signing in rotation or to include signature metadata if local signing is permitted. <br><strong>Parameters & return:</strong> rowPayload, evidenceRef optional, signerKeyId optional, synchronous flag. Returns {success, chainIndex, rowHash, <code>signedEnvelopeHash|null</code>, <code>errorCode|null</code>}. <br><strong>Primary invariants (must/shall):</strong><br>1. PrevHash continuity: the prevHash included must equal the current tail lastRowHash at append time; if a conflict exists due to concurrent appends or rotation, the call should retry deterministically up to maxAttempts then fail with PQ_AUDIT_CHAIN_CONFLICT. <br>2. Signing discipline: rows intended for immediate trust must be signed in the rotation that includes them; if signerKeyId is not available, row is marked unsigned and scheduled for RotateAndSign. Unsigned critical rows impede exports until signed and recorded with an explicit audit operator override. <br>3. Atomicity: chained append must commit atomically with respect to tail updates to avoid orphan rows; atomic file append or atomic replace strategies with commit markers are required. <br><strong>Algorithm & implementation notes:</strong><br>1. Read lastRowHash atomically from tail metadata. Build envelope = {prevHash, timestamp, paramsHash, evidenceRef, module, procedure, appendContext}. <br>2. Optionally compute local signature over envelope if signerKeyId present and key available. Include signature metadata in envelope and persist via AppendAuditRow synchronous semantics. <br>3. Handle concurrent rotation or tail advancement by retrying after a small deterministic backoff; if rotation occurred between read and append, recompute prevHash and try again. <br><strong>Edge cases:</strong> signer key unavailable for required-signing flow: mark envelope as unsigned, append but emit pq.audit.chained.append.unsigned and schedule out-of-band signing. Multiple append actors on separate hosts require optimistic concurrency resolution via deterministic retry. <br><strong>Observability:</strong> pq.audit.chained.append.start, pq.audit.chained.append.complete, pq.audit.chained.append.retry, pq.audit.chained.append.failure. <br><strong>Tests & CI:</strong> concurrent chained append simulation, unsigned-row handling tests, out-of-band signing reconciliation tests. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>RotateAndSignAuditTail(rotationWindowSeconds=3600, signerKeyId, maxRowsPerRotation=10000, force=false)</strong> — rotation & signing workflow<br><strong>Purpose & contract:</strong> gather pending tail rows into a rotation segment, compute rotationRootHash, sign rotation bundle using signerKeyId (prefer HSM), and publish rotation manifest used by VerifyAuditChain. Rotation snapshots improve verification efficiency and provide signed checkpoints for regulatory submission. <br><strong>Parameters & return:</strong> rotationWindowSeconds, signerKeyId (identifier mapping to key store/HSM policy), maxRowsPerRotation, force. Returns rotationManifest {rotationSequence, rotationHash, signedBy, rowCount, artifactRef, timestamp} or error. <br><strong>Primary invariants (must/shall):</strong><br>1. Monotonic rotationSequence numbering maintained across all controllers. <br>2. Rotation manifest must include prevRotationHash linking to prior rotationRootHash, enabling a chain-of-rotation continuity check. <br>3. Signed manifest cannot be re-signed with differing contents; idempotent rotation of identical content returns same manifest. <br><strong>Algorithm & implementation notes:</strong><br>1. Determine rotation cut point deterministically (time-based or row-count) using tail snapshot to avoid including rows appended after rotation started. <br>2. Serialize rotationSegment as canonical binary (CBOR/Protobuf canonical encoding) containing ordered rowHashes and minimal projection, optionally including signed envelopes for row-level proofs. <br>3. Compute rotationRootHash via Merkle tree over rowHashes or deterministic digest of rotationSegment bytes. <br>4. Create rotationManifest with rotationSequenceNumber, prevRotationHash, rotationRootHash, rowRange, signerKeyId reference, and timestamp. Sign manifest using signerKeyId for production flows; include signer certificate chain or reference to release manifest. <br>5. Persist rotationSegment and signed manifest atomically using AtomicWrite; update rotation registry. <br>6. Publish pointer to rotation manifest registry and emit pq.audit.rotate.complete with manifestHash and rotationSequence. <br><strong>Cross-platform & operational notes:</strong> use canonical encodings for cross-language verification; HSM or KMS integration preferred for regulated flows; when HSM is unavailable implement an offline signing workflow with documented out-of-band signing and evidenceRef recorded in audits. <br><strong>Edge cases:</strong> leader election or collision when multiple controllers attempt rotation—resolve via deterministic policy (e.g., controller with lowest controllerId wins) or optimistic commit with retries. Partial rotation artifacts should be quarantined and repairable via AuditRepairTempRecords. <br><strong>Observability:</strong> pq.audit.rotate.start, pq.audit.rotate.complete, pq.audit.rotate.failure, pq.audit.rotate.conflict. <br><strong>Tests & CI:</strong> rotation signature goldens across multiple languages, leader collision injection, offline signing and later verification tests, rotation manifest replay tests. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>VerifyAuditChain(knownRotationManifests[], verifyKeys=trustedKeys)</strong> — chain verification primitive<br><strong>Purpose & contract:</strong> verify cryptographic continuity and integrity of audit rotations and optionally row-level hashes. Used in CI, daily monitoring, and for forensic verification. Returns detailed verificationReport enumerating passes and anomalies. <br><strong>Parameters & return:</strong> list of rotation manifests or registry pointer, verifyKeys set. Returns {passed:boolean, anomalies:[{rotationSequence,error}], verifiedRowsCount:int, verificationDurationMs:int}. <br><strong>Primary invariants (must/shall):</strong><br>1. Signature verification must validate certificate chain and check revocation status as configured. <br>2. PrevRotationHash continuity strictly enforced. Any mismatch triggers immediate alert and forensics. <br>3. Verification should be non-destructive and able to run against archives or live tail. <br><strong>Algorithm & notes:</strong><br>1. Iterate rotation manifests in sequence: validate signature using verifyKeys, re-compute rotationRootHash from rotationSegment and compare, ensure prevRotationHash equals previous rotationRootHash. <br>2. Optionally recompute rowHashes from canonicalized payloads stored in evidence to do row-level verification. This is expensive and may be limited to random sampling or targeted investigations. <br>3. Produce anomaly report linking to evidenceRefs for each mismatch. <br><strong>Edge cases:</strong> key rotation requires a key-rotation manifest mapping old signerKeyId to verification keys; missing rotation segment necessitates a forensic_manifest with missing ranges. <br><strong>Observability:</strong> pq.audit.verify.start, pq.audit.verify.result, pq.audit.verify.detail per anomaly. <br><strong>Tests & CI:</strong> signature verification with current and historical keys, partial archive simulation, cross-language signature verification goldens. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>QueryAuditRows(filters={correlationId,module,procedure,timeRange,paramsHash}, paging={cursor,limit}, compact=true)</strong> — query API for UI and operator consumption<br><strong>Purpose & contract:</strong> read-only query interface returning ordered minimal projections for rows that match filters. Exposes evidenceRef pointers but not raw PII. Must be low-latency for hot window and gracefully degrade to archive-backed reads. <br><strong>Parameters & return:</strong> filters object, paging cursor, limit. Returns {rows:[{timestamp,module,procedure,paramsHash,rowHash,evidenceRef,appendPosition}], nextCursor, totalEstimate}. <br><strong>Primary invariants (must/shall):</strong><br>1. Deterministic ordering by appendPosition ensures consistent paging. <br>2. No raw PII in top-level fields; evidenceRef used to fetch sanitized payload when needed. <br><strong>Algorithm & implementation notes:</strong><br>1. Query hot in-memory index for recent rows; for older ranges, consult compacted archives and rehydrate minimal projection. <br>2. Provide a consistency watermark indicating the last fully-signed rotation included in the results to help operators reason about tamper-evidence. <br>3. If query spans rotation boundary, return metadata indicating which rotations are partial and whether verification is pending. <br><strong>Edge cases:</strong> cross-archive queries may return partial results and a nextCursor to resume; UI must present "consistent to rotation X" markers. <br><strong>Observability:</strong> pq.audit.query.start and pq.audit.query.complete with row counts and elapsed time. <br><strong>Tests:</strong> hot-window performance tests, archive-backed query path tests, paging cursor correctness tests. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>ExportAuditTail(outputPath, includeEvidenceRefs=false, compress=true, checksumAlgorithm=sha256, maxAttempts=3)</strong> — atomic export with checksum and staged fallback<br><strong>Purpose & contract:</strong> produce a deterministically serialized, signed, and checksummed export containing a contiguous range of audit rows and optionally embedded evidence blobs (when policy permits). Exports intended for regulator submission must reference release-manifest and signerKeyId. Exports must be atomic and verified by checksum. <br><strong>Parameters & return:</strong> outputPath (URI or local path), includeEvidenceRefs bool, compress bool, checksumAlgorithm, maxAttempts. Returns {success, artifactUri, artifactChecksum, attempts, elapsedMs, <code>errorCode|null</code>}. <br><strong>Detailed behavior (must/shall):</strong><br>1. Validate destination and export policy; regulated exports require manifest and signerKeyId for signed export manifest. <br>2. Collate rotation manifests and row segments for requested time range; include rotation signatures and chain-of-rotation metadata. <br>3. If includeEvidenceRefs true and evidence blobs are permitted for export, embed encrypted evidence or include a zipped bucket of evidence files; otherwise produce an evidence manifest mapping evidenceRefs to archive URIs. <br>4. Stream serialize export deterministically while computing checksum; write to local tempPath using AtomicWrite semantics to prevent truncated artifacts. <br>5. On success, rename tempPath to final outputPath atomically; compute and store artifactChecksum and emit pq.audit.export.complete with artifact checksum and destination. <br>6. If remote upload fails, attempt staged local fallback and emit pq.audit.export.degraded with reason; if fallback used, ensure operator notified and SRE alerted for persistent failures. <br><strong>Cross-platform & governance notes:</strong> output container must preserve signing metadata; produce both machine-readable and human-readable manifest for regulator packages. Use canonical encodings to allow cross-language verification. <br><strong>Recovery & runbook:</strong> on verification failure, rerun ExportAuditTail with verifyOnly mode to detect transient corruption; run AtomicWriteRepair if temp artifacts remain. <br><strong>Observability:</strong> pq.audit.export.attempt, pq.audit.export.complete, pq.audit.export.failure, pq.audit.export.degraded. <br><strong>Tests & CI:</strong> export parity tests, signed manifest verification, network failure injection, staged fallback validation. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>CompactAuditTail(compactionStrategy="time-window", windowSeconds=86400, maxSegmentSize=50MB, retainIndex=true)</strong> — compaction & archival primitive<br><strong>Purpose & contract:</strong> compact older tail rows into archive segments to reduce hot index size while preserving provenance and rehydration capability. Compaction reduces operational cost while keeping chain verifiability. <br><strong>Behavior & steps:</strong><br>1. Identify candidate row ranges older than windowSeconds and not under legal hold or flagged for immediate replay. <br>2. Produce compactSegment containing rowHashes, minimal projections (timestamp,module,procedure,paramsHash,rowHash,evidenceRef), and a mapping to evidenceRef for large payloads; compress segment with deterministic compressor. <br>3. Persist compactSegment using AtomicWrite and update index mapping to point queries to compactSegment. <br>4. Emit pq.audit.compact.start and pq.audit.compact.complete with segmentChecksum, rowRange, and indexUpdate. <br><strong>Edge cases & failover:</strong> if compaction interrupted, temp artifact left must be handled by AuditRepairTempRecords; do not delete hot tail rows until rotation manifest confirmed and compactSegment verified. <br><strong>Tests:</strong> rehydration parity test: rehydrate compact segment and verify top-level projection matches original tail rows; compaction concurrency tests ensuring no data loss. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>AuditReplay(correlationId, evidenceFetchPolicy="local-first", dryRun=true, deterministic=true)</strong> — deterministic replay engine for forensic analysis<br><strong>Purpose & contract:</strong> reconstruct and re-execute recorded operator actions using persisted audit rows and evidence artifacts to reproduce observable outputs deterministically. Supports dry-run verification producing diffs, and full-run sandbox execution for non-production reproductions. <br><strong>Parameters & return:</strong> correlationId, evidenceFetchPolicy, dryRun, deterministic. Returns replayReport {replayedSteps, producedArtifactChecksums, diffs, success, diagnostics}. <br><strong>Algorithm & notes:</strong><br>1. Query audit rows by correlationId; fetch required evidenceRef artifacts via AuditEvidenceStoreRead following evidenceFetchPolicy and enforcing ACL and two-person approval for regulated data if required. <br>2. Restore RNG serialized state, SafeRound canonical inputs, parameter snapshots, and environment configuration from evidence. <br>3. Execute pipeline in a sandbox with deterministic RNG and SafeRound algorithm configured to match production algorithm version. Capture produced artifacts and checksums. <br>4. Compare produced checksums with archived artifactChecksum values in audit rows and produce diff report identifying byte-level or semantic differences. <br>5. Emit pq.audit.replay.start and pq.audit.replay.complete with outcome and evidence pointers. <br><strong>Operator guidance:</strong> prefer dryRun mode for routine forensic checks; only perform non-dry replays in controlled sandbox to avoid accidental re-injection. <br><strong>Tests & CI:</strong> golden replay tests across language runtimes; RNG and SafeRound parity checks; replay under altered evidence to affirm mismatch detection is precise. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>AuditTailReader(streamSliceSize=1000, backpressure=true)</strong> — streaming reader for UI and diagnostics<br><strong>Purpose & contract:</strong> provide streaming consumption of the hot audit tail to UIs and monitoring tools. Supports resumption via cursor, long-poll for new rows, and consistent markers across rotation boundaries. Must support backpressure so slow consumers do not overwhelm buffer. <br><strong>API and behavior:</strong> subscribe(cursor=null) returns a stream of rowSummaries with heartbeats; pause/resume and checkpointing supported; reconnect attempts resume from last cursor if within retention horizon. <br><strong>Implementation notes:</strong> maintain in-memory circular queue for last N rows and a compact persistent cursor index for resume; use long-polls when socket streams unavailable; expose a watermark for last signed rotation included. <br><strong>Observability:</strong> pq.audit.stream.connect, pq.audit.stream.disconnect, pq.audit.stream.lag metrics. <br><strong>Tests:</strong> long-poll fallback tests, resume across rotation tests, backpressure and slow-consumer scenarios. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>AuditFlush(correlationId=null, force=false, flushTimeoutMs=5000)</strong> — flush buffered audit tail to durable storage<br><strong>Purpose & contract:</strong> force immediate persist of in-memory buffer to tail segment using AtomicWrite semantics. Use prior to critical operations (job persistence, export) to ensure appended rows are durable. If force=true perform synchronous flush with blocking semantics (must be executed off UI thread or scheduled). <br><strong>Return object:</strong> {queued:boolean, flushedRows:int, durationMs:int, <code>errorCode|null</code>}. <br><strong>Edge cases:</strong> flushTimeout exceeded returns partialFlush with details and schedules retry; emit pq.audit.flush.attempt and pq.audit.flush.complete. <br><strong>Tests:</strong> flush under sustained append rates and crash recovery ensuring no dropped rows. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>AuditRetentionSweep(policyName="hot-warm-cold", retentionConfig)</strong> — retention & archival enforcement<br><strong>Purpose & contract:</strong> enforce retention policies by moving compacted archives to warm or cold storage, deleting hot data per policy, and producing proofs-of-delete for regulatory audits. Legal holds and regulator orders override deletion. <br><strong>Behavior & steps:</strong><br>1. Determine candidate artifacts per retention policy and exclude items under legal hold. <br>2. Validate artifact integrity by checksum prior to transfer. <br>3. Transfer artifacts to warm/cold storage and update retention index. <br>4. For hot deletions, create proof-of-delete artifacts and retention manifest, then emit pq.audit.retention.sweep with summary and forensic_manifest pointer. <br><strong>Edge cases:</strong> legal hold detection and skip logic must be robust and audited; failed transfer must trigger retry policy and operator alert. <br><strong>Observability:</strong> pq.audit.retention.sweep rows with counts and URIs. <br><strong>Tests:</strong> legal hold scenarios, proof-of-delete verification, cross-region archival transfer tests. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>AuditEvidenceStoreWrite(evidencePayload, correlationId, encrypt=true, aclPolicy)</strong> — evidence persistence primitive<br><strong>Purpose & contract:</strong> persist sanitized or full evidence artifacts (parameter sets, RNG state, SafeRound canonical decimals, traces) to encrypted evidence store and return evidenceRef pointer for use in top-level audit rows. Evidence store enforces ACLs, PII detection, redaction, and retention metadata. <br><strong>Parameters & return:</strong> evidencePayload (JSON/binary), correlationId, encrypt boolean, aclPolicy. Returns {evidenceRef, evidenceHash, storageUri, expiryPolicy}. <br><strong>Primary invariants:</strong> evidence persisted encrypted at rest; top-level audit rows only reference evidenceRef, not raw payloads; PII detection enforced. <br><strong>Edge cases & mitigation:</strong> if PII present and policy forbids storing it in evidence store, return PQ_EVIDENCE_PII_REJECT and provide guidance on allowed redaction or privacy-safe summarization. <br><strong>Observability:</strong> pq.audit.evidence.written(correlationId, evidenceHash, storageUri). <br><strong>Tests:</strong> write/read parity, PII detection tests, ACL enforcement tests including two-person access gating for regulated evidence. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>AuditEvidenceStoreRead(evidenceRef, authorizationToken, redactPolicy="safe")</strong> — evidence read primitive for replay and verification<br><strong>Purpose & contract:</strong> read evidence artifacts enforcing ACLs and redaction policies. Each read must produce an access audit record (who, when, why). Evidence reads for regulated content may require two-person approval and off-line key release. <br><strong>Return:</strong> sanitizedPayload or fullPayload when authorized. <br><strong>Security notes:</strong> evidence store must never leak raw PII in logs; all reads recorded for chain-of-custody. <br><strong>Observability:</strong> pq.audit.evidence.read(correlationId, evidenceRefFingerprint, requesterId, outcome). <br><strong>Tests:</strong> access control enforcement tests, two-person approval workflows, read logging verification. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>AuditEmitUserAction(correlationId, operatorId, module, procedure, paramsHash, uiContext=null)</strong> — standardized user action emitter<br><strong>Purpose & contract:</strong> capture ribbon-driven operator actions in a normalized schema for PQ flows: preview, inject, export, refresh. Sanitizes params and delegates full payload persistence to evidence store when required. Ensures operatorId and correlationId presence for governance. <br><strong>Fields & invariants:</strong> include operatorId (or operator-presence marker), module, procedure, paramsHash, evidenceRef pointer if full params stored. UI context must avoid raw PII. <br><strong>Operator guidance:</strong> call at ribbon handler entry before performing downstream operations; ensures immediate user-action audit exists even if downstream operations later fail. <br><strong>Tests:</strong> UI instrumentation tests verifying emission for all ribbon controls. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>AuditEmitSystemEvent(correlationId, module, procedure, level, message, diagnosticRef=null)</strong> — system event audit emitter<br><strong>Purpose & contract:</strong> record non-user events: background refresh completions, connector dependency results, ensureDeps findings, plugin load errors. Levels: info, warn, error, critical. DiagnosticRef points to evidence for deep diagnostics when permitted. High severity events trigger metrics and alerting. <br><strong>Observability:</strong> pq.audit.systemevent rows and optional SRE alert triggers for critical events. <br><strong>Tests:</strong> severity to alert mapping, diagnosticRef linking. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>AuditMetricsSnapshot(correlationId, metricSet, timestamp)</strong> — local metrics snapshot for telemetry uplink<br><strong>Purpose & contract:</strong> snapshot metrics like append latency histograms, rotation durations, export throughput, verify success rate; persist snapshot via AtomicWrite for CORE_Telemetry upload. Avoid network I/O in fast paths. Emit pq.audit.metrics.snapshot referencing snapshot artifact. <br><strong>Tests:</strong> snapshot integrity and periodic upload registration tests. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>AuditRepairTempRecords(tempPaths[], correlationId, force=false)</strong> — repair utility for leftover temp artifacts<br><strong>Purpose & contract:</strong> inspect leftover temp artifacts from failed AtomicWrite or rotation, verify payloads against evidenceHash, and either finalize atomic rename under maintenance or quarantine with forensic_manifest for operator action. Returns repairReport enumerating actions and outcomes. <br><strong>Behavior:</strong> validate temp artifact checksum; attempt atomic rename if safe and if target path free; otherwise quarantine and produce forensic_manifest. Emit pq.audit.atomic.temp.inspect and pq.audit.atomic.repair.*. <br><strong>Edge cases:</strong> mismatched payloads require quarantine and SRE escalation. <br><strong>Tests:</strong> ENOENT, ENOSPC, permission denied repair flows. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>Cross-cutting Observability, Telemetry & Error catalog:</strong><br><strong>Audit schema mandatory fields:</strong> timestamp, correlationId, module=PQ_Audit, procedure, operatorId optional, paramsHash, rowHash, evidenceRef optional, prevHash optional, rotationSequence optional, appendPosition, metadata object with duration_ms, attempts, artifactChecksum, tempPathList, errorCode optional. <br><strong>PII policy:</strong> top-level audit rows must never contain raw PII; sanitized full params stored encrypted in evidence store and referenced by evidenceRef. <br><strong>Representative ErrorCodes:</strong> PQ_AUDIT_MISSING_CORRELATION, PQ_AUDIT_SERIALIZATION_FAIL, PQ_AUDIT_DUPLICATE, PQ_AUDIT_APPEND_ENOSPC, PQ_AUDIT_APPEND_EPERM, PQ_AUDIT_ROTATE_CONFLICT, PQ_AUDIT_ROTATE_SIGN_FAIL, PQ_AUDIT_VERIFY_FAIL, PQ_AUDIT_EXPORT_FAIL, PQ_EVIDENCE_PII_REJECT, PQ_AUDIT_REPLAY_MISMATCH. Each maps to remediation guidance in PQ_Error. <br><strong>Metrics names:</strong> pq.audit.append.latency_ms, pq.audit.append.success_rate, pq.audit.rotate.duration_ms, pq.audit.verify.success_rate, pq.audit.export.throughput_bytes_s, pq.audit.replay.time_ms. Metrics are buffered locally and uploaded in audited batches by CORE_Telemetry. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>Testing matrix, property tests, and cross-language golden governance:</strong><br><strong>Unit tests (must include):</strong><br>1. Append idempotency and duplicate detection with varied payload permutations and correlationIds. <br>2. Chained append concurrency tests verifying monotonic appendPosition under parallel writers. <br>3. Rotation signing goldens across Python, VBA, JS implementations. <br>4. Export atomicity tests with induced rename/fsync failures using FS mocks and staged fallback validation. <br>5. Evidence store PII detection and ACL enforcement tests. <br><strong>Integration tests:</strong><br>1. End-to-end PQ_Ribbon -> AppendChainedAudit -> RotateAndSign -> ExportAuditTail -> VerifyAuditChain CI pipeline test. <br>2. AuditReplay validation: generate test inject run, persist evidence, and replay to verify checksum parity. <br>3. Retention sweep simulation with legal hold scenarios. <br><strong>Property tests:</strong><br>1. Chain invariants: recomputing rotation root from archived rows must equal manifest root. <br>2. Replay determinism for seeded RNG and persisted evidence. <br><strong>CI golden gating:</strong> rotation signature parity, canonical encoding tests, static analyzer blocking forbidden UI-thread FS operations. Changes to audit schema require migration manifest and owner approvals. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>Developer guidance, allowed & forbidden patterns:</strong><br><strong>Required usage patterns:</strong><br>1. Always use AuditEmitUserAction for every ribbon-driven operation that will produce artifacts downstream. <br>2. Persist full parameters, RNG seeds, or large payloads with AuditEvidenceStoreWrite and reference via evidenceRef. <br>3. Use AppendChainedAudit for regulated or tamper-evident flows. <br>4. RotateAndSignAuditTail regularly; do not leave unsigned tail entries for prolonged periods. <br><strong>Forbidden practices:</strong><br>1. Do not embed raw PII in top-level audit rows. <br>2. Do not perform blocking fsync operations on the UI thread. <br>3. Do not export evidence unencrypted to third-party endpoints without governance approval. <br>4. Do not rely on global non-deterministic RNG for operator-visible sampling. <br><strong>Code-review checklist:</strong> verify AuditEmitUserAction use at UI entry points, evidenceRef usage for large payloads, AppendChainedAudit for regulated flows, RotateAndSign included in release manifest, and static analyzer checks for forbidden patterns. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>Operational runbook & incident playbooks (executable steps):</strong><br><strong>Audit append ENOSPC runbook:</strong><br>1. Query pq.audit.append.failure rows for ENOSPC with correlationId and target path. <br>2. Collect disk metrics for target mount (df -h, iostat, vmstat) and attach to incident. <br>3. Use AuditFlush to persist buffered rows to alternate local volume (--stage-local) and run AuditRepairTempRecords to finalize artifacts. <br>4. If capacity cannot be freed, escalate to SRE with forensic_manifest and audit_tail excerpt. <br><strong>Rotation verification anomaly triage:</strong><br>1. Run VerifyAuditChain to localize rotation anomaly. <br>2. Retrieve implicated rotation manifests and segments; verify signature chain against release manifest. <br>3. If signature invalid, place rotation on legal hold and escalate to security; issue freeze on exports dependent on the corrupted rotation. <br><strong>Non-deterministic replay complaint triage:</strong><br>1. Pull pq.audit.evidence.written and pq.audit.evidence.read entries for correlationId to find RNG serialized state and SafeRound inputs. <br>2. Execute AuditReplay deterministic dry-run and compare produced checksums; if mismatch persists, collect cross-language golden vectors and escalate to implementation owners. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>Extremely detailed narratives & examples (multiple scenarios):</strong><br><strong>Scenario 1 — Template injection audit & deterministic replay (complete trace):</strong><br>1. Operator clicks Inject in PQ_Ribbon with correlationId p-20260117-abc. PQ_Ribbon calls AuditEmitUserAction which appends a minimal useraction row with paramsHash and evidenceRef placeholder. <br>2. PQ_Injector serializes full parameter set including previewSeed, numeric fidelity flags, and canonical decimal snapshots and calls AuditEvidenceStoreWrite. Evidence store returns evidenceRef e-123 and evidenceHash. pq.audit.evidence.written emitted. <br>3. PQ_Injector constructs a chained envelope {prevHash, timestamp, paramsHash, evidenceRef e-123, module=PQ_Injector, procedure=inject} and calls AppendChainedAudit. AppendChainedAudit writes the row to the tail (buffered or durable per context) and returns appendPosition and rowHash. pq.audit.chained.append.complete emitted. <br>4. Rotation executes within configured window: RotateAndSignAuditTail snapshots rows up to the cut, builds rotationSegment, computes rotationRootHash and signs rotationManifest with signerKeyId release-key-v3. Signed manifest and rotationSegment persisted via AtomicWrite and pq.audit.rotate.complete emitted. <br>5. ExportAuditTail creates a regulator package including rotationManifests, evidenceRef map, and an export manifest signed by the release signing key. Export persisted atomic and pq.audit.export.complete emitted with artifactChecksum and destinationUri. <br>6. Later, compliance requests deterministic replay: operator triggers AuditReplay(correlationId p-20260117-abc) with authorized evidence access. AuditReplay fetches evidenceRef e-123 (AuditEvidenceStoreRead with two-person approval if required), restores RNG state and canonical decimals, executes deterministic pipeline in sandbox reproducing artifactChecksum identical to archived artifactChecksum. pq.audit.replay.complete emitted with producedArtifactChecksums. <br><strong>Narrative takeaway:</strong> full chain from UI action → persisted parameters → chained append → rotation signing → export → replay demonstrates reproducibility and tamper-evidence with minimal PII exposure. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>Scenario 2 — Preview with seeded sampling and audit linkage:</strong><br>1. Operator requests Preview; PQ_Ribbon computes previewSeed = HMAC_SHA256(<code>correlationId | &quot;preview-v2&quot;</code>) and packages seed and sampling parameters into evidence payload. <br>2. Call AuditEvidenceStoreWrite to persist seed and sampling metadata; receive evidenceRef pseed-01. <br>3. AuditEmitUserAction appends a user action row referencing pseed-01 and paramsHash; pq.audit.append.complete emitted. <br>4. Preview engine receives seed as parameter and performs deterministic sampling producing preview set; preview metadata (mChecksum) appended to audit tail. <br>5. If operator disputes sample selection, AuditReplay rehydrates seed and selection algorithm from evidenceRef to reproduce exact preview selection for forensic review. <br><strong>Narrative takeaway:</strong> seeding previews and persisting seeds to the evidence store enables reproducible previews across sessions and hosts. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>Scenario 3 — Refresh diagnostics & export pipeline:</strong><br>1. Operator triggers Refresh; AuditEmitUserAction records the refresh intent with job descriptor paramsHash and evidenceRef referencing connector config. <br>2. Background refresh worker logs step-level events: pq_refresh.start, pq_refresh.conn.resolve, pq_refresh.query.exec, pq_refresh.collect.diagnostics; each step references evidenceRef to large diagnostic traces stored in evidence store. <br>3. On completion, ExportAuditTail packages M text and diagnostics, produces artifactChecksum and persists via AtomicWrite. pq.audit.export.complete emitted. <br>4. If export fails due to network, ExportAuditTail attempts staged local fallback and emits pq.audit.export.degraded. Operator UI displays degraded export status with a link to retry or download the local artifact. <br><strong>Narrative takeaway:</strong> fine-grained step-level audits plus archived diagnostics enable SRE to triage provider-level failures and reproduce full diagnostic contexts for root-cause analysis. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>Scenario 4 — Template update governance for regulated templates:</strong><br>1. Template author opens PR modifying a regulated template tagged requiresHighPrecision. CI runs static checks ensuring mChecksum updated and no removal of requiresHighPrecision. <br>2. The PR approval actions (owner approvals, test signatures) are captured by AppendChainedAudit rows with evidenceRef pointing to PR diff and approval notes. <br>3. Release proceeds only after two-person approval recorded as chained audit rows and RotateAndSignAuditTail includes those approvals in a signed rotation manifest. <br>4. The signed release manifest acts as regulator-evidence that owner approvals and golden tests passed prior to release. <br><strong>Narrative takeaway:</strong> governance enforced by chained audit rows and signed rotations provides regulator-proof change history. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>Scenario 5 — MatchMerge deterministic tie-breaking & audit trail:</strong><br>1. MatchMerge runs as a background job with correlationId mm-20260117-9; worker seeds DeterministicRNG(seed_source=correlationId, salt="matchmerge-v1") and persists serialized RNG state via AuditEvidenceStoreWrite to evidenceRef rng-evidence. <br>2. Match candidate tie-break ordering performed using deterministic shuffle keyed by tieBreakerKeys plus RNG-derived ordering; proposed merges persisted as merge proposal artifacts via AtomicWrite and recorded in AppendChainedAudit rows referencing proposal artifact checksum and RNG evidenceRef. <br>3. Operator reviews proposal; accept action recorded via AppendChainedAudit and saved apply plan persisted by AtomicWrite. If inline-apply chosen, additional two-person approvals captured in chained audits. <br>4. Forensics: AuditReplay can restore RNG state, reproduce candidate ordering, and reproduce exact merge proposals for audit. <br><strong>Narrative takeaway:</strong> deterministic RNG and persisted RNG state plus chained audit entries ensure reproducible matching decisions and auditable merge proposals. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>Conceptual Power Query (M) patterns — mapping PQ_Audit principles to PQ workflows (expanded):</strong><br><strong>Context:</strong> M runtime differences across hosts, decimal fidelity variability, and limited file-system guarantees require orchestrated helpers around M to achieve PQ_Audit guarantees. <br><strong>Patterns & recommended practices:</strong><br>1. <strong>Preview seeding pattern:</strong> compute previewSeed in ribbon layer; persist seed to evidence store and include seed fingerprint in preview audit. For simple preview selections, pass seed into template as parameter. For larger sampling, perform sampling in worker using DeterministicRNG and store selection snapshot as evidenceRef. <br>2. <strong>Injection atomicity pattern:</strong> M cannot guarantee atomic replace semantics on all hosts. Produce authoritative M query artifact in a trusted helper (signed XLAM or worker) that writes artifact to disk using AtomicWrite and emits a chained audit row with artifactChecksum. The injector then calls workbook.Queries.Add with the persisted artifact to ensure the injected query exactly matches audited artifact. <br>3. <strong>High-precision numeric pattern:</strong> mark templates requiring strict rounding as requiresHighPrecision. For such templates, offload numeric aggregation and rounding to worker-side SafeRound flows. Template should emit canonicalized numeric payload and evidenceRef; worker returns final rounded artifact persisted via AtomicWrite and recorded by AppendChainedAudit. <br>4. <strong>Retry & idempotency mapping:</strong> M refresh orchestration should be handled outside M by the add-in or a worker that applies Retry with idempotent tokens. M templates should remain declarative and stateless. <br>5. <strong>Diagnostics & traces pattern:</strong> M diagnostics can be large; store them in evidence store with evidenceRef and emit minimal diagnostic pointers in PQ_Audit rows. This keeps top-level rows lightweight and non-PII. <br><strong>Operator guidance:</strong> prefer worker-side finalization for numeric-critical or auditable steps; treat M templates as deterministic transformations parametrized by persisted seeds and canonical inputs; use evidenceRefs liberally for snapshots needed in replay. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>Conceptual DAX patterns — mapping PQ_Audit to semantic model governance (expanded):</strong><br><strong>Context:</strong> DAX is read-time expression language and cannot perform side-effects or persist external artifacts. ETL must handle rounding and chain-of-custody. <br><strong>Patterns & recommendations:</strong><br>1. <strong>RunMetadata table pattern:</strong> ETL writes an atomic RunMetadata record persisted with AtomicWrite and referenced by AppendChainedAudit including fields correlationId, configHash, artifactChecksum, runTimestamp, auditStatus. DAX measures can reference RunMetadata to display run provenance and verification status in reports. <br>2. <strong>Offload rounding/residuals to ETL:</strong> perform SafeRoundResiduals in ETL and store resolved cents as integers. DAX consumes pre-rounded values ensuring model logic remains deterministic and avoids cumulative rounding bias. <br>3. <strong>Deterministic sampling for model surfaces:</strong> compute a stable HashKey in ETL (e.g., HMAC(<code>primaryKey | runSeed</code>)) and persist sampling selection criteria with correlationId; DAX can use HashKey MOD N < k to present deterministic cohort selections. <br>4. <strong>Model-level checksum reconciliation:</strong> ETL writes datasetChecksum to artifact manifest and model RunMetadata; DAX measure exposes a ReconciliationFlag comparing current model checksum to expected artifactChecksum for downstream consumer confidence. <br><strong>Narrative takeaway:</strong> DAX should be a deterministic read surface referencing ETL-persisted audit metadata rather than performing transformational or cryptographic responsibilities. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>Appendices: forensic artifacts, evidence paths & recommended retention (expanded):</strong><br><strong>Minimum forensic artifacts to collect for PQ incidents:</strong><br>1. audit_tail.csv excerpt containing useraction and pq.audit.* rows spanning the correlationId timeframe. <br>2. rotation manifests and rotationSegment archives, including signer certificate chain and prevRotationHash chain. <br>3. evidence blobs referenced by evidenceRef such as serialized RNG state, canonical decimal snapshots, parameter sets, and diagnostic traces. <br>4. exported artifacts (M files, diagnostics bundle) with recorded artifactChecksum and export manifest. <br>5. jobDescriptor persisted via AtomicWrite with jobId and correlationId. <br>6. AuditReplay report and diff output showing producedArtifactChecksums and delta analysis. <br>7. AtomicWrite temp artifact listing and any atomic repair logs. <br><strong>Evidence storage & retention patterns:</strong><br>1. Hot evidence store: \\evidence\hot\<module>\<correlationId>\; retain 30 days with restricted access for rapid replay. <br>2. Warm encrypted archive: secure long-term store for regulatory retention (7 years) with access controls and chain-of-custody metadata. <br>3. Cold archive: immutable storage per regulatory schedule with signed manifests proving content integrity. <br><strong>Forensic_manifest contents:</strong> the forensic_manifest enumerates artifacts, rotation manifests, signer certificates, evidenceRef URIs, checksums, and access instructions for decryption keys (key IDs not secrets). <br><strong>Retention verification cadence:</strong> monthly AuditRetentionSweep with pq.audit.retention.sweep row; proof-of-delete artifacts preserved for compliance. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>Acceptance checklist before PQ_Audit release (detailed):</strong><br>1. OWNERS listed and contactable in OWNERS.md. <br>2. Public API stable and documented with backward-compatible versioning and changelog. <br>3. Unit, integration, and golden tests for chaining and rotation signing pass across supported languages. <br>4. EvidenceStore encryption, ACL, and PII detection tests pass and security review completed. <br>5. Static analyzer prohibits UI-thread blocking FS operations and raw PII in top-level rows. <br>6. Release manifest includes signer keys, and injection helpers are code-signed where required. <br><strong>Blocking conditions:</strong> missing audit emits on PQ_Ribbon flows, rotation signature verification failures, or top-level PII leakage detected by static analysis. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>Extremely detailed test plan highlights & scripts (explicit, conceptual):</strong><br><strong>Unit tests:</strong><br>1. Append idempotency: call AppendAuditRow concurrently with identical payloads and verify single canonical appendPosition returned. <br>2. Chained append concurrency: multiple writers append chained rows concurrently; verify monotonic appendPosition ordering and prevHash continuity maintained. <br>3. Evidence store PII detection: supply payloads containing simulated PII patterns and verify rejection or required redaction. <br>4. Rotate signing: mock HSM signing and validate verifier implementations in multiple languages can validate signature. <br><strong>Integration tests:</strong><br>1. Full pipeline: PQ_Ribbon user action → AppendChainedAudit → RotateAndSignAuditTail → ExportAuditTail → VerifyAuditChain. Validate artifactChecksum parity and rotation signature verification. <br>2. Export failure injection: simulate remote storage failure and ensure staged local fallback and pq.audit.export.degraded emitted. <br>3. AuditReplay roundtrip: build a complex inject run, persist RNG and SafeRound inputs, replay deterministically and compare artifact checksums. <br><strong>Performance tests:</strong><br>1. Append throughput: synthetic stress test with 10k concurrent append producers; measure median append latency and flush behavior. <br>2. Rotation signing throughput: sign 100k rows and measure rotation duration; ensure it meets performance SLOs. <br>3. Export throughput with compression vs. no-compression under constrained network conditions. <br><strong>CI gating:</strong> no merge to main until unit, integration, golden signatures, and static analyzer checks pass; performance regressions require owner sign-off. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>Operator runbook quick commands & examples (concise and prescriptive):</strong><br>1. diagnostics collect --correlation p-YYYYMMDD-abc — collects audit_tail excerpt, rotation manifests, evidenceRef snapshots, and forensic_manifest.json. <br>2. audit export --range start,end --dest /secure/share/pq-audit --include-evidence true — create signed export for regulator submission and store artifactChecksum for reconciliation. <br>3. audit replay --correlation p-... --dry-run — deterministic replay for operator verification; returns diffs and producedArtifactChecksums. <br>4. audit repair-temp --temp /tmp/pq-audit.part.* — attempt to repair leftover temp artifacts using AtomicWriteRepair and produce pq.audit.atomic.repair entries. <br>5. audit flush --force — synchronous flush to ensure rows are persisted before scheduling critical jobs. <br><strong>When to call SRE:</strong> repeated pq.audit.export.failure with staged fallback used, rotation verification anomalies, evidence store key access failures, or repeated ENOSPC on append persistence. Provide forensic_manifest and rotation manifests in SRE ticket. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>Final notes, governance & mandatory constraints (firm):</strong><br>1. Never place raw PII in top-level audit rows; use evidenceRef to reference encrypted evidence. <br>2. AppendChainedAudit is mandatory for regulated outputs and artifacts consumed by downstream services; unsigned tails are unacceptable for production exports. <br>3. Deterministic RNG seeds for operator-visible sampling must be persisted to evidence store and evidenceRef recorded in top-level audit rows. <br>4. Offload numeric-sensitive transforms to worker SafeRound flows and persist inputs to evidence store for reproduction. <br>5. All critical operations must emit audit rows and attach evidenceRef where necessary for forensics. <br>6. Periodic chain verification must run in CI and monitoring; verification failures open automated incidents and require forensics per runbook. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>Appendix A — Example audit row schema (descriptive, not a code snippet):</strong><br><strong>Fields required for PQ_Audit rows:</strong> timestamp, correlationId, module, procedure, operatorId optional, paramsHash, rowHash, evidenceRef optional, prevHash optional, rotationSequence optional, appendPosition, metadata object with duration_ms, attempts, artifactChecksum, tempPathList. <br><strong>Policy note:</strong> top-level audit rows must not contain PII; sanitized full params stored in evidence store and referenced by evidenceRef. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>Appendix B — Common failure modes & mitigations (expanded):</strong><br><strong>Failure mode: partial tail write observed</strong><br>1. Cause: non-atomic append or host-specific rename semantics differing from POSIX; remote filesystem edge cases. <br>2. Mitigation: enforce AtomicWrite on persistence, run AuditRepairTempRecords to inspect temp files and finalize safe renames; add host compatibility checks and recommend same-volume writes where possible. <br><strong>Failure mode: rotation signature verification failure</strong><br>1. Cause: signer key rotated without publishing verification keys, rotation artifact corruption, or HSM failure. <br>2. Mitigation: place rotation on legal hold, disable exports associated with suspect rows, retrieve historical key manifest and verify mapping; if needed, restore from last verified rotation and escalate to security owners. <br><strong>Failure mode: non-deterministic replay</strong><br>1. Cause: RNG state not persisted, SafeRound inputs missing, cross-host decimal normalisation differences, or evidenceRef missing. <br>2. Mitigation: persist RNG serialize_state and SafeRound canonical inputs to evidence store; maintain cross-language golden vectors and implement canonical decimal normalization library in all runtimes. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>Appendix C — Governance checklists & PR requirements (explicit):</strong><br>1. PR must include unit tests for any new behavior, golden vectors if deterministic sequences or encodings changed, and audit emission verification. <br>2. Changes to rotation or signature algorithm require migration manifest, owner approvals, and cross-language goldens. <br>3. Evidence store modifications require security review and tests ensuring PII not surfaced in top-level rows. <br>4. Release manifest must be updated and OWNERS approval acquired for production changes affecting injection or regulated outputs. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>Appendix D — Long-form incident reconstruction example (ordered):</strong><br><strong>Incident synopsis:</strong> "Injected query mismatch for correlation p-20260112-455" — artifact differs from expected template output observed by operator. <br><strong>Forensic reconstruction steps (ordered):</strong><br>1. Retrieve useraction and pq.audit.* rows for correlationId from audit_tail to build timeline. <br>2. Pull evidenceRef blobs for parameter sets and serialized RNG state via AuditEvidenceStoreRead with proper authorization. <br>3. Execute AuditReplay in deterministic dry-run mode restoring RNG and SafeRound state to produce replay artifacts and producedArtifactChecksums. <br>4. Compare producedArtifactChecksums to archived artifactChecksum from pq.audit.append.complete; if mismatch found, check atomic_write.verification_failed and rotation manifests for corruption. <br>5. Inspect AtomicWrite temp artifacts and run AuditRepairTempRecords to finalize or quarantine items. <br>6. If difference due to numeric rounding, restore canonical decimal snapshots and run SafeRoundResiduals reproduction; produce granular diff report showing where rounding diverged and why. <br>7. Package forensic_manifest.json enumerating rotation manifests, evidence blobs, artifact checksums, and replay logs; escalate to compliance if regulated. <br><strong>Outcome:</strong> reproduction yields validation that M runtime decimal handling differed; mitigation included marking template requiresHighPrecision, shifting final rounding to worker SafeRound, adding cross-language golden vectors, and releasing a signed update with AppendChainedAudit recorded approvals. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>Appendix E — PQ & DAX short checklists for template authors and report builders:</strong><br><strong>PQ Template author checklist:</strong><br>1. Include mChecksum and templateVersion in metadata. <br>2. Mark requiresHighPrecision for templates with regulated numeric requirements. <br>3. Parameterize previewSeed and ensure previewSeed persisted via evidence store and recorded in preview audit. <br>4. Offload final numeric aggregation to worker SafeRound when requiresHighPrecision true. <br>5. Emit audit hooks for major template actions and link to correlationId. <br><strong>DAX/report builder checklist:</strong><br>1. Consume RunMetadata table for run provenance and artifactChecksum. <br>2. Avoid allocation or rounding residual logic in DAX; perform in ETL and persist resolved integers. <br>3. Use RunMetadata.auditStatus to display verification badges and guide downstream consumers. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>Appendix F — Example operator scenarios & sample playbooks (expanded):</strong><br><strong>Operator scenario: "Export failed — staged fallback used"</strong><br>1. Operator sees PQ UI indicating export degraded. <br>2. Run diagnostics collect --correlation <id> to gather audit_tail, export failure row, and forensic_manifest. <br>3. If staged local artifact present, download and verify artifact checksum with local copy. <br>4. Re-run ExportAuditTail to alternate destination or rerun after network recovery; attach forensic_manifest and pq.audit.export.failure to SRE ticket if repeated. <br><strong>Operator scenario: "Need to reproduce a preview selection"</strong><br>1. Use audit query to find pq_preview row and evidenceRef containing previewSeed. <br>2. Request AuditReplay with deterministic=true and dryRun to reproduce preview in sandbox; present produced selection to stakeholder. <br>3. If reproduced selection differs, escalate to platform parity owners with evidenceRef and replay logs. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>Appendix G — Implementation notes & recommended library contracts (conceptual):</strong><br><strong>Canonical JSON rules for audit payloads:</strong><br>1. Sort object keys lexicographically. <br>2. Represent numbers in normalized decimal string form for canonicalization of SafeRound inputs. <br>3. Use RFC3339 UTC timestamps and avoid locale-dependent formatting. <br><strong>EvidenceRef format:</strong> evidenceRef should be an opaque URI-like token referencing encrypted evidence store object and include a short fingerprint to allow verification without revealing the payload. <br><strong>Rotation manifest format:</strong> include rotationSequence, prevRotationHash, rotationRootHash, rotationRowRange, signerKeyId, signingCertFingerprint, timestamp, and optional human-readable summary; manifest signed with signerKeyId. <br><strong>AtomicWrite contract with audit:</strong> AtomicWrite must return artifactChecksum and tempPaths inspected in pq.audit.atomic.repair entries when failures occur. Evidence store writes must return evidenceHash used in top-level audit rows as paramsHash. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>Closing operational constraint (must not be bypassed):</strong><br>All PQ operations that produce artifacts consumed by downstream systems must: emit a user action audit row, persist full parameters and seeds to evidence store via AuditEvidenceStoreWrite, append a chained audit row using AppendChainedAudit, ensure rotation signing via RotateAndSignAuditTail, and export only signed artifacts. This chain is mandatory for regulated outputs, PII-touching workflows, and artifacts requiring downstream trust. </td></tr><tr><td data-label="PQ_Audit — Per-function Expert Technical Breakdown"> <strong>Final verification note:</strong> The PQ_Audit design above is intentionally prescriptive: it enforces deterministic chaining, evidence-backed replay, atomic persistence semantics, rotation signing, and strict PII handling. Implementations must provide cross-language canonical encodings and golden vectors to ensure parity across Excel/VBA, worker runtimes, and CI verification. </td></tr></tbody></table></div><div class="row-count">Rows: 44</div></div><script src="assets/xlsx.full.min.js?v=1758605028" defer></script>
<script src="assets/script.js?v=1759748863" defer></script>
<script src="assets/worker.js?v=1758331710" defer></script>
<script>
(function(){
  const template = "{table}_{date}";
  const userVal = "";
  const hrefPrefix = "assets";
  function formatName(tableName) {
    const date = (new Date()).toISOString().slice(0,10);
    return template.replace('{table}', tableName).replace('{date}', date).replace('{user}', userVal);
  }
  const btn = document.getElementById('exportBtn');
  if(btn) {
    btn.addEventListener('click', async function() {
      try {
        const html = document.documentElement.outerHTML;
        if(html.length > 2000000) { alert('Export refused: html too large'); return; }
        if(window.Worker) {
          const workerUrl = (function(){ try{ return hrefPrefix + '/worker.js'; }catch(e){ return null; } })();
          if(workerUrl) {
            try {
              const worker = new Worker(workerUrl);
              worker.postMessage({html: html, format: 'pdf'});
              worker.onmessage = function(e) { console.log('worker:', e.data); alert('Worker replied: '+(e.data.msg||e.data.status)); };
            } catch(err) { console.warn('Worker create failed', err); alert('Worker not available'); }
          } else { alert('Worker not available'); }
        } else { alert('Export worker not supported in this environment.'); }
      } catch(err) { console.warn('Export failed', err); alert('Export worker not available. See console for details.'); }
    });
  }
})();
window.addEventListener('load', function(){ try{ document.querySelectorAll('.table-wrapper').forEach(function(e){ e.style.opacity='1'; }); }catch(e){} });
</script>
</div>
</body>
</html>