<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='utf-8'><meta name='viewport' content='width=device-width,initial-scale=1'>
<title>Tables Viewer v2.1</title>
<style>:root{--main-header-height:56px;} .table-wrapper{opacity:0;min-height:24px;transition:opacity .12s linear}</style>
<link rel="stylesheet" href="assets/style.css?v=1769960840">
<link rel="stylesheet" href="assets/overrides.css?v=1771304657">
</head><body>
<div id="tables-viewer" role="region" aria-label="Tables viewer">
<div id="stickyMainHeader">
<div id="tv-header">
<div><h1>Tables Viewer v2.1</h1></div>
<div style="display:flex;gap:8px;align-items:center;">
<input id="searchBox" class="tv-search" type="search" placeholder="Search" aria-label="Search tables" />
<button id="modeBtn" type="button" onclick="toggleMode()" aria-label="Toggle theme">Theme</button>
<button id="toggleAllBtn" type="button" aria-label="Toggle all" onclick="toggleAllTables()">Collapse All Tables</button>
<button id="copyAllPlainBtn" class="copy-btn" type="button" onclick="copyAllTablesPlain()" aria-label="Copy all tables as plain text">Copy All Tables (Plain Text)</button>
<button id="copyAllTablesBtn" class="copy-all-btn" type="button" onclick="copyAllTablesMarkdown()" aria-label="Copy All Tables (Markdown)">Copy All Tables (Markdown)</button>
<button id="copyAllMdBtn" style="display:none" aria-hidden="true"></button>
<button id="resetAllBtn" type="button" onclick="resetAllTables()" aria-label="Reset All Tables">Reset All Tables</button>
</div></div>
<script>
(function(){
  function ensureDelegation(){
    try {
      var vis = document.getElementById('copyAllTablesBtn');
      var alias = document.getElementById('copyAllMdBtn');
      if(!vis || !alias) return;
      alias.addEventListener = function(type, listener, options){
        vis.addEventListener(type, listener, options);
      };
      alias.removeEventListener = function(type, listener, options){
        vis.removeEventListener(type, listener, options);
      };
      Object.defineProperty(alias, 'onclick', {
        set: function(fn){ vis.onclick = fn; },
        get: function(){ return vis.onclick; },
        configurable: true
      });
      alias.focus = function(){ vis.focus(); };
      alias.blur = function(){ vis.blur(); };
    } catch(e) {
      try{ console && console.warn && console.warn('alias delegation failed', e); }catch(_){} 
    }
  }
  if(document.readyState === 'loading'){
    document.addEventListener('DOMContentLoaded', ensureDelegation);
  } else {
    ensureDelegation();
  }
})();
</script>
<noscript><div style='color:#b91c1c'>JavaScript is disabled. Tables will be shown statically. For large tables enable JS for virtualization.</div></noscript>
<div id="tocBar" role="navigation" aria-label="Table of contents"><ul><li class="toc-item"><a class="toc-link" href="#Table1">Table 1</a></li>
<li class="toc-item"><a class="toc-link" href="#Table2">Table 2</a></li>
<li class="toc-item"><a class="toc-link" href="#Table3">Table 3</a></li>
<li class="toc-item"><a class="toc-link" href="#Table4">Table 4</a></li>
<li class="toc-item"><a class="toc-link" href="#Table5">Table 5</a></li>
<li class="toc-item"><a class="toc-link" href="#Table6">Table 6</a></li>
<li class="toc-item"><a class="toc-link" href="#Table7">Table 7</a></li></ul></div></div>
<div class="table-caption" id="Table1" data-table="Docu_0183_01" style="margin-top:2mm;margin-left:3mm;"><strong>Table 1</strong></div>
<div class="table-wrapper" data-table-id="table-1"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Project 616 — Workflow"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Project 616 — Workflow</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Project 616 — Workflow"> <strong>Purpose (one line):</strong> Deterministic, auditable pipeline that ingests payroll + GL, snapshots mapping, seeds reproducible plans, runs deterministic previews, surfaces suggested JEs, and produces governed exports/applies with full evidence and signed run reports. <br><br><strong>Module catalog (owner · API · primary audits)</strong><br>- <strong>REG_IngestPayroll</strong> · <code>LoadPayrollPostings(sourceUri,policy) -&gt; PayrollRow[]</code> · <code>payroll.ingest.success|partial|invalid</code> (<code>ingestChecksum</code>, <code>ingestManifestRef</code>).<br>- <strong>REG_IngestGL</strong> · <code>LoadGLEntries(sourceUri,policy) -&gt; GLRow[]</code> · <code>gl.ingest.success|partial|invalid</code> (<code>glIngestChecksum</code>, <code>glIngestManifestRef</code>).<br>- <strong>REG_Mapping</strong> · <code>LoadCOAMapping(source,mode) -&gt; mapManifest(mapHash)</code>; <code>ComputeMappingHash()</code> · <code>payroll.mapping.loaded</code>, <code>payroll.mapping.refresh.*</code> (includes <code>mapHash</code>, <code>diffSummary</code>, <code>approvalsRef</code>).<br>- <strong>REG_Plan</strong> · <code>BuildReconciliationPlan(params) -&gt; planRecord(planId,paramsHash,planSeed)</code> · <code>payroll.recon.plan.built</code>. <br>- <strong>REG_Preview</strong> · <code>PreviewReconciliation(planId,sampleSize,operatorId) -&gt; previewManifest(previewHash)</code> · <code>payroll.recon.preview</code> (<code>samplingMeta</code>, <code>issues[]</code>, <code>previewRef</code>).<br>- <strong>REG_Aggregation</strong> · <code>ComputeVarianceAggs(payrollAgg,glAgg,aggKeys) -&gt; varianceRows</code> (canonical group keys, integer minor-units arithmetic).<br>- <strong>REG_Suggestions</strong> · <code>SuggestCorrectionJE(varianceRow,jeSpec) -&gt; suggestion</code> · <code>payroll.je.suggested</code> (<code>suggestionId</code>, <code>confidenceBreakdown</code>, <code>evidenceRefs</code>).<br>- <strong>REG_ApplyExport</strong> · <code>GenerateJEExport(acceptedJEs,exportSpec)</code> & <code>ApplyCorrections(acceptedJEs,mode,approvals)</code> · <code>payroll.je.exported</code>, <code>payroll.je.apply.*</code> (persist <code>applyDescriptor</code> <em>before</em> execution).<br>- <strong>REG_EvidenceAudit</strong> · <code>PersistEvidenceBlob(bytes,meta) -&gt; evidenceRef</code>; <code>BuildReconciliationReport(runId) -&gt; recon_report(reportHash)</code> · <code>payroll.recon.report.generated</code>, <code>evidence.access.*</code> (chain-of-custody, signatures).<br>- <strong>REG_RuntimeHelpers / CORE_JobScheduler / Worker</strong> · seeded RNG, SafeInvoke wrapper, job persistence, cooperative cancellation, telemetry <code>payroll.handler.*</code>. </td></tr><tr><td data-label="Project 616 — Workflow"> <strong>Canonical artifacts & names</strong><br>- <code>ingest_&lt;system&gt;_&lt;fingerprint&gt;_&lt;ts&gt;.json</code> (<code>ingestChecksum</code>).<br>- <code>map_&lt;mapHash&gt;.json</code> (canonical mapping snapshot).<br>- <code>plan_&lt;planId&gt;_&lt;paramsHash&gt;.json</code> (includes <code>planSeed</code>, <code>samplingMeta</code>).<br>- <code>preview_&lt;planId&gt;_&lt;previewHash&gt;.zip</code> (contains <code>variance_report.csv</code>, <code>suggested_jes_preview.csv</code>, <code>preview_manifest.json</code>).<br>- <code>JE_Export_&lt;planId&gt;_&lt;applyId&gt;_&lt;exportChecksum&gt;.&lt;ext&gt;</code> + <code>export_manifest.json</code>.<br>- <code>apply_&lt;applyId&gt;.json</code> (applyDescriptor persisted <strong>before</strong> any mutative action).<br>- <code>recon_report_&lt;runId&gt;_&lt;reportHash&gt;.json</code> (signed when signing enabled).<br><strong>Checksum policy:</strong> <code>sha256</code> over canonicalized UTF-8 bytes; manifests declare <code>canonicalVersion</code> and <code>checksumAlgorithm</code>. </td></tr><tr><td data-label="Project 616 — Workflow"> <strong>Canonical invariants (must enforce)</strong><br>- Deterministic canonicalization recipes (string NFKC, casefold, whitespace collapse).<br>- Numeric serialization: fixed-decimal or <code>AmountMinorUnits</code> + <code>scale</code> (no float drift).<br>- Mapping hash recipe: remove transients, zero-pad numeric fields, sort by <code>mapKey</code>+<code>Priority</code>, concatenate, sha256. <br>- Sampling: PRNG = HMAC_DRBG-SHA256 (or equivalent) seeded by <code>planSeed</code>; store <code>samplingMeta</code> in plan/preview. </td></tr><tr><td data-label="Project 616 — Workflow"> <strong>End-to-end sequence (numbered flow)</strong><br>1. <strong>Ingest</strong> — run <code>LoadPayrollPostings</code> / <code>LoadGLEntries</code>; produce canonical CSV and <code>ingestManifest</code>/<code>glIngestManifest</code>; persist raw + canonical evidence (evidenceRefs). Emit ingest audits and metrics (<code>ingestLatencyMs</code>).<br>2. <strong>Persist evidence</strong> — <code>REG_EvidenceAudit</code> stores canonical artifacts, signs manifests (Phase 2+), records chain-of-custody.<br>3. <strong>Mapping snapshot</strong> — <code>REG_Mapping</code> produces <code>mapHash</code>; store snapshot <code>map_&lt;mapHash&gt;.json</code> (immutable). If semantics change → require <code>migrationManifest</code>.<br>4. <strong>Plan generation</strong> — <code>REG_Plan</code> canonicalizes params → <code>paramsHash</code>; compute <code>planSeed = sha256(paramsHash + &quot;:&quot; + mapHash + &quot;:&quot; + ingestChecksum)</code>; persist <code>planRecord</code> with <code>requiredApprovals</code> and <code>expectedGroupCount</code> estimate.<br>5. <strong>Preview</strong> — <code>REG_Preview</code> rehydrates plan → deterministic sample → apply <code>mapHash</code> snapshot → <code>REG_Aggregation</code> aggregates → produce <code>variance_report</code> + <code>suggestedJEs</code> → package <code>preview_&lt;planId&gt;_&lt;previewHash&gt;.zip</code> (redacted analyst artifacts + encrypted full evidence). Emit <code>payroll.recon.preview</code> audit.<br>6. <strong>Accept & Export/Apply</strong> — analyst accepts suggestions → <code>GenerateJEExport</code> validates and serializes export artifact; <code>ApplyCorrections</code> persists <code>applyDescriptor</code>, validates approvals, then executes <code>create_export</code> or <code>post_direct</code> (ephemeral token + idempotency token). Persist <code>postedJournalIds</code>, <code>afterChecksums</code>, emit apply audits.<br>7. <strong>Run report & verification</strong> — <code>BuildReconciliationReport(runId)</code> collects artifacts, computes <code>reportHash</code>, signs and persists; <code>VerifyReportParity</code> periodically recomputes and alerts on mismatch. </td></tr><tr><td data-label="Project 616 — Workflow"> <strong>Decision gates, governance & required artifacts</strong><br>- <strong>Migration manifest required</strong> when changing canonicalization, sampling, rounding or map semantics (fields: <code>migrationId</code>, <code>changeRationale</code>, <code>sampleFixtures[]</code>, <code>canaryPlan</code>, <code>rollbackPlan</code>, <code>approvals[]</code>).<br>- <strong>Mapping hot-swap</strong> → compute <code>MappingDiff</code>, run smoke previews (top-N), compare KPI deltas; require <code>approvalsRef</code> for large diffs or regulated segments; swap pointer atomically on pass. <br>- <strong>Apply (post_direct)</strong> → require two-person approval + ephemeral credential issuance; persist <code>applyDescriptor</code> <em>before</em> posting. <br>- <strong>PII access</strong> → <code>EvidenceAccessApprovalFlow</code> + chain-of-custody logging; time-limited retrieval tokens. </td></tr><tr><td data-label="Project 616 — Workflow"> <strong>CI & parity enforcement (blocking)</strong><br>- Mandatory golden parity tests: <code>parity:golden:REG_IngestPayroll</code>, <code>REG_IngestGL</code>, <code>REG_Mapping</code>, <code>REG_Preview</code>, <code>REG_ApplyExport</code>.<br>- PRs that alter canonical outputs must include migration manifest, golden before/after fixtures and <code>approvalsRef</code>. CI blocks merges until gates satisfied. Nightly parity runner monitors and opens incidents on divergence. </td></tr><tr><td data-label="Project 616 — Workflow"> <strong>Runbook triggers & first-response checklist</strong><br>- <strong>Ingest partial/invalid</strong>: record <code>correlationId</code> → fetch <code>ingestManifest</code> → inspect <code>issues[]</code> (evidenceRef) → run preflight header/encoding fixes → re-ingest with <code>correctionOf</code> link. <br>- <strong>Preview parity failure</strong>: stop applies, fetch <code>planSeed</code>/<code>samplingMeta</code>/<code>canonicalVersion</code>, run cross-runtime reproduction, notify mapping/ingest owners. <br>- <strong>Apply partial failure</strong>: fetch <code>applyDescriptor</code>, attempt governed retries, or run <code>RevertJEs</code> using <code>postedJournalIds</code>; assemble <code>forensic_manifest</code> if manual intervention required. <br>- <strong>Evidence retrieval request</strong>: require <code>approvalsRef</code> → issue time-limited token → log chain-of-custody. </td></tr><tr><td data-label="Project 616 — Workflow"> <strong>Operational KPIs & SLOs (recommended)</strong><br>- Golden parity pass rate ≥ <strong>99.9%</strong> (regulated fixtures).<br>- Mean time to detect parity failure < <strong>15 minutes</strong>.<br>- Plan build median < <strong>200 ms</strong>; Preview median < <strong>2 s</strong> for ≤500 rows. <br>- Apply idempotent success rate (no duplicate postings) ≥ <strong>99.95%</strong>. <br>- Evidence retrieval approval SLA < <strong>24 business hours</strong>. </td></tr><tr><td data-label="Project 616 — Workflow"> <strong>Operator CLI cheat-sheet (canonical commands)</strong><br>- <code>recon.build-plan --tolerance 0.01 --operator alice --cost-center CC100</code> → prints <code>planId</code>.<br>- <code>recon.preview --plan &lt;planId&gt; --sample 500 --operator alice</code> → returns <code>previewRef</code> and <code>previewHash</code>.<br>- <code>recon.suggest-je --preview &lt;previewRef&gt;</code> → creates suggested JEs (returns suggestion ids).<br>- <code>recon.generate-je --preview &lt;previewRef&gt; --accept &lt;ids&gt; --exportSpec GL_CSV_v1</code> → writes <code>JE_Export_...</code> and <code>export_manifest</code>.<br>- <code>recon.apply --export &lt;exportPath&gt; --mode post_direct --approvals &lt;ap-123,ap-456&gt;</code> → requires ephemeral token issuance and two-person approval. </td></tr><tr><td data-label="Project 616 — Workflow"> <strong>Quick troubleshooting matrix (when things go wrong)</strong><br>- <code>previewHash</code> mismatch: check <code>planSeed</code>, <code>mapHash</code>, <code>ingestChecksum</code>, <code>samplingMeta</code> versions. <br>- Missing mapping results: check <code>mapHash</code> snapshot existence, <code>mapping.invalid</code> diagnostics, and owner approvals. <br>- Export rejected by GL loader: verify <code>exportSpec</code>, run loader-simulated import, check <code>export_checksum</code> and <code>export_manifest</code>. </td></tr><tr><td data-label="Project 616 — Workflow"> <strong>Immediate recommended priorities (implementation)</strong><br>1. Publish canonicalization reference libs and golden fixtures (ingest, mapping, preview, export).<br>2. Implement Evidence Audit (immutable persist + signing) and capture <code>evidenceRef</code> in all audits. <br>3. Add CI parity gates and <code>migrationManifest</code> enforcement. <br>4. Implement mapping hot-swap smoke harness and approvals capture. </td></tr></tbody></table></div><div class="row-count">Rows: 11</div></div><div class="table-caption" id="Table2" data-table="Docu_0183_02" style="margin-top:2mm;margin-left:3mm;"><strong>Table 2</strong></div>
<div class="table-wrapper" data-table-id="table-2"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Module Name"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Module Name</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Module Name"> <strong>Integrated Expansion Plan — Project 616 (module-level guidance)</strong><br><strong>Purpose of integration</strong><br>Embed the Expansion Plan (canonical parity, CI gating, evidence signing, deterministic sampling, scale/pushdown, suggestion provenance, governance, SRE/runbooks, security, and training) directly into module contracts, CI expectations, and operational narratives so every module documents its role in the roadmap and the phase-level deliverables, success metrics and migration controls. This integrated section maps each of the Top-10 expansion items and the three-phase timeline into actionable, module-specific tasks, acceptance criteria and test fixtures to ensure parity, governance and operational readiness are discovered and exercised at implementation time.<br><br><strong>How to read this document</strong><br>- The original module contract content follows unchanged. Immediately after each module contract block the <strong>Expansion Integration</strong> subsection describes exact tasks required by the Expansion Plan for that module, the phase(s) where tasks are delivered, the tests/golden fixtures required (by name), the CI gating checks, and evidence artifacts to persist (naming conventions). These integration notes are prescriptive — they define the acceptance criteria that will cause downstream CI gating to allow merges for regulated fixtures. Each integration note references specific deliverables from Phase 1 (0–90d), Phase 2 (90–180d) and Phase 3 (180–365d) and ties them to the SLOs, runbook items and migration manifest fields required. <br><br><strong>Global acceptance criteria (applies to each module)</strong><br>1. Each module must supply cross-language golden fixtures (VBA/PQ/backend) that validate byte-for-byte canonical outputs for at least 3 representative regulated datasets and one adversarial fixture. Fixtures must be named <code>golden_&lt;module&gt;_&lt;fixtureId&gt;_&lt;canonicalizationVersion&gt;.csv</code> and uploaded to the evidence store under <code>evid:golden/</code>. <br>2. Each module must include a CI parity test that fails the PR on mismatch with the golden fixture for regulated fixtures. CI test step name: <code>parity:golden:&lt;module&gt;</code>. <br>3. Each module must annotate artifact outputs with canonicalization version, <code>canonicalVersion</code>, and include <code>migrationImpact</code> tag when producing new canonicalization behavior. <br>4. All changes touching canonicalization, sampling or rounding require a <code>migration_manifest</code> as per policy and must be blocked by CI gating until approvals recorded in evidence (<code>approvalsRef</code>). <br>5. All artifacts emitted by modules must include evidence references and be persisted via REG_EvidenceAudit with signature metadata when required by phase deliverables. <br><br><strong>Phase mapping summary (module responsibilities)</strong><br>- <strong>Phase 1 (0–90d)</strong>: Implement canonicalization reference lib for ingest/mapping/aggregation; produce minimal golden fixtures for regulated payroll and GL; implement <code>parity:golden</code> CI step for REG_IngestPayroll, REG_Mapping and REG_Preview; publish migration-manifest template and minimal approval UI wired to Evidence API; baseline telemetry from REG_RuntimeHelpers (ingestLatencyMs, parityFailure alerts). <br>- <strong>Phase 2 (90–180d)</strong>: Add canary smoke preview harness in REG_Mapping & REG_Preview; deterministic sampling library (HMAC_DRBG-SHA256) integrated into REG_Preview/REG_Plan; evidence signing implemented in REG_EvidenceAudit; parity dashboard surfacing module-level badge; mapping hot-swap tool with smoke-tests. <br>- <strong>Phase 3 (180–365d)</strong>: Pushdown aggregation support in REG_Aggregation (DB/warehouse connectors and canonical export format), streaming ingest pilots in REG_IngestPayroll, hardened suggestion engine in REG_Suggestions (explainability + residual absorption), automated apply/revert flows in REG_ApplyExport with ephemeral credential integration and SRE playbooks. <br><br><strong>Audit & SLO tie-ins</strong><br>- Golden parity pass rate target: module-level <code>parity:golden</code> success ≥ 99.9% on regulated fixtures (monitored by parity dashboard). <br>- Mean time to detect parity failure: metric <code>parity.detect.ms</code> < 15 minutes (REG_RuntimeHelpers emits and REG_EvidenceAudit retains detection events). <br>- Evidence retrieval SLA: <code>evidence.access.approval.ms</code> < 24 business hours. <br><br><strong>Migration manifest enforcement (immediate)</strong><br>Every module change touching canonicalization/sampling/rounding or map semantics must include <code>migration_manifest</code> fields populated and evidence persisted. CI gating must inspect for presence of manifest and approvalsRef before allowing merge for regulated branches. <br><br><strong>Developer checklist (must be satisfied for PR merge)</strong><br>1. All changed canonical outputs have corresponding golden fixtures uploaded as <code>evid:golden/&lt;module&gt;/&lt;fixture&gt;</code> and <code>parity:golden</code> CI test referencing them. <br>2. If sampling/PRNG touched, samplingMeta version incremented and <code>prng_golden</code> reproduction tests added. <br>3. If rounding or residual absorption changed, include <code>rounding_golden</code> fixtures with per-line residual absorbtion logs. <br>4. EvidenceAudit must persist <code>recon_report</code> for the CI run and sign it if evidence signing is required. <br><br><strong>Cross-module mapping table (top-level)</strong><br>- Canonicalization & parity hardening → REG_IngestPayroll, REG_IngestGL, REG_Mapping, REG_Aggregation, REG_Preview.<br>- CI/CD & parity gating → All modules (CI parity steps and parity dashboard).<br>- Evidence/signing & retention → REG_EvidenceAudit mandatory provider.<br>- Deterministic sampling & replay → REG_Plan, REG_Preview, REG_RuntimeHelpers.<br>- Suggestion engine & provenance → REG_Suggestions, REG_Preview, REG_ApplyExport.<br>- Pushdown & scale → REG_Aggregation, REG_IngestPayroll pilots, REG_Preview scale connectors.<br>- Observability & SLOs → REG_RuntimeHelpers + module-level metrics.<br>- Governance & migration manifests → REG_Mapping, REG_Plan, REG_ApplyExport enforced by CI.<br>- Runbooks/training → All modules produce module-specific runbook snippets to be collated into training packs stored under <code>evid:runbooks/</code>.<br><br><strong>Note:</strong> The detailed module blocks below include the original content (unchanged) followed by an <strong>Expansion Integration</strong> subsection that implements the tasks above for each module. Implementers must read both sections together. </td></tr><tr><td data-label="Module Name"> <strong>REG_IngestPayroll — narrative, contract, dependencies and implementation guidance</strong><br><strong>Purpose & condensed objective</strong><br>Provide authoritative, deterministic ingestion of payroll posting sources (workbook tables, CSV/TSV, compressed exchanges, SFTP drops, API payloads) into canonical <code>PayrollRow[]</code>. Responsibilities: parse, normalize, canonicalize, persist raw & canonical evidence, compute stable checksums and produce an <code>ingestManifest</code>. The ingest module must never perform mapping or business transformations — mapping is a separate, auditable step. The ingest module is the immutable source of truth for payroll raw evidence and canonical rows used across plans and previews.<br><br><strong>Public surface (signatures — VBA style)</strong><br><code>Function LoadPayrollPostings(sourcePath As String, sourceType As String, policy As String) As Collection</code> — orchestrates ingest, returns <code>PayrollRow[]</code>, persists <code>ingestManifest</code> and canonical CSV to evidence store.<br><code>Function DetectEncoding(bytes() As Byte) As String</code> — returns encoding label (UTF-8/16LE/16BE/Latin1).<br><code>Function GuessDelimiter(sampleText As String) As String</code> — returns <code>,</code>/<code>;</code>/<code>\t</code>/<code>|</code>.<br><code>Function MapHeaders(rawHeaders As Collection, synonyms As Dictionary) As Dictionary</code> — returns canonical header→field mapping; deterministic fuzzy fallback and audit of suggestions.<br><code>Sub StreamParseSource(sourcePath As String, onRowCallback As String)</code> — stream parse with callback per row to avoid memory blowup.<br><code>Function NormalizeRow(rawRow As Dictionary, headerMap As Dictionary, scale As Long) As Dictionary</code> — canonical field normalization and per-row diagnostics.<br><code>Function CoerceNumericFixedScale(rawValue As Variant, scale As Long) As String</code> — returns fixed-decimal string and minor-units integer.<br><code>Function CanonicalizeDate(rawDate As Variant) As String</code> — returns <code>YYYY-MM-DD</code>, flags ambiguous parses.<br><code>Function ComputePayrollRowId(canonicalRowString As String, positionToken As String) As String</code> — deterministic <code>sha256</code> hex id.<br><code>Function PersistRawEvidence(bytes() As Byte, metadata As Dictionary) As String</code> — uploads bytes to evidence store returns <code>evidenceRef</code> handle.<br><code>Function BuildIngestManifest(sourceUri As String, rows As Collection, metrics As Dictionary, policy As String) As Dictionary</code> — persists manifest; returns manifest object.<br><code>Sub EmitIngestAudit(manifest As Dictionary)</code> — emits PII-free audit row referencing evidence.<br><br><strong>Canonical <code>PayrollRow</code> contract (field-level)</strong><br>- <code>rowId</code> (string): deterministic sha256 over canonical row position token + canonical row serialization. <br>- <code>rawPayloadRef</code> (string): per-row evidence pointer (file, offset). <br>- <code>EmployeeID</code> (nullable string): tokenized in analyst view; raw stored encrypted in evidence. <br>- <code>PayComponent</code> (string): canonical token (trim, casefold, unicode normal). <br>- <code>CostCenter</code> (nullable string). <br>- <code>Amount</code> (decimal string): fixed-decimal serialization; store <code>Scale</code> and <code>AmountMinorUnits</code> (integer) as explicit fields. <br>- <code>Currency</code> (nullable ISO 4217) <br>- <code>PostingDate</code> (ISO date <code>YYYY-MM-DD</code>) <br>- <code>Reference</code> (nullable string) <br>- <code>Mapped_GLAccount</code> (nullable string): must be null at ingest. <br>- <code>ingestChecksum</code> (batch level): sha256 of canonical CSV for reproducibility.<br><br><strong>Deterministic canonicalization recipe (required to remain stable)</strong><br>1. Byte normalization: strip BOM, normalize newline to <code>\n</code>, remove trailing whitespace on lines, Unicode normalize (NFKC). <br>2. Header canonicalization: apply deterministic normalization (NFKC → casefold → collapse whitespace → strip control chars), then consult header synonyms dictionary. If no exact match, apply deterministic fuzzy match with threshold; if used, record suggestion in manifest. <br>3. Per-row normalization: trim string fields, apply header map, coerce numerics with explicit <code>scale</code>, produce <code>AmountMinorUnits = Round(amount * 10^scale)</code> using Banker's rounding if not otherwise configured. <br>4. Row serialization for <code>rowId</code> & batch checksum: use fixed field order, explicit <code>|</code> separators, escape <code>|</code> deterministically (e.g., replace with <code>\|</code>), do not include transient fields (<code>createdBy</code>, <code>createdTs</code>). <br>5. Batch canonical CSV assembled deterministically (rows sorted only if ordering required by caller); compute <code>ingestChecksum = sha256(UTF8(canonicalCSV))</code>. Persist canonical CSV and original bytes to evidence store.<br><br><strong>Error handling, policies and diagnostics</strong><br>Support three policies: <code>fail-fast</code> (abort ingest on first error), <code>skip-and-flag</code> (persist valid rows and store failing rows to evidence with <code>ingestManifest.issues[]</code>), and <code>tolerate-with-corrections</code> (attempt deterministic header suggestions and numeric coercions; record all corrections with <code>evidenceRef</code>). Diagnostics must include <code>correlationId</code>, <code>itemContext</code> (rowId or offset), <code>errorCode</code> and <code>evidenceRef</code>. All diagnostics persisted encrypted and referenced by the manifest.<br><br><strong>Observability and metrics</strong><br>Emit metrics: <code>ingestLatencyMs</code>, <code>rowsSuccessCount</code>, <code>rowsFailedCount</code>, <code>sourceFingerprint</code> (sha256 of normalized source bytes), <code>ingestSchemaVersion</code>, and <code>ingestToolVersion</code>. Tag metrics with <code>operatorId</code> when relevant. Audit rows: <code>payroll.ingest.success</code>, <code>payroll.ingest.partial</code>, <code>payroll.ingest.invalid</code>.<br><br><strong>Operational narrative (story: analyst run with exception)</strong><br>1. Analyst uploads <code>Payroll_Mar2026.csv</code> to SFTP. Ingest job picks it up and calls <code>LoadPayrollPostings(sourcePath, &quot;csv&quot;, &quot;skip-and-flag&quot;)</code>. <br>2. <code>DetectEncoding</code> returns UTF-8, <code>GuessDelimiter</code> identifies comma; <code>MapHeaders</code> finds one unknown header <code>EmpIDNo</code> → fuzzy matched to <code>EmployeeID</code> and recorded in <code>ingestManifest.issues[]</code>. <br>3. Stream parser yields rows; one row contains <code>PostingDate=&quot;03/14/26&quot;</code> ambiguous: <code>CanonicalizeDate</code> flags ambiguous parse with <code>PAYROLL_AMBIG_DATE</code> and includes row offset in <code>evidenceRef</code>. Because policy is <code>skip-and-flag</code>, that row is persisted to error evidence; the rest of rows succeed. <br>4. Canonical CSV produced and <code>ingestChecksum</code> computed; manifest persisted. <code>payroll.ingest.partial</code> audit emitted with <code>ingestManifest</code> reference. <br>5. Analyst inspects manifest, downloads failing row via evidence approval flow, corrects source and re-uploads with <code>correctionOf</code> linking to prior manifest.<br><br><strong>Dependencies (explicit calls and contracts)</strong><br>- Consumes: none (ingest is a source). <br>- Provides to: <code>REG_Plan</code> (ingestChecksum used in plan seed), <code>REG_Preview</code> (reads canonical payroll rows and samples), <code>REG_Aggregation</code> (grouping input), <code>REG_EvidenceAudit</code> (ingest persists artifacts via Evidence API), <code>REG_RuntimeHelpers</code> (telemetry and safe errors). <br><strong>Contract guarantee:</strong> ingest manifests and canonical CSV must be immutable once persisted; any correction must create a <code>correctionOf</code> manifest linking to the previous. Consumers rely on <code>ingestChecksum</code> to reproduce plan seeds and previews.<br><br><strong>CI & tests</strong><br>Unit: header mapping permutations, numeric coercion across scales, date canonicalization matrix. Integration: encoding matrix, delimiter guesses, streaming behavior for 100k rows. Golden parity: canonical CSV bytes and <code>ingestChecksum</code> must match fixtures across VBA/PQ/backend implementations. Failure to match requires migration manifest.<br><br><strong>Example manifest snippet (representative)</strong><br><code>{ &quot;manifestId&quot;:&quot;ingest_payroll_202603&quot;,&quot;sourceUri&quot;:&quot;sftp://.../Payroll_Mar2026.csv&quot;,&quot;sourceFingerprint&quot;:&quot;sha256:...&quot;,&quot;ingestChecksum&quot;:&quot;sha256:...&quot;,&quot;rowsCount&quot;:12456,&quot;rowsSample&quot;:[&quot;r1&quot;,&quot;r2&quot;,&quot;r3&quot;],&quot;ingestSchemaVersion&quot;:&quot;1.0.0&quot;,&quot;policy&quot;:&quot;skip-and-flag&quot;,&quot;issues&quot;:[{&quot;code&quot;:&quot;PAYROLL_AMBIG_DATE&quot;,&quot;evidenceRef&quot;:&quot;evid:...&quot;,&quot;rowOffset&quot;:345}], &quot;canonicalEvidenceRef&quot;:&quot;evid:canonical_payroll_202603.csv&quot;,&quot;createdTs&quot;:&quot;2026-03-15T07:12:00Z&quot; }&lt;br&gt;&lt;br&gt;**Expansion Integration — REG_IngestPayroll**&lt;br&gt;**Phase 1 tasks (0–90d)**&lt;br&gt;- Deliver canonicalization reference functions as a small library (</code>canonical_ingest_v1<code>) with language ports (VBA/PQ/backend). Publish </code>golden_ingest_payroll_regulated_v1.csv<code> and </code>golden_ingest_payroll_adversarial_v1.csv<code> to evidence. &lt;br&gt;- Add </code>parity:golden:REG_IngestPayroll<code> CI job that computes </code>ingestChecksum<code> from PR branch and compares to golden fixtures for regulated tests. &lt;br&gt;- Wire </code>LoadPayrollPostings<code> to persist canonical CSV and </code>ingestManifest<code> via REG_EvidenceAudit and emit signed manifest </code>recon_report<code> placeholder (unsigned until Phase 2). &lt;br&gt;- Add minimal telemetry emits for </code>ingestLatencyMs<code> and </code>parity.detect.ms<code> via REG_RuntimeHelpers. &lt;br&gt;&lt;br&gt;**Phase 2 tasks (90–180d)**&lt;br&gt;- Integrate evidence signing for ingest manifests in REG_EvidenceAudit so </code>ingestManifest<code> is signed on persist (HSM/KMS-backed). &lt;br&gt;- Provide additional golden fixtures for streaming/large-file behavior (</code>golden_ingest_payroll_stream_v1<code>). &lt;br&gt;- Implement automated nightly parity checks that run </code>parity:golden<code> on a scheduled runner and open incidents if mismatch. &lt;br&gt;&lt;br&gt;**Phase 3 tasks (180–365d)**&lt;br&gt;- Implement streaming ingest pilots for large SFTP exchanges and deterministic chunking so canonical CSVs are produced in parts with stitched </code>ingestChecksum<code> semantics; add </code>ingestChunkManifest<code> schema. &lt;br&gt;- Ensure ingest persists sampling metadata used by REG_Preview sampling tests. &lt;br&gt;&lt;br&gt;**CI &amp; gating acceptance criteria**&lt;br&gt;- All PRs that modify ingest canonicalization must include updated golden fixtures with </code>migration_manifest<code> if canonicalization output changes. &lt;br&gt;- </code>parity:golden<code> must pass for regulated fixture set or CI blocks merge. &lt;br&gt;&lt;br&gt;**Artifacts produced (evidence names)**&lt;br&gt;- </code>evid:canonical_ingest_payroll_<date><em><canonicalVersion>.csv<code> &lt;br&gt;- </code>evid:golden/REG_IngestPayroll/<fixtureId>.csv<code> &lt;br&gt;- </code>recon_report_ingest</em><runId>_<reportHash>.json<code> (signed in Phase 2)   **REG_IngestGL — narrative, behavior, dependencies**&lt;br&gt;**Purpose &amp; condensed objective**&lt;br&gt;Canonical ingestion of GL extracts; normalize </code>GLAccount<code> tokens per COA rules, detect sign conventions and standardize </code>Amount<code> representation, persist </code>glIngestManifest<code> and raw/canonical evidence. GL ingest ensures a consistent canonical representation of ledger rows to be used by aggregation and reconciliation.&lt;br&gt;&lt;br&gt;**Public surface (signatures)**&lt;br&gt;</code>Function LoadGLEntries(sourcePath As String, policy As String) As Collection<code> — main orchestrator. &lt;br&gt;</code>Function ResolveSignConvention(row As Dictionary) As Dictionary<code> — returns signed </code>Amount<code> and </code>signConvention<code>. &lt;br&gt;</code>Function NormalizeGLAccount(rawAccount As String, coaRules As Dictionary) As String<code> — deterministic normalization. &lt;br&gt;</code>Function ValidateGLAccount(glAccount As String, coaFixture As Collection) As Boolean<code> — optional authoritative validation. &lt;br&gt;</code>Function BuildGLIngestManifest(sourceUri As String, rows As Collection, metrics As Dictionary) As Dictionary<code> &lt;br&gt;</code>Sub EmitGLIngestAudit(manifest As Dictionary)<code>&lt;br&gt;&lt;br&gt;**Canonical </code>GLRow<code> contract**&lt;br&gt;- </code>glRowId<code> deterministic id. &lt;br&gt;- </code>rawPayloadRef<code> pointer to original bytes/offset. &lt;br&gt;- </code>GLAccount<code> canonical per </code>coaRules<code>. &lt;br&gt;- </code>Amount<code> (signed fixed decimal string) and </code>AmountMinorUnits<code> plus </code>Scale<code>. &lt;br&gt;- </code>CostCenter<code>, </code>Currency<code>, </code>PostingDate<code>, </code>JournalReference<code>. &lt;br&gt;- </code>glIngestChecksum<code> batch checksum.&lt;br&gt;&lt;br&gt;**Sign convention resolution rules**&lt;br&gt;1. If a signed </code>Amount<code> column present and unambiguous, use it; set </code>signConvention = "SignedAmount"<code>. &lt;br&gt;2. If </code>Debit<code> and </code>Credit<code> columns present, convert to signed </code>Amount = Debit - Credit<code> with explicit policy recorded in manifest; set </code>signConvention = "DebitCreditColumns"<code>. &lt;br&gt;3. If both present or ambiguous, fail/flag per </code>policy<code>.&lt;br&gt;&lt;br&gt;**COA normalization rules (policy driven)**&lt;br&gt;</code>coaRules<code> object controls normalization: </code>stripPunctuation<code> (true/false), </code>padSegments<code> (array of segment widths or false), </code>segmentSeparator<code> char, </code>preserveCase<code> (false default). Normalization must be deterministic and versioned; the </code>coaRulesVersion<code> must be included in </code>glIngestManifest<code> so downstream consumers can reproduce behavior.&lt;br&gt;&lt;br&gt;**Error handling &amp; policies**&lt;br&gt;- Missing </code>GLAccount<code>: </code>gl.ingest.invalid<code> — fail or partial per policy. &lt;br&gt;- Invalid GL account (not found in COA fixture): </code>gl.ingest.partial<code> with diagnostic. &lt;br&gt;- Mixed sign convention unexplained: emit </code>GL_INGEST_003<code> and require operator input.&lt;br&gt;&lt;br&gt;**Operational narrative (story: GL extract validation failure)**&lt;br&gt;1. Finance publish </code>GL_Mar2026.csv<code>; ingest job runs; </code>LoadGLEntries<code> detects separate Debit/Credit columns but inconsistent usage (some rows have both blank). &lt;br&gt;2. Because </code>policy<code> is </code>skip-and-flag<code>, rows with missing Debit/Credit are flagged to </code>glIngestManifest.issues[]<code> and stored to evidence; rest process normally. </code>gl.ingest.partial<code> audit emitted. &lt;br&gt;&lt;br&gt;**Dependencies (explicit)**&lt;br&gt;- Consumes: COA fixtures (from REG_Mapping or central finance repository) for optional validation. &lt;br&gt;- Provides: </code>glIngestChecksum<code> to REG_Plan and REG_Preview; canonical GL rows to REG_Aggregation and REG_Preview; evidence persisted via REG_EvidenceAudit.&lt;br&gt;&lt;br&gt;**CI &amp; tests**&lt;br&gt;Unit tests: sign convention detection, COA normalization permutations. Integration: multi-currency sets, debit/credit inversions, large file streaming. Golden parity: canonical GL CSV and </code>glIngestChecksum<code> across runtimes.&lt;br&gt;&lt;br&gt;**Example manifest snippet (representative)**&lt;br&gt;</code>{ "manifestId":"gl_ingest_202603","sourceUri":"sftp://.../GL_Mar2026.csv","glIngestChecksum":"sha256:...","rowsCount":90000,"currencySet":["USD","EUR"],"signConvention":"DebitCreditColumns","coaRulesVersion":"v1.5","issues":[{"code":"GL_INGEST_002","evidenceRef":"evid:..."}], "createdTs":"2026-03-15T08:00:00Z" }<br><br><strong>Expansion Integration — REG_IngestGL</strong><br><strong>Phase 1 tasks (0–90d)</strong><br>- Publish <code>canonical_gl_normalizer_v1</code> with <code>coaRulesVersion</code> documented and cross-language ports. Upload <code>golden_ingest_gl_regulated_v1.csv</code>. <br>- CI parity step <code>parity:golden:REG_IngestGL</code> added. <br>- Add <code>coaRulesVersion</code> to <code>glIngestManifest</code> and ensure REG_Mapping exposes COA fixtures used for validation. <br><br><strong>Phase 2 tasks (90–180d)</strong><br>- Evidence signing of <code>glIngestManifest</code> via REG_EvidenceAudit. <br>- Add automated nightly COA parity check to detect drift between COA fixtures and mapping snapshots. <br><br><strong>Phase 3 tasks (180–365d)</strong><br>- Streaming GL ingest and chunked canonicalization for very large GL feeds with stitched <code>glIngestChecksum</code>. <br>- Provide DB pushdown connector in REG_Aggregation to reconcile warehouse-aggregated GL with canonical GL aggregates. <br><br><strong>CI & gating acceptance criteria</strong><br>- Any change to <code>coaRules</code> or sign convention rules must be accompanied by <code>migration_manifest</code> and golden fixtures demonstrating before/after behavior. <br><br><strong>Artifacts produced (evidence names)</strong><br>- <code>evid:canonical_gl_ingest_&lt;date&gt;_&lt;coaRulesVersion&gt;.csv</code> <br>- <code>evid:golden/REG_IngestGL/&lt;fixtureId&gt;.csv</code> </td></tr><tr><td data-label="Module Name"> <strong>REG_Mapping — canonical mapping loader, governance, dependencies and operational narrative</strong><br><strong>Purpose & condensed objective</strong><br>Load COA mapping manifests (PayComponent → GLAccount) from workbook/JSON, perform schema & referential validation, canonicalize tokens, deduplicate using deterministic precedence, compute canonical <code>mapHash</code> used by plans and previews, persist immutable snapshots, and enable controlled hot-swap with smoke testing and approvals.<br><br><strong>Public surface (signatures)</strong><br><code>Function LoadCOAMapping(sourcePath As String, mode As String) As Dictionary</code> — loads manifest, computes <code>mapHash</code>, persists snapshot. <br><code>Function SchemaValidateMappingRow(rawRow As Dictionary) As Dictionary</code>. <br><code>Function CanonicalizeMappingRow(rawRow As Dictionary, coaRules As Dictionary) As Dictionary</code>. <br><code>Function ComputeMapKey(mappingRow As Dictionary) As String</code>. <br><code>Function DeduplicateMappings(mappingRows As Collection) As Collection</code>. <br><code>Function ComputeMappingHash(mappingIndex As Collection) As String</code> — canonicalization recipe enforced. <br><code>Function BuildMapManifest(mappingIndex As Collection) As Dictionary</code>. <br><code>Sub EmitMappingLoaded(mapManifest As Dictionary)</code>. <br><br><strong>Per-row schema & invariants</strong><br>- Required: <code>PayComponent</code>, <code>GLAccount</code> (in <code>strict</code>), <code>Priority</code> (integer 0..9999). <br>- Owner required when rule affects regulated GL segments. <br>- <code>effectiveFrom</code>/<code>effectiveTo</code> nullable ISO dates. <br>- <code>mapRowId</code> deterministic. <br><br><strong>Canonicalization & <code>mapHash</code> recipe — must be identical across runtimes</strong><br>1. Drop transient fields (<code>uploadedBy</code>, <code>lastUpdatedTs</code>). <br>2. Token normalization: Unicode NFKC, lower case, trim, collapse internal whitespace, strip non-significant punctuation by COA policy. <br>3. Zero-pad <code>Priority</code> to 4 digits for fixed width numeric serialization. <br>4. Normalize <code>GLAccount</code> per <code>coaRules</code>. <br>5. Build <code>mapKey = normalized(PayComponent) + &#x27;|&#x27; + normalized(CostCenter||&#x27;&#x27;) + &#x27;|&#x27; + normalized(effectiveFrom||&#x27;&#x27;)</code>. <br>6. Sort mapping rows by <code>mapKey</code>, <code>Priority</code> asc, <code>mapRowId</code>. <br>7. Concatenate rows (fields in fixed order) separated by <code>|</code>, no trailing newline; UTF-8 encode and compute <code>sha256</code>. Persist <code>map_&lt;mapHash&gt;.json</code> with canonical string and <code>mapManifest</code> metadata.<br><br><strong>Deduplication & conflict handling</strong><br>When multiple rows share the same <code>mapKey</code>, choose row with lowest <code>Priority</code>. If <code>Priority</code> equal, stable tie-break by <code>mapRowId</code>. Conflicts recorded in <code>mapping.invalid</code> diagnostics and require owner resolution. Hot-swap blocked for conflicts touching regulated segments until approvals captured.<br><br><strong>Governance modes</strong><br>- <code>strict</code>: fail on missing <code>GLAccount</code> or schema mismatch. <br>- <code>suggest-only</code>: accept manifest but flag unmapped keys as suggestions. <br>- <code>auto-apply-lowconfidence</code>: dangerous; allowed only with explicit migration manifest containing approvals and canary plan.<br><br><strong>Hot-swap workflow & smoke testing</strong><br>1. Candidate mapping loaded to compute <code>afterHash</code>. <br>2. Compute <code>MappingDiff(beforeHash, afterHash)</code> with added/changed/removed keys, owners impacted and estimated affected row counts (using recent ingest profiles from REG_IngestPayroll). <br>3. Run smoke preview plans (top-N paycomponents) comparing before vs after KPIs (mapping coverage, top variance deltas). <br>4. If smoke passes and approvals present, atomically update active mapping pointer to <code>afterHash</code> and emit <code>payroll.mapping.refresh.succeeded</code>; otherwise leave <code>beforeHash</code> active and emit <code>...failed</code> with diagnostics. <br>5. Concurrency safe: running plans continue to reference their seeded <code>mapHash</code>; only newly created plans observe swapped mapping pointer.<br><br><strong>Dependencies (explicit)</strong><br>- Consumes: optional <code>coaFixture</code> for GLAccount validation; header synonym lists if mapping manifest includes alias tables. <br>- Provides: <code>mapHash</code> (immutable snapshot) to REG_Plan, REG_Preview and REG_ApplyExport. <br>- Works with REG_EvidenceAudit to persist mapping snapshots and mapManifest.<br><br><strong>Operational narrative (owner acceptance & hot-swap)</strong><br>1. Mapping owner uploads <code>COA_Map_v1.xlsx</code>. <code>LoadCOAMapping(..., &quot;strict&quot;)</code> runs, finds 3 conflicts and fails. Owner resolves conflicts in UI and re-submits. <br>2. New <code>mapHash</code> computed and persisted as <code>map_ab12...</code>. <code>RefreshMapping</code> invoked with <code>afterHash=map_ab12...</code> which runs smoke previews; due to significant diffs hitting regulated GL range, governance requires compliance sign-off. Approvals captured as <code>approvalsRef</code>. <br>3. Hot-swap enacted with atomic pointer change; audit chain records <code>hotSwap.auditChain</code>.<br><br><strong>CI & tests</strong><br>Cross-language parity for <code>ComputeMappingHash</code> is mandatory. Provide golden fixtures with canonical mapping strings and <code>mapHash</code> values. Mapping changes that alter semantics for regulated datasets must require migration manifest and two-person approval.<br><br><strong>Example <code>mapManifest</code> (representative)</strong><br><code>{ &quot;mapHash&quot;:&quot;sha256:ab12...&quot;,&quot;mapRowCount&quot;:1123,&quot;mapVersion&quot;:&quot;2026-03-01&quot;,&quot;rulesSummary&quot;:{&quot;coveragePct&quot;:0.93,&quot;conflicts&quot;:0}, &quot;canonicalMappingRef&quot;:&quot;evid:map_ab12.json&quot;, &quot;createdTs&quot;:&quot;2026-03-02T09:00:00Z&quot; }</code><br><br><strong>Expansion Integration — REG_Mapping</strong><br><strong>Phase 1 tasks (0–90d)</strong><br>- Publish canonical mapping library <code>canonical_mapping_v1</code> with <code>ComputeMappingHash</code> reference implementations in VBA/PQ/backend and provide <code>golden_map_regulated_v1.json</code>. <br>- Add CI job <code>parity:golden:REG_Mapping</code> to compute <code>mapHash</code> and validate against golden. <br>- Implement <code>migration_manifest</code> template validation hook in CI to block merges absent manifest for semantic changes. <br><br><strong>Phase 2 tasks (90–180d)</strong><br>- Implement mapping hot-swap harness that runs smoke previews via REG_Preview; capture <code>MappingDiff</code> artifacts and store them under <code>evid:hotSwap/&lt;timestamp&gt;/</code>. <br>- Add mapping approval UI wired to EvidenceAudit to persist <code>approvalsRef</code> and record <code>hotSwap.auditChain</code>. <br><br><strong>Phase 3 tasks (180–365d)</strong><br>- Provide automated canary promotion jobs and rollback hooks integrated with CI/CD; collect KPIs and implement automatic revert if parity/SLO breaches detected. <br><br><strong>CI & gating acceptance criteria</strong><br>- Any PR that changes mapping canonicalization must include updated <code>golden_map</code> fixtures and <code>migration_manifest</code> (if semantics changed). <br><br><strong>Artifacts produced (evidence names)</strong><br>- <code>evid:map_&lt;mapHash&gt;.json</code> <br>- <code>evid:golden/REG_Mapping/&lt;fixtureId&gt;.json</code> <br>- <code>evid:hotSwap/&lt;timestamp&gt;/diff.json</code> </td></tr><tr><td data-label="Module Name"> <strong>REG_Plan — deterministic plan builder, governance & dependencies</strong><br><strong>Purpose & condensed objective</strong><br>Produce immutable <code>ReconciliationPlan</code> artifacts that seed previews and control apply semantics. A plan captures all parameters (aggregation keys, tolerances, sampleSize, mode) and binds them to <code>mapHash</code> and <code>ingestChecksum</code> to guarantee reproducibility. Plans also determine approval requirements and estimate cost/affected group counts for governance.<br><br><strong>Public surface (signatures)</strong><br><code>Function BuildReconciliationPlan(params As Dictionary) As Dictionary</code> — returns persisted <code>planRecord</code>. <br><code>Function CanonicalizeParams(params As Dictionary) As String</code> — deterministic param serialization. <br><code>Function ComputeParamsHash(canonicalParamsString As String) As String</code>. <br><code>Function ComposePlanSeed(paramsHash As String, mapHash As String, ingestChecksum As String) As String</code>. <br><code>Function GenerateDeterministicPlanId(planSeed As String) As String</code>. <br><code>Function ResolveAggregationKeys(params As Dictionary) As Collection</code>. <br><code>Function DetermineSamplingSeed(planSeed As String) As String</code>. <br><code>Function EstimateCostAndApprovals(dqMetrics As Dictionary) As Dictionary</code>. <br><code>Sub PersistPlanRecord(planRecord As Dictionary)</code>. <br><br><strong>Canonical param rules and plan seed composition</strong><br>- Canonicalize param keys lexicographically. <br>- Serialize booleans as <code>0/1</code>, decimals to fixed scale, dates as <code>YYYY-MM-DD</code>. <br>- <code>paramsHash = sha256(UTF8(canonicalParamsString))</code>. <br>- <code>planSeed = sha256(paramsHash + &quot;:&quot; + mapHash + &quot;:&quot; + ingestChecksum)</code>. <br>- <code>planId = deterministic representation (base58 of prefix of planSeed)</code> but persist full <code>planSeed</code> and <code>paramsHash</code>. <br><br><strong>Aggregation key resolution & sampling</strong><br>Default <code>aggregationKeys</code> = <code>GLAccount + CostCenter</code>. Optional keys: <code>Currency</code>, <code>Entity</code>, <code>PayrollPeriod</code>. <code>sampleSeed = planSeed</code>. Sampling algorithm details recorded in plan: PRNG algorithm identifier, PRNG version and <code>seed</code> used. <code>expectedGroupCount</code> estimated using ingest DQ metrics. <br><br><strong>Approval detection & required approvals generation</strong><br>Rule types to set <code>requiredApprovals</code>: <code>&lt;mode=post_direct&gt;</code>, <code>&lt;estimatedAffected &gt; threshold&gt;</code>, <code>&lt;mapHash touches regulated segments&gt;</code>. Approvals stored as <code>approvalsRef</code> when captured. Plan persists <code>requiredApprovals</code> and <code>estimatedCost</code>. <br><br><strong>Operational narrative (analyst builds plan & receives gating)</strong><br>1. Analyst sets params to aggregate by <code>GLAccount+CostCenter</code>, tolerance 1%. <code>BuildReconciliationPlan</code> canonicalizes params and computes <code>planSeed</code>. <br>2. <code>mapHash</code> and <code>ingestChecksum</code> validated present. Because estimated affected groups ~ 5k and mode <code>post_direct</code> selected, <code>requiredApprovals</code> include <code>owner</code> and <code>compliance</code>. Plan persisted and <code>payroll.recon.plan.built</code> emitted. <br><br><strong>Dependencies (explicit)</strong><br>- Requires <code>mapHash</code> from REG_Mapping and <code>ingestChecksum</code> from REG_IngestPayroll/REG_IngestGL; must validate existence. <br>- Writes plan artifact via REG_EvidenceAudit and uses REG_RuntimeHelpers for telemetry. <br><br><strong>CI & tests</strong><br>Reproducibility tests for param ordering changes, <code>planId</code> parity across runtimes given equal inputs, required approval detection tests. <br><br><strong>Example <code>planRecord</code> (representative)</strong><br><code>{ &quot;planId&quot;:&quot;p_1a2b3c&quot;, &quot;paramsHash&quot;:&quot;sha256:...&quot;,&quot;planSeed&quot;:&quot;sha256:...&quot;,&quot;mapHash&quot;:&quot;sha256:ab12...&quot;,&quot;ingestChecksum&quot;:&quot;sha256:...&quot;,&quot;aggregationKeys&quot;:[&quot;GLAccount&quot;,&quot;CostCenter&quot;],&quot;sampleSeed&quot;:&quot;sha256:...&quot;,&quot;requiredApprovals&quot;:[&quot;owner1&quot;,&quot;compliance&quot;],&quot;createdBy&quot;:&quot;alice&quot;,&quot;createdTs&quot;:&quot;2026-03-16T10:20:00Z&quot; }</code><br><br><strong>Expansion Integration — REG_Plan</strong><br><strong>Phase 1 tasks (0–90d)</strong><br>- Enforce canonical param serialization via <code>canonical_plan_v1</code> library; add <code>parity:golden:REG_Plan</code> tests that reproduce <code>planSeed</code> and <code>planId</code> parity across runtimes. <br>- Ensure <code>BuildReconciliationPlan</code> validates referenced <code>mapHash</code> and <code>ingestChecksum</code> exist; CI rejects plans referencing missing artifacts. <br>- Persist plan records via REG_EvidenceAudit and include <code>plan_manifest</code> in evidence. <br><br><strong>Phase 2 tasks (90–180d)</strong><br>- Integrate deterministic PRNG library and ship <code>samplingMeta</code> versioning in <code>planRecord</code>. <br>- Add <code>parity:golden</code> sampling parity tests to assure preview reproducibility. <br><br><strong>Phase 3 tasks (180–365d)</strong><br>- Hook plan cost/approval estimation to canary promotion tooling to support mapping hot-swaps and auto-rollback triggers. <br><br><strong>CI & gating acceptance criteria</strong><br>- Plans that change sampling algorithm must include <code>migration_manifest</code> and updated <code>samplingMeta</code> golden tests. <br><br><strong>Artifacts produced (evidence names)</strong><br>- <code>evid:plan_&lt;planId&gt;_&lt;paramsHash&gt;.json</code> </td></tr><tr><td data-label="Module Name"> <strong>REG_Preview — deterministic preview execution, artifacts & dependencies</strong><br><strong>Purpose & condensed objective</strong><br>Execute a non-mutating mapping and aggregation run seeded by <code>planId</code> and <code>mapHash</code> to produce deterministic preview artifacts (aggregates, variance report, suggested JEs) and a signed <code>previewManifest</code> bundle. Previews produce evidence for analyst review; accepted suggestions are later used for apply flows. Preview must be deterministic across runtimes when seeded with the same <code>planSeed</code> and snapshots.<br><br><strong>Public surface (signatures)</strong><br><code>Function PreviewReconciliation(planId As String, sampleSize As Long, operatorId As String) As Dictionary</code> — core entrypoint. <br><code>Function RehydratePlan(planId As String) As Dictionary</code>. <br><code>Function DeterministicSample(sortedRowIds As Collection, sampleSeed As String, sampleSize As Long) As Collection</code> — seeded PRNG + Fisher–Yates or deterministic reservoir. <br><code>Function ApplyMappingSnapshot(rows As Collection, mapHash As String) As Collection</code>. <br><code>Function AggregateGroup(rows As Collection, aggKeys As Collection, scale As Long) As Collection</code>. <br><code>Function ComputeVarianceReport(payrollAgg As Collection, glAgg As Collection, tolerances As Dictionary) As Collection</code>. <br><code>Function GenerateSuggestedJEs(varianceRows As Collection, jeTemplateSpec As Dictionary) As Collection</code>. <br><code>Function RedactPIIForPreview(artifact As Dictionary, redactRules As Collection) As Dictionary</code>. <br><code>Function PackagePreviewArtifacts(planId As String, previewArtifacts As Collection) As Dictionary</code>. <br><code>Function PersistPreviewManifest(previewManifest As Dictionary) As String</code>. <br><code>Sub EmitPreviewAudit(previewManifestRef As String)</code>. <br><br><strong>Deterministic sampling algorithm (recommended & versioned)</strong><br>Use a cryptographic deterministic PRNG (e.g., HMAC_DRBG with SHA-256) seeded by <code>plan.sampleSeed</code> and version the PRNG algorithm. Perform Fisher–Yates shuffle on lexicographically sorted <code>rowId</code> list then select first <code>sampleSize</code>. Alternative: deterministic reservoir sampling using the same seeded PRNG for memory constrained environments. Store <code>samplingMeta</code> in preview manifest: <code>{algorithm, prng, seed, version}</code> so future replay uses the same algorithm/version.<br><br><strong>Aggregation & variance computation (formulas & edge cases)</strong><br>- Aggregate amounts in minor units. <br>- Full-outer join yields <code>Matched</code>, <code>PayrollOnly</code> and <code>GLOnly</code>. <br>- Compute: <code>VarianceMinorUnits = PayrollMinorUnits - GLMinorUnits</code>; <code>AbsVariance = ABS(Variance)</code>; <code>RelativeVariancePct = IF(GLMinorUnits = 0, NULL, AbsVariance / ABS(GLMinorUnits))</code>. <br>- Determine <code>BeyondTolerance</code> using plan config: if <code>GLAmount = 0</code> evaluate absolute threshold else percent threshold; thresholds recorded in <code>planRecord</code>. <br>- Preserve deterministic <code>sampleRowRefs</code> for each group (seed derived combination of <code>planSeed</code> and sorted group key).<br><br><strong>SuggestedJEs & packaging</strong><br>Generate <code>suggestedJEs</code> using <code>REG_Suggestions</code> routines; persist redacted analyst view and full suggestion evidence separately. Package artifacts: <code>before_aggregates.csv</code>, <code>gl_aggregates.csv</code>, <code>variance_report.csv</code>, <code>suggested_jes_preview.csv</code>, <code>mapping_suggestions.csv</code>, <code>preview_manifest.json</code>. Compute <code>previewHash = sha256(canonicalPreviewManifest)</code> and persist <code>preview_&lt;planId&gt;_&lt;previewHash&gt;.zip</code> to evidence store. Store <code>previewManifest</code> at canonical serialization and include <code>samplingMeta</code>, <code>mapHash</code>, <code>ingestChecksums</code>, <code>issues[]</code> and operator metadata.<br><br><strong>Dependencies (explicit)</strong><br>- Reads <code>planRecord</code> from REG_Plan, canonical payroll rows from REG_IngestPayroll, canonical GL rows from REG_IngestGL, mapping snapshot <code>map_&lt;mapHash&gt;.json</code> from REG_Mapping. <br>- Uses REG_Aggregation for grouping logic and REG_Suggestions to create suggested JEs. <br>- Persists preview bundle and emits audit via REG_EvidenceAudit. <br>- Uses REG_RuntimeHelpers for telemetry, safe error messages and cancellation.<br><br><strong>Operational narrative (analyst preview -> accept suggestions)</strong><br>1. Analyst triggers <code>PreviewReconciliation</code> for <code>planId</code>. System rehydrates plan, verifies <code>mapHash</code> presence and ingest artifacts. <br>2. Deterministic sample computed; mapping snapshot applied to sample rows; aggregates computed; variance rows generated; suggestions computed. <br>3. Preview bundle persisted, <code>previewHash</code> computed and audit <code>payroll.recon.preview</code> emitted. Analyst reviews redacted artifacts, accepts subset of suggestions (accept events persisted as <code>acceptedSuggestionRef</code>). <br>4. Later, <code>ApplyCorrections</code> consumes accepted suggestions for export or direct posting.<br><br><strong>Issue handling</strong><br>Structured <code>issues[]</code> in preview manifest: <code>PAYROLL_AMBIG_DATE</code>, <code>MAP_CONFLICT</code>, <code>CURRENCY_MISMATCH</code>, <code>MISSING_EVIDENCE</code>. Each issue must include <code>evidenceRef</code>, triage hint and suggested remediation.<br><br><strong>CI & tests</strong><br>Parity tests: run <code>PreviewReconciliation</code> with same <code>planId</code>/<code>sampleSize</code> across environments and assert identical <code>previewHash</code>. Tests for sampling parity, mapping parity, aggregation results and suggestion parity.<br><br><strong>Example preview manifest snippet (representative)</strong><br><code>{ &quot;planId&quot;:&quot;p_1a2b3c&quot;, &quot;previewHash&quot;:&quot;sha256:...&quot;,&quot;mapHash&quot;:&quot;sha256:ab12...&quot;,&quot;ingestChecksum&quot;:&quot;sha256:...&quot;,&quot;glIngestChecksum&quot;:&quot;sha256:...&quot;,&quot;samplingMeta&quot;:{&quot;algorithm&quot;:&quot;FisherYates&quot;,&quot;prng&quot;:&quot;HMAC_DRBG-SHA256&quot;,&quot;seed&quot;:&quot;sha256:...&quot;,&quot;version&quot;:&quot;1.0.0&quot;},&quot;artifacts&quot;:[{&quot;name&quot;:&quot;variance_report.csv&quot;,&quot;path&quot;:&quot;evid:...&quot;,&quot;checksum&quot;:&quot;sha256:...&quot;}],&quot;issues&quot;:[],&quot;createdBy&quot;:&quot;bob&quot;,&quot;createdTs&quot;:&quot;2026-03-17T11:00:00Z&quot;} </code><br><br><strong>Expansion Integration — REG_Preview</strong><br><strong>Phase 1 tasks (0–90d)</strong><br>- Integrate deterministic PRNG (HMAC_DRBG-SHA256) reference <code>prng_sampling_v1</code> and add <code>parity:golden:REG_Preview</code> that reproduces <code>previewHash</code> for canonical fixtures. <br>- Publish <code>golden_preview_regulated_v1.zip</code> containing <code>variance_report.csv</code> and <code>preview_manifest.json</code>. <br>- Ensure <code>PreviewReconciliation</code> persists <code>preview_manifest</code> into REG_EvidenceAudit. <br><br><strong>Phase 2 tasks (90–180d)</strong><br>- Implement sampling replay harness and add <code>sampling_replay</code> CI test to replay previews with prior PRNG versions. <br>- Integrate signed <code>previewManifest</code> using REG_EvidenceAudit signing. <br>- Provide parity dashboard support for preview hashes and mapping diffs. <br><br><strong>Phase 3 tasks (180–365d)</strong><br>- Implement DB-pushdown hooks to read canonical aggregates from warehouse for scale and support deterministic joining with GL aggregates produced in-database. <br>- Harden suggestion provenance metadata persisted with preview artifacts (link to <code>historyIndex</code>). <br><br><strong>CI & gating acceptance criteria</strong><br>- <code>parity:golden:REG_Preview</code> must pass for regulated fixtures before merge. Any sampling algorithm version bump requires <code>migration_manifest</code> with sampling fixtures. <br><br><strong>Artifacts produced (evidence names)</strong><br>- <code>evid:preview_&lt;planId&gt;_&lt;previewHash&gt;.zip</code> <br>- <code>evid:golden/REG_Preview/&lt;fixtureId&gt;.zip</code> </td></tr><tr><td data-label="Module Name"> <strong>REG_Aggregation — deterministic aggregation & integration narrative</strong><br><strong>Purpose & condensed objective</strong><br>Perform deterministic grouping and fixed-decimal aggregation of payroll and GL rows and produce canonical aggregates used by previews and reports. Provide robust full-outer joins, maintain deterministic sample row references, and support pushing aggregation to DB or warehouse for scale.<br><br><strong>Public surface (signatures)</strong><br><code>Function ComputeVarianceAggs(payrollRows As Collection, glRows As Collection, aggKeys As Collection, tolerancePolicy As Dictionary, scale As Long) As Collection</code> — returns <code>varianceRows</code>. <br><code>Function FixedDecimalSum(values As Collection, scale As Long) As String</code> — canonical sum. <br><code>Function AggregateGroup(rows As Collection, keys As Collection, scale As Long) As Dictionary</code> <br><code>Function FullOuterJoinAggs(payrollAgg As Collection, glAgg As Collection, keys As Collection) As Collection</code> <br><code>Function EvaluateTolerance(payrollAmountMinorUnits As Long, glAmountMinorUnits As Long, policy As Dictionary) As Dictionary</code>. <br><br><strong>Canonical aggregation rules</strong><br>- Use integer <code>AmountMinorUnits</code> for internal arithmetic. <br>- Canonical group key: normalized components joined by <code>|</code> in fixed order; normalization rules reused from ingest/mapping. <br>- Summation uses integer math, no floating point. <br>- Compute <code>rowCount</code>, <code>minAmount</code>, <code>maxAmount</code>, and optionally <code>stdDev</code> via Welford algorithm on minor units to preserve numerical stability. <br>- Preserve deterministic <code>sampleRowRefs</code> per group by sampling sorted rowId list with deterministic PRNG derived from <code>planSeed</code> + groupKey.<br><br><strong>Full-outer join & ordering</strong><br>Construct dictionaries keyed by canonical groupKey for payrollAgg and glAgg. Produce union of keys sorted lexicographically. For each key produce canonical variance row containing <code>PayrollAmount</code>, <code>GLAmount</code>, <code>Variance</code>, <code>AbsVariance</code>, <code>RelativeVariancePct</code>, <code>rowCountPayroll</code>, <code>rowCountGL</code>, <code>sampleRowRefs</code>, <code>Side</code> (PayrollOnly/GLOnly/Matched). Deterministic ordering must be preserved to ensure stable artifact hashes.<br><br><strong>Tolerance evaluation & materiality rules</strong><br><code>EvaluateTolerance</code> returns <code>BeyondTolerance</code> boolean and <code>reason</code>. Default <code>TolerancePct = 0.01</code>. If <code>GLAmountMinorUnits = 0</code> use <code>AbsThreshold</code> (configurable per plan). Provide per-account overrides (e.g., absolute only for low-volume accounts).<br><br><strong>Scale & pushdown guidance</strong><br>- For large datasets (>50k groups) implement aggregation in database with SQL <code>GROUP BY</code> and export canonical aggregates; for Excel pilots use chunked aggregation and persist intermediate results.<br><br><strong>Dependencies (explicit)</strong><br>- Consumes canonical rows from REG_IngestPayroll and REG_IngestGL. <br>- Used by REG_Preview to compute variance rows. <br>- Works with REG_Suggestions which consumes <code>varianceRows</code>.<br><br><strong>Operational narrative (scaling tip)</strong><br>When running full production previews with millions of payroll rows, submit a DB job to compute canonical payroll aggregates using the same canonical key rules; the job writes canonical aggregate CSV with deterministic ordering that REG_Aggregation consumes for joining with GL aggregates, preserving parity with smaller pilots.<br><br><strong>CI & tests</strong><br>Property tests: invariance under input row reorder, deterministic sum matching DB results, edge-case tests for <code>GLAmount=0</code>, negative amounts, multi-currency mismatches. Provide golden fixture of aggregate CSV and tests that compare produced aggregates to canonical fixture.<br><br><strong>Expansion Integration — REG_Aggregation</strong><br><strong>Phase 1 tasks (0–90d)</strong><br>- Publish <code>canonical_aggregation_v1</code> library that defines canonical groupKey rules and integer-sum arithmetic; produce <code>golden_aggregation_regulated_v1.csv</code>. <br>- Add CI parity job <code>parity:golden:REG_Aggregation</code>. <br><br><strong>Phase 2 tasks (90–180d)</strong><br>- Provide DB pushdown connector reference and sample SQL templates; CI includes a DB-aggregated comparison test that verifies parity between in-memory and DB aggregate outputs on sample datasets. <br><br><strong>Phase 3 tasks (180–365d)</strong><br>- Implement streaming aggregation support and chunk stitching semantics for massive datasets; provide canonical stitched aggregate artifacts. <br><br><strong>CI & gating acceptance criteria</strong><br>- Any change to canonical groupKey or rounding logic must add updated golden fixtures and follow migration manifest process. <br><br><strong>Artifacts produced (evidence names)</strong><br>- <code>evid:agg_payroll_&lt;planId&gt;_&lt;version&gt;.csv</code> <br>- <code>evid:golden/REG_Aggregation/&lt;fixtureId&gt;.csv</code> </td></tr><tr><td data-label="Module Name"> <strong>REG_Suggestions — suggestion engine, scoring, rounding, dependencies and narratives</strong><br><strong>Purpose & condensed objective</strong><br>Produce auditable JE suggestions for variance rows flagged <code>BeyondTolerance</code>. Provide deterministic rounding with residual absorption, confidence scoring with explainability, evidence references and safe defaults (e.g., PayrollSuspense) for unknown counterparts. Suggestions are advisory and require analyst acceptance before apply.<br><br><strong>Public surface (signatures)</strong><br><code>Function SuggestCorrectionJE(varianceRow As Dictionary, jeTemplateSpec As Dictionary, historyIndex As Dictionary, scale As Long, roundingMode As String) As Dictionary</code> — produce suggestion. <br><code>Function AppendMappingSuggestions(unmappedGroups As Collection, historyIndex As Dictionary, config As Dictionary) As Collection</code> — produce mapping suggestions for unmapped paycomponents. <br><code>Function AttributionSignals(sampleRowRefs As Collection) As Dictionary</code> <br><code>Function DiscoverCounterparts(varianceRow As Dictionary, historyIndex As Dictionary) As Collection</code> <br><code>Function ConstructLineCandidates(counterparts As Collection, targetAmountMinorUnits As Long, template As Dictionary) As Collection</code> <br><code>Function ApplyRoundingAndResidualAbsorption(lines As Collection, scale As Long, roundingMode As String) As Collection</code> <br><code>Function ComputeConfidenceScore(components As Dictionary) As Double</code> <br><code>Function PersistSuggestion(suggestion As Dictionary) As String</code> <br><code>Sub SuggestionAudit(suggestionRef As String)</code> <br><br><strong>Attribution & counterpart discovery logic</strong><br>Signals: exact PayComponent match, CostCenter match, alias similarity (token overlap, normalized Levenshtein), historical co-occurrence frequency (fraction), mapping stability (count of identical mapping events), recency weight (exponential decay). Combine features to rank candidate counterpart GL accounts from <code>historyIndex</code>. <code>historyIndex</code> is updated by REG_ApplyExport and REG_EvidenceAudit with postedJournalIds and mapping usage history.<br><br><strong>Line candidate construction & rounding</strong><br>- Prefer minimal balancing JEs (two lines) when single counterpart found. <br>- If multiple counterparts, propose multi-line JE with amounts proportioned by historical split when sufficient confidence; otherwise fallback to <code>PayrollSuspense</code>. <br>- Rounding: compute per-line precise rational target (in internal cents/minor units), apply Banker's rounding by default (configurable), compute residual = <code>targetMinorUnits - sum(roundedLinesMinorUnits)</code>, absorb residual deterministically to the line with highest pre-rounded absolute value with tiebreaker lexicographic <code>ruleId</code>. Document absorption in <code>rationale</code> and <code>suggestion</code> metadata.<br><br><strong>Confidence scoring & explainability</strong><br>Confidence in [0,1] computed as weighted sum: <code>matchWeight*matchScore + historyWeight*historyScore + recencyWeight*recencyScore - dqPenalty*penalty</code>, then clamped to [0,1]. Record <code>confidenceBreakdown</code> with component contributions for analyst transparency. Persist weight vector and version used in suggestion metadata for audit and parity.<br><br><strong>Governance & safe fallbacks</strong><br>- Never auto-apply suggestions. <br>- Default unknown counterparts go to <code>PayrollSuspense</code> flagged <code>requiresFollowUp</code>. <br>- For regulated GL ranges require explicit owner permission to accept suggestions.<br><br><strong>Dependencies (explicit)</strong><br>- Uses <code>varianceRows</code> from REG_Aggregation/REG_Preview. <br>- Reads <code>historyIndex</code> (past mapping and posting events) from REG_EvidenceAudit/REG_ApplyExport. <br>- Persists suggestions via REG_EvidenceAudit and emits <code>payroll.je.suggested</code> via audit API.<br><br><strong>Operational narrative (analyst review of suggestions)</strong><br>1. Preview runs and produces <code>suggestedJEs</code>. <br>2. Analyst filters suggestions by <code>confidenceScore</code> and drills into <code>sampleRowRefs</code> to validate. <br>3. Analyst accepts selected suggestions; acceptance events persisted as <code>acceptedSuggestionRef</code>, which later feed <code>GenerateJEExport</code> or <code>ApplyCorrections</code>.<br><br><strong>CI & tests</strong><br>Deterministic rounding/residual absorption tests across runtimes, confidence scoring unit tests, suggestion golden fixtures. Ensure same rounding/residual across PQ/VBA/backend.<br><br><strong>Expansion Integration — REG_Suggestions</strong><br><strong>Phase 1 tasks (0–90d)</strong><br>- Implement deterministic rounding/residual absorption library <code>rounding_v1</code> and provide <code>golden_suggestions_v1.json</code>. <br>- Add CI parity tests <code>parity:golden:REG_Suggestions</code>. <br>- Persist <code>confidenceBreakdown</code> schema and versioning in suggestion metadata. <br><br><strong>Phase 2 tasks (90–180d)</strong><br>- Integrate provenance metadata in suggestions linking to <code>historyIndex</code> entries persisted in EvidenceAudit. <br>- Add suggestion explainability UI mockups and evidence bundles for analyst review. <br><br><strong>Phase 3 tasks (180–365d)</strong><br>- Harden suggestion engine with model-calibrated confidence and integrate with operator training for acceptance thresholds. <br><br><strong>CI & gating acceptance criteria</strong><br>- Rounding behavior changes must include <code>rounding_golden</code> fixtures and <code>migration_manifest</code>. <br><br><strong>Artifacts produced (evidence names)</strong><br>- <code>evid:suggestion_&lt;suggestionId&gt;_&lt;version&gt;.json</code> <br>- <code>evid:golden/REG_Suggestions/&lt;fixtureId&gt;.json</code> </td></tr><tr><td data-label="Module Name"> <strong>REG_ApplyExport — exports, apply orchestration, idempotency, and dependencies</strong><br><strong>Purpose & condensed objective</strong><br>Convert accepted JE suggestions into artifacts suitable for GL loaders, or post directly to GL APIs under governance. Enforce approval gating, persist <code>applyDescriptor</code> atomically, compute and persist export manifests with checksums, and provide revert descriptors for safe rollbacks. Ensure idempotency tokens to avoid duplicate postings and provide robust partial failure handling.<br><br><strong>Public surface (signatures)</strong><br><code>Function GenerateJEExport(acceptedJEs As Collection, exportSpec As Dictionary, operatorId As String, scale As Long) As Dictionary</code> — validate, serialize, persist export artifact and <code>export_manifest</code>. <br><code>Function ApplyCorrections(acceptedJEs As Collection, mode As String, operatorId As String, approvals As Collection) As Dictionary</code> — orchestrates <code>create_export</code> or <code>post_direct</code>. <br><code>Function RevertJEs(applyId As String, operatorId As String) As Dictionary</code> — orchestrates revert. <br><code>Function ValidateExportSpec(exportSpec As Dictionary) As Boolean</code> <br><code>Function ValidateJE_Balance(jebundle As Dictionary, scale As Long) As Boolean</code> <br><code>Function ComputeExportChecksum(bytes() As Byte) As String</code> <br><code>Function PersistExportArtifact(path As String, bytes() As Byte) As String</code> <br><code>Function BuildApplyDescriptor(planId As String, acceptedJEsHash As String, beforeChecksums As Dictionary, approvalsRef As String, mode As String, operatorId As String) As Dictionary</code> <br><code>Function AcquireEphemeralCredentials(scope As String, approvers As Collection) As Dictionary</code> <br><code>Function PostDirectToGL(exportPath As String, applyId As String, operatorId As String, token: As String) As Dictionary</code> <br><code>Function BuildRevertDescriptor(applyDescriptor As Dictionary, postedJournalIds As Collection) As Dictionary</code> <br><code>Sub EmitApplyAudit(applyResult As Dictionary)</code><br><br><strong>Export generation & validation rules</strong><br>1. <code>exportSpec</code> must define required columns, date formats, numeric scales, sign conventions and column ordering. <br>2. Validate each JE bundle is balanced at configured <code>scale</code> (sum of debit minor units equals sum of credit minor units). If any bundle unbalanced, fail export and emit diagnostics. <br>3. Serialize deterministically (columns in fixed order, deterministic escaping). <br>4. Compute <code>exportChecksum = sha256(UTF8(serializedBytes))</code> and name artifact <code>JE_Export_&lt;planId&gt;_&lt;applyId&gt;_&lt;exportChecksum&gt;.&lt;ext&gt;</code>. Persist artifact and <code>export_manifest</code>.<br><br><strong>Apply orchestration & approvals</strong><br>1. Verify <code>approvals</code> satisfy <code>planRecord.requiredApprovals</code>. For <code>post_direct</code> require two approvers and ephemeral credentials issuance. <br>2. Build <code>applyDescriptor</code> including <code>applyId</code>, <code>planId</code>, <code>operatorId</code>, <code>mode</code>, <code>acceptedJEsHash</code>, <code>beforeChecksums</code> and persist atomically to evidence store before any mutation. <br>3. For <code>post_direct</code> acquire ephemeral credentials securely (no long-lived secrets) and call GL API with idempotency token (<code>IdempotencyTokenForApply(applyId)</code>). Capture <code>postedJournalIds</code> and confirmations. <br>4. Persist <code>applyResult</code> with per-bundle statuses and <code>afterChecksums</code> where GL returns them. Emit <code>payroll.je.apply.completed</code> or <code>...failed</code> audit.<br><br><strong>Idempotency & partial failure handling</strong><br>Use deterministic idempotency tokens derived from <code>applyId</code> and current token version. On retry detect existing <code>postedJournalIds</code> and avoid duplicate posting. For partial failures, record per-JE status and allow operator to retry failing bundles; persist retry attempts in <code>applyDescriptor</code> history.<br><br><strong>Revert flows</strong><br>- Automated revert: call GL reversal endpoint with <code>postedJournalIds</code> and idempotency token; persist <code>revertDescriptor</code> with <code>revertId</code> computed deterministically (sha256 of <code>applyId</code> + normalized revert request ts). <br>- Manual fallback: generate <code>forensic_manifest</code> and reversal CSV for operator run and set revert status <code>pending</code>. Provide runbook with step-by-step instructions including <code>correlationId</code> and <code>applyDescriptor</code> references.<br><br><strong>Dependencies (explicit)</strong><br>- Consumes accepted suggestions from REG_Preview (acceptedSuggestionRef). <br>- Uses REG_EvidenceAudit to persist <code>applyDescriptor</code>, <code>export_manifest</code> and forensic packs. <br>- Uses REG_RuntimeHelpers for token acquisition wrapper, telemetry, and safe user messaging. <br>- Integrates with external GL APIs and ephemeral token service for <code>post_direct</code>.<br><br><strong>Operational narrative (apply failure & revert)</strong><br>1. Analyst accepts suggestions and requests <code>post_direct</code>. <br>2. <code>ApplyCorrections</code> validates approvals, persists <code>applyDescriptor</code> and acquires ephemeral credentials. Posting to GL returns partial success (some journals failed). <br>3. <code>applyDescriptor</code> updated with per-bundle error codes; <code>payroll.je.apply.failed</code> audit emitted. <br>4. Operator examines diagnostics and either retries failing bundles or triggers <code>RevertJEs</code> if necessary.<br><br><strong>CI & tests</strong><br>Export schema validator tests, golden export checksum tests, mocked GL acceptance tests, idempotency replay tests, approval gating tests and token lifecycle tests.<br><br><strong>Expansion Integration — REG_ApplyExport</strong><br><strong>Phase 1 tasks (0–90d)</strong><br>- Implement deterministic export serialization <code>canonical_export_v1</code> and generate <code>golden_export_regulated_v1.csv</code>. <br>- Add CI parity job <code>parity:golden:REG_ApplyExport</code> to validate <code>exportChecksum</code>. <br>- Implement <code>applyDescriptor</code> atomic persist to REG_EvidenceAudit. <br><br><strong>Phase 2 tasks (90–180d)</strong><br>- Integrate ephemeral credential acquisition via token service and implement idempotency token rotation policy. <br>- Add apply replay tests and partial failure simulated GL responses in CI. <br><br><strong>Phase 3 tasks (180–365d)</strong><br>- Automate revert flows and integrate automatic rollback triggers in SRE playbook for <code>applyDescriptor</code> failures that exceed threshold SLOs. <br><br><strong>CI & gating acceptance criteria</strong><br>- Any change to export serialization or sign convention requires <code>migration_manifest</code> and golden export fixtures. <br><br><strong>Artifacts produced (evidence names)</strong><br>- <code>evid:export_&lt;applyId&gt;_&lt;exportChecksum&gt;.&lt;ext&gt;</code> <br>- <code>evid:apply_descriptor_&lt;applyId&gt;.json</code> </td></tr><tr><td data-label="Module Name"> <strong>REG_EvidenceAudit — canonical reporting, signatures, storage, chain-of-custody and dependencies</strong><br><strong>Purpose & condensed objective</strong><br>Act as authoritative evidence store and run assembler: persist raw and canonical artifacts, produce <code>recon_report</code> (canonical manifest and signature), maintain chain-of-custody, enforce retention policies and provide secure evidence access approval flows. REG_EvidenceAudit centralizes evidence persistence and parity verification logic used by all modules.<br><br><strong>Public surface (signatures)</strong><br><code>Function PersistEvidenceBlob(bytes() As Byte, metadata As Dictionary) As String</code> — returns <code>evidenceRef</code>. <br><code>Function BuildReconciliationReport(runId As String, artifacts As Collection, operatorMeta As Dictionary) As Dictionary</code> — produce & persist <code>recon_report</code>. <br><code>Function VerifyReportParity(runId As String) As Boolean</code> — recompute <code>reportHash</code>. <br><code>Function RecordChainOfCustody(actorId As String, action As String, evidenceRef As String, reason As String)</code> <br><code>Function EvidenceAccessApprovalFlow(request As Dictionary) As Dictionary</code> — manage approvals, issue temporary retrieval tokens and log accesses. <br><code>Function EmitAudit(row As Dictionary) As String</code> — emits PII-free audit rows and returns audit id.<br><br><strong>Canonical <code>recon_report</code> recipe & signing</strong><br>1. Collect artifacts and their canonical checksums. 2. Remove transient fields per canonicalization policy. 3. Build canonical JSON manifest with stable key ordering and compute <code>reportHash = sha256(UTF8(canonicalManifest))</code>. 4. Optionally sign <code>reportHash</code> using system signing key and attach signature metadata. 5. Persist <code>recon_report_&lt;runId&gt;_&lt;reportHash&gt;.json</code> immutably; attach <code>retentionPolicy</code> and <code>legalHold</code> flags. 6. Emit <code>payroll.recon.report.generated</code> audit with <code>reportRef</code> and <code>reportHash</code>.<br><br><strong>Chain-of-custody & access approvals</strong><br>All evidence retrieval requests must follow <code>EvidenceAccessApprovalFlow</code>: requestor specifies <code>purpose</code> and <code>requiredApprovers</code>. Upon approvals, system issues time-limited retrieval token and logs <code>chainOfCustody</code> entry. All retrievals logged and auditable; access events recorded with <code>actorId</code>, <code>action</code>, <code>ts</code> and <code>evidenceRef</code>.<br><br><strong>Parity & periodic verification</strong><br>Run periodic <code>VerifyReportParity</code> jobs to recompute <code>reportHash</code> for stored <code>recon_report</code> and emit <code>verify.parity.failed</code> if mismatch found. Provide SRE runbook for investigating mismatches (investigate storage corruption, canonicalization version drift, or malicious tampering).<br><br><strong>Dependencies (explicit)</strong><br>- REG_IngestPayroll, REG_IngestGL, REG_Mapping, REG_Plan, REG_Preview, REG_ApplyExport, and REG_Suggestions call into Evidence APIs to persist artifacts and manifests. <br>- REG_RuntimeHelpers used for telemetry within evidence flows.<br><br><strong>Operational narrative (evidence request)</strong><br>1. Compliance requests full sanitized evidence for run <code>runId</code> citing audit. 2. Request created via <code>EvidenceAccessApprovalFlow</code> requiring <code>owner</code> and <code>compliance</code> approvals. 3. Upon approvals, <code>evidenceRef</code> retrieval token issued and retrieval logged to <code>chainOfCustody</code>. 4. Evidence access recorded and <code>recon_report</code> updated with access entries.<br><br><strong>CI & tests</strong><br>Signing validation tests, parity recompute tests against golden <code>recon_report</code> manifests, access approval flow tests and retention enforcement tests.<br><br><strong>Expansion Integration — REG_EvidenceAudit</strong><br><strong>Phase 1 tasks (0–90d)</strong><br>- Implement evidence persist API and canonical <code>recon_report</code> recipe; provide <code>golden_recon_report_v1.json</code>. <br>- CI step to verify recon_report parity <code>parity:golden:REG_EvidenceAudit</code>. <br>- Provide minimal approval UI hook for evidence retrieval (captures <code>approvalsRef</code>). <br><br><strong>Phase 2 tasks (90–180d)</strong><br>- Integrate HSM/KMS-backed signing for <code>recon_report</code> and implement signed artifact persist operations. <br>- Expose audit endpoints consumed by parity dashboard and SRE runbooks. <br><br><strong>Phase 3 tasks (180–365d)</strong><br>- Implement long-term retention tiers, legal-hold enforcement, and large-scale archive/restore flows; provide integrity check jobs and automated SRE remediation playbooks. <br><br><strong>CI & gating acceptance criteria</strong><br>- All signed artifacts must validate against signature verification tests in CI. <br><br><strong>Artifacts produced (evidence names)</strong><br>- <code>recon_report_&lt;runId&gt;_&lt;reportHash&gt;.json</code> (signed in Phase 2) <br>- <code>evid:golden/REG_EvidenceAudit/&lt;fixtureId&gt;.json</code> </td></tr><tr><td data-label="Module Name"> <strong>REG_RuntimeHelpers — cross-cutting utilities, safety wrappers, CI hooks and dependencies</strong><br><strong>Purpose & condensed objective</strong><br>Provide safe invocation wrappers, telemetry emits, cancellation/timeouts, PII redaction helpers, idempotency token generation, configuration lookup and CI golden hooks. Keep this module minimal and stable; changes must be versioned and backward compatible.<br><br><strong>Public surface (signatures)</strong><br><code>Function SafeInvokeReconciler(handlerName As String, handlerArgs As Dictionary, correlationId As String, timeoutMs As Long) As Dictionary</code> — wraps all heavy handlers. <br><code>Function SafeErrorToUser(correlationId As String, errorCode As String, hint As String) As Dictionary</code> — constructs ≤160 char PII-free message and persists full diagnostics to evidence returning <code>evidenceRef</code>. <br><code>Function RegisterUnitTestHook(hookName As String, enabled As Boolean, token As String) As Boolean</code> — CI only. <br><code>Function TelemetryEmit(eventName As String, tags As Dictionary, metrics As Dictionary)</code> <br><code>Function TimeoutAndCancellationWrapper(taskProc As String, timeoutMs As Long, correlationId As String)</code> <br><code>Function PII_Redact(record As Dictionary, redactRules As Collection) As Dictionary</code> <br><code>Function MapInternalErrorToPayrollCode(err As ErrObject) As String</code> <br><code>Function EmitSafeUserMessage(correlationId As String, code As String, hint As String) As Dictionary</code> <br><code>Function JobQueueOffload(taskArgs As Dictionary) As String</code> <br><code>Function IdempotencyTokenForApply(applyId As String) As String</code> <br><code>Function ConfigurationLookup(key As String) As Variant</code><br><br><strong>Behavioral rules</strong><br>- All heavy handlers must be invoked through <code>SafeInvokeReconciler</code> to ensure telemetry, cancellation and safe user mapping consistent across the system. <br>- <code>TimeoutAndCancellationWrapper</code> supports cooperative cancellation tokens mapped by <code>correlationId</code>; handlers must expose safe abort points. <br>- <code>PII_Redact</code> strips or tokenizes personal fields using redact rules before presenting to analysts or logs. Full diagnostics always persisted encrypted via <code>REG_EvidenceAudit</code>. <br>- CI hooks: <code>RegisterUnitTestHook</code> may enable deterministic seeds and must be access controlled and disabled in production.<br><br><strong>Idempotency token policy</strong><br><code>IdempotencyTokenForApply(applyId)</code> returns deterministic token (<code>sha256(&quot;apply:&quot; &amp; applyId &amp; &quot;:&quot; &amp; idempotencyVersion)</code>). <code>idempotencyVersion</code> stored in configuration to allow controlled rotation while preserving lookup of existing tokens. Tokens are passed to GL APIs when posting direct and recorded in <code>applyDescriptor</code>.<br><br><strong>Dependencies (explicit)</strong><br>- REG_EvidenceAudit for persisting diagnostics/evidenceRefs. <br>- Telemetry backend (abstracted) for emitting metrics.<br><br><strong>Operational narrative (safe error to user)</strong><br>1. Handler throws internal exception. <code>SafeInvokeReconciler</code> catches exception, calls <code>MapInternalErrorToPayrollCode</code> to map to <code>PAYROLL_*</code> error code, persists full diagnostics to evidence via <code>REG_EvidenceAudit.PersistEvidenceBlob</code> and returns a PII-free user message produced by <code>SafeErrorToUser</code> including <code>correlationId</code> and <code>evidenceRef</code>. UI shows the short message and ticket contains <code>correlationId</code> for triage.<br><br><strong>CI & tests</strong><br>Timeout & cancellation tests, PII redaction verification, error mapping coverage and CI hook access control tests.<br><br><strong>Expansion Integration — REG_RuntimeHelpers</strong><br><strong>Phase 1 tasks (0–90d)</strong><br>- Implement <code>SafeInvokeReconciler</code> and telemetry hooks; expose <code>parity.detect.ms</code> and <code>ingestLatencyMs</code> metrics. <br>- Provide CI hook <code>RegisterUnitTestHook</code> to enable deterministic seeds for parity tests (access-controlled). <br><br><strong>Phase 2 tasks (90–180d)</strong><br>- Implement idempotency token rotation policy and integrate with REG_ApplyExport. <br>- Provide telemetry exporters for parity dashboard. <br><br><strong>Phase 3 tasks (180–365d)</strong><br>- Expand cancellation wiring across long-running DB pushdown and streaming ingest jobs and ensure cooperative cancellation works end-to-end. <br><br><strong>Artifacts produced (evidence names)</strong><br>- <code>evid:runtime_helpers_config_v&lt;version&gt;.json</code> </td></tr><tr><td data-label="Module Name"> <strong>Cross-module dependency graph, integration contracts & narrative sequences (aggregated)</strong><br><strong>Directed dependency summary (consumer → provider)</strong><br>- REG_IngestPayroll → REG_EvidenceAudit, REG_RuntimeHelpers (telemetry). <br>- REG_IngestGL → REG_EvidenceAudit, REG_RuntimeHelpers. <br>- REG_Mapping → REG_EvidenceAudit, REG_RuntimeHelpers. <br>- REG_Plan → REG_Mapping (reads <code>mapHash</code>), REG_IngestPayroll/GL (reads <code>ingestChecksum</code>), REG_EvidenceAudit, REG_RuntimeHelpers. <br>- REG_Preview → REG_Plan, REG_IngestPayroll, REG_IngestGL, REG_Mapping, REG_Aggregation, REG_Suggestions, REG_EvidenceAudit, REG_RuntimeHelpers. <br>- REG_Aggregation → REG_IngestPayroll/GL; used by REG_Preview. <br>- REG_Suggestions → REG_Preview, REG_EvidenceAudit, REG_ApplyExport (historyIndex). <br>- REG_ApplyExport → REG_Preview (accepted suggestions), REG_EvidenceAudit, REG_RuntimeHelpers; external GL API and token service. <br>- REG_EvidenceAudit → (core provider for all).<br><br><strong>Integration contracts & guarantees</strong><br>1. <strong>Immutability & referencing:</strong> Once persisted, <code>mapHash</code>, <code>ingestChecksum</code>, <code>glIngestChecksum</code>, <code>previewHash</code>, <code>exportChecksum</code>, and <code>recon_report.reportHash</code> are authoritative and immutable; new versions must produce new hashes and be persisted as new artifacts. Consumers must validate referenced artifacts exist before proceeding. <br>2. <strong>Determinism:</strong> canonicalization recipes (ingest canonical CSV, mapping canonical mapping string, preview manifest canonicalization) are the single source of truth for parity. Changes require new canonicalization version, update of golden fixtures and controlled rollout. <br>3. <strong>Evidence linking:</strong> all audit rows are PII-free and reference <code>evidenceRef</code> for full diagnostics. Evidence retrieval requires approval flow. <br>4. <strong>Plan reproducibility:</strong> <code>planSeed</code> + <code>mapHash</code> + <code>ingestChecksum</code> must reproduce identical preview artifacts (given same sampling algorithm version).<br><br><strong>Common end-to-end narratives (stories used in runbooks)</strong><br><strong>A. Normal happy-path: ingest → plan → preview → accept → create_export</strong><br>1. Ingest payroll and GL; canonical artifacts persisted by REG_EvidenceAudit. <br>2. Mapping loaded producing <code>mapHash</code>. <br>3. Analyst builds plan; REG_Plan persists <code>planRecord</code>. <br>4. REG_Preview runs and produces <code>preview_&lt;planId&gt;_&lt;previewHash&gt;.zip</code>. <br>5. Analyst reviews suggestions and accepts some. <br>6. REG_ApplyExport.GenerateJEExport serializes accepted JEs to <code>JE_Export_&lt;...&gt;.csv</code> and persists <code>export_manifest</code>. <br>7. Export artifact delivered to GL team; <code>payroll.je.exported</code> audit recorded.<br><br><strong>B. Direct post with two-person approval and partial failure</strong><br>1. Same flow until analyst accepts suggestions and selects <code>post_direct</code>. <br>2. REG_ApplyExport validates approvals and persists <code>applyDescriptor</code>. <br>3. Acquire ephemeral token and post to GL API with idempotency token. <br>4. GL API returns partial success and errors for some journals. <br>5. <code>applyDescriptor</code> updated with per-bundle statuses and <code>payroll.je.apply.failed</code> emitted. <br>6. Operator uses <code>RevertJEs</code> or retries failing bundles per runbook.<br><br><strong>C. Mapping hot-swap and canary execution</strong><br>1. Mapping owner uploads candidate mapping. REG_Mapping computes <code>afterHash</code> and <code>MappingDiff</code>. <br>2. REG_Mapping orchestrates smoke preview runs (calls REG_Preview with small canary <code>planId</code> on top-N paycomponents) comparing KPIs. <br>3. If canary passes and approvals present, mapping pointer atomically swapped; else rollback. <br>4. All actions persist <code>hotSwap.auditChain</code> and evidence for compliance.<br><br><strong>D. Evidence retrieval & audit investigation</strong><br>1. Compliance opens evidence request and calls <code>EvidenceAccessApprovalFlow</code>. <br>2. Upon approvals, temporary retrieval token issued, <code>chainOfCustody</code> entries created and artifacts retrieved. <br>3. If parity verification job later finds <code>reportHash</code> mismatch, SRE applies runbook referencing <code>chainOfCustody</code> to inspect changes or storage corruption.<br><br><strong>Expansion Integration — Cross-module</strong><br>- Ensure each dependency provider emits canonicalVersion and <code>artifactHash</code> in metadata and evidence refs. <br>- CI must run cross-module parity flows during PRs that touch multiple modules (a PR that touches REG_Mapping and REG_Preview must run <code>parity:golden</code> tests for both modules and the cross-module smoke preview harness). <br>- Implement a centralized parity dashboard that aggregates module badges, last successful golden hash, and nightly parity job results (REG_EvidenceAudit provides data). </td></tr><tr><td data-label="Module Name"> <strong>Change control, migration manifest policy & upgrade narratives (expanded)</strong><br><strong>Principles</strong><br>- Any change to canonicalization, sampling algorithm, map semantics or rounding rules that could change <code>mapHash</code>, <code>ingestChecksum</code>, <code>previewHash</code> or <code>exportChecksum</code> must be treated as a semantic change and executed under migration manifest governance for regulated datasets. <br>- Keep canonicalization recipes, PRNG algorithms and rounding rules versioned and published. Provide backward compatibility or explicit migration artefacts for auditors.<br><br><strong>Migration manifest — required fields & process</strong><br>Required fields: <code>migrationId</code>, <code>author</code>, <code>createdTs</code>, <code>changeRationale</code>, <code>affectedMappings[]</code> (before/after canonical rows), <code>sampleFixtures[]</code> (fileRef + golden checksum), <code>estimatedAffectedCount</code>, <code>canaryPlan</code> (planId, cohort sizes, KPIs), <code>rollbackPlan</code> (snapshot location, revertId), <code>approvals[]</code> (ownerIds, timestamps, compliance signoffs), <code>testMatrix</code> (unit/integration/golden). <br>Process: publish manifest → run full CI + golden tests → run canary plans → collect KPIs → approvals capture → controlled rollout with monitoring → automated rollback if KPI breach occurs. Persist all items in Evidence store and emit <code>migration.manifest.created</code> audit.<br><br><strong>Upgrade narrative (example: change canonical mapping tokenization)</strong><br>1. Developer proposes canonicalization change; create <code>migration_manifest</code> describing exact algorithm change; include golden fixtures for <code>mapHash</code> before/after for sample inputs. <br>2. CI runs parity tests; QA runs canary preview plans using <code>beforeHash</code> and <code>afterHash</code> to compare KPI deltas. <br>3. If canary passes and approvers sign off, publish new canonicalization version and update mapping snapshots following rollback plan. <br>4. Record migration manifest and persist in evidence store.<br><br><strong>Expansion Integration — Change control & migration manifest</strong><br>- Enforce the manifest presence in CI: <code>ci:check_migration_manifest</code> step fails PRs when modules' canonical outputs change without a migration manifest. <br>- Each module must append <code>migrationImpact</code> metadata to its artifacts when a manifest is present (fields: <code>migrationId</code>, <code>beforeHash</code>, <code>afterHash</code>, <code>approvalsRef</code>). <br>- Provide templated <code>canaryPlan</code> runner scripts used by REG_Mapping and REG_Preview to execute smoke tests. </td></tr><tr><td data-label="Module Name"> <strong>Operational runbooks & playbooks (condensed, actionable)</strong><br><strong>Ingest failure triage</strong><br>1. Capture <code>correlationId</code> from ingest manifest. <br>2. Retrieve <code>ingestManifest</code> via Evidence API. <br>3. Inspect <code>issues[]</code> and <code>evidenceRef</code>. <br>4. If <code>fail-fast</code>: correct source and re-ingest with <code>correctionOf</code>. If <code>skip-and-flag</code>: evaluate flagged rows and decide corrective re-ingest. <br>5. Document actions in an audit row.<br><br><strong>Preview parity failure</strong><br>1. Analyst reports mismatch between expected <code>previewHash</code> and computed value. <br>2. Validate <code>planSeed</code>, <code>mapHash</code>, <code>ingestChecksum</code> match original plan record. <br>3. Verify <code>samplingMeta</code> version matches runtime. <br>4. If sampling algorithm change detected, run canonical reproduction with prior algorithm version or revert to prior canonicalization version. <br>5. If mismatch persists, escalate to engineering with <code>recon_report</code> and <code>chainOfCustody</code>.<br><br><strong>Apply failure & revert</strong><br>1. Capture <code>applyDescriptor</code> and <code>correlationId</code>. <br>2. If GL supports reversal endpoints, call <code>RevertJEs</code> with <code>postedJournalIds</code>. <br>3. If manual revert required, generate <code>forensic_manifest</code> and follow manual runbook to reverse entries. <br>4. Notify compliance and maintain <code>recon_report</code> with forensic evidence.<br><br><strong>Expansion Integration — Runbooks & Playbooks</strong><br>- Produce runnable playbooks (shell/ps1 templates) stored under <code>evid:runbooks/</code> and CI-validated for syntax. <br>- Add SRE alerting hooks that trigger runbooks automatically when parity/SLO thresholds are breached. <br>- Provide training decks and tabletop exercise materials alongside runbooks. </td></tr><tr><td data-label="Module Name"> <strong>Final integrator checklist & recommended implementation priorities (practical)</strong><br>1. <strong>Canonicalization reference library:</strong> implement single authoritative canonicalization functions (string normalization, numeric serialization, mapping canonicalization) and port to Power Query, VBA and backend languages; include exhaustive unit tests asserting byte-for-byte equality of canonical outputs. <br>2. <strong>Evidence store & signing:</strong> implement REG_EvidenceAudit with immutable writes, versioned signing keys, retention and <code>chainOfCustody</code>. <br>3. <strong>Parity CI:</strong> include golden fixtures for <code>ingestChecksum</code>, <code>mapHash</code>, <code>previewHash</code>, <code>exportChecksum</code> and block merges when golden parity fails for regulated datasets. <br>4. <strong>Safe invocation & telemetry:</strong> implement <code>SafeInvokeReconciler</code> and ensure all heavy handlers use it; configure interactive thresholds and job queue offload. <br>5. <strong>Mapping hot-swap harness:</strong> implement smoke preview harness, diff tool and approvals capture. <br>6. <strong>Apply idempotency tokens & ephemeral credentials:</strong> integrate with token service and GL test harness to simulate direct posts in CI. <br>7. <strong>Operator runbooks & training:</strong> publish runbooks for ingest issues, preview parity, mapping hot-swap, apply failures and evidence retrieval. <br>8. <strong>Monitoring & SLOs:</strong> instrument plan build latency, preview duration and apply failure rate; set alerts for golden parity failures. <br>9. <strong>Governance automation:</strong> implement enforcement for migration manifests, two-person approvals for <code>post_direct</code> and capture <code>approvalsRef</code> artifacts in evidence. <br>10. <strong>Data protection:</strong> strictly enforce PII redaction on UIs and analyst artifacts; evidence retrieval only after approvals with <code>chainOfCustody</code> recording.<br><br><strong>Expansion Integration — Final priorities mapping to phases</strong><br>- Phase 1 (0–90d): Items 1, 3, 4, 7 (minimal), 10 (baseline). <br>- Phase 2 (90–180d): Items 2, 5, 6, 8, 9 (CI enforcement & dashboard). <br>- Phase 3 (180–365d): Items 5 (scale), 6 (automation), 7 (training program completion), 8 (SLO enforcement at scale). </td></tr><tr><td data-label="Module Name"> <strong>Closing operational note</strong><br>This expanded artifact is intended as a canonical implementer and operator reference for Project 616. It codifies deterministic canonicalization, separation of concerns, audit-first design, evidence handling, mapping governance and safe apply flows. For any planned deviation (new canonicalization, PRNG change, rounding policy change), prepare a migration manifest, update golden fixtures, run canary previews and obtain required approvals before rolling out changes to production. <br><br><strong>Success metrics (measure world-class)</strong><br>• Golden parity pass rate ≥ 99.9% for regulated fixtures.<br>• Mean time to detect ingest/preview parity failure < 15 minutes.<br>• % applies with idempotent success ≥ 99.95% (no duplicate postings).<br>• Evidence retrieval approval lead time < 24 business hours.<br>• Analyst acceptance rate for high-confidence suggestions ≥ 85%.<br>• Time to recover from partial apply (revert/retry) < 4 hours. <br><br><strong>Immediate next tactical steps (first 30 days)</strong><br>1. Form the 3-team task force: Canonicalization (libs & fixtures), CI/Parity (pipelines & gating), Evidence/Governance (manifest + signing design).<br>2. Produce a minimal golden fixture set (payroll regulated sample, gl regulated sample, mapping sample) and failing parity test to validate CI gating.<br>3. Draft migration-manifest and approval UX wireframe; run one tabletop on a planned mapping change and persist findings as <code>evid:runbooks/tabletop_&lt;date&gt;.md</code>.<br><br><strong>Risks & mitigations</strong><br>Risk: canonicalization drift across runtimes → mitigation: cross-language test vectors, automated nightly parity checks. Risk: rollout friction → mitigation: canary plans + approvals + rollback paths. Risk: evidence store scalability → mitigation: archive tiers, signed manifests and integrity checks. <br><br><strong>Closing prioritization note</strong><br>Prioritise canonical parity + CI gating + migration governance first — these three yield the largest immediate reduction in risk and the fastest path to measurable, auditable improvements. Implementers must ensure each module provides the evidence artifacts and golden fixtures required by CI before expanding to Phase 2 items. </td></tr></tbody></table></div><div class="row-count">Rows: 15</div></div><div class="table-caption" id="Table3" data-table="Docu_0183_03" style="margin-top:2mm;margin-left:3mm;"><strong>Table 3</strong></div>
<div class="table-wrapper" data-table-id="table-3"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by REG_IngestPayroll — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">REG_IngestPayroll — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong>Scope & purpose (single sentence):</strong> Deterministic, auditable ingestion of payroll posting sources into canonical <code>PayrollRow[]</code>, producing immutable <code>ingestManifest</code> and canonical CSV (byte-for-byte reproducible), preserving full sanitized evidence behind <code>evidenceRef</code>, exposing deterministic hooks for CI/golden parity, and operating safely within Excel/VBA hosts with clear migration and governance controls. <br><br><strong>Important invariants & overall guarantees (must be preserved):</strong><br>1. Determinism: identical logical inputs (raw bytes, canonicalization version, header synonyms, numeric scale, position tokens) produce identical canonical CSV bytes and identical <code>ingestChecksum</code>. <br>2. Immutability: persisted canonical CSVs and <code>ingestManifest</code>s are immutable; corrections create new manifests with <code>correctionOf</code>. <br>3. Separation: ingest performs normalization and canonicalization only; mapping to GL is not executed here. <br>4. Audit-first: every public API call writes a PII-free audit row containing <code>correlationId</code> and <code>evidenceRef</code> pointers for full diagnostics. <br>5. PII minimization: UI previews show tokenized/pseudonymized values; full sanitized PII is stored encrypted in <code>evidenceRef</code> and requires approval to access. <br>6. CI/golden parity: cross-runtime golden fixtures (VBA, Power Query, backend) must reproduce canonical bytes; changes to canonicalization require <code>migrationManifest</code> and approval. </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong>Canonicalization — authoritative recipe (versioned)</strong><br><strong>Purpose:</strong> ensure identical bytes across implementations; version this recipe (e.g., <code>canonicalVersion = &quot;ingest-v1.0.0&quot;</code>) and record in manifests. <br><strong>Steps (deterministic, ordered):</strong><br>1. Byte-level normalization: strip BOM; normalize newlines to <code>\n</code>; remove trailing whitespace on each line; ensure UTF-8 encoding before hashing. <br>2. Header normalization: Unicode NFKC → fold to lower-case (only for header matching, not for UI display) → collapse internal whitespace → strip control characters → map via synonyms dictionary; store usedStrategy per header. <br>3. Field normalization: for each field apply NFKC, trim outer whitespace, collapse multiple internal spaces to single space, remove control chars, and apply field-specific transforms (casefold for tokens like <code>PayComponent</code>, preserve case for <code>Reference</code> unless configured). <br>4. Numeric normalization: represent <code>Amount</code> as fixed-decimal string with explicit <code>scale</code> (e.g., "1234.50") and also store <code>AmountMinorUnits</code> integer = Round(amount × 10^scale) using <strong>banker's rounding</strong> (round-half-to-even) by default. <br>5. Date normalization: canonicalize to ISO <code>YYYY-MM-DD</code> when possible; ambiguous parses flagged with <code>PAYROLL_AMBIG_DATE</code> and recorded in <code>ingestManifest.issues[]</code>. <br>6. Row serialization for hashing: define canonical field order (documented and versioned), join by pipe <code>|</code>, escape <code>|</code> as <code>\|</code>, produce per-row canonical string; do not include transient metadata like <code>createdBy</code> or <code>createdTs</code>. <br>7. Batch canonical CSV assembly: deterministic ordering (either preserve original order if caller requested or sort by a documented stable key if caller requests canonical sort). <br>8. Compute <code>ingestChecksum = sha256(UTF8(canonicalCSV))</code>. Persist CSV and manifest. </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <code>Function LoadPayrollPostings(sourcePath As String, sourceType As String, policy As String, Optional correlationId As String) As Collection</code> — full ingest flow (entrypoint). <br><code>Function DetectEncoding(bytes() As Byte) As Dictionary</code> — returns <code>{encoding, confidence}</code>. <br><code>Function GuessDelimiter(sampleText As String) As Dictionary</code> — returns <code>{delimiter, confidence, rationale}</code>. <br><code>Function MapHeaders(rawHeaders As Collection, synonyms As Dictionary, Optional correlationId As String) As Dictionary</code> — returns <code>{headerMap, suggestions[], diagnostics}</code>. <br><code>Sub StreamParseSource(sourcePath As String, onRowCallback As String, Optional chunkSize As Long, Optional correlationId As String)</code> — streaming parse, callback per row. <br><code>Function NormalizeRow(rawRow As Dictionary, headerMap As Dictionary, scale As Long, Optional correlationId As String) As Dictionary</code> — returns canonical <code>PayrollRow</code> and <code>rowDiagnostics</code>. <br><code>Function CoerceNumericFixedScale(rawValue As Variant, scale As Long) As Dictionary</code> — returns <code>{decimalString, minorUnits, ok, diagnostics}</code>. <br><code>Function CanonicalizeDate(rawDate As Variant, Optional preferredLocale As String) As Dictionary</code> — returns <code>{isoDate, confidence, parsePath}</code>. <br><code>Function ComputePayrollRowId(canonicalRowString As String, positionToken As String) As String</code> — returns sha256 hex. <br><code>Function PersistRawEvidence(bytes() As Byte, metadata As Dictionary) As String</code> — persists evidence and returns <code>evidenceRef</code>. <br><code>Function BuildIngestManifest(sourceUri As String, rows As Collection, metrics As Dictionary, policy As String, Optional correlationId As String) As Dictionary</code> — returns persisted manifest object. <br><code>Sub EmitIngestAudit(manifest As Dictionary, Optional correlationId As String)</code> — appends PII-free audit row referencing manifest. </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong>Function-level deep breakdown, examples, and narratives</strong> </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong><code>LoadPayrollPostings(sourcePath, sourceType, policy, correlationId)</code> — end-to-end orchestration (detailed)</strong><br><strong>Purpose & high-level narrative:</strong> orchestrates source fingerprinting, encoding detection, delimiter guessing, header mapping, streaming parse, per-row normalization, canonical CSV assembly, checksum computation, evidence persistence, manifest persist, and audit emission. Designed for Excel/VBA host but with clear separation for backend/hybrid execution. <br><strong>Parameters & options:</strong><br>1. <code>sourcePath</code>: file path, SFTP URI, or API payload token. <br>2. <code>sourceType</code>: <code>&quot;csv&quot;|&quot;tsv&quot;|&quot;xlsx&quot;|&quot;compressed&quot;|&quot;api&quot;</code> — instructs parse method. <br>3. <code>policy</code>: <code>&quot;fail-fast&quot;|&quot;skip-and-flag&quot;|&quot;tolerate-with-corrections&quot;</code>. <br>4. <code>correlationId</code>: reproducible unique id for tracing (if omitted, generate <code>r-YYYYMMDD-&lt;random&gt;</code>). <br><strong>Detailed flow (step-by-step, deterministic, ordered):</strong><br>1. <strong>Create correlation context:</strong> record <code>correlationId</code> if present otherwise generate one; all subsequent logs and audit rows embed it. <br>2. <strong>Read raw bytes:</strong> fetch file bytes (for <code>xlsx</code>, extract the primary sheet payload or the exported CSV bytes from the workbook). Use binary reads to avoid codepage conversions. <br>3. <strong>Source fingerprint:</strong> compute <code>sourceFingerprint = sha256(normalizeBytes(rawBytes))</code> where <code>normalizeBytes</code> applies byte-level normalization (strip BOM, newline normalization) to produce a stable fingerprint independent of incidental differences. <br>4. <strong>Detect encoding:</strong> call <code>DetectEncoding</code> on first N bytes and record <code>encoding</code> with confidence. <br>5. <strong>If CSV/TSV:</strong> sample first M lines (e.g., 200 lines) and call <code>GuessDelimiter</code>. For Excel/XLSX, read header row directly. <br>6. <strong>Header mapping:</strong> extract raw headers, call <code>MapHeaders</code> with authoritative synonyms dictionary (persisted and versioned). Record headerMap and suggestions. <br>7. <strong>Streaming parse:</strong> call <code>StreamParseSource</code> with <code>onRowCallback</code> bound to local <code>ProcessRow</code>. <code>StreamParseSource</code> reads chunks, yields rows, and provides reliable resumption offsets; store parse cursor periodically in <code>staging</code> if job long-running. <br>8. <strong>Per row normalization:</strong> <code>NormalizeRow</code> converts raw row to canonical <code>PayrollRow</code> and returns <code>rowDiagnostics</code>. For each normalized row write canonical CSV row to staging canonical stream. <br>9. <strong>Error handling per policy:</strong><br>- <code>fail-fast</code>: on first unrecoverable error abort, persist diagnostics evidence, emit <code>payroll.ingest.invalid</code> audit, return error. <br>- <code>skip-and-flag</code>: persist valid rows, store failing-row raw bytes to evidence, note in <code>ingestManifest.issues[]</code>. <br>- <code>tolerate-with-corrections</code>: attempt deterministic corrections (e.g., numeric coercion heuristics, header suggestion) and record changes in <code>rowDiagnostics</code> and <code>ingestManifest.corrections[]</code>. <br>10. <strong>Stitch canonical CSV:</strong> after streaming, ensure canonical CSV built deterministically (chunk ordering deterministic), close canonical stream and compute <code>ingestChecksum = sha256(UTF8(canonicalCSV))</code>. <br>11. <strong>Persist canonical CSV & raw evidence:</strong> call <code>PersistRawEvidence</code> for raw bytes and canonical CSV; returns <code>rawEvidenceRef</code> and <code>canonicalEvidenceRef</code>. <br>12. <strong>Build and persist <code>ingestManifest</code>:</strong> call <code>BuildIngestManifest</code> (see function breakdown below) to persist manifest including <code>ingestChecksum</code>, <code>rowsCount</code>, <code>rowsSample</code>, <code>issues[]</code>, <code>canonicalEvidenceRef</code>, <code>rawEvidenceRef</code>. <br>13. <strong>Emit ingest audit:</strong> call <code>EmitIngestAudit(manifest, correlationId)</code>. <br>14. <strong>Return canonical rows (optional)</strong>: if caller requested rows in memory and dataset small, return <code>PayrollRow[]</code> as <code>Collection</code>; otherwise return a descriptor <code>{manifestRef, canonicalEvidenceRef, rowsCount}</code> pointing caller to persisted artifacts for downstream processing like plan seeding. <br><strong>Example scenario (concrete):</strong> operator triggers <code>LoadPayrollPostings(&quot;sftp://finance/Payroll_Apr2026.csv&quot;,&quot;csv&quot;,&quot;skip-and-flag&quot;,&quot;r-20260120-xx1&quot;)</code>. DetectEncoding => <code>utf-8</code>; GuessDelimiter => <code>,</code> confidence 0.97; header <code>EmpIDNo</code> fuzzy-matched to <code>EmployeeID</code> with suggestion recorded; stream parse finds 12 rows with ambiguous <code>PostingDate</code> using <code>03/04/05</code> format; <code>policy=skip-and-flag</code> persists valid rows; manifest issues include 12 <code>PAYROLL_AMBIG_DATE</code> entries referencing <code>evidenceRefs</code> for failing rows. Audit <code>payroll.ingest.partial</code> emitted with manifestRef and <code>ingestChecksum</code>. </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong><code>DetectEncoding(bytes())</code> — expanded algorithm & VBA notes</strong> <br><strong>Return schema:</strong> <code>{encoding: &quot;utf-8&quot;|&quot;utf-16le&quot;|&quot;utf-16be&quot;|&quot;latin1&quot;|&quot;unknown&quot;, confidence: 0..1, rationale}</code>. <br><strong>Algorithm deterministic steps:</strong><br>1. Detect BOM sequences (UTF-8 <code>EF BB BF</code>, UTF-16LE <code>FF FE</code>, UTF-16BE <code>FE FF</code>) → immediate match confidence=1.0. <br>2. Try strict UTF-8 decode attempt on sample block; if succeeds with no invalid sequences → return UTF-8 with high confidence. <br>3. Test UTF-16 plausibility (even byte zero distribution pattern) → return UTF-16LE/BE if pattern matches with confidence 0.8. <br>4. Else fallback to Latin1 (ISO-8859-1) with moderate confidence. <br><strong>VBA implementation notes:</strong> use binary read into <code>Byte</code> array, check BOM by byte prefix comparisons, avoid <code>StrConv</code> until encoding decided. For reliability, implement in a COM helper DLL or call backend microservice for heavy encoding detection if available. <br><strong>Test cases:</strong> BOM present, accented chars without BOM (UTF-8), simulating windows-1252 vs Latin1 differences. </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong><code>GuessDelimiter(sampleText)</code> — scoring, tie-breaks, and PQ companion notes</strong> <br><strong>Output schema:</strong> <code>{delimiter: String, confidence: Double, rationale: String}</code>. <br><strong>Deterministic scoring steps:</strong><br>1. Candidate delimiters = [<code>,</code>,<code>;</code>,<code>\t</code>,<code>|</code>]. <br>2. For each candidate: parse sample lines ignoring quoted segments, compute observed column counts per line, compute mode count and variance. <br>3. Score = (modeCount × consistencyWeight) - variancePenalty + localeDelimiterBias (if locale hint indicates comma vs semicolon preference). <br>4. Choose delimiter with highest score; if tie apply deterministic preference order <code>\t</code>, <code>,</code>, <code>;</code>, <code>|</code>. <br><strong>Edge behavior:</strong> if confidence < threshold (e.g., 0.6) emit <code>ingestManifest.issues[]</code> recommending manual review. <br><strong>Power Query note:</strong> PQ <code>Csv.Document</code> supports delimiter parameter; store <code>mChecksum</code> for the <code>GuessDelimiter</code> decision for parity. </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong><code>MapHeaders(rawHeaders, synonyms, correlationId)</code> — strategy, fuzzy matching details</strong> <br><strong>Return schema:</strong> <code>{headerMap: Dictionary, suggestions: Collection, diagnostics: Collection}</code>. <br><strong>Strategy (deterministic):</strong><br>1. Normalize raw header: NFKC → lower-case → trim → strip control chars. <br>2. Exact match against canonical set → mark <code>usedStrategy=&quot;exact&quot;</code>. <br>3. Synonyms dictionary lookup (language-specific) → mark <code>usedStrategy=&quot;synonym&quot;</code>. <br>4. Deterministic fuzzy match: Levenshtein normalized by length; threshold default = 0.2 (i.e., distance/length ≤ 0.2) → <code>usedStrategy=&quot;fuzzy&quot;</code> with score. <br>5. If no match and <code>policy</code> allows corrections, create <code>suggestions</code> plus <code>suggestionEvidenceRef</code> recording raw header sample for operator review. <br><strong>Example mapping results:</strong> <code>{&quot;EmpIDNo&quot;:&quot;EmployeeID&quot;, &quot;PayComp&quot;:&quot;PayComponent&quot;, &quot;Amt&quot;:&quot;Amount&quot;}</code> with <code>suggestions</code> containing <code>{&quot;raw&quot;:&quot;Gross Pay&quot;,&quot;suggested&quot;:&quot;Amount&quot;,&quot;strategy&quot;:&quot;fuzzy&quot;,&quot;score&quot;:0.87}</code>. <br><strong>Diagnostics:</strong> record any headers requiring manual resolution and which mapping strategy was used per header. </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong><code>StreamParseSource(sourcePath,onRowCallback,chunkSize,correlationId)</code> — streaming architecture & robust resume</strong> <br><strong>Goal:</strong> process large files in bounded memory and support resume after interruption. <br><strong>Pattern:</strong><br>1. Open input stream in binary mode. <br>2. Read bytes up to chunkSize or until chunk contains a complete set of logical CSV records (handle quoted multi-line fields). <br>3. Convert chunk bytes to text using <code>DetectEncoding</code> result (deterministic decoder). <br>4. Split into rows respecting quoted fields and yield rows to <code>onRowCallback(rowDict, rowOffset)</code> with <code>positionToken</code> for each row. <br>5. Periodically persist parse cursor (byte offset or chunk index) in ephemeral staging manifest for resume. <br>6. On error, allow resume from last persisted parse cursor. <br><strong>VBA host constraints & best practice:</strong><br>1. Excel is single-threaded; avoid long synchronous loops that freeze UI — process limited batches and yield to UI using <code>DoEvents</code>. <br>2. For very large files prefer delegating parse to a helper COM server or a lightweight backend worker; the helper posts back to Excel by persisting canonical chunks to a shared storage and returning <code>manifestRef</code>. <br>3. Use <code>ADODB.Stream</code> binary reads to avoid codepage issues. <br><strong>Resume example:</strong> last persisted cursor <code>offset=12,345,678</code>. After crash, reopen file, seek to offset, resume chunking and parsing. </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong><code>NormalizeRow(rawRow,headerMap,scale,correlationId)</code> — precise field-level canonicalization & tokenization</strong> <br><strong>Return schema:</strong> canonical <code>PayrollRow</code> <code>{rowId, rawPayloadRef, EmployeeID (tokenized), PayComponent, CostCenter, Amount, AmountMinorUnits, Scale, Currency, PostingDate, Reference, Mapped_GLAccount = Null, rowDiagnostics}</code>. <br><strong>Deterministic step order (must be adhered to exactly):</strong><br>1. Map headers using <code>headerMap</code>. <br>2. For each string field: apply Unicode NFKC → trim outer whitespace → replace sequences of whitespace with single space → remove control characters. <br>3. For <code>EmployeeID</code>: compute <code>EmployeeToken = HMAC_SHA256(utf8(employeeRaw), tokenKey)</code> where <code>tokenKey</code> is rotation-capable and stored in config; persist full encrypted <code>employeeRaw</code> to <code>evidenceRef</code>. <br>4. For <code>PayComponent</code> and <code>CostCenter</code> apply canonical tokenization: lower-case, collapse punctuation as per <code>canonicalMappingRules</code>. <br>5. For <code>Amount</code>: call <code>CoerceNumericFixedScale(amountRaw, scale)</code> to obtain <code>decimalString</code> and <code>AmountMinorUnits</code>. <br>6. For <code>PostingDate</code>: call <code>CanonicalizeDate(postingDateRaw, optionalLocale)</code>. <br>7. Build canonicalRowString with fixed field order and escaping. <br>8. Compute <code>rowId = ComputePayrollRowId(canonicalRowString, positionToken)</code>. <br>9. Attach <code>rowDiagnostics</code> (coercions applied, parseConfidence, originalRawMap) to manifest but not in canonical CSV. <br><strong>Notes on <code>rowDiagnostics</code>:</strong> store per-row diagnostics separately referenced by <code>evidenceRef</code> to avoid PII leakage in primary artifacts; do not include raw PII in audit rows. <br><strong>Example normalized row:</strong><br><code>{rowId:&quot;sha256:abc...&quot;, rawPayloadRef:&quot;evid:raw/ingest/..&quot;, EmployeeID:&quot;tok:3a5f...&quot;, PayComponent:&quot;ot&quot;, CostCenter:&quot;CC100&quot;, Amount:&quot;1234.50&quot;, AmountMinorUnits:123450, Scale:2, Currency:&quot;USD&quot;, PostingDate:&quot;2026-03-14&quot;, Reference:&quot;INV-001&quot;, Mapped_GLAccount:Null, rowDiagnostics:{coercedAmount:true, originalAmount:&quot;$1,234.50&quot;}}</code> </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong><code>CoerceNumericFixedScale(rawValue, scale)</code> — exact algorithm & edge-cases</strong> <br><strong>Precise deterministic algorithm:</strong><br>1. Pre-clean: remove surrounding quotes, non-numeric punctuation except decimal separator; normalize unicode minus signs to ASCII <code>-</code>; trim whitespace. <br>2. Determine decimal separator: if both <code>.</code> and <code>,</code> present use deterministically chosen rule (e.g., choose separator that occurs only once or use <code>localeHint</code>); if ambiguous fallback to <code>.</code> and record a low-confidence diagnostic. <br>3. Build an arbitrary-precision decimal string (no floating point) by splitting integer and fractional parts. <br>4. Multiply decimal by 10^scale to compute rational target integer; apply banker's rounding: if fractional portion is exactly 0.5 then round to even integer; otherwise standard nearest. <br>5. Output <code>decimalString = formatted with exactly scale digits</code> and <code>minorUnits = integer</code>. <br><strong>Return:</strong> <code>{decimalString, minorUnits, ok:Boolean, diagnostics:{parseConfidence, originalRaw}}</code>. <br><strong>Edge cases handled:</strong> negative zero, extremely large magnitudes (cap and flag), currency symbols embedded in parentheses for negative accounting amounts <code>($1,234.50)</code>, scientific notation (e.g., <code>1.2345e3</code>) parsed deterministically if allowed by policy. </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong><code>CanonicalizeDate(rawDate, preferredLocale)</code> — tie-break rules & audit signals</strong> <br><strong>Deterministic precedence chain:</strong><br>1. If input matches ISO <code>YYYY-MM-DD</code> or ISO with time → accept. <br>2. If explicit <code>format</code> provided by ingestion context → attempt parse. <br>3. PreferredLocale provided by caller → use locale rules (e.g., <code>en-US</code> month/day/year). <br>4. Heuristic parse: choose format that yields realistic dates (e.g., day ≤ 31, month ≤ 12). <br>5. Two-digit years mapped deterministically: if <code>yy &lt;= cutoff</code> map to 2000+yy else 1900+yy; <code>cutoff</code> stored in ingest config (e.g., 69). <br>6. If ambiguous outcome (multiple plausible parses) → return <code>isoDate = NULL</code>, <code>confidence</code> low, and record <code>evidenceRef</code> for failing row; follow policy: skip-and-flag or fail-fast. <br><strong>Return example:</strong> <code>{isoDate:&quot;2026-03-14&quot;, confidence:0.98, parsePath:&quot;locale=en-US&quot;}</code>. </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong><code>ComputePayrollRowId(canonicalRowString, positionToken)</code> — hashing contract</strong> <br><strong>Deterministic spec:</strong> compute <code>sha256</code> over UTF-8 bytes of <code>positionToken + &quot;:&quot; + canonicalRowString</code>; output lower-case hex string. Use identical SHA-256 implementation across hosts (recommend vetted library). Document canonical field order used to build <code>canonicalRowString</code> in spec and version it. <br><strong>PositionToken choices & effects:</strong> using file offset vs row ordinal changes <code>rowId</code> if source re-ordered; choose token aligned with reproducibility goals (use stable row ordinal when reingesting corrected files to preserve identity). </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong><code>PersistRawEvidence(bytes(), metadata)</code> — evidence store contract & naming</strong> <br><strong>Contract summary:</strong> persist bytes atomically to evidence store, attach metadata (<code>correlationId</code>, <code>sourceUri</code>, <code>sourceFingerprint</code>, <code>canonicalVersion</code>, <code>ingestSchemaVersion</code>, <code>retentionPolicy</code>, <code>legalTags</code>), return an opaque <code>evidenceRef</code>. Evidence store should support immutability for regulated artifacts and optional signing. <br><strong>Recommended naming:</strong><br>- Raw file: <code>evid:raw/ingest/&lt;sourceFingerprint&gt;_&lt;ts&gt;.bin</code><br>- Canonical CSV: <code>evid:canonical/ingest_&lt;sourceFingerprint&gt;_v&lt;canonicalVersion&gt;.csv</code><br>- Manifest: <code>evid:manifests/ingest_&lt;manifestId&gt;.json</code><br><strong>Behavior on failure:</strong> transient errors → exponential backoff, persistent failures → stage artifact to secure local path and reference staged path in manifest with <code>stagedEvidence</code> flag; operator must run <code>evidence.retryPersist(stagedPath)</code> later. </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong><code>BuildIngestManifest(sourceUri, rows, metrics, policy, correlationId)</code> — manifest schema & examples</strong> <br><strong>Canonical manifest fields (recommended):</strong><br>1. <code>manifestId</code> (unique id) <br>2. <code>sourceUri</code> <br>3. <code>sourceFingerprint</code> <br>4. <code>ingestChecksum</code> (sha256) <br>5. <code>canonicalVersion</code>, <code>ingestSchemaVersion</code>, <code>ingestToolVersion</code> <br>6. <code>rowsCount</code> <br>7. <code>rowsSample[]</code> (top N rowIds) <br>8. <code>policy</code> <br>9. <code>issues[]</code> (structured diagnostics: code, severity, evidenceRef, rowOffset or rowId) <br>10. <code>rawEvidenceRef</code>, <code>canonicalEvidenceRef</code> <br>11. <code>createdBy</code>, <code>createdTs</code> <br>12. <code>parityMeta</code> (CI parity run result reference if available) <br>13. <code>migrationImpact</code> (if migration manifest present) <br><strong>Example manifest (JSON-like):</strong><br><code>{ &quot;manifestId&quot;:&quot;ingest_payroll_20260120_001&quot;, &quot;sourceUri&quot;:&quot;sftp://finance/Payroll_Apr2026.csv&quot;, &quot;sourceFingerprint&quot;:&quot;sha256:abc...&quot;, &quot;ingestChecksum&quot;:&quot;sha256:def...&quot;, &quot;canonicalVersion&quot;:&quot;ingest-v1.0.0&quot;, &quot;ingestSchemaVersion&quot;:&quot;1.0.0&quot;, &quot;rowsCount&quot;:12456, &quot;rowsSample&quot;:[&quot;sha256:...&quot;,&quot;sha256:...&quot;], &quot;policy&quot;:&quot;skip-and-flag&quot;, &quot;issues&quot;:[{&quot;code&quot;:&quot;PAYROLL_AMBIG_DATE&quot;,&quot;severity&quot;:&quot;warn&quot;,&quot;rowOffset&quot;:345,&quot;evidenceRef&quot;:&quot;evid:issue/ingest_345.bin&quot;}], &quot;rawEvidenceRef&quot;:&quot;evid:raw/ingest/sha...&quot;, &quot;canonicalEvidenceRef&quot;:&quot;evid:canonical/ingest_sha.csv&quot;, &quot;createdBy&quot;:&quot;alice&quot;,&quot;createdTs&quot;:&quot;2026-01-20T10:12:00Z&quot; }</code> <br><strong>Persistence contract:</strong> manifest persisted via <code>REG_EvidenceAudit.PersistEvidenceBlob</code> and its <code>evidenceRef</code> included in <code>EmitIngestAudit</code>. </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong><code>EmitIngestAudit(manifest, correlationId)</code> — audit row fields & retention</strong> <br><strong>Required audit row fields (PII-free):</strong><br>1. <code>timestamp</code> <br>2. <code>correlationId</code> <br>3. <code>module = &quot;REG_IngestPayroll&quot;</code> <br>4. <code>procedure = &quot;ingest.success&quot;|&quot;ingest.partial&quot;|&quot;ingest.invalid&quot;|&quot;ingest.failed&quot;</code> <br>5. <code>manifestRef</code> (evidenceRef) <br>6. <code>ingestChecksum</code> <br>7. <code>rowsCount</code> <br>8. <code>issuesSummary</code> (counts by severity/code) <br>9. <code>operatorId</code> or automation id <br>10. <code>canonicalVersion</code> <br><strong>Retention & integrity:</strong> audit rows append-only; include <code>prevHash</code> to enable simple chain-of-audits integrity checks; recon reports include signed snapshots. </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong>Extended narratives — real-world scenarios with step-by-step traces</strong> </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong>Scenario 1: "Large SFTP file with streaming ingest and resume" (very detailed)</strong> <br><strong>Context:</strong> finance drops <code>Payroll_2026_Q1.zip</code> containing <code>Payroll_Jan.csv</code> <code>Payroll_Feb.csv</code>... each 1.2M rows. Backend job picks up archive and calls <code>LoadPayrollPostings</code> for each file via backend worker; VBA only used for operator review. <br><strong>Flow highlights:</strong><br>1. Worker extracts CSVs to a staging bucket and computes <code>sourceFingerprint</code> per file. <br>2. <code>DetectEncoding</code> returns UTF-8; <code>GuessDelimiter</code> finds comma. <br>3. <code>StreamParseSource</code> processes file in 100k-row chunks, writing canonical chunk files (<code>canonical_chunk_0001.csv</code>, ...) with deterministic chunk naming (sourceFingerprint + partIndex). Each chunk's canonical bytes hashed into <code>partChecksum</code>. <br>4. After all chunks persisted, compute <code>ingestChecksum = sha256(concat(part1, part2,...))</code> using deterministic concatenation; persist <code>canonicalEvidenceRef</code> that references a manifest listing <code>partChecksums</code> to allow partial verification. <br>5. On transient worker crash after part 3, job restarts and resumes at part 4 using persisted parse cursor; no duplication or reorder occurs due to deterministic chunk indexing. <br>6. <code>ingestManifest</code> includes <code>parts[]</code> with <code>partIndex</code>, <code>partChecksum</code>, <code>canonicalPartRef</code>. <br><strong>Operator actions:</strong> operator can view partial canonical preview for a chunk by invoking <code>PreviewChunk(partRef)</code> which reproduces canonical rows for audit. </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong>Scenario 2: "Mailbox upload with multiple encodings (mixed sources) and header synonyms"</strong> <br><strong>Context:</strong> multiple country offices upload payroll files in different encodings with local header labels. <br><strong>Flow highlights:</strong><br>1. Ingest pipeline enumerates incoming files, applies <code>DetectEncoding</code> and <code>GuessDelimiter</code>, then <code>MapHeaders</code> with localized synonyms dictionary (per-country). <br>2. For a French office file using semicolon delimiter and ISO-8859-1, <code>DetectEncoding</code> returns <code>latin1</code> and <code>GuessDelimiter</code> returns <code>;</code>. <code>MapHeaders</code> uses French synonyms mapping <code>[&quot;Matricule&quot;-&gt;&quot;EmployeeID&quot;,&quot;DatePaie&quot;-&gt;&quot;PostingDate&quot;]</code>. <br>3. Canonicalization NFKC applied to ensure accent handling (<code>José</code> -> canonical handling for matching, but tokenization retains proper evidence storage). <br>4. Manifest stores <code>localeTag=fr-FR</code> and <code>headerSynonymsVersion</code>. <br><strong>Operator notes:</strong> country-specific synonyms and canonicalization exceptions must be maintained in a versioned repo; upgrades to synonyms require migration manifest when they affect canonical outputs. </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong>Deep conceptual Power Query (M) guidance and pseudo-implementations</strong> </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong>Conceptual M function patterns (pseudocode) — canonical ingestion</strong> <br><strong>Goal:</strong> provide parity with VBA canonicalization for PQ pilots. <br><strong>Pseudo-M:</strong><br><code>(sourcePath as text, canonicalVersion as text, scale as number) =&gt;</code><br><code>let</code><br><code>  Raw = Csv.Document(File.Contents(sourcePath), [Delimiter=&quot;,&quot;, Columns=... , Encoding=65001, QuoteStyle=QuoteStyle.Csv]),</code><br><code>  Promoted = Table.PromoteHeaders(Raw, [PromoteAllScalars=true]),</code><br><code>  HeaderMap = FnNormalizeHeaders(Table.ColumnNames(Promoted)),</code><br><code>  Mapped = Table.RenameColumns(Promoted, HeaderMap, MissingField.UseNull),</code><br><code>  NormalizeText = (t) =&gt; Text.Clean(Text.Trim(Text.Normalize(t, &quot;NFKC&quot;))),</code><br><code>  NormalizeRow = Table.TransformColumns(Mapped, List.Transform(ColumnList, each {_, each if _ is text then NormalizeText(_) else _, type text})),</code><br><code>  CoerceAmounts = Table.TransformColumns(NormalizeRow, {&quot;Amount&quot;, each FnCoerceNumeric(_, scale), type record}),</code><br><code>  CanonicalCSV = FnBuildCanonicalCsv(CoerceAmounts, canonicalVersion),</code><br><code>  IngestChecksum = FnSha256Binary(Text.ToBinary(CanonicalCSV)),</code><br><code>in</code><br><code>  [Table = CoerceAmounts, IngestChecksum = IngestChecksum]</code> <br><strong>Notes:</strong> implement <code>FnCoerceNumeric</code> using string manipulations rather than locale-dependent <code>Number.From</code> to preserve deterministic behavior; use <code>Binary.FromText</code> and SHA-256 helper implemented in a custom connector when needed. </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong>Power Query parity pitfalls & mitigation</strong> <br>1. PQ <code>Text</code> operations may differ across host versions — pin PQ feature version and test golden parity across Excel bitness. <br>2. Avoid <code>Number.FromText</code> with no culture specified; prefer explicit parsing logic in M or call out to signed helper extension for canonical decimal parsing. <br>3. Ensure <code>Text.Normalize</code> implementation parity; where unavailable, implement minimal NFKC-like normalizer in M or prefer backend canonicalization for heavy parity-critical flows. </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong>Conceptual DAX designs for downstream validation and dashboards</strong> </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong>Integer-first arithmetic pattern (recommended)</strong> <br><strong>Reason:</strong> avoid floating point drift when presenting totals and variances; use minor-units integers aggregated and only convert to decimal at display. <br><strong>Example measures:</strong><br><code>Payroll_TotalMinor = SUM(Payroll[AmountMinorUnits])</code><br><code>GL_TotalMinor = SUM(GL[AmountMinorUnits])</code><br><code>VarianceMinor = [Payroll_TotalMinor] - [GL_TotalMinor]</code><br><code>VarianceAmount = DIVIDE([VarianceMinor], POWER(10, SELECTEDVALUE(Params[Scale], 2)))</code><br><code>VariancePct = IF([GL_TotalMinor]=0, BLANK(), ABS([VarianceMinor]) / ABS([GL_TotalMinor]))</code><br><code>BeyondTolerance = IF( [GL_TotalMinor]=0, ABS([VarianceMinor]) &gt; SELECTEDVALUE(Params[AbsThreshold], 1000), [VariancePct] &gt; SELECTEDVALUE(Params[TolerancePct], 0.01) )</code><br><strong>Visualization patterns:</strong> show <code>VarianceAmount</code> with <code>BeyondTolerance</code> filter; allow drill-through to <code>sampleRowRefs</code> stored in preview artifact for triage. </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong>Operator runbooks (exhaustive, prescriptive)</strong> </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong>Ingest partial result triage (step-by-step)</strong> <br>1. Capture <code>correlationId</code> from alert. <br>2. Fetch <code>ingestManifest</code> via Evidence UI using manifestRef from the audit row. <br>3. Inspect <code>issues[]</code>; for each issue follow its <code>evidenceRef</code> to download failing-row sample (requires approvals). <br>4. If <code>PAYROLL_AMBIG_DATE</code> -> open <code>evidenceRef</code> sample and determine correct locale/format; update the source or update ingest <code>policy</code> and re-run with <code>correctionOf</code> referencing previous manifestId. <br>5. If <code>coercion</code> failures found for amounts due to strange numeric formats, document heuristic and update canonicalization <code>paramsHash</code> if you change policy; for regulated changes follow migration manifest path. <br>6. If storage persistence failure occurred, run <code>evidence.retryPersist(stagedPath)</code> or contact storage SRE. </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong>Apply/Export readiness checklist (pre-apply)</strong> <br>1. Confirm <code>ingestChecksum</code>, <code>mapHash</code>, and <code>planRecord</code> present and persisted. <br>2. Run smoke preview with canonical <code>planSeed</code> and ensure <code>previewHash</code> reproducible. <br>3. Confirm <code>suggestedJEs</code> production and <code>acceptedSuggestionRef</code> exist. <br>4. Validate <code>exportSpec</code> schema and run <code>GenerateJEExport</code> to compute <code>exportChecksum</code> and test loader-simulated import in staging GL. <br>5. For <code>post_direct</code> ensure <code>approvalsRef</code> present and ephemeral token issuance flow tested. </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong>Error codes, diagnostics, and remediation actions (practical catalog)</strong> </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong>Ingest-related codes (recommended)</strong><br><code>PAYROLL_INGEST_001</code> — <code>INVALID_SCHEMA</code> (required header missing) — fix: reformat source with required headers. <br><code>PAYROLL_INGEST_002</code> — <code>ENCODING_UNDETERMINED</code> — fix: source re-encode to UTF-8 or configure encoding hint. <br><code>PAYROLL_INGEST_003</code> — <code>AMBIGUOUS_DATE</code> — fix: provide locale or correct date in source. <br><code>PAYROLL_INGEST_004</code> — <code>NUMERIC_PARSE_FAIL</code> — fix: correct amount formatting or extend parsing heuristics and run tests. <br><code>PAYROLL_INGEST_005</code> — <code>EVIDENCE_PERSIST_FAIL</code> — SRE: check evidence store service, retry, or use staged fallback. <br><code>PAYROLL_INGEST_006</code> — <code>GOLDEN_PARITY_MISMATCH</code> — CI blocked: attach <code>migrationManifest</code> explaining canonical change. </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong>Security, PII, and compliance controls (detailed)</strong> </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong>PII policy for <code>EmployeeID</code> & sensitive fields</strong> <br>1. UI surfaces must show tokenized <code>EmployeeID</code> using <code>EmployeeToken = HMAC(employeeRaw, tokenKey)</code>; do not reveal raw employee values in analytics or logs. <br>2. Raw sensitive values persisted encrypted in evidence store; access requires <code>approvalsRef</code> and recorded chain-of-custody. <br>3. Evidence retrieval audit entries must include <code>approverIds</code>, <code>approvalTs</code>, <code>purpose</code>, and retrieval token expiration. <br>4. Automated redaction tools should be applied to snapshots before export to non-authorized consumers. </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong>Secrets & external connectors</strong> <br>1. No long-lived credentials in workbook or code; use ephemeral tokens fetched via secure token service (e.g., <code>modSecurity.getEphemeralToken(scope)</code>) for SFTP/API connections. <br>2. Token issuance events are audited but token values never logged. <br>3. For connectors requiring secrets in backend, use vault injection at runtime and rotate keys regularly. </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong>CI/CD & golden-parity governance (how to integrate)</strong> </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> 1. Maintain golden fixtures in <code>evid:golden/REG_IngestPayroll/</code> and lock them for regulated branches. <br>2. CI step <code>parity:golden:REG_IngestPayroll</code> runs canonical ingestion across all implementation targets (VBA macro harness, PQ script, and backend reference) using fixed <code>correlationId</code> and <code>canonicalVersion</code>. <br>3. If <code>ingestChecksum</code> differs from golden, CI produces <code>ci.golden.diff</code> with byte-level diff and blocks merge for regulated code until <code>migrationManifest</code> and approvals present. <br>4. Nightly parity runner monitors parity drift and creates incidents when golden diffs appear unexpectedly. </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong>Testing matrix (comprehensive)</strong> </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> 1. Unit tests: header mapping permutations, numeric coercion edge cases, date ambiguity matrix (locales), <code>ComputePayrollRowId</code> parity across runtimes. <br>2. Integration: streaming parse resume, chunked canonical CSV stitch, large-file memory profile. <br>3. Golden parity: fixed fixtures run across VBA/PQ/backend produce identical <code>ingestChecksum</code>. <br>4. Property tests: invariance to row reordering when caller requests canonical sort, determinism of sampling seeds for previews. <br>5. Security tests: PII redaction verification, evidence access approval flows, forbidden API scans for plaintext secrets. <br>6. Performance: 50k/200k/1M row tests with CPU/memory telemetry and ingestion SLO verification. </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong>Implementation notes & high-quality VBA patterns</strong> </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> 1. Avoid blocking UI: implement worker patterns using <code>Application.OnTime</code> to schedule queued micro-tasks and <code>DoEvents</code> within loops for responsiveness. <br>2. Use <code>ADODB.Stream</code> or binary <code>Open ... For Binary</code> to read raw bytes and avoid system codepage conversions. <br>3. Implement SHA-256 in a vetted VBA module or call a signed COM/.NET shim; ensure UTF-8 canonical byte generation identical across environments. <br>4. Minimize Excel Range operations — write staging arrays and batch-write to sheets only for small datasets or for operator views. <br>5. For heavy ingest or parity-critical tasks prefer delegating to backend microservice invoked by VBA with job descriptor; retain manifest linkage. </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong>Migration manifest example (ingest canonical change)</strong> </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong>Fields:</strong> <code>migrationId</code>, <code>author</code>, <code>createdTs</code>, <code>changeRationale</code>, <code>affectedFixtures[]</code> (list of sample source files + beforeChecksum + afterChecksum), <code>estimatedAffectedCount</code>, <code>canaryPlan</code> (planId + cohortSize + KPIs), <code>rollbackPlan</code> (snapshotRef + revertSteps), <code>approvals[]</code> (owner + compliance + timestamps), <code>testMatrix</code> (unit/integration/golden references). <br><strong>Example snippet:</strong><br><code>{ &quot;migrationId&quot;:&quot;mig_20260120_ingest_norm_v2&quot;, &quot;author&quot;:&quot;eng@company&quot;, &quot;createdTs&quot;:&quot;2026-01-20T12:00:00Z&quot;, &quot;changeRationale&quot;:&quot;normalize diacritics and update date precedence to prefer ISO&quot;, &quot;affectedFixtures&quot;:[{&quot;fixture&quot;:&quot;golden_ingest_payroll_regulated_v1.csv&quot;,&quot;before&quot;:&quot;sha256:aaa...&quot;,&quot;after&quot;:&quot;sha256:bbb...&quot;}], &quot;estimatedAffectedCount&quot;:12456, &quot;canaryPlan&quot;:{ &quot;planId&quot;:&quot;p_canary_001&quot;,&quot;cohortSize&quot;:1000,&quot;KPIs&quot;:[&quot;parityMismatchPct&quot;,&quot;previewDeltaPct&quot;] }, &quot;rollbackPlan&quot;:{&quot;snapshotRef&quot;:&quot;evid:manifests/ingest_before_v1.json&quot;,&quot;revertId&quot;:&quot;revert_20260120_mig1&quot;}, &quot;approvals&quot;:[{&quot;role&quot;:&quot;owner&quot;,&quot;id&quot;:&quot;owner@example.com&quot;,&quot;ts&quot;:&quot;2026-01-20T12:10Z&quot;}, {&quot;role&quot;:&quot;compliance&quot;,&quot;id&quot;:&quot;compliance@example.com&quot;,&quot;ts&quot;:&quot;2026-01-20T13:00Z&quot;}], &quot;testMatrix&quot;:{ &quot;unit&quot;:&quot;ci/unit/ingest&quot;,&quot;integration&quot;:&quot;ci/integration/ingest&quot;,&quot;golden&quot;:&quot;ci/golden/ingest&quot;} }</code> </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong>Forensic runbook (incidents & evidence packaging)</strong> </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> 1. On detection of high-severity parity failure or data leak, capture <code>correlationId</code> and lock ingest-export flows to read-only to contain spread. <br>2. Run <code>forensic_pack --correlation &lt;cid&gt;</code> to assemble: <code>ingestManifest</code>, <code>canonical csv</code>, <code>rawEvidenceRef</code>, <code>audit_tail.csv</code> rows covering <code>cid</code>, <code>worker logs</code>, <code>migrationManifest</code> if applicable. <br>3. Compute checksums for all artifacts, persist to WORM storage, and record chain-of-custody. <br>4. Notify compliance and SRE with package link and evidence hashes; follow legal disclosure processes. </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong>Long-form conceptual examples (detailed)</strong> </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong>Example: complete ingest run with diagnostics and operator actions (full trace)</strong> <br>1. Operator runs <code>LoadPayrollPostings(&quot;file://\\share\\Payroll_Mar2026.csv&quot;,&quot;csv&quot;,&quot;skip-and-flag&quot;,&quot;r-20260120-x1&quot;)</code>. <br>2. System computes <code>sourceFingerprint=sha256:5fcd...</code>, detects <code>utf-8</code>, chooses <code>,</code> delimiter. <br>3. MapHeaders finds <code>EmpIDNo</code> fuzzy to <code>EmployeeID</code> and records suggestion <code>s-001</code> with evidenceRef <code>evid:hdrSuggest_s-001</code>. <br>4. StreamParseSource reads 20k rows in 5 chunk parts; while processing chunk 2 finds 8 rows with <code>03/04/05</code> ambiguous dates — under <code>skip-and-flag</code> these rows persisted to <code>evid:issues/ingest_chunk2_ambig.bin</code>, marked in <code>manifest.issues[]</code>. <br>5. Canonical CSV chunk files produced with deterministic ordering across chunks; stitched canonical CSV computed and <code>ingestChecksum=sha256:fe23...</code>. <br>6. Persist canonical CSV to <code>evid:canonical/ingest_5fcd_v1.csv</code>; persist manifest with <code>canonicalEvidenceRef</code>. <br>7. Emit <code>payroll.ingest.partial</code> audit with manifestRef and <code>issuesSummary</code> indicating 8 ambiguous date rows. <br>8. Operator inspects <code>issues[]</code>, obtains approval to retrieve evidence, fixes rows, and re-uploads corrected CSV invoking <code>LoadPayrollPostings</code> with <code>correctionOf</code> linking previous manifestId; new manifest references prior manifest via <code>correctionOf</code>. </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong>Developer handoff & deliverables checklist</strong> </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> 1. Provide <code>canonical_ingest_v1</code> reference libs for VBA, M (Power Query), and backend with unit tests and golden fixtures. <br>2. Provide CI <code>parity:golden:REG_IngestPayroll</code> job that runs all implementations against golden fixtures and fails on byte-for-byte mismatch. <br>3. Provide evidence API hook examples and <code>PersistRawEvidence</code> integration tests. <br>4. Provide sample <code>migrationManifest</code> template and <code>hotSwap</code> smoke-test harness for mapping-level changes. <br>5. Provide operator runbooks and <code>forensic_pack</code> helper script. </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong>Final acceptance & release gates (must be satisfied)</strong> </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> 1. Unit + integration tests for canonicalization and parsing pass. <br>2. Golden parity checks across VBA/PQ/backend produce identical <code>ingestChecksum</code> on regulated fixtures. <br>3. Evidence store persists raw + canonical artifacts; manifests signed (Phase 2+) and audit chain present. <br>4. Migration manifest required for any canonicalization semantic change; approvals recorded. <br>5. Operator runbooks validated with tabletop exercises; SRE alerting wired to parity/mismatch thresholds. </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong>Appendix: compact cheat sheets and examples (copy/paste-ready)</strong> </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong>Canonical CSV row example (escaped)</strong><br><code>ot|CC100|tok:3a5f...|1234.50|123450|2|USD|2026-03-14|INV-001</code> <br><strong>Example manifest minimal JSON (single-line compact):</strong><br><code>{ &quot;manifestId&quot;:&quot;ingest_20260120_001&quot;,&quot;sourceUri&quot;:&quot;file://payroll.csv&quot;,&quot;sourceFingerprint&quot;:&quot;sha256:5fcd...&quot;,&quot;ingestChecksum&quot;:&quot;sha256:fe23...&quot;,&quot;rowsCount&quot;:12456,&quot;canonicalEvidenceRef&quot;:&quot;evid:canonical/ingest_5fcd_v1.csv&quot;,&quot;createdTs&quot;:&quot;2026-01-20T10:12:00Z&quot; }</code> <br><strong>Example DAX measure for variance percent:</strong><br><code>VariancePct = IF([GLTotalMinor]=0, BLANK(), ABS([PayrollTotalMinor]-[GLTotalMinor]) / ABS([GLTotalMinor]))</code> </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong>If you want next (pick one):</strong> <br>1. Provide full Power Query M reference implementation sketch (copyable M code) for <code>canonical_ingest_v1</code> and <code>FnCoerceNumeric</code>. <br>2. Provide robust VBA skeletons for <code>LoadPayrollPostings</code>, <code>StreamParseSource</code>, and SHA-256 helper module with error handling. <br>3. Provide CI YAML example demonstrating <code>parity:golden:REG_IngestPayroll</code>, including fixtures and migration manifest check step. <br>4. Produce printable PDF of this entire specification (packaged). </td></tr><tr><td data-label="REG_IngestPayroll — Per-function Expert Technical Breakdown"> <strong>Verification & 10× checks performed:</strong><br>1. Canonicalization recipe is present and versioned. <br>2. Row-level id/hash recipe specified and stable across runtimes. <br>3. Streaming and resume approach documented for large files. <br>4. Numeric and date coercion algorithms specified with tie-breakers. <br>5. Evidence persist and naming conventions defined. <br>6. Audit contract and PII redaction policies included. <br>7. CI/golden parity governance and migration manifest requirements enumerated. <br>8. Power Query and DAX conceptual implementations provided for parity and downstream validations. <br>9. Operator runbooks and forensic packaging steps included. <br>10. Developer handoff checklist and deliverables enumerated. </td></tr></tbody></table></div><div class="row-count">Rows: 52</div></div><div class="table-caption" id="Table4" data-table="Docu_0183_04" style="margin-top:2mm;margin-left:3mm;"><strong>Table 4</strong></div>
<div class="table-wrapper" data-table-id="table-4"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by REG_IngestGL — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">REG_IngestGL — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="REG_IngestGL — Per-function Expert Technical Breakdown"> <strong>Preface & scope (concise)</strong><br>This document is the definitive, implementation-and-operations oriented expansion of the REG_IngestGL module specification. It assumes the reader will implement ingest in multiple runtimes (VBA for Excel-hosted pilots, Power Query transformations for workbook-embedded pipelines, and backend services for production-scale processing). The focus is determinism, auditability, evidence integrity, governance, testability (golden parity), security, and operational resilience. The content below includes: deep narrative explanations of each function; extended examples and edge-case treatment; conceptual guidance for Power Query transforms (no code); conceptual DAX measures for analytic workbooks (no code); exhaustive test and CI matrices; migration and governance recipes; SRE runbooks and operator procedures; manifest templates and naming conventions; and detailed QA acceptance criteria. The canonical field names and behavior contracts from the prior specification are preserved and extended. </td></tr><tr><td data-label="REG_IngestGL — Per-function Expert Technical Breakdown"> <strong>Table of contents (quick jump guide)</strong><br>1. Overview & purpose<br>2. Contract & canonical GLRow recap<br>3. Architectural model & dataflow sequences<br>4. Per-function deep dive (purpose, inputs, outputs, invariants, diagnostics, examples) — includes <code>LoadGLEntries</code>, <code>DetectEncoding</code>, <code>GuessDelimiter</code>, <code>MapHeaders</code>, <code>ResolveSignConvention</code>, <code>CoerceAmountToFixedScale</code>, <code>NormalizeGLAccount</code>, <code>ValidateGLAccount</code>, <code>ComputeGLRowId</code>, <code>CanonicalSerializeBatch</code>, <code>BuildGLIngestManifest</code>, <code>PersistRawEvidence</code>, <code>EmitGLIngestAudit</code>, <code>StreamParseSource</code> and helpers<br>5. Extremely detailed narratives & runbooks (typical flows, failure stories, partial recovery) — several story arcs with step-by-step operator actions<br>6. Chunking & stitched checksum semantics for huge feeds<br>7. Deterministic sampling & parity considerations (PRNG choices and sampling replay) — linkage to downstream preview reproducibility<br>8. Conceptual Power Query guidance (transform sequences, M expression management, template strategy, PQ-specific pitfalls) — conceptual only, no code<br>9. Conceptual DAX guidance (canonical measures, variance calculations, materiality logic) — conceptual only, no code<br>10. Golden fixtures, CI gating, migration-manifest workflows, approvals and audit-signing requirements<br>11. Tests & CI matrix (exhaustive) and cross-runtime parity tests<br>12. Security, PII, and evidence access controls (detailed) <br>13. Implementation checklists and recommended immediate steps for teams<br>14. Appendices: manifest examples, sample <code>glIngestManifest</code>, operator commands, error-code catalog, mapping between evidence artifacts and audit rows. </td></tr><tr><td data-label="REG_IngestGL — Per-function Expert Technical Breakdown"> <strong>1) Overview & purpose (expanded)</strong><br>REG_IngestGL is the canonical ingestion surface for GL data. Its end goals are: produce reproducible canonical artifacts that downstream modules (planning, preview, aggregation, suggestion engines, apply paths) can rely on; provide immutable evidence and a tamper-evident audit chain; detect and surface data quality and schema issues early; and provide operator-friendly remediation paths while protecting PII and preserving legal compliance. The module supports both small-scale interactive Excel/Power Query pilots and large-scale backend ingest pipelines. Implementation must remain consistent across runtimes and versions; canonicalization algorithms and <code>coaRules</code> are versioned and must be referenced in manifests so that every artifact is reproducible. </td></tr><tr><td data-label="REG_IngestGL — Per-function Expert Technical Breakdown"> <strong>2) Contract & canonical <code>GLRow</code> recap (detailed)</strong><br>- <code>glRowId</code> (deterministic string): sha256 over <code>positionToken</code> + canonical row string. <br>- <code>rawPayloadRef</code> (evidenceRef string): pointer to raw source bytes + offset. <br>- <code>GLAccount</code> (canonical string): normalized according to <code>coaRulesVersion</code>. <br>- <code>Amount</code> (decimal string): canonical fixed-decimal string, includes sign. <br>- <code>AmountMinorUnits</code> (integer): integer representation at <code>Scale</code>. <br>- <code>Scale</code> (integer): decimal places used (explicit). <br>- <code>Currency</code> (nullable ISO 4217 string). <br>- <code>PostingDate</code> (ISO <code>YYYY-MM-DD</code>). <br>- <code>JournalReference</code> (nullable string). <br>- <code>signConvention</code> (enum): <code>DebitCreditColumns | SignedAmount | Inferred</code>. <br>- <code>glIngestChecksum</code> (string): sha256 over canonical CSV bytes or stitched-chunks. <br><strong>Important invariants:</strong> <br>1. Canonicalization steps (string normalization, numeric formatting, field ordering, escaping semantics) are the single source of truth for parity and must be ported exactly. <br>2. All manifests include <code>canonicalVersion</code> and <code>coaRulesVersion</code>. <br>3. Evidence artifacts are immutable; corrections create new manifests with <code>correctionOf</code> references. </td></tr><tr><td data-label="REG_IngestGL — Per-function Expert Technical Breakdown"> <strong>3) Architecture & dataflow sequences (expanded)</strong><br>High-level sequence (for any runtime): <br>1. Input acquisition (poll SFTP, pick up upload, read API payload, user-drop) → raw bytes captured and persisted to evidence store with <code>sourceFingerprint</code>. <br>2. <code>DetectEncoding</code> → produce normalized bytes. <br>3. <code>GuessDelimiter</code> → determine separating char. <br>4. <code>StreamParseSource</code> → produce per-row raw dictionaries and file offsets. <br>5. <code>MapHeaders</code> → canonical header mapping, fuzzy suggestions persisted. <br>6. Per-row processing: <code>ResolveSignConvention</code> → <code>CoerceAmountToFixedScale</code> → <code>NormalizeGLAccount</code> → <code>ValidateGLAccount</code> → <code>ComputeGLRowId</code>. <br>7. Batch canonicalization: <code>CanonicalSerializeBatch</code> → compute <code>glIngestChecksum</code> and persist canonical CSV. <br>8. <code>BuildGLIngestManifest</code> → persist manifest to evidence (signed as needed) and emit <code>gl.ingest.*</code> audit row. <br>9. Downstream modules read the manifest and canonical CSV via evidence refs. <br><strong>Notes on parallelism & chunking:</strong> <br>- Parsing and normalization operate ideally as streaming transforms; chunk-level persistence and final stitch are supported for scale. <br>- <code>StreamParseSource</code> can yield rows to multiple worker threads in backend implementations, but canonicalization order must be preserved for canonical checksum unless <code>sortRows=true</code> provided explicitly with manifest documentation. </td></tr><tr><td data-label="REG_IngestGL — Per-function Expert Technical Breakdown"> <strong>4) Per-function deep dive — exhaustive</strong> </td></tr><tr><td data-label="REG_IngestGL — Per-function Expert Technical Breakdown"> <strong><code>LoadGLEntries(sourcePath As String, policy As String, options As Dictionary) -&gt; (GLRow[], manifestRef)</code></strong><br><strong>Role:</strong> master orchestrator; implements the end-to-end ingest behavior and policy enforcement. <br><strong>Contract details:</strong> <br>- Inputs: <code>sourcePath</code>, <code>policy</code> (string: <code>fail-fast | skip-and-flag | tolerate-with-corrections</code>), <code>options</code> (dictionary: <code>coaRulesVersion</code>, <code>desiredScale</code>, <code>encodingOverride</code>, <code>delimiterHint</code>, <code>chunkSize</code>, <code>sortRows</code>, <code>signManifest</code>). <br>- Output: canonical <code>GLRow[]</code> (if returning rows in memory), <code>manifestRef</code>. <br><strong>Detailed flow:</strong> <br>1. Acquire raw bytes from <code>sourcePath</code>. Persist raw artifact using <code>PersistRawEvidence</code> and compute <code>sourceFingerprint</code>. <br>2. Run <code>DetectEncoding</code>; if <code>encodingOverride</code> provided, use it but record <code>encodingOverride</code> in manifest and audit. <br>3. Determine delimiter using <code>GuessDelimiter</code> (use <code>delimiterHint</code> to bias if provided). <br>4. Stream parse using <code>StreamParseSource</code> with <code>chunkSize</code> and call <code>onRowCallback</code> which runs header mapping and per-row canonicalizers. <br>5. Perform sign resolution at batch-level (first pass heuristics) then per-row resolution with <code>ResolveSignConvention</code>. <br>6. Determine <code>desiredScale</code> either from options or batch inference. <br>7. Normalize <code>GLAccount</code> using <code>NormalizeGLAccount</code> with <code>coaRulesVersion</code> from options. <br>8. Validate <code>GLAccount</code> with <code>ValidateGLAccount</code> if <code>coaFixture</code> provided; collect suggestions. <br>9. Compute <code>glRowId</code> for each row and collect row-level diagnostics. <br>10. Build canonical CSV(s) using <code>CanonicalSerializeBatch</code> (per-chunk or single file) and compute <code>glIngestChecksum</code> (or stitched-checksum if chunked). <br>11. Build <code>glIngestManifest</code> using <code>BuildGLIngestManifest</code> and persist via <code>PersistRawEvidence</code> (manifest signed if <code>signManifest</code>). <br>12. Emit <code>gl.ingest.success|partial|invalid</code> audit via <code>EmitGLIngestAudit</code>. <br><strong>Policy behavior examples:</strong> <br>• <code>fail-fast</code>: if any critical issue (missing GLAccount column or ambiguous sign usage) → abort ingest and emit <code>gl.ingest.invalid</code>. <br>• <code>skip-and-flag</code>: persist valid rows and put invalid rows into evidence with issues and emit <code>gl.ingest.partial</code>. <br>• <code>tolerate-with-corrections</code>: attempt deterministic corrections (strip punctuation in GLAccount, coerce ambiguous numeric notation) and annotate manifest with <code>corrections[]</code> and <code>approvalsRef</code> if necessary. <br><strong>Diagnostics & manifest fields recorded:</strong> <code>sourceFingerprint</code>, <code>encodingDetected</code>, <code>delimiterChosen</code>, <code>headerMap</code>, <code>rowsCount</code>, <code>rowsFailedCount</code>, <code>rowsSuccessCount</code>, <code>glIngestChecksum</code>, <code>coaRulesVersion</code>, <code>signConvention</code>, <code>issues[]</code> (codes, rowOffsets, evidenceRefs), <code>canonicalVersion</code>, <code>ingestToolVersion</code>, <code>createdTs</code>, <code>ingestPolicy</code>. </td></tr><tr><td data-label="REG_IngestGL — Per-function Expert Technical Breakdown"> <strong><code>DetectEncoding(bytes() As Byte) -&gt; (encodingLabel, normalizedBytes, evidenceRef?)</code></strong><br><strong>Expanded behavior & heuristics:</strong> <br>1. Step A — BOM detection: if BOM present, set encoding accordingly and strip BOM for processing, record BOM in metadata. <br>2. Step B — byte-pattern heuristics: examine byte ranges to distinguish UTF-8 from single-byte encodings. <br>3. Step C — fallback and ambiguity handling: if bytes are ambiguous between UTF-8 and Windows-1252, pick UTF-8 as default but mark <code>encodingAmbiguous=true</code> and store a short sample in evidence for manual review. <br>4. Step D — newline normalization: convert <code>\r\n</code> and <code>\r</code> to <code>\n</code> to present normalized bytes to parser. <br><strong>Edge notes:</strong> multi-lingual GL extracts (non-ASCII characters in notes) often require UTF-8; failing to detect properly leads to canonical checksum mismatch across systems. Always persist <code>sourceFingerprint</code> computed over normalized bytes. </td></tr><tr><td data-label="REG_IngestGL — Per-function Expert Technical Breakdown"> <strong><code>GuessDelimiter(sampleText) -&gt; chosenDelimiter</code></strong><br><strong>Robustness details:</strong> <br>1. Evaluate a candidate set (<code>\t</code>, <code>,</code>, <code>;</code>, <code>|</code>) across first N lines (configurable), counting occurrences and column stability. <br>2. Score candidates based on consistency of column counts and quoted-field validity. <br>3. Apply deterministic tie-breaker order if scores equal (e.g., prefer tab, then comma, then semicolon, then pipe) — must be documented and stable. <br>4. If sample has heavy quoting containing embedded delimiters, use quoted-field heuristics (count balanced quotes) to avoid false positives. <br>5. When ambiguous, persist sample to evidence and raise <code>GL_INGEST_005</code>. </td></tr><tr><td data-label="REG_IngestGL — Per-function Expert Technical Breakdown"> <strong><code>MapHeaders(rawHeaders, synonyms, fuzzyThreshold) -&gt; headerMap</code></strong><br><strong>Deterministic mapping rules:</strong> <br>1. Normalize header text: NFKC → lowercase → strip control chars → collapse multiple spaces → trim. <br>2. Attempt exact match on <code>synonyms</code> dictionary. <br>3. If no exact match, compute normalized edit distance vs candidate canonical header names and pick the candidate if <code>distanceScore &gt;= fuzzyThreshold</code> or if deterministic tiebreaker applies. <br>4. If no candidate meets threshold, mark header as <code>unknown</code> in <code>headerMap</code> and include suggested mapping candidates in <code>manifest.headerSuggestions[]</code> for operator review. <br>5. Persist <code>headerMap</code> in manifest with <code>mapConfidence</code> and <code>mappingAlgorithmVersion</code>. </td></tr><tr><td data-label="REG_IngestGL — Per-function Expert Technical Breakdown"> <strong><code>ResolveSignConvention(batchRows, headerMap, policy) -&gt; (batchSignConvention, annotatedRows, issues)</code></strong><br><strong>Detailed rules and heuristics:</strong> <br>1. Gather presence counts for <code>Amount</code>, <code>Debit</code>, and <code>Credit</code> columns across sample or whole batch. <br>2. Heuristics: if <code>Amount</code> present in ≥ threshold% of rows (default 95%), adopt <code>SignedAmount</code>. Else if both <code>Debit</code> and <code>Credit</code> present in ≥ threshold% of rows adopt <code>DebitCreditColumns</code>. Else mark as <code>Inferred</code> and proceed with fallback rules. <br>3. If both <code>SignedAmount</code> and <code>DebitCreditColumns</code> occur significantly, emit <code>GL_INGEST_003</code> with sample evidence and follow <code>policy</code>. <br>4. For <code>DebitCreditColumns</code>, compute <code>signedAmount = Debit - Credit</code> and record <code>rawDebit</code>/<code>rawCredit</code> in diagnostics if values non-zero in both fields. <br>5. For <code>SignedAmount</code>, coercion rules apply to parse parentheses and locale-specific decimals. <br>6. Always annotate each row with <code>parseConfidence</code> for amount resolution, and include problematic row offsets in <code>manifest.issues[]</code>. </td></tr><tr><td data-label="REG_IngestGL — Per-function Expert Technical Breakdown"> <strong><code>CoerceAmountToFixedScale(rawValue, desiredScale, roundingMode) -&gt; {AmountString, Scale, AmountMinorUnits, parseConfidence, diagnostics}</code></strong><br><strong>Precision & rounding policy:</strong> <br>1. Parsing: strip currency symbols, whitespace, and thousands separators deterministically; detect parentheses for negative numbers; accept either dot or comma as decimal separator depending on locale heuristics or explicit option. <br>2. Scale selection: <code>desiredScale</code> passed in options or inferred as max decimals in sample (but inference recorded in manifest). <br>3. Rounding: default banker's rounding (round-half-to-even) to minimize bias; configurable to other rounding modes with manifest documentation. <br>4. Integer conversion: compute <code>AmountMinorUnits = round(Amount * 10^Scale)</code> using exact decimal math. <br>5. Diagnostics: include <code>parseConfidence</code> metric (0-1) based on how many heuristics needed, presence of ambiguous separators, multi-match of parse patterns, or locales. <br><strong>Edge cases & policy choices:</strong> <br>- If <code>rawValue</code> is blank and both Debit and Credit blank in a <code>DebitCreditColumns</code> scenario, treat as zero with <code>parseConfidence=1.0</code>. <br>- If <code>rawValue</code> has multiple decimal separators, set <code>parseConfidence</code> low and flag for review. <br><strong>Example calculations (conceptual):</strong> <br>- Input <code>&quot;(1,234.50)&quot;</code> with <code>Scale=2</code> → <code>Amount = -1234.50</code> → <code>AmountMinorUnits = -123450</code>. <br>- Input <code>&quot;£1.234,567&quot;</code> with ambiguous locale → <code>parseConfidence</code> low, store raw sample in evidence. </td></tr><tr><td data-label="REG_IngestGL — Per-function Expert Technical Breakdown"> <strong><code>NormalizeGLAccount(rawAccount, coaRules) -&gt; {canonicalAccount, mappingDetails}</code></strong><br><strong>COA rules mechanics:</strong> <br>1. Unicode NFKC normalize and trim. <br>2. Replace variety of separators (<code>/</code>, <code>.</code>, <code>-</code>, spaces) with canonical separator (configurable, e.g., <code>|</code>). <br>3. Split into segments, apply <code>padSegments</code> rules if fixed width required; zero-pad left where required. <br>4. Strip non-significant punctuation if <code>stripPunctuation=true</code>. <br>5. Optionally lower-case or preserve case depending on <code>preserveCase</code> flag in <code>coaRules</code>. <br>6. Produce <code>mappingDetails</code> containing original segments, transformations applied, and removed characters for forensic reconstructability. <br><strong>Governance:</strong> <br>- <code>coaRulesVersion</code> must be included in manifest. <br>- Any change to <code>coaRulesVersion</code> that affects canonical accounts for regulated segments must be managed via <code>migrationManifest</code>. <br><strong>Examples:</strong> <br>- Raw <code>6-100.10</code> with <code>segmentWidths=[2,3,2]</code> → canonical <code>06|100|10</code> and mappingDetails shows all steps. </td></tr><tr><td data-label="REG_IngestGL — Per-function Expert Technical Breakdown"> <strong><code>ValidateGLAccount</code> (detailed)</strong><br><strong>Validation behavior and suggestions:</strong> <br>1. Exact match: confirm presence in <code>coaFixture</code>. <br>2. If missing, produce candidate suggestions using deterministic ranking: prefix matches, segment-subset matches, normalized Levenshtein across canonical tokens. <br>3. Provide <code>suggestionConfidence</code> and rank of alternatives. <br>4. If <code>policy</code> is <code>strict</code>, reject ingest on missing account; if <code>suggest-only</code>, accept but flag in manifest and link to <code>mappingSuggestions</code> artifact. <br>5. Persist validation logs in evidence for later audit and now include <code>ownerId</code> if COA fixture items include owners for automated notification. </td></tr><tr><td data-label="REG_IngestGL — Per-function Expert Technical Breakdown"> <strong><code>ComputeGLRowId(canonicalRowString, positionToken) -&gt; glRowId</code></strong><br><strong>Row ID recipe & reproducibility:</strong> <br>1. Fixed fields and order: <code>GLAccount|AmountMinorUnits|Scale|Currency|PostingDate|JournalReference</code>. <br>2. Escape special separator characters in field content deterministically (replace <code>|</code> with <code>\|</code> or equivalent escape sequence) before concatenation. <br>3. Compute <code>sha256(positionToken + &quot;:&quot; + canonicalRowString)</code> and return hex digest. <br>4. PositionToken recommendation: use <code>filePath:lineNumber</code> or file <code>byteOffset</code> to ensure uniqueness for duplicate content. <br><strong>Determinism note:</strong> Row IDs must be identical across repeated ingest of the same canonical CSV and position token usage. </td></tr><tr><td data-label="REG_IngestGL — Per-function Expert Technical Breakdown"> <strong><code>CanonicalSerializeBatch(rows, canonicalVersion, sortRows=false) -&gt; (canonicalCSVBytes, glIngestChecksum)</code></strong><br><strong>Serialization rules & escaping semantics:</strong> <br>1. Header order fixed and documented in canonicalVersion; do not include transient metadata columns. <br>2. Fields serialized with deterministic escaping: escape separator character, escape backslashes, and ensure text fields are NFKC-normalized before serialization. <br>3. Line endings strictly <code>\n</code>. <br>4. Optional <code>sortRows=true</code> performs lexicographic sort by grouping key; when used, record <code>sortRows=true</code> in manifest, because downstream consumers must know behavior. <br>5. Compute <code>glIngestChecksum = sha256(UTF8(canonicalCSVBytes))</code>. <br>6. For chunked mode produce per-chunk canonical CSVs and chunk checksums, then compute stitched <code>glIngestChecksum</code> per chosen stitch scheme. <br><strong>Checks:</strong> provide a <code>canonicalPreview</code> (first N bytes) for quick regression verification in CI logs to expedite debugging of parity failures. </td></tr><tr><td data-label="REG_IngestGL — Per-function Expert Technical Breakdown"> <strong><code>BuildGLIngestManifest(sourceUri, rowsCount, glIngestChecksum, metrics, options) -&gt; manifest</code></strong><br><strong>Fields and governance metadata (exhaustive):</strong> <br>- <code>manifestId</code>, <code>sourceUri</code>, <code>sourceFingerprint</code>, <code>glIngestChecksum</code>, <code>rowsCount</code>, <code>currencySet</code>, <code>signConvention</code>, <code>coaRulesVersion</code>, <code>canonicalVersion</code>, <code>ingestToolVersion</code>, <code>policy</code>, <code>createdTs</code>, <code>createdBy</code>, <code>correctionOf</code> (optional), <code>approvalsRef</code> (if corrections require approvals), <code>issues[]</code> (structured), <code>headerMap</code>, <code>sampleRows</code> (glRowIds), <code>metrics</code> (rowsSuccessCount, rowsFailedCount, parseConfidenceSummary), <code>evidenceRefs</code> (rawRef, canonicalCSVRef, snippetsRef), <code>stitchScheme</code> (if chunked), <code>migrationId</code> (if applicable). <br><strong>Signature & signing:</strong> <br>- If evidence signing is available, compute <code>manifestHash</code> and request HSM/KMS signature; attach <code>signedBy</code>, <code>signedTs</code>, and <code>signatureRef</code> in manifest. <br><strong>Persistence:</strong> pass manifest to <code>PersistRawEvidence</code> for immutability and return <code>manifestRef</code>. <br><strong>Audit:</strong> call <code>EmitGLIngestAudit(manifestRef, manifest)</code> to create append-only audit row. </td></tr><tr><td data-label="REG_IngestGL — Per-function Expert Technical Breakdown"> <strong><code>PersistRawEvidence(bytes, metadata) -&gt; evidenceRef</code></strong><br><strong>Operational considerations:</strong> <br>1. Evidence store must support immutable writes, retention tags, and optional cryptographic signing. <br>2. For large files use chunked uploads with final manifest and store chunk-level checksums. <br>3. Evidence metadata must include <code>ingestManifestId</code> and <code>canonicalVersion</code> so retrieval is straightforward for audits. <br>4. Access control: evidence retrieval must go through <code>EvidenceAccessApprovalFlow</code> and chain-of-custody logging. </td></tr><tr><td data-label="REG_IngestGL — Per-function Expert Technical Breakdown"> <strong><code>EmitGLIngestAudit(manifestRef, manifest)</code></strong><br><strong>Audit row structure and PII rules:</strong> <br>1. Audit rows must be PII-free and contain: <code>timestamp</code>, <code>correlationId</code>, <code>module=REG_IngestGL</code>, <code>procedure=gl.ingest.success|partial|invalid</code>, <code>manifestRef</code>, <code>glIngestChecksum</code>, <code>rowsCount</code>, <code>issuesSummary</code>, <code>coaRulesVersion</code>, <code>canonicalVersion</code>. <br>2. Audit entries must persist append-only in an audit store and be chained (include <code>prevHash</code> or equivalent). <br>3. For regulated runs, rotate and sign the daily audit slice and preserve in WORM storage with retention per legal requirements. </td></tr><tr><td data-label="REG_IngestGL — Per-function Expert Technical Breakdown"> <strong><code>StreamParseSource(sourcePath, onRowCallback, chunkSize)</code></strong><br><strong>Streaming architecture & resume behavior:</strong> <br>1. Read normalized bytes and split to lines using <code>\n</code> normalized delimiter. <br>2. For every chunk of <code>chunkSize</code> rows, call <code>onRowCallback</code> with rows and starting offset. <br>3. Persist chunk offset and chunk status in job descriptor for idempotent resume. <br>4. Handle partial line at chunk boundary by buffering remainder until full line is available. <br>5. On failure, re-run from last successfully persisted chunk offset. <br>6. If using a queue/worker model, ensure chunk descriptors include <code>correlationId</code> and <code>manifestId</code> to maintain traceability. </td></tr><tr><td data-label="REG_IngestGL — Per-function Expert Technical Breakdown"> <strong>5) Extremely detailed narratives & runbooks — scenarios and operator steps</strong> </td></tr><tr><td data-label="REG_IngestGL — Per-function Expert Technical Breakdown"> <strong>Narrative 1 — daily SFTP feed (happy path, full trace)</strong><br>1. Operator places <code>GL_20260121.csv</code> into SFTP inbound. <br>2. Scheduler creates job with <code>correlationId</code> <code>r-20260121-abc123</code> and invokes <code>LoadGLEntries(..., policy=&#x27;skip-and-flag&#x27;, options={coaRulesVersion:&#x27;v2.0&#x27;, desiredScale:2, chunkSize:500000})</code>.<br>3. <code>PersistRawEvidence</code> stores raw file, returns <code>rawRef</code>. <br>4. <code>DetectEncoding</code> returns <code>utf-8</code> → <code>normalizedBytes</code> used by parser. <br>5. <code>GuessDelimiter</code> returns <code>comma</code>. <br>6. <code>StreamParseSource</code> reads in 500k row chunks; for each chunk: map headers via <code>MapHeaders</code>, call <code>ResolveSignConvention</code> (batch-level found <code>DebitCreditColumns</code>), run per-row <code>CoerceAmountToFixedScale</code> with Scale=2 and banker's rounding, normalize <code>GLAccount</code> with <code>coaRules.v2.0</code>, validate accounts against COA fixture (found in local cache), compute <code>glRowId</code> and stream into a chunk CSV via <code>CanonicalSerializeBatch</code>. <br>7. Each chunk CSV persisted with <code>PersistRawEvidence</code>, chunk checksum computed and recorded in job descriptor. <br>8. After final chunk, stitch chunk checksums in deterministic order into final <code>glIngestChecksum</code>. <br>9. <code>BuildGLIngestManifest</code> creates manifest with <code>stitchScheme=&#x27;concat&#x27;</code>, includes chunk evidenceRefs, <code>rowsCount</code>, <code>currencySet</code>, <code>coaRulesVersion=&#x27;v2.0&#x27;</code>, <code>signConvention=&#x27;DebitCreditColumns&#x27;</code>. <br>10. Manifest persisted & <code>EmitGLIngestAudit</code> invoked; audit stored and administrators notified via monitoring that <code>gl.ingest.success</code> was recorded. <br>11. Downstream <code>REG_Preview</code> references manifestRef to perform deterministic preview seeded by plan seed. </td></tr><tr><td data-label="REG_IngestGL — Per-function Expert Technical Breakdown"> <strong>Narrative 2 — mixed-format feed & sign ambiguity (triage)</strong><br>1. An external supplier combined two small GL feeds into one file — first part uses <code>Amount</code>, second part uses <code>Debit</code>/<code>Credit</code>. <br>2. <code>ResolveSignConvention</code> reports <code>GL_INGEST_003: ambiguous_sign_convention</code> because both patterns exceed thresholds in the same batch. <br>3. With <code>policy=fail-fast</code>, ingest aborts and <code>gl.ingest.invalid</code> audit is emitted with <code>manifest.issues[]</code> referencing sample evidence showing the two patterns. <br>4. Operator runs <code>evidence.pull</code> to retrieve sample, inspects, and requests corrected feed from supplier or decides to split file into two passes: run <code>LoadGLEntries</code> twice with offsets (or split file at known boundary) and re-ingest each with the correct <code>policy</code>/options. <br>5. Once corrected, downstream parity tests confirm <code>glIngestChecksum</code> matches expected golden sample. <br>6. If the operator must proceed while supplier fixes, they can set <code>policy=tolerate-with-corrections</code> and provide deterministic correction rules (e.g., treat portion with <code>debit</code>/<code>credit</code> for rows where <code>Amount</code> missing) and attach <code>approvalsRef</code> to the manifest for governance traceability. </td></tr><tr><td data-label="REG_IngestGL — Per-function Expert Technical Breakdown"> <strong>Narrative 3 — COA drift and migration management (owner/approval)</strong><br>1. Finance publishes a new COA that changes punctuation and segment widths for an entire department. <br>2. Ingest runs show high <code>GL_INGEST_004</code> counts for unknown GLAccounts. <br>3. The mapping team creates a <code>migrationManifest</code> covering COA change with sample fixtures showing before/after canonicalization and a canary plan to reprocess a small historical cohort. <br>4. Run canary ingest and preview; compare KPIs (mapping coverage, variance deltas). <br>5. If canary meets thresholds and owners approve, <code>coaRulesVersion</code> updated to <code>v3.0</code> and new ingest runs use <code>NormalizeGLAccount</code> with v3 rules. All manifests reference <code>migrationId</code>. <br>6. If KPIs breach thresholds, follow rollback plan: revert <code>coaRulesVersion</code> pointer to previous v2 and notify stakeholders. </td></tr><tr><td data-label="REG_IngestGL — Per-function Expert Technical Breakdown"> <strong>6) Chunking & stitched checksum semantics (very detailed)</strong><br><strong>Rationale:</strong> very large GL files cannot be fully held in memory; chunking enables parallel processing and fault-tolerant resumption while still producing deterministic canonical output and a single authoritative checksum. <br><strong>Design choices:</strong> <br>1. <strong>Per-chunk canonical CSV + chunk checksum</strong>: each chunk is canonicalized independently and persisted; chunk checksum computed as sha256 over chunk canonical CSV bytes. <br>2. <strong>Stitching semantics</strong>: final <code>glIngestChecksum</code> computed as sha256 over concatenation of chunk checksums in the <em>original source order</em> (not parallel completion order) or via a Merkle-tree approach where final root is computed deterministically. Document <code>stitchScheme</code> in manifest. <br>3. <strong>Resume behavior</strong>: job descriptor records last successful chunk index and offset; on failure restart reads remaining chunks only and recomputes stitched checksum requiring only the unchanged chunk checksums. <br>4. <strong>Determinism enforcement</strong>: chunk size and ordering are deterministic (configurable but fixed per ingest) and recorded in manifest so reproducing the stitched checksum is possible. <br><strong>Edge-case handling:</strong> <br>1. If chunking changes between runs (e.g., different chunkSize), the final stitched checksum will differ; treat as semantics change and require manifest documentation. <br>2. If rows cross chunk boundary, stream parser ensures row integrity by buffering partial lines; manifest records any row fragmentation events for audits. <br><strong>Operational hints:</strong> keep chunkSize large enough to amortize overhead but small enough to fit worker memory and allow fast retries. For recommended values, test on representative data sets. </td></tr><tr><td data-label="REG_IngestGL — Per-function Expert Technical Breakdown"> <strong>7) Deterministic sampling & parity considerations (detailed)</strong><br><strong>Why sampling matters:</strong> preview reproducibility relies on deterministic sampling; if sampling algorithm changes or seed differs, preview artifacts will not match golden fixtures. <br><strong>Algorithm recommendation & metadata:</strong> <br>1. Use a cryptographic deterministic PRNG such as HMAC_DRBG-SHA256 seeded by <code>planSeed</code> derived from <code>paramsHash + &quot;:&quot; + mapHash + &quot;:&quot; + ingestChecksum</code>. <br>2. Sampling implementation: perform a deterministic Fisher–Yates shuffle on lexicographically sorted <code>glRowId</code> list seeded by PRNG and take first N entries for sample. Alternative: deterministic reservoir sampling using seeded PRNG for memory boundedness. <br>3. Store full <code>samplingMeta</code> in plan and preview manifests: <code>{algorithm:&quot;FisherYates&quot;, prng:&quot;HMAC_DRBG-SHA256&quot;, seed:&quot;sha256:...&quot;, version:&quot;1.0.0&quot;}</code>. <br>4. Parity expectations: sampling algorithm and PRNG version must be stable across ports; any change requires updated golden fixtures and migration manifest. <br><strong>Replay & debug:</strong> store <code>seed</code> and a small sample listing of sampled <code>glRowId</code> for fast reproduction. For debug, re-run sampling with same <code>seed</code> and confirm identical <code>previewHash</code>. </td></tr><tr><td data-label="REG_IngestGL — Per-function Expert Technical Breakdown"> <strong>8) Conceptual Power Query (PQ) guidance (no code snippets)</strong><br><strong>Design approach for workbook-hosted pilots:</strong> <br>1. <strong>Parameterize transformations</strong>: Keep <code>coaRulesVersion</code>, <code>headerSynonyms</code>, <code>desiredScale</code>, and <code>delimiterHint</code> as parameters that are externalized (hidden sheet or remote config) rather than hard-coded into M expressions. This supports rapid iteration and CI parity. <br>2. <strong>Deferred evaluation & performance</strong>: design PQ steps so heavy parsing and normalizations are performed in explicit, refreshable stages. Avoid forcing evaluation of entire data unless necessary; use query folding to push work to data source when possible. <br>3. <strong>Canonicalization sequence (conceptual)</strong>: <br>1. Normalize bytes/encoding (handled by connector where possible). <br>2. Promote headers and apply header mapping as parameterized step. <br>3. Normalize GLAccount: apply text normalization, tokenization on separators, and segment padding via transform sequence. <br>4. Numeric parsing: define canonical parse step for amounts, convert to decimal with explicit number of decimals and then compute minor units via integer multiplication to avoid float drift. <br>5. Output stage: produce canonical CSV preview and compute a checksum using deterministic serialization step (e.g., explicit column order, stable escaping); persist checksum on export. <br>4. <strong>Testing:</strong> export intermediate deterministic snapshots and their checksums as part of PQ test runs; CI should run PQ transformations headless against fixtures (via automated runners) to validate parity. <br>5. <strong>Templates & maintainability:</strong> prefer small, composable templates that implement single responsibilities (header mapping, GL normalization, numeric coercion) rather than monolithic M scripts so changes to canonicalization are localized. <br><strong>Operational PQ pitfalls & mitigations:</strong> <br>- PQ may inadvertently apply localized number parsing using workbook locale; always set explicit parse format parameters or perform textual normalization first. <br>- Large files in Excel may require staging to disk or using Power BI dataset mode instead of workbook import. <br>- Hidden sheet synonyms are easy to tamper with; sign and version control template repositories externally and import by reference if possible. </td></tr><tr><td data-label="REG_IngestGL — Per-function Expert Technical Breakdown"> <strong>9) Conceptual DAX guidance (no code snippets)</strong><br><strong>Purpose:</strong> support downstream analytic measures for variance detection, materiality evaluation, trend analysis, and audit traceability. <br><strong>Core model design:</strong> <br>1. Import GL canonical aggregates using integer minor-unit columns and <code>Scale</code> metadata; prefer pre-aggregated tables where feasible. <br>2. Store <code>planId</code>, <code>mapHash</code>, <code>glIngestChecksum</code>, and <code>previewHash</code> as dataset-level metadata to permit slice-and-dice by run. <br>3. Canonical grouping keys: <code>GLAccount</code>, <code>CostCenter</code>, <code>Currency</code>, <code>Entity</code> where multi-entity. <br><strong>Key conceptual measures:</strong> <br>1. <code>PayrollAmount</code> and <code>GLAmount</code> computed using integer sums on <code>AmountMinorUnits</code> aggregated at group key; represent display amounts by dividing by <code>10^Scale</code>. <br>2. <code>VarianceMinorUnits = PayrollSumMinorUnits - GLSumMinorUnits</code>. <br>3. <code>AbsVariance = absolute(VarianceMinorUnits)</code>. <br>4. <code>RelativeVariancePct = if(GLSumMinorUnits = 0, BLANK(), AbsVariance / ABS(GLSumMinorUnits))</code>. <br>5. <code>BeyondTolerance</code> computed using plan-configured <code>TolerancePct</code> and <code>AbsThreshold</code> rules: <code>if(GLSumMinorUnits = 0, AbsVariance &gt; AbsThreshold, RelativeVariancePct &gt; TolerancePct)</code>. <br><strong>Materiality & filters:</strong> implement slicers for <code>TolerancePct</code> and <code>AbsThreshold</code> and allow operator to apply account-level overrides. <br><strong>Explorability & audit linkage:</strong> include clickable drill-through linking group to <code>previewRef</code> and <code>glRowId</code> sample lists (redacted on UI) so analysts can quickly inspect source-level evidence. <br><strong>Performance hints:</strong> precompute aggregates in ETL for large datasets; use calculated columns only for small dimensional logic; avoid row-level iteration in DAX for millions of rows. </td></tr><tr><td data-label="REG_IngestGL — Per-function Expert Technical Breakdown"> <strong>10) Golden fixtures, CI gating & migration manifest workflows (detailed)</strong><br><strong>Golden fixtures purpose & lifecycle:</strong> <br>1. Fixed canonical inputs and expected canonical outputs (canonical CSV bytes and <code>glIngestChecksum</code>) used by CI to assert cross-runtime parity. <br>2. Each golden fixture pair must include: raw source sample, expected canonical CSV, expected <code>glIngestChecksum</code>, <code>canonicalVersion</code>, <code>coaRulesVersion</code>, and small replay plan to reproduce. <br>3. Golden fixtures are stored in <code>golden/gl_ingest</code> path with immutable names and checksums; updates require PR with <code>migrationManifest</code>. <br><strong>CI gates and enforcement:</strong> <br>1. CI steps include unit tests, integration tests, property tests, then golden parity tests. <br>2. Golden parity failure blocks merges for regulated module changes; developer must either fix code or attach <code>migrationManifest</code> with clear owner approvals and regression test runs. <br>3. Nightly parity runner runs full golden suite and triggers SRE alerts on divergence. <br><strong>Migration manifest required contents (must be included with PRs that change semantics):</strong> <br>- <code>migrationId</code> (unique), <code>author</code>, <code>createdTs</code> <br>- <code>changeRationale</code> <br>- <code>affectedFunctions</code> list (e.g., <code>NormalizeGLAccount</code>, <code>CoerceAmountToFixedScale</code>) <br>- <code>sampleFixtures[]</code> (raw sample ref, expected before/after canonical CSV checksums) <br>- <code>canaryPlan</code> (<code>planId</code>, cohort sizes, kpis & thresholds) <br>- <code>rollbackPlan</code> (previous artifacts and revert steps) <br>- <code>approvals[]</code> (ownerIDs, timestamps) <br>- <code>testMatrix</code> (unit/integration/golden entries) <br><strong>Approval workflow:</strong> PRs changing canonicalization must capture <code>approvalsRef</code> in CI, and the deployment pipeline must require <code>approvalRef</code> for promotion to production. </td></tr><tr><td data-label="REG_IngestGL — Per-function Expert Technical Breakdown"> <strong>11) Tests & CI matrix (exhaustive)</strong><br><strong>Unit tests (function-level):</strong> <br>1. <code>DetectEncoding</code> tests: BOM variants, UTF-16 surrogate pairs, ambiguous bytes. <br>2. <code>GuessDelimiter</code> tests: quoted fields, mixed delimiters, Excel line endings. <br>3. <code>MapHeaders</code> tests: synonym permutations, fuzzy-match determinism, special characters in headers. <br>4. <code>ResolveSignConvention</code> tests: <code>Amount</code> only, <code>Debit/Credit</code> only, mixed patterns, parentheses negative. <br>5. <code>CoerceAmountToFixedScale</code> tests: thousands separators, locales, negative parentheses, varied decimal counts up to 6 decimals, rounding edge-case half-values. <br>6. <code>NormalizeGLAccount</code> tests: segmentation permutations, padding rules, punctuation stripping, case handling. <br>7. <code>ComputeGLRowId</code> tests: identical canonical row strings and position tokens yield same rowId. <br>8. <code>CanonicalSerializeBatch</code> tests: escaping rules, newline normalization, deterministic header order. <br><strong>Integration tests:</strong> <br>1. Full run with small sample raw file from source to manifest, validate canonical CSV bytes and manifest contents. <br>2. Streaming chunk reassembly tests including resume from offset, failure injection mid-chunk. <br>3. COA fixture validation tests with missing accounts and suggestion generation. <br><strong>Golden tests:</strong> <br>1. End-to-end run on canonical fixture pair — assert canonical CSV bytes and <code>glIngestChecksum</code> match golden. <br>2. Cross-runtime parity: run same fixture in VBA, PQ, backend and assert identical canonical bytes/hash. <br><strong>Property tests:</strong> <br>1. Shuffle-row invariance tests when <code>sortRows=false</code> should produce differing checksums (and test ensures this is acceptable per option), but when <code>sortRows=true</code> canonical checksum must be order-invariant. <br>2. Deterministic <code>glRowId</code> under repeated runs. <br><strong>Performance tests & SLO verification:</strong> stress test ingest throughput, chunking throughput, and memory consumption for lookup table handling. <br><strong>Security & compliance tests:</strong> forbidden-API detection (no secrets), evidence retrieval gating simulation, signature verification for signed manifests. </td></tr><tr><td data-label="REG_IngestGL — Per-function Expert Technical Breakdown"> <strong>12) Security, PII & evidence access controls (exhaustive)</strong><br><strong>PII handling rules:</strong> <br>1. GL payloads often include <code>JournalReference</code> or employee references: treat anything that could identify a person as PII. <br>2. Audit rows must be PII-free and contain only <code>evidenceRef</code> for full sanitized logs. <br>3. Evidence store must encrypt sanitized raw artifacts at rest and only support retrieval through <code>EvidenceAccessApprovalFlow</code> with two-person approvals for regulated disclosures. <br><strong>Secrets & detection:</strong> <br>1. If payload contains probable credentials (pattern detection) or tokens, ingest must block and emit <code>GL_INGEST_999</code> and alert security. <br>2. Forbidden APIs: code that reads from local secrets stores in UI/runtime path must fail static analysis. <br><strong>Chain-of-custody & approvals:</strong> <br>1. Evidence retrieval requests include <code>purpose</code>, <code>requesterId</code>, and <code>approvers[]</code>. <br>2. On approval, issue time-limited retrieval token and record retrieval event in <code>chainOfCustody[]</code> with <code>actorId</code>, <code>action</code>, <code>ts</code>, and <code>reason</code>. <br><strong>Signing and non-repudiation:</strong> <br>1. Use KMS/HSM-backed signing for manifests and optionally audit batches. <br>2. Store signature metadata in manifest and have verification steps in CI to assert signature validity. </td></tr><tr><td data-label="REG_IngestGL — Per-function Expert Technical Breakdown"> <strong>13) Implementation checklist & recommended immediate steps for teams</strong><br><strong>Developer & release checklist:</strong> <br>1. Implement canonicalization functions as isolated library modules and port to each runtime (VBA, PQ, backend). <br>2. Produce golden fixtures for representative regulated datasets and verify cross-runtime parity. <br>3. Add CI jobs: unit, integration, property, golden parity, security checks, and performance microbenchmarks. <br>4. Implement Evidence store persist APIs and sign manifests in staging. <br>5. Draft migration-manifest template and approvals capture UI flow. <br>6. Implement chunked ingest with stitch scheme and resume semantics. <br>7. Add operational dashboards for parity failures, ingest failure rates, COA validation stats and <code>parity.detect.ms</code>. <br><strong>Ops checklist:</strong> <br>1. Prepare SRE runbooks for ingest failure, parity failure, and apply revert flows. <br>2. Schedule runbook tabletop exercises for mapping hot-swap and migration scenarios. <br>3. Configure alerts for <code>GL_INGEST_004</code> spikes (COA mismatches) and golden parity failure. </td></tr><tr><td data-label="REG_IngestGL — Per-function Expert Technical Breakdown"> <strong>14) Appendices (templates, sample manifests, error codes)</strong> </td></tr><tr><td data-label="REG_IngestGL — Per-function Expert Technical Breakdown"> <strong>Sample <code>glIngestManifest</code> (exhaustive fields example)</strong><br><code>{ &quot;manifestId&quot;:&quot;gl_ingest_20260121_001&quot;, &quot;sourceUri&quot;:&quot;sftp://inbound/GL_20260121.csv&quot;, &quot;sourceFingerprint&quot;:&quot;sha256:abc123...&quot;, &quot;glIngestChecksum&quot;:&quot;sha256:def456...&quot;, &quot;rowsCount&quot;:1200000, &quot;currencySet&quot;:[&quot;USD&quot;], &quot;signConvention&quot;:&quot;DebitCreditColumns&quot;, &quot;coaRulesVersion&quot;:&quot;v2.0&quot;, &quot;canonicalVersion&quot;:&quot;1.4.0&quot;, &quot;ingestToolVersion&quot;:&quot;reg-ingestgl-1.2.3&quot;, &quot;policy&quot;:&quot;skip-and-flag&quot;, &quot;createdBy&quot;:&quot;system_ingest_scheduler&quot;, &quot;createdTs&quot;:&quot;2026-01-21T03:01:00Z&quot;, &quot;correctionOf&quot;: null, &quot;approvalsRef&quot;: null, &quot;stitchScheme&quot;:&quot;concat&quot;, &quot;chunkChecksums&quot;:[ &quot;sha256:ch1...&quot;, &quot;sha256:ch2...&quot;, &quot;sha256:ch3...&quot; ], &quot;sampleRows&quot;:[ &quot;sha256:row1...&quot;, &quot;sha256:row2...&quot; ], &quot;headerMap&quot;:{ &quot;acct_no&quot;:&quot;GLAccount&quot;, &quot;dr_amt&quot;:&quot;Debit&quot;, &quot;cr_amt&quot;:&quot;Credit&quot;, &quot;cur&quot;:&quot;Currency&quot;, &quot;post_dt&quot;:&quot;PostingDate&quot; }, &quot;issues&quot;:[ { &quot;code&quot;:&quot;GL_INGEST_004&quot;, &quot;description&quot;:&quot;COA validation failures for 24 accounts&quot;, &quot;evidenceRef&quot;:&quot;evid:sugg_coa_20260121.json&quot;, &quot;count&quot;:24 } ], &quot;metrics&quot;:{ &quot;rowsSuccessCount&quot;:1199976, &quot;rowsFailedCount&quot;:24, &quot;parseConfidenceSummary&quot;:{ &quot;median&quot;:0.98,&quot;p10&quot;:0.85 } }, &quot;evidenceRefs&quot;:{ &quot;rawRef&quot;:&quot;evid:raw_gl_abc123.bin&quot;,&quot;canonicalCSVRef&quot;:&quot;evid:canonical_gl_ingest_20260121_v1_4_0.csv&quot;,&quot;validationReport&quot;:&quot;evid:gl_validation_20260121.json&quot; }, &quot;migrationId&quot;: null }</code> </td></tr><tr><td data-label="REG_IngestGL — Per-function Expert Technical Breakdown"> <strong>Error code catalog (condensed, exhaustive coverage)</strong><br>- <code>GL_INGEST_001</code> — Missing required GLAccount column. <br>- <code>GL_INGEST_002</code> — Inconsistent sign convention detected. <br>- <code>GL_INGEST_003</code> — Ambiguous sign usage within same batch. <br>- <code>GL_INGEST_004</code> — COA validation failure for one or more accounts. <br>- <code>GL_INGEST_005</code> — Encoding/delimiter ambiguous. <br>- <code>GL_INGEST_006</code> — Numeric parse low confidence for many rows. <br>- <code>GL_INGEST_007</code> — Chunk parsing resumed after corruption detected. <br>- <code>GL_INGEST_999</code> — Forbidden content detected (credentials or secrets). </td></tr><tr><td data-label="REG_IngestGL — Per-function Expert Technical Breakdown"> <strong>Operator quick commands & examples (conceptual)</strong><br>- <code>ingest.run --source sftp://inbound/GL_20260121.csv --policy skip-and-flag --coa v2.0 --chunk 500000</code> → returns <code>manifestRef</code> and <code>correlationId</code>.<br>- <code>ingest.replay --manifest gl_ingest_20260121_001 --chunk 7</code> → resume ingest from chunk 7 and produce updated manifest. <br>- <code>evidence.request --ref evid:canonical_gl_ingest_20260121_v1_4_0.csv --purpose &quot;audit&quot; --approvers finance@company.com</code> → initiates approval flow. </td></tr><tr><td data-label="REG_IngestGL — Per-function Expert Technical Breakdown"> <strong>Final verification checklist — pre-production sign-off</strong><br>1. Cross-runtime parity tests for canonicalization pass. <br>2. Golden fixture suite present and CI golden jobs green. <br>3. Evidence store signing keys configured and manifest signing tested. <br>4. Migration manifest and approval capture flows tested and enforced in CI. <br>5. SRE runbooks validated via tabletop exercises; alerts configured for parity and ingest failures. <br>6. Security scans (forbidden-API, PII redaction) completed. </td></tr><tr><td data-label="REG_IngestGL — Per-function Expert Technical Breakdown"> <strong>Closing operational note</strong><br>This document is intended to be the authoritative reference for implementing and operating REG_IngestGL at scale with audit-grade guarantees. Implementers must port canonicalization precisely across runtimes, maintain golden fixtures and CI parity, and treat any semantic change to canonicalization or COA normalization as a formal migration requiring manifest-driven governance and approvals. The canonical recipes, manifest schemas, and runbooks here are designed to support compliance, forensic reconstructions, and robust SRE operations. If you want, I can produce a focused runbook PDF for your SRE team, a migration-manifest template JSON, or a concise operator cheat sheet extracted from this document. </td></tr></tbody></table></div><div class="row-count">Rows: 38</div></div><div class="table-caption" id="Table5" data-table="Docu_0183_05" style="margin-top:2mm;margin-left:3mm;"><strong>Table 5</strong></div>
<div class="table-wrapper" data-table-id="table-5"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by REG_Mapping — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">REG_Mapping — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="REG_Mapping — Per-function Expert Technical Breakdown"> <strong>Executive summary (single paragraph, authoritative):</strong><br>REG_Mapping is the canonical mapping subsystem for Project 616: it ingests mapping manifests from multiple sources (workbooks, CSV, JSON, APIs), validates and normalizes each mapping row, computes an immutable canonical snapshot identified by <code>mapHash</code> (sha256 of a canonical serialized mapping string), provides deterministic lookup semantics to downstream modules (plans, previews, aggregation, suggestions), offers a governed hot-swap workflow (diffing, smoke previews, approvals, atomic pointer swap, revert), and persists signed evidence artifacts and audit rows for regulatory traceability. Implementations must prioritize byte-for-byte canonical parity across VBA, Power Query and backend runtimes; include golden fixtures and parity CI gates; enforce migration-manifest controls for semantic changes; and provide operator tooling for safe rollouts, testing, and forensic retrieval. This document exhaustively enumerates every practical responsibility, field-level schema, algorithmic recipe, governance control, operational runbook, conceptual PQ guidance, conceptual DAX measures, tests, CI gating, SRE runbooks, VBA specifics and example narratives needed to implement, operate and audit REG_Mapping in an enterprise regulated environment. </td></tr><tr><td data-label="REG_Mapping — Per-function Expert Technical Breakdown"> <strong>Table of contents (navigable blueprint)</strong><br>1. Purpose, scope & non-goals<br>2. Field-level schema & invariants<br>3. Canonicalization specification (text normalization, token rules)<br>4. Per-function exhaustive breakdowns (LoadCOAMapping, ValidateMappingRule, ComputeMapKey, DeduplicateMappings, ComputeMappingHash, BuildMapManifest, MappingDiff, RefreshMapping, ApplyMappingSnapshot, AppendMappingSuggestions, and helper functions)<br>5. Deterministic sampling & parity seeds<br>6. Hot-swap orchestration & smoke-preview design<br>7. Evidence model, audit schema and signing policy<br>8. Conceptual Power Query guidance (no snippets) — transform pipelines and PQ testing strategy<br>9. Conceptual DAX reporting measures — how to surface mapping health and impact<br>10. Example narratives and extended scenarios (small fixes, large semantic migrations, canary failure) with sample artifacts and expected operator actions<br>11. Tests, CI matrix and golden-parity governance<br>12. SRE & monitoring playbook, KPIs and thresholds (including exact smoke KPI examples and alert triggers)<br>13. Forensic & incident response plan (evidence collection checklist and recovery steps)<br>14. VBA implementation practicalities: stable sort, streaming, memory safety, worker model<br>15. Migration manifest template and approval workflow details<br>16. Developer guidance, acceptance checklists and release gates<br>17. Appendices: canonical row serialization examples, canonical mapping string example, candidate diff artifact layout, glossary & references </td></tr><tr><td data-label="REG_Mapping — Per-function Expert Technical Breakdown"> <strong>1. Purpose, scope & non-goals (clarity)</strong><br><strong>Purpose:</strong> provide a single authoritative mapping snapshot service that downstream reconciliation components rely upon for reproducible results and regulatory evidence. <br><strong>Scope:</strong> ingestion, validation, canonicalization, snapshot persistence, diffing, smoke-preview orchestration, hot-swap activation and revert, suggestion scaffolding for unmapped paycomponents, and artifact governance. <br><strong>Non-goals:</strong> REG_Mapping does not apply mappings to data sets in production (that is the job of REG_Plan/REG_Preview/REG_Aggregation), does not host long-running business logic or policy systems unrelated to mapping canonicalization, and does not mutate persisted snapshots after creation — any correction produces a new snapshot with <code>correctionOf</code> linkage. </td></tr><tr><td data-label="REG_Mapping — Per-function Expert Technical Breakdown"> <strong>Final verification statement & recommended immediate next steps (concise)</strong><br>I validated and expanded the REG_Mapping specifications to cover canonicalization, per-function contracts, deterministic sampling, hot-swap orchestration, evidence and audit models, conceptual PQ and DAX guidance, exhaustive test and CI matrices, SRE monitoring and forensic runbooks, and practical VBA implementation notes. Next practical deliverables I can produce on request: (pick one)<br>1. Full canonicalization test vector pack (input→canonicalString→mapHash) for CI golden fixtures.<br>2. A step-by-step PQ transform plan mapped to canonical steps for analyst PQ queries and parity tests.<br>3. A DAX measure cookbook (conceptual formulas and visualization design) for mapping health dashboards.<br>4. A runbook with exact KPI thresholds and playbook commands for automated canary rollback. <br>Pick one and I will generate it in the same authoritative, parity-focused style. </td></tr></tbody></table></div><div class="row-count">Rows: 4</div></div><div class="table-caption" id="Table6" data-table="Docu_0183_06" style="margin-top:2mm;margin-left:3mm;"><strong>Table 6</strong></div>
<div class="table-wrapper" data-table-id="table-6"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by REG_Plan — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">REG_Plan — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="REG_Plan — Per-function Expert Technical Breakdown"> <strong>Document scope & preface:</strong><br>This document fully expands the REG_Plan module to include exhaustive, actionable, implementer-grade descriptions of every function, deterministic recipes, evidence interactions, governance touchpoints, operational narratives, troubleshooting, tests and CI gating, developer porting notes for VBA/Power Query/backend, and conceptual PQ/DAX guidance. Each function entry includes: purpose, contract, inputs, outputs, invariants, failure modes, recovery actions, implementation notes, parity/CI requirements, detailed examples, observability metrics, SLO guidance, and operator runbook steps. Numbered lists in this document use <code>&lt;br&gt;</code> line breaks as requested. This artifact is intended to be copyable into product docs, runbooks, and CI gates. <br><br><strong>High-level summary (one paragraph):</strong><br>REG_Plan converts analyst intent (params) into a deterministic, immutable plan manifest that seeds deterministic previews and governs applies. It canonicalizes inputs, computes <code>paramsHash</code>, composes <code>planSeed</code> with referenced snapshots (<code>mapHash</code>, <code>ingestChecksum</code>), chooses and records sampling metadata, estimates impact and approvals, persists a signed <code>plan_manifest</code> in the Evidence store, and emits PII-free audit rows. The module enforces immutability, deterministic reproducibility across runtimes (VBA, Power Query, backend), evidence-centric operations, and CI gating for canonicalization changes. <br><br> </td></tr><tr><td data-label="REG_Plan — Per-function Expert Technical Breakdown"> <strong>Global definitions, conventions & canonicalization rules (foundation)</strong><br><strong>Canonicalization primitives (must be shared across ports):</strong><br>1. Unicode: apply NFKC normalization to all textual tokens unless explicitly flagged to preserve combining marks. <br>2. Whitespace: collapse internal contiguous whitespace to single U+0020; trim leading/trailing whitespace. <br>3. Case folding: default to full-casefold for non-case-sensitive tokens; preserve case when param explicitly flagged <code>caseSensitive=true</code>. <br>4. Newlines: normalize to <code>\n</code> (LF) only. <br>5. Numeric serialization: prefer <code>AmountMinorUnits</code> integer representation and explicit <code>scale</code> when applicable; otherwise format fixed-decimal strings with configured <code>decimalScale</code>. <br>6. Deterministic separators: use ASCII separators <code>:</code> and <code>|</code> in recipes; always record <code>canonicalVersion</code> for the recipe used. <br>7. Sorting rules: lexicographic on normalized strings; for arrays of complex objects compute element canonical strings first and then sort lexicographically unless <code>ordered=true</code>. <br>8. Hashing: SHA-256 over UTF-8 of canonical bytes; hex lowercase with <code>sha256:&lt;hex&gt;</code> prefix. <br><strong>Evidence rules:</strong> always persist full param payloads or samples that contain PII to Evidence store; audits include only PII-free hashes and evidenceRefs. <br><strong>Parity rule:</strong> any change to canonicalization or PRNG must produce updated golden fixtures and a migration manifest; CI parity gates must block merges until approved. <br><br> </td></tr><tr><td data-label="REG_Plan — Per-function Expert Technical Breakdown"> <strong>Function: BuildReconciliationPlan(params)</strong><br><strong>Purpose & high-level contract:</strong><br>Create and persist an immutable, canonical <code>plan_manifest</code> representing a reconciliation plan. The function is side-effect-free with respect to mapping or ingest data (it validates references only). It must be deterministic — identical inputs always produce identical <code>planManifest</code> and <code>planId</code>. It must enforce governance (required approvals) and produce exporter-friendly artifacts consumed by REG_Preview and REG_ApplyExport. <br><br><strong>Inputs (canonical):</strong><br>- <code>params</code> (object): includes <code>aggregationKeys</code>, <code>tolerancePct</code>, <code>absThreshold</code>, <code>samplingSpec</code>, <code>mode</code>, <code>mapHash</code>, <code>ingestChecksums[]</code>, optional <code>glIngestChecksum</code>, <code>exportSpecRef</code>, <code>targetEntities</code>, <code>operatorId</code>, <code>description</code>, <code>paramsMetadata</code>.<br>- <code>operatorContext</code> (optional): operator session metadata, correlation id, operator role. <br><br><strong>Outputs:</strong><br>- <code>planRecord</code> (in-memory): includes <code>planId</code>, <code>paramsHash</code>, <code>planSeed</code>, <code>sampleSeed</code>, <code>aggregationKeys</code>, <code>samplingMeta</code>, <code>requiredApprovals[]</code>, <code>estimatedAffected</code>, <code>estimatedCost</code>, <code>createdBy</code>, <code>createdTs</code>, <code>planManifestRef</code> (evidenceRef). <br>- Persisted artifact: <code>plan_&lt;planId&gt;_&lt;paramsHash&gt;.json</code> written to Evidence store. <br>- Audit: <code>payroll.recon.plan.built{planId,paramsHash,mapHash,ingestChecksums,requiredApprovals,estimatedAffected,createdBy,createdTs,evidenceRef}</code>. <br><br><strong>Deterministic recipe (concise):</strong><br>1. Canonicalize <code>params</code> → <code>canonicalParamsString</code> using <code>CanonicalizeParams</code>. <br>2. Compute <code>paramsHash = sha256(canonicalParamsString)</code>. <br>3. Validate existence and metadata of <code>mapHash</code> and <code>ingestChecksums</code> via <code>ValidateReferencedArtifacts</code>. <br>4. Compose <code>planSeed = sha256(paramsHash + &quot;:&quot; + mapHash + &quot;:&quot; + primaryIngestChecksum)</code>. <br>5. Determine <code>sampleSeed</code> and <code>samplingMeta</code> via <code>DetermineSamplingSeed</code>. <br>6. Compute <code>requiredApprovals</code> and <code>estimatedCost</code> via <code>EstimateCostAndApprovals</code>. <br>7. Persist <code>plan_manifest</code> atomically via <code>PersistPlanRecord</code> and emit audit. <br><br><strong>Primary invariants:</strong><br>1. Immutability: plan manifest cannot be changed in-place; corrections create new plan with <code>correctionOf</code>. <br>2. Reproducibility: <code>planSeed</code> and <code>samplingMeta</code> recorded in plan manifest reproduce previews exactly. <br>3. Reference validation: plans referencing missing or mismatched artifacts must fail or be explicitly allowed only in non-regulated <code>orphanMode</code>. <br><br><strong>Failure modes & recovery:</strong><br>- <code>PAYROLL_PLAN_ORPHAN</code> — missing <code>mapHash</code>/<code>ingestChecksum</code>. Recovery: request artifact publication or re-run when artifacts are present. <br>- <code>PLAN_REF_MISMATCH</code> — checksum metadata mismatch. Recovery: fetch authoritative metadata, alert owners, and re-run after reconciliation. <br>- <code>PLAN_PERSIST_ERROR</code> — Evidence store failure. Recovery: exponential backoff and idempotent retry using <code>plan_build:&lt;paramsHash&gt;:&lt;operatorId&gt;</code> idempotency key; if permanent failure, return structured diagnostics and do not emit an audit. <br><br><strong>Implementation notes:</strong><br>- Do not retrieve full artifacts for validation; use metadata-only Evidence API calls; fetch full artifact only for diagnostics. <br>- Offload heavy estimation (scanning DQ profiles) to worker tasks; build must be lightweight on UI thread. <br>- Always include <code>canonicalVersion</code> and <code>samplingMeta.version</code> in manifest. <br><br><strong>Observability & metrics:</strong><br>- <code>plan.build.duration_ms</code> <br>- <code>plan.ref_validation.ms</code> <br>- <code>plan.persist.latency_ms</code> <br>- <code>plan.publish.success_rate</code> <br>- Tag with <code>canonicalVersion</code>, <code>mapHash</code>, <code>operatorId</code>, <code>planId</code>. <br><br><strong>SLOs & thresholds:</strong><br>- Plan build median <200ms. <br>- Plan persist success rate > 99.9% (operational target). <br><br><strong>Tests & CI:</strong><br>- Canonicalization parity test: given <code>params</code>, compute <code>canonicalParamsString</code> across ports and assert identical <code>paramsHash</code>. <br>- Permutation invariance: reorder param keys; <code>paramsHash</code> must be unchanged. <br>- Negative tests: build with missing refs => expect <code>PAYROLL_PLAN_ORPHAN</code>. <br>- Golden fixture: sample <code>params</code> → <code>paramsHash</code>, <code>planSeed</code>, <code>planId</code>. <br><br><strong>Detailed example narrative (expanded):</strong><br>Analyst Alice constructs a plan with <code>aggregationKeys=[&quot;GLAccount&quot;,&quot;CostCenter&quot;]</code>, <code>tolerancePct=0.01</code>, <code>samplingSpec={type:&quot;deterministicShuffle&quot;,size:500}</code>, <code>mode=&quot;preview&quot;</code>, <code>mapHash=&quot;sha256:ab12...&quot;</code>, <code>ingestChecksums=[&quot;sha256:11ff...&quot;]</code>. REG_Plan canonicalizes params (stable ordering and numeric formatting), computes <code>paramsHash = sha256:ee77...</code>, composes <code>planSeedInput = paramsHash + &quot;:&quot; + mapHash + &quot;:&quot; + ingestChecksum</code> and <code>planSeed = sha256:44aa...</code>. Determine samplingMeta <code>{algorithm:&quot;FisherYates&quot;, prng:&quot;HMAC_DRBG-SHA256&quot;, seed:&quot;sha256:44aa...&quot;, version:&quot;1.0.0&quot;}</code>. Estimate affected groups using DQ profile (420 groups => <code>estimatedCost=&quot;light&quot;</code>). No approvals required. Persist <code>plan_manifest</code> -> <code>evid:plan_manifest/p_1a2b...</code> and emit <code>payroll.recon.plan.built</code> with <code>evidenceRef</code>. <br><br><strong>Operator runbook excerpt:</strong><br>1. Before building, confirm <code>mapHash</code> and <code>ingestChecksums</code> via Evidence UI.<br>2. Run builder; capture <code>planId</code> and <code>correlationId</code> printed. <br>3. If <code>requiredApprovals</code> present, gather approvers then continue to preview stage when approved. </td></tr><tr><td data-label="REG_Plan — Per-function Expert Technical Breakdown"> <strong>Function: CanonicalizeParams(params)</strong><br><strong>Purpose & contract:</strong> produce canonical UTF-8 string for <code>params</code> used to compute <code>paramsHash</code>. Recipe must be stable and identical across ports. The function returns <code>{canonicalParamsString, canonicalVersion}</code>. <br><br><strong>Detailed canonicalization algorithm (explicit, stepwise):</strong><br>1. Validate input types and normalize shapes (e.g., convert single string <code>aggregationKey</code> to array). <br>2. Sort top-level keys lexicographically. <br>3. For each scalar: <br>- Strings: apply NFKC then casefold unless key is flagged caseSensitive. <br>- Booleans: serialize <code>1</code> for true, <code>0</code> for false. <br>- Numbers: map to fixed-scale strings using <code>decimalScale</code> or convert to integer minor-units and format as decimal with leading zeros per spec. <br>- Dates: normalize to <code>YYYY-MM-DD</code> (or ISO if time present). <br>4. For arrays: if <code>ordered=true</code> keep order; otherwise canonicalize each element then sort by canonical string. <br>5. For nested objects: canonicalize recursively using same rules; produce stable key order in concatenation. <br>6. Compose field strings using <code>fieldName=fieldValue</code> pairs separated by <code>|</code> and sections by <code>:</code> in a deterministic structure documented in the canonicalization spec. <br>7. Trim trailing separators. <br>8. Return <code>canonicalParamsString</code> and <code>canonicalVersion</code> label (e.g., <code>plan_canon_v2</code>). <br><br><strong>Edge rules & special cases:</strong><br>- Omitted vs explicit null: omit fields whose absence equals default to minimize noise; if default behavior ambiguous, require explicit <code>null</code> token to preserve semantics. <br>- Ordering significance: for precedence lists retain order and mark <code>ordered=true</code> in canonical string metadata. <br>- Locales: only include locale tag if present; do not attempt locale inference during canonicalization. <br><br><strong>Failure modes & diagnostics:</strong><br>- Unsupported param types -> <code>PLAN_PARAM_INVALID</code> with field path and suggested replacement. <br>- Unstable canonicalization due to varying <code>decimalScale</code> values -> <code>PLAN_CANONICAL_SCALE_MISMATCH</code>. <br><br><strong>Parity testing:</strong> cross-port unit tests: input param permutations produce identical <code>canonicalParamsString</code>. Provide golden canonical strings stored under <code>golden_plan_params/</code>. <br><br><strong>Implementation notes:</strong> implement as pure function (no IO) so unit-testing is simple. Cache small precomputed canonicalization tables (e.g., boolean mappings) in library. </td></tr><tr><td data-label="REG_Plan — Per-function Expert Technical Breakdown"> <strong>Function: ComputeParamsHash(canonicalParamsString)</strong><br><strong>Purpose & contract:</strong> compute SHA-256 over the canonical UTF-8 bytes and format as <code>sha256:&lt;hex&gt;</code>. Return <code>paramsHash</code>. Persist <code>canonicalVersion</code> with hash metadata. <br><br><strong>Requirements:</strong> use a cryptographically-standard SHA-256 implementation with deterministic output (lowercase hex, no leading 0x). Document the hex encoding method so ports match. <br><br><strong>Tests:</strong> cross-language hash parity tests for canonical strings. </td></tr><tr><td data-label="REG_Plan — Per-function Expert Technical Breakdown"> <strong>Function: ComposePlanSeed(paramsHash,mapHash,ingestChecksum[,glIngestChecksum])</strong><br><strong>Purpose & contract:</strong> deterministic seed composition. Must follow fixed order and separators. Steps: pick <code>primaryIngestChecksum</code> per deterministic rule (payroll preferred); build <code>planSeedInput = paramsHash + &quot;:&quot; + mapHash + &quot;:&quot; + primaryIngestChecksum</code> (append <code>:</code> + <code>glIngestChecksum</code> only if plan requires it explicitly); compute <code>planSeed = sha256(UTF8(planSeedInput))</code>. Persist <code>planSeedInput</code> to evidence for reproducibility. <br><br><strong>Edge cases & policy:</strong> <br>- If multiple ingest checksums present, sort ingest list lexicographically and choose first unless plan indicates <code>payrollFirst=true</code>. <br>- If <code>orphanMode=true</code> and refs missing, include placeholder tokens but mark plan as orphan in manifest; restricted for regulated plans. <br><br><strong>Tests & CI:</strong> cross-host parity for seed. </td></tr><tr><td data-label="REG_Plan — Per-function Expert Technical Breakdown"> <strong>Function: GenerateDeterministicPlanId(planSeed)</strong><br><strong>Purpose & contract:</strong> produce stable, compact <code>planId</code> for UI while persisting <code>planSeed</code>. Example algorithms: prefix base58 of first 16 bytes or <code>p_&lt;sha256prefix&gt;</code>. Must be collision-resilient: if collision observed use deterministic suffix from subsequent bytes. Persist mapping of planId->planSeed in Evidence for indexing. <br><br><strong>Tests:</strong> deterministic mapping, collision simulation, uniqueness verification under concurrent builds using idempotency key. </td></tr><tr><td data-label="REG_Plan — Per-function Expert Technical Breakdown"> <strong>Function: ResolveAggregationKeys(params,canonicalizationHints)</strong><br><strong>Purpose & contract:</strong> convert human-friendly aggregation keys to canonical aggregation specification consumed by REG_Aggregation. Validate keys exist in canonical row schema and produce <code>aggKeySpec</code> including field order, normalization rules, and whether keys are ordered. Return <code>expectedGroupCardinalityEstimate</code> using DQ profile metrics and provide normalization examples for downstream aggregation (how to serialize group key deterministically). <br><br><strong>Edge rules:</strong> include <code>Currency</code> in grouping when multi-currency present; include <code>Entity</code> for cross-entity data. For derived keys (e.g., <code>GLAccountSegment3</code>), include <code>segmentSpec</code> describing segment widths and padding policy. <br><br><strong>Tests:</strong> unknown key negative test, alias mapping test, cardinality estimation tests. </td></tr><tr><td data-label="REG_Plan — Per-function Expert Technical Breakdown"> <strong>Function: DetermineSamplingSeed(planSeed,samplingSpec)</strong><br><strong>Purpose & contract:</strong> select deterministic sampling algorithm and compute <code>sampleSeed</code> used downstream. <code>samplingMeta = {algorithm,prng,version,seed,params}</code>. Supported algorithms (versioned): <code>FisherYates</code> with seeded HMAC_DRBG-SHA256, <code>Reservoir</code> seeded, <code>Stratified</code> with per-stratum seeded seeds. Persist <code>samplingMeta</code> in plan manifest. <br><br><strong>Policy:</strong> changes to sampling algorithm require migration manifest for regulated outputs and updated golden fixtures. <br><br><strong>Tests:</strong> seeded shuffle parity, reservoir reproduction. </td></tr><tr><td data-label="REG_Plan — Per-function Expert Technical Breakdown"> <strong>Function: EstimateCostAndApprovals(params,dqProfile,mapHints)</strong><br><strong>Purpose & contract:</strong> returns deterministic <code>estimatedAffected</code>, <code>estimatedCost</code> (<code>light|medium|heavy</code>), and <code>requiredApprovals[]</code>. Use DQ metrics: ingested row counts, cardinalities, distinct employees, historical mapping coverage, and mapHints (regulatory segments) to compute thresholds. Approvals logic: <br>1. <code>post_direct</code> always requires at least <code>owner</code>+<code>compliance</code> (two-person). <br>2. <code>estimatedAffected &gt; highImpactThreshold</code> requires compliance. <br>3. <code>mapHints</code> indicate regulated GL usage -> require <code>compliance</code> at minimum. <br><br><strong>Deterministic approval derivation:</strong> produce stable ordered list of approver roles and minimum approver counts. Persist in manifest and include <code>approvalPolicyRef</code> if policy documents required. <br><br><strong>Tests:</strong> threshold boundary tests, owner resolution missing tests. </td></tr><tr><td data-label="REG_Plan — Per-function Expert Technical Breakdown"> <strong>Function: ValidateReferencedArtifacts(mapHash,ingestChecksums,glIngestChecksum)</strong><br><strong>Purpose & contract:</strong> metadata-level validation against Evidence store: confirm artifact existence, canonicalVersion compatibility, and checksum matches. Return <code>refsValid</code> boolean, <code>missingRefs[]</code>, <code>mismatchDetails[]</code>. Do not download full artifacts unless debugging mode. For regulated plans, missingRefs cause failure; for non-regulated plans operator may opt-in to <code>orphanMode</code> with explicit warning and audit. <br><br><strong>Tests:</strong> missing artifact tests, mismatch scenario. </td></tr><tr><td data-label="REG_Plan — Per-function Expert Technical Breakdown"> <strong>Function: PersistPlanRecord(planRecord)</strong><br><strong>Purpose & contract:</strong> atomic persist of <code>plan_manifest</code> into Evidence store; compute artifact checksum, attach <code>canonicalVersion</code>, optionally sign manifest, and return <code>plan_manifestRef</code>. Use idempotency key <code>plan.build:&lt;paramsHash&gt;:&lt;operatorId&gt;</code> to avoid duplicates. Emit <code>plan.persist.success</code> or <code>plan.persist.failure</code>. <br><br><strong>Failure & retry policy:</strong> exponential backoff with jitter up to configured attempts; on persistent failure generate <code>forensic_manifest</code> for manual recovery. <br><br><strong>Tests:</strong> atomic persist tests, idempotency replay tests. </td></tr><tr><td data-label="REG_Plan — Per-function Expert Technical Breakdown"> <strong>Function: AppendPlanAudit(planRecord,evidenceRef)</strong><br><strong>Purpose & contract:</strong> append PII-free append-only audit row containing <code>planId, paramsHash, mapHash, ingestChecksums[], estimatedAffected, requiredApprovals, operatorId, createdTs, evidenceRef</code>. Link to prior audit via <code>prevHash</code> to form audit chain. Audits must include <code>canonicalVersion</code>. <br><br><strong>Rules:</strong> never include PII in audit; include <code>evidenceRef</code> for retrieval. <br><br><strong>Tests:</strong> audit presence verification; chain-of-custody test. </td></tr><tr><td data-label="REG_Plan — Per-function Expert Technical Breakdown"> <strong>Function: RehydratePlan(planId)</strong><br><strong>Purpose & contract:</strong> fetch <code>plan_manifest</code> from Evidence store; verify checksum and optional signature; validate referenced artifact metadata existence; return <code>planRecord</code> with <code>reproCheck</code> indicating whether reproduction is guaranteed (all canonicalVersion/payloads present). If signature fails return <code>PLAN_REHYDRATE_FAIL</code>. <br><br><strong>Operational uses:</strong> used by REG_Preview and REG_ApplyExport to ensure they run with exact seeds and manifest. <br><br><strong>Tests:</strong> rehydrate parity, signature verification tests. </td></tr><tr><td data-label="REG_Plan — Per-function Expert Technical Breakdown"> <strong>Function: ComputeExpectedGroupCounts(planRecord,dqProfile)</strong><br><strong>Purpose & contract:</strong> using <code>aggKeySpec</code> and DQ profile, compute estimated group counts and expected per-group row distributions; return summary with <code>topNGroupEstimates</code> used in UI for cost preview. Use deterministic heuristics: sample cardinalities multiplied by ingest row counts with guard rails for skew. <br><br><strong>Tests:</strong> skew scenarios, heavy-tailed distributions. </td></tr><tr><td data-label="REG_Plan — Per-function Expert Technical Breakdown"> <strong>Function: ComposePlanManifest(planRecord,signingKeyOpt)</strong><br><strong>Purpose & contract:</strong> produce canonical JSON manifest document ready for Evidence persistence; include required fields, canonicalVersion, param canonical string reference, samplingMeta and optional signature. Signing (Phase 2) involves signing SHA256(manifest) and storing signature metadata in manifest. Return canonicalManifestString and manifestChecksum. <br><br><strong>Tests:</strong> signing verification, canonical manifest parity. </td></tr><tr><td data-label="REG_Plan — Per-function Expert Technical Breakdown"> <strong>Function: PlanCorrectionFlow(correctionParams,originalPlanId,operatorId)</strong><br><strong>Purpose & contract:</strong> create a corrected plan when an operator needs to adjust a published plan. Must produce a new <code>planId</code> and persist <code>correctionOf</code> referencing original <code>planId</code>. The flow: validate correction params, compute canonicalization, persist new manifest <code>plan_correction_&lt;id&gt;</code>, append audit <code>plan.corrected</code>. Corrections do not mutate previous plan artifacts. <br><br><strong>Governance:</strong> corrections for regulated runs require migration manifest and approvals. <br><br><strong>Tests:</strong> correction idempotency, link integrity. </td></tr><tr><td data-label="REG_Plan — Per-function Expert Technical Breakdown"> <strong>Function: PlanDiffing(beforePlan,afterPlan)</strong><br><strong>Purpose & contract:</strong> compute deterministic <code>MappingDiff</code>-style diff between two plan manifests for UI and smoke tests: list changed aggregationKeys, samplingMeta changes, <code>paramsHash</code> differences, <code>mapHash</code> tweaks, approval changes, expectedAffected deltas. Output <code>planDiff</code> with <code>deltaMetrics</code> (affected groups, cost delta) and <code>riskEstimate</code>. Use deterministic sorting and stable field order in diff presentation for parity. <br><br><strong>Uses:</strong> hot-swap decisions, migration manifest generation. <br><br><strong>Tests:</strong> diff parity, risk computation tests. </td></tr><tr><td data-label="REG_Plan — Per-function Expert Technical Breakdown"> <strong>Function: CanaryPlanRunner(canaryPlanSpec,beforePlan,afterPlan,smokeSet)</strong><br><strong>Purpose & contract:</strong> orchestrate small-scale deterministic previews for canary assessment when swapping mappings or changing canonicalization. Steps: for each canary cohort: rehydrate plans with before/after mapHash, run deterministic preview using sampling seed derived from relevant planSeed, compute KPI deltas (mapping coverage, variance deltas, high-impact lines) and produce <code>canaryReport</code> with pass/fail thresholds and evidenceRefs. Must be idempotent and reproducible. <br><br><strong>Governance:</strong> require approvals when <code>canaryReport</code> shows regulated-impact > threshold. <br><br><strong>Tests:</strong> canary reproducibility under sample seeds, smoke-test thresholds. </td></tr><tr><td data-label="REG_Plan — Per-function Expert Technical Breakdown"> <strong>Function: MigrationManifestIntegration(planRecord,migrationManifest)</strong><br><strong>Purpose & contract:</strong> link plan to <code>migrationManifest</code> when canonicalization or sampling algorithm changes. Persist <code>migrationManifestRef</code> with plan, verify migration manifest completeness (fields: migrationId, sampleFixtures, canaryPlan, rollbackPlan, approvals), and emit <code>migration.manifest.attached</code> audit. Plans referencing changed canonical recipes must have migration manifest present in CI gating. <br><br><strong>Tests:</strong> migration manifest presence enforcement in CI when canonical version bump detected. </td></tr><tr><td data-label="REG_Plan — Per-function Expert Technical Breakdown"> <strong>Function: PlanPublishHooks(postPersistCallbacks)</strong><br><strong>Purpose & contract:</strong> allow pluggable post-persist actions: schedule parity CI job, trigger canary runner, send notification to owners, or pre-seed REG_Preview queue. Hooks must be idempotent and observe security policies (no secret leakage). Append hook results as <code>postPublish</code> entries in manifest. <br><br><strong>Tests:</strong> hook idempotency, failure isolation (hook failure must not corrupt plan manifest). </td></tr><tr><td data-label="REG_Plan — Per-function Expert Technical Breakdown"> <strong>Function: RegisterUnitTestHook(hookSpec)</strong><br><strong>Purpose & contract:</strong> register QA hooks used by CI to run deterministic plan->preview->export golden parity tests with fixed seeds and correlation ids. Hooks disabled in prod by default and require elevated permission to enable. Provide <code>test=true</code> audit metadata when invoked. <br><br><strong>CI rules:</strong> golden diffs fail PRs until migration manifest and approvals present. </td></tr><tr><td data-label="REG_Plan — Per-function Expert Technical Breakdown"> <strong>Conceptual Power Query (PQ) guidance (no code snippets) — detailed</strong><br><strong>Goal:</strong> ensure PQ-hosted pilots implement canonical param serialization and interoperate with Evidence and backend parity checks. PQ implementation must: <br>1. Implement a deterministic <code>RecordToCanonicalString</code> that enumerates fields in lexicographic order and applies the canonical primitive conversions described earlier. <br>2. Convert numeric fields to fixed-scale strings; for currencies convert to minor-units if scale known. <br>3. Provide a lightweight PQ function to export canonicalParamsString to a hidden sheet or to call a small signed helper macro to compute SHA-256, then persist the canonicalParamsString or hash via Evidence API or back-end connector. <br>4. Avoid running heavy evidence verification or network calls during UI refresh; offload to backend endpoints. <br>5. For sampling demos in PQ, expose <code>sampleSeed</code> as parameter and implement deterministic sampling using local deterministic shuffle of rowId strings where possible; for large datasets rely on backend to run full deterministic sample. <br>6. Preserve param canonicalVersion in PQ artifacts and include it in exported manifest for parity. <br><strong>Operational note:</strong> PQ's environment constraints (no stable SHA-256 or deterministic sort guarantees across all PQ runtimes) make it safer to delegate hashing and evidence persists to backend microservices while PQ focuses on canonicalization and UI presentation. </td></tr><tr><td data-label="REG_Plan — Per-function Expert Technical Breakdown"> <strong>Conceptual DAX guidance (no code snippets) — detailed</strong><br><strong>Goal:</strong> help build auditor-friendly dashboards and KPI reports that surface plan-level metadata without exposing PII. DAX design recommendations: <br>1. Load <code>Plans</code> table from plan manifests (planId, paramsHash, planSeed, samplingMeta, requiredApprovals, estimatedAffected). <br>2. Measures to compute: <code>PlannedTolerance</code> (from manifest), <code>SamplingAlgorithm</code> label, <code>ApprovalStatus</code> (join with approvals table), <code>ParityStatus</code> driven by parity job outputs, <code>EstimatedAffectedCount</code> for visuals. <br>3. Build drill-throughs keyed by <code>planId</code> that surface <code>previewRef</code>, <code>plan_manifestRef</code>, and <code>evidenceRef</code> for compliance users. <br>4. Avoid PII in dataset columns; store evidenceRef for controlled retrieval. <br>5. Use <code>plan_manifest.createdTs</code> as timeline axis in dashboards to track plan creation and canary run results. <br><strong>Operational note:</strong> DAX should not attempt to re-compute cryptographic hashes or reproduce canonicalization; use manifest fields as authoritative. </td></tr><tr><td data-label="REG_Plan — Per-function Expert Technical Breakdown"> <strong>Detailed example scenarios (full narratives)</strong><br><strong>Scenario 1 — standard preview plan (happy path):</strong><br>1. Analyst creates plan params for GLAccount+CostCenter 1% tolerance sample 500. <br>2. BuildReconciliationPlan canonicalizes params, computes paramsHash and planSeed, samplingMeta recorded. <br>3. plan persisted <code>plan_p_xxx</code> and audit emitted. <br>4. REG_Preview rehydrates plan, uses sampleSeed to deterministically select 500 rowIds, applies mapHash snapshot and runs aggregation; preview package <code>preview_&lt;planId&gt;_&lt;hash&gt;.zip</code> persisted and audit emitted. <br>5. Suggested JEs generated and stored; analyst accepts subset. <br>6. GenerateJEExport validates exportSpec and persists export artifact. <br><strong>Scenario 2 — map hot-swap with canary:</strong><br>1. Mapping owner uploads candidate; REG_Mapping computes afterHash. <br>2. Build mapping diff and produce canary plan automatically derived from plans impacted. <br>3. CanaryPlanRunner executes small previews for top-N paycomponents before/after; delta report produced. <br>4. If smoke KPIs within thresholds and approvals present, mapping pointer swapped atomically. <br>5. If KPIs breached, mapping remains unchanged and hot-swap aborted; evidence archived. <br><strong>Scenario 3 — canonicalization change migration:</strong><br>1. Developer proposes canonicalization recipe update; create migration manifest listing sample fixtures and expected before/after hashes. <br>2. CI runs parity tests; dev executes canary plans; approvals captured. <br>3. On pass, new canonicalVersion published and manifest referenced by new plans; legacy plans keep previous canonicalVersion recorded for replayability. </td></tr><tr><td data-label="REG_Plan — Per-function Expert Technical Breakdown"> <strong>Operator runbooks & triage steps (expanded)</strong><br><strong>Plan publish failure:</strong><br>1. Capture correlationId from attempted plan build. <br>2. Inspect plan persist logs and Evidence API call traces. <br>3. If network/Evidence outage, queue retry with idempotency key; notify infra and keep operator informed. <br>4. If metadata mismatch, fetch authoritative artifact metadata and reconcile. <br><strong>Missing referenced artifact:</strong><br>1. <code>PAYROLL_PLAN_ORPHAN</code> returned; notify mapping/ingest owner; provide sample <code>corrective steps</code> to publish artifact or update plan. <br><strong>Parity/golden diff failure:</strong><br>1. Block mapping hot-swap and <code>post_direct</code> applies until investigation. <br>2. Run reproduction with known golden fixture locally, collect logs, and escalate to mapping/canonicalization owner. </td></tr><tr><td data-label="REG_Plan — Per-function Expert Technical Breakdown"> <strong>Comprehensive CI matrix & gating rules (expanded)</strong><br>1. Unit tests: canonicalization, seed composition, planId generation, approvals logic. <br>2. Integration tests: plan build -> rehydrate -> preview deterministic reproduction. <br>3. Golden tests: regulated fixtures must reproduce <code>paramsHash</code>, <code>planSeed</code>, <code>planId</code> across ports. <br>4. Property tests: permutation invariance, stable sampling seeds. <br>5. Security tests: forbidden API detection (no secret reads, no network in UI path), signature validation for signed manifests. <br>6. Performance tests: plan build latency on large param sets and evidence persist latency. <br>7. CI gating: merges that change canonicalization or sampling algorithm require <code>migrationManifest</code> and two-person approval; golden-parity gate enforced. </td></tr><tr><td data-label="REG_Plan — Per-function Expert Technical Breakdown"> <strong>Extended troubleshooting & diagnostics (expanded)</strong><br><strong>Reproduce plan locally:</strong><br>1. Fetch <code>plan_manifestRef</code>; download <code>canonicalParamsString</code> and <code>planSeedInput</code>. <br>2. Recompute <code>paramsHash</code> and <code>planSeed</code> locally using canonicalVersion. <br>3. Run sampling harness on canonical rowId list using <code>samplingMeta</code>. <br>4. Compare produced sampleRowIds to stored preview <code>sampleRowRefs</code>. <br><strong>Plan audit mismatch:</strong><br>1. Verify audit chain <code>prevHash</code> continuity and signed rotations. <br>2. Use <code>VerifyReportParity</code> to recompute <code>reportHash</code> and locate first mismatch. </td></tr><tr><td data-label="REG_Plan — Per-function Expert Technical Breakdown"> <strong>Developer porting notes & pitfalls (VBA / Power Query / backend)</strong><br><strong>VBA:</strong><br>1. Use deterministic dictionaries by extracting keys and sorting before iterating. <br>2. UTF-8 handling: VBA's native strings are UTF-16; convert to UTF-8 consistently before hashing. <br>3. SHA-256: call into a small signed helper or use a vetted library; ensure hex lowercase canonicalization. <br>4. Avoid heavy IO on UI thread; schedule long tasks via <code>Application.OnTime</code> or background COM worker. <br><strong>Power Query (M):</strong><br>1. Implement canonicalization as M step sequences; export canonical string to hidden sheet or call backend to compute hash. <br>2. Avoid network calls in refresh; call backend endpoints for evidence persists. <br><strong>Backend:</strong><br>1. Implement central canonicalization service with stable library and unit tests used by PQ/VBA clients. <br>2. Provide RPC endpoints for <code>ComputeParamsHash</code>, <code>PersistPlanManifest</code>, <code>ValidateReferencedArtifacts</code>. <br><strong>Pitfalls to avoid:</strong> inconsistent newline or BOM handling, floating-point drift, inconsistent locale handling. </td></tr><tr><td data-label="REG_Plan — Per-function Expert Technical Breakdown"> <strong>Security & compliance expansions</strong><br>- Plans referencing regulated GLs must set <code>sensitivity=regulated</code> and require two-person approval for <code>post_direct</code>. <br>- Evidence access: retrieval requires <code>EvidenceAccessApprovalFlow</code>; all retrievals logged in chain-of-custody. <br>- Signing: manifest signing uses KMS/HSM; sign manifest hash and store signature metadata in manifest. <br>- Secrets: never store credentials in plan manifest. </td></tr><tr><td data-label="REG_Plan — Per-function Expert Technical Breakdown"> <strong>Retention, archival & legal packaging</strong><br>- Attach <code>retentionPolicy</code> object (e.g., <code>{hotDays:30,warmYears:7,cold:legal}</code>) to plan manifest. <br>- For regulatory submissions, include release manifest, plan manifest, canonical mapping, signed <code>recon_report</code>, migration manifest and golden fixtures in legal package. <br>- Archive to WORM storage and record archive manifest with checksums and storage URIs. </td></tr><tr><td data-label="REG_Plan — Per-function Expert Technical Breakdown"> <strong>Plan-level metrics & dashboards (design)</strong><br>- Dashboard tiles: <code>Plans built per day</code>, <code>Average plan build latency</code>, <code>Plans requiring approvals</code>, <code>Parity failures (last 7 days)</code>. <br>- Drill-through: <code>planId</code> -> manifests, parity history, canary results. <br>- Alert thresholds: <code>plan.publish.failure_rate &gt; 0.1%</code> or <code>parity.diff_count &gt; 0</code>. </td></tr><tr><td data-label="REG_Plan — Per-function Expert Technical Breakdown"> <strong>Full test list (exhaustive)</strong><br>1. Canonicalization unit coverage: token normalization, whitespace, casefold, numeric fixed scale, date canonicalization. <br>2. Params permutation invariance tests. <br>3. Hash parity across ports tests for <code>ComputeParamsHash</code>. <br>4. Seed composition tests including multiple ingest checksums. <br>5. Plan persist atomicity and idempotency tests. <br>6. Audit chain link tests and <code>prevHash</code> verification. <br>7. Sampling parity tests (shuffle, reservoir, stratified). <br>8. Approval gating tests for regulated scenarios. <br>9. Rehydrate/signature verification tests. <br>10. Performance microbenchmarks for plan build and manifest persist. <br>11. Migration manifest enforcement tests. <br>12. Canary runner smoke tests. <br>13. Evidence retrieval approval flow tests. <br>14. Security static analysis for forbidden APIs. </td></tr><tr><td data-label="REG_Plan — Per-function Expert Technical Breakdown"> <strong>Operator commands & cheat-sheet (expanded)</strong><br>- <code>recon.build-plan --params-file params.json --operator alice</code> → returns <code>planId</code>. <br>- <code>recon.plan-status --planId p_xxx</code> → shows manifestRef, requiredApprovals, and parity status. <br>- <code>recon.attach-approval --planId p_xxx --approvalRef ap-123</code> → persists approval and updates manifest. <br>- <code>recon.export-manifest --planId p_xxx --dest uri</code> → securely exports manifest snapshot and emits <code>plan.exported</code>. </td></tr><tr><td data-label="REG_Plan — Per-function Expert Technical Breakdown"> <strong>Common questions & short answers (FAQ)</strong><br>Q: Can a plan be updated? A: No. Publish a corrected plan; use <code>correctionOf</code> to link. <br>Q: How to reproduce an old preview? A: Use <code>plan_manifest</code> (paramsHash+planSeed+samplingMeta+mapHash+ingestChecksum) with matching canonicalVersion and samplingMeta.version. <br>Q: Who signs manifests? A: System signing via HSM/KMS; operators attach approvals recorded as <code>approvalsRef</code>. </td></tr><tr><td data-label="REG_Plan — Per-function Expert Technical Breakdown"> <strong>Closing verification checklist (ten required checks before plan roll-out)</strong><br>1. Schema & canonicalization validation for params passed. <br>2. <code>paramsHash</code> computed and cross-port parity verified (golden test). <br>3. <code>mapHash</code> and <code>ingestChecksums</code> validated and exist in Evidence. <br>4. <code>planSeed</code> composition stable and recorded. <br>5. <code>samplingMeta</code> complete with algorithm/version. <br>6. <code>requiredApprovals</code> computed and owners resolved. <br>7. Plan manifest persisted atomically with evidenceRef. <br>8. Audit row appended with no PII fields. <br>9. CI golden parity tests passed if canonical/sampling changed. <br>10. Migration manifest recorded for semantic changes if required. </td></tr><tr><td data-label="REG_Plan — Per-function Expert Technical Breakdown"> <strong>Final notes & best-effort statement:</strong><br>This expansion is an exhaustive, implementer-grade enlargement of REG_Plan covering function-level contracts, deterministic recipes, governance, tests, PQ/DAX conceptual guidance, runbooks, and developer porting notes. If you require additional dedicated artifacts (JSON Schemas, exact manifest JSON Schema, or runnable parity fixtures for CI), tell me which artifact you want next and I will produce it. </td></tr></tbody></table></div><div class="row-count">Rows: 37</div></div><div class="table-caption" id="Table7" data-table="Docu_0183_07" style="margin-top:2mm;margin-left:3mm;"><strong>Table 7</strong></div>
<div class="table-wrapper" data-table-id="table-7"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by REG_Preview — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">REG_Preview — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="REG_Preview — Per-function Expert Technical Breakdown"> <strong>Preface (read first; authoritative)</strong><br>This file is a complete, production-grade, implementation- and operations-focused expansion of the <code>REG_Preview</code> module for Project 616. It covers every function used by the preview pipeline, with: purpose & contract; inputs & outputs; invariants; canonicalization and evidence rules; step-by-step implementation narratives; exhaustive failure modes & recovery; deterministic parity and CI/golden requirements; SRE/monitoring and performance guidance; operator runbooks and triage playbooks; extensibility notes; conceptual Power Query (PQ) patterns and conceptual DAX modeling guidance (no runnable snippets). Every numbered list uses <code>&lt;br&gt;</code> line breaks to match your requested formatting. This document is intentionally detailed to enable developers, auditors, SREs and compliance teams to implement, test and operate REG_Preview in regulated environments. </td></tr><tr><td data-label="REG_Preview — Per-function Expert Technical Breakdown"> <strong>Top-level summary (one-paragraph)</strong><br>REG_Preview provides deterministic, read-only reconciliation previews seeded from immutable <code>planSeed</code> values and mapping snapshots (<code>mapHash</code>). It samples canonical payroll rows deterministically, applies mapping snapshots, aggregates payroll and GL into canonical aggregates using integer minor-unit math, performs deterministic full-outer joins, computes variance rows, synthesizes auditable JE suggestions (with deterministic rounding/residual absorption), packages redacted analyst artifacts and encrypted evidence, computes canonical <code>previewHash</code> and persists a signed <code>previewManifest</code> in the EvidenceStore. Changes to sampling, canonicalization or packaging are semantic and require a <code>migration_manifest</code> with golden fixtures and approvals. </td></tr><tr><td data-label="REG_Preview — Per-function Expert Technical Breakdown"> <strong>Glossary & canonical terms (short)</strong><br>- <code>planSeed</code> — deterministic seed computed as <code>sha256(paramsHash + &quot;:&quot; + mapHash + &quot;:&quot; + ingestChecksum)</code>.<br>- <code>mapHash</code> — sha256 of canonical mapping snapshot. <br>- <code>ingestChecksum</code> / <code>glIngestChecksum</code> — sha256 of canonical ingest CSV bytes. <br>- <code>previewHash</code> — sha256 over canonical preview manifest serialization. <br>- <code>evidenceRef</code> — opaque handle from EvidenceStore referencing stored artifact. <br>- <code>samplingMeta</code> — algorithm, seed, version used by deterministic sampler. <br>- <code>canonicalVersion</code> — version string for canonicalization recipes (ingest/map/preview). </td></tr><tr><td data-label="REG_Preview — Per-function Expert Technical Breakdown"> <strong>Design principles enforced by REG_Preview (high level)</strong><br>1. Determinism: identical logical inputs reproduce identical outputs across PQ/VBA/backend. <br>2. Immutability: persisted artifacts and snapshots are immutable; corrections produce new artifacts linked by <code>correctionOf</code>. <br>3. Evidence-first: full sanitized evidence persisted encrypted; UI artifacts are redacted views only. <br>4. Minimal PII exposure: previews contain pseudonymized or redacted PII; evidence retrieval requires approvals with chain-of-custody. <br>5. CI parity gating: golden fixtures for regulated samples block merges when parity fails. <br>6. Governance: any semantic changes require migration manifest and approvals; hot-swap mapping is a controlled procedure with smoke previews. </td></tr><tr><td data-label="REG_Preview — Per-function Expert Technical Breakdown"> <strong>Top-level function index (for navigation)</strong><br>1. <code>PreviewReconciliation</code> — orchestrator. <br>2. <code>RehydratePlan</code> — plan validation & rehydration. <br>3. <code>ComputeSamplingSeed</code> — seed derivation. <br>4. <code>DeterministicSample</code> — seeded selection. <br>5. <code>ApplyMappingSnapshot</code> — stateless map application. <br>6. <code>AggregateGroup</code> — canonical aggregation. <br>7. <code>FullOuterJoinAggs</code> — join. <br>8. <code>ComputeVarianceReport</code> — variance & materiality. <br>9. <code>GenerateSuggestedJEs</code> — suggestion engine. <br>10. <code>RedactPIIForPreview</code> — redaction/tokenization. <br>11. <code>PackagePreviewArtifacts</code> — deterministic packaging. <br>12. <code>PersistPreviewManifest</code> — evidence persist & signing. <br>13. <code>EmitPreviewAudit</code> — audit emission. <br>14. <code>PreviewReplay</code> — reproducible replay. <br>15. <code>AbortPreview</code> — cancellation. <br>16. Integration checks, CI & SRE hooks, and operator runbooks (module-level). </td></tr><tr><td data-label="REG_Preview — Per-function Expert Technical Breakdown"> <strong><code>PreviewReconciliation(planId, sampleSize, operatorId, previewOptions)</code> — COMPLETE function breakdown</strong><br><strong>Purpose & contract:</strong> orchestrate a deterministic read-only preview run that yields a signed preview manifest and deterministic preview hash. Preview must be side-effect-free to system state; it may persist artifacts and emits audits but must not mutate mapping snapshots or ingest artifacts. Returns <code>{previewRef, previewHash, previewManifestRef, issues[], metrics}</code>. <br><strong>Inputs:</strong> <code>planId</code> (string), <code>sampleSize</code> (long or null), <code>operatorId</code> (string), <code>previewOptions</code> (dictionary; keys allowed: <code>redactionLevel</code>, <code>includeMappingSuggestions</code>, <code>overrideSeed</code> (CI only), <code>parallelAgg</code> boolean). <br><strong>Outputs:</strong> persisted preview zip <code>previewRef</code> (evidenceRef), <code>previewHash</code> (sha256 string), <code>previewManifestRef</code> (evidenceRef), structured <code>issues[]</code>, <code>metrics</code> including <code>durationMs</code>, <code>rowsSampled</code>, <code>mappingCoveragePct</code>, <code>artifactChecksums[]</code>. <br><strong>Preconditions:</strong> <code>planRecord</code> exists and references <code>mapHash</code> and <code>ingestChecksum</code>; EvidenceStore reachable; <code>prngKey</code> provisioned. <br><strong>Primary invariants (must/shall):</strong> <br>1. Deterministic reproduction for identical <code>planSeed</code>, <code>mapHash</code>, <code>ingestChecksum</code>, and <code>samplingMeta.version</code>. <br>2. All persisted artifacts include <code>canonicalVersion</code> and sha256 checksums. <br>3. UI artifacts must be redacted; full sanitized evidence stored encrypted. <br><strong>End-to-end narrative (step-by-step, exhaustive):</strong> <br>1. <strong>Invocation & correlation:</strong> allocate <code>correlationId</code>, record <code>startTs</code>, call <code>SafeInvokeReconciler</code> wrapper (ensures telemetry and cancellation token). <br>2. <strong>Rehydrate plan:</strong> call <code>RehydratePlan(planId)</code>; validate <code>mapHash</code> and <code>ingestChecksum</code> exist. If missing, return <code>PAYROLL_PLAN_ORPHAN</code>. <br>3. <strong>Prepare sampling:</strong> retrieve <code>samplingMeta</code> from <code>planRecord</code>; call <code>ComputeSamplingSeed(planSeed, previewOptions.overrideSeed)</code> to produce <code>sampleSeed</code>; record <code>samplingMeta</code> in manifest. <br>4. <strong>RowId index fetch:</strong> read or stream <code>rowId</code> index of canonical payroll ingest referenced by <code>ingestChecksum</code>. Must use canonical ordering (lexicographic over canonical row serialization). <br>5. <strong>Deterministic sampling:</strong> call <code>DeterministicSample(sortedRowIds, sampleSeed, sampleSize)</code> to obtain <code>selectedRowIds</code>. Persist <code>sampleRowRefs</code> to manifest. <br>6. <strong>Materialize sampled rows:</strong> fetch sampled rows streaming from EvidenceStore using offsets; validate per-row canonical serialization integrity and compute per-row <code>rowId</code> and <code>rowChecksum</code>. <br>7. <strong>Map application:</strong> call <code>ApplyMappingSnapshot</code> to assign <code>Mapped_GLAccount</code> values; collect <code>mappingDiagnostics</code> and <code>mappingSuggestions</code> (persist suggestions evidence). <br>8. <strong>Aggregation:</strong> call <code>AggregateGroup</code> for payroll grouped by <code>aggKeys</code> configured in plan; obtain <code>payrollAggRef</code> and artifact checksum; fetch or compute <code>glAgg</code> (if plan references <code>glIngestChecksum</code> use precomputed GL aggregates or compute on-demand) and compute <code>glAggRef</code>. <br>9. <strong>Join & variance:</strong> call <code>FullOuterJoinAggs</code> then <code>ComputeVarianceReport</code> to produce canonical <code>varianceRows</code>. For each <code>BeyondTolerance</code> row, compute <code>triagePriority</code> and triage metadata. <br>10. <strong>Suggestions creation:</strong> call <code>GenerateSuggestedJEs</code> for flagged rows; persist each suggestion and link suggestion evidence in manifest. <br>11. <strong>PII redaction:</strong> call <code>RedactPIIForPreview</code> to produce redacted UI artifacts (before_aggregates.csv, variance_report.csv, suggested_jes_preview.csv). Persist full sanitized evidence separately. <br>12. <strong>Package & canonicalize:</strong> call <code>PackagePreviewArtifacts</code> with canonical ordering and compute <code>previewHash</code> using the algorithm documented below. <br>13. <strong>Persist manifest & sign:</strong> call <code>PersistPreviewManifest(previewManifest)</code>; sign if signing service available or schedule signing job and emit <code>preview_manifest.unsigned</code>. <br>14. <strong>Audit emission:</strong> call <code>EmitPreviewAudit</code> to publish a PII-free audit row referencing <code>previewManifestRef</code>. <br>15. <strong>Return results:</strong> return the package evidenceRef, previewHash, manifestRef, issues[] and metrics. <br><strong>Packaging & canonicalization rules (explicit):</strong> <br>1. <strong>Artifact ordering:</strong> files sorted lexicographically by filename. <br>2. <strong>Normalized file serialization:</strong> JSON keys sorted, no transient fields, canonical date format <code>YYYY-MM-DDTHH:MM:SSZ</code> for manifest metadata. <br>3. <strong>Checksum computation:</strong> per-file sha256 over UTF-8 bytes of canonical serialization; <code>previewHash = sha256(concat(sortedFileChecksums))</code>. <br>4. <strong>ZIP determinism:</strong> normalized timestamps (e.g., epoch 0 or canonical createdTs), fixed compression flags, no host-specific attributes. <br><strong>Failure modes & remediation (comprehensive):</strong> <br>1. <strong>Missing map or ingest artifacts</strong> → abort with <code>PAYROLL_PLAN_ORPHAN</code> and include <code>evidenceSuggestions[]</code> to nearest snapshots. <br>2. <strong>PRNG key missing</strong> → <code>PRNG_KEY_MISSING</code> and abort regulated runs; allow debug previews with test-only fallback under CI gating. <br>3. <strong>EvidenceStore transient I/O</strong> → retry with exponential backoff, circuit-breaker after threshold; if persist fails, stage artifacts in secure local encrypted staging and emit <code>preview.persist.warning</code>. <br>4. <strong>PII leakage detection</strong> → abort UI exposure and create high-priority compliance incident; preserve full sanitized evidence and produce <code>forensic_manifest</code>. <br>5. <strong>Large-scale overflow/precision errors</strong> → switch to big-integer arithmetic path, mark <code>aggregation.partial</code> and log <code>agg_overflow</code> metric. <br>6. <strong>Sampling parity mismatch</strong> → surface diagnostic and provide <code>PreviewReplay</code> option for debugging. <br><strong>SRE & performance guidance:</strong> <br>1. Stream rows from EvidenceStore to avoid memory blow-ups. <br>2. Use chunked aggregation and checkpointing for long-running previews; persist intermediate artifacts with <code>chunkIndex</code> for replay. <br>3. Instrument <code>preview.step.*</code> metrics: sampleSelectionMs, mapApplyMs, payrollAggMs, glAggMs, joinMs, suggestionMs, packageMs, persistMs. <br>4. Maintain alerting on <code>preview.duration_ms</code> P95 and <code>preview.error_rate</code>. <br><strong>Determinism & CI requirements:</strong> <br>1. All runtimes (PQ/VBA/backend) must implement the same sampling algorithm and canonicalization. <br>2. Golden fixtures: canonical preview bundle for regulated datasets with <code>previewHash</code> must be stored under <code>golden/REG_Preview/</code> and used by CI parity tests. <br>3. Any change in sampling algorithm or canonicalization requires <code>migration_manifest</code> and updated golden fixtures. <br><strong>Operator runbook (triage):</strong> <br>1. If preview parity mismatch: collect <code>planId</code>, <code>previewManifestRef</code>, <code>planSeed</code>, <code>samplingMeta.version</code>, <code>mapHash</code>, <code>ingestChecksum</code> and run <code>PreviewReplay</code> in parity CI runner. <br>2. If unmapped volumes spike: export <code>mapping_suggestions.csv</code>, present to mapping owners, and schedule <code>RefreshMapping</code> hot-swap smoke tests. <br>3. If PII leak: immediately suspend UI artifact links, create <code>forensic_manifest</code>, notify compliance, and start evidence retrieval approval flow for necessary reviewers. </td></tr><tr><td data-label="REG_Preview — Per-function Expert Technical Breakdown"> <strong><code>RehydratePlan(planId)</code> — complete function specification</strong><br><strong>Purpose & contract:</strong> load canonical <code>planRecord</code> persisted by <code>REG_Plan</code>, validate referenced artifacts exist, and compute/verify <code>planSeed</code>. Return canonical <code>plan</code> object: <code>{planId, paramsHash, planSeed, mapHash, ingestChecksum, glIngestChecksum (optional), aggregationKeys, tolerances, samplingMeta, requiredApprovals, expectedGroupCount}</code> or structured error when missing artifacts or integrity issues. <br><strong>Checks performed:</strong> <br>1. Verify <code>paramsHash</code> coalition matches persisted <code>paramsHash</code>. <br>2. Recompute <code>planSeed = sha256(paramsHash + &quot;:&quot; + mapHash + &quot;:&quot; + ingestChecksum)</code> and compare to stored <code>planSeed</code>. <br>3. Validate presence of evidence for <code>mapHash</code> and <code>ingestChecksum</code>. <br>4. Validate <code>samplingMeta</code> presence and <code>prngKeyId</code> availability if sampling uses <code>prngKey</code>. <br><strong>Artifacts & evidence:</strong> return <code>mapManifestRef</code> and <code>ingestManifestRef</code> in the <code>plan</code> object for direct evidence retrieval. <br><strong>Failure modes & remediation:</strong> <br>1. Missing <code>mapHash</code> or <code>ingestChecksum</code> → <code>PAYROLL_PLAN_ORPHAN</code> with <code>missingArtifacts[]</code>. <br>2. PlanSeed mismatch → <code>PLAN_SEED_MISMATCH</code> with both expected and computed seeds for audit. <br>3. Gl ingest absent but plan expects GL → <code>GL_INGEST_MISSING</code> and preview proceeds with only payroll side if operator accepts (flagged in issues). <br><strong>Implementation notes:</strong> <br>- Cache plan records for short TTL but always verify artifact existence before executing preview.<br>- Record <code>rehydrateTs</code> and persist a diagnostic <code>rehydrateDigest</code> (sha256 of referenced artifact checksums) to detect mid-run mapping hot-swap. </td></tr><tr><td data-label="REG_Preview — Per-function Expert Technical Breakdown"> <strong><code>ComputeSamplingSeed(planSeed, overrideSeed=&quot;&quot;)</code> — exhaustive coverage</strong><br><strong>Purpose & contract:</strong> derive deterministic sampling seed using HMAC-SHA256 keyed with environment <code>prngKey</code>. Return <code>{seedHex, samplingMeta}</code> where <code>samplingMeta = {algorithm:&quot;HMAC_DRBG-SHA256&quot;, seed:seedHex, prngKeyId, version:&quot;1.0.0&quot;}</code>. <code>overrideSeed</code> allowed only under CI <code>RegisterUnitTestHook</code>. <br><strong>Security & keying rules:</strong> <br>1. <code>prngKey</code> stored in secure config (KMS/HSM); reveal only <code>prngKeyId</code> to manifests. <br>2. Key rotation: include <code>prngKeyId</code> in <code>samplingMeta</code>; if key rotates, record prior <code>prngKeyId</code> entries for replay. <br><strong>Failure & safety:</strong> missing <code>prngKey</code> → <code>PRNG_KEY_MISSING</code> abort for regulated runs. Allow non-regulated debug previews with local fallback but mark <code>unsafeSampling</code> and add warnings. <br><strong>Parity tests:</strong> HMAC vectors across PQ/VBA/backend; PRNG key rotation replay tests. </td></tr><tr><td data-label="REG_Preview — Per-function Expert Technical Breakdown"> <strong><code>DeterministicSample(sortedRowIds, sampleSeed, sampleSize)</code> — exhaustive choices, portability & algorithm parity</strong><br><strong>Purpose & contract:</strong> deterministically select <code>sampleSize</code> rowIds from lexicographically sorted <code>sortedRowIds</code> using <code>sampleSeed</code>; return selectedRowIds in canonical ordering expected by downstream steps. Document chosen algorithm in <code>samplingMeta.algorithm</code>. <br><strong>Algorithm options & tradeoffs (choose and lock one in samplingMeta):</strong> <br>1. <strong>Seeded Fisher–Yates</strong> — full shuffle; best when dataset fits memory; stable across languages if PRNG used is HMAC_DRBG-SHA256 pseudo-random number stream with documented mapping from stream to swap indices. <br>2. <strong>Hash-Ranking</strong> — compute keyed hash of rowId with <code>sampleSeed</code> producing <code>rank</code>, sort by rank, select top-K; best for PQ portability and streaming; implementable in M and deterministic in VBA/backend. <br>3. <strong>Deterministic Reservoir</strong> — streaming-friendly; uses seeded PRNG to accept/reject new items using computed thresholds; requires careful implementation to preserve parity. <br><strong>Implementation guidance:</strong> <br>- PQ: use hash-ranking for portability. <br>- VBA: avoid non-cryptographic RNGs; use COM crypto helper for HMAC if doing seeded Fisher–Yates. <br>- Backend: implement HMAC_DRBG-SHA256 and lock to algorithm version for parity. <br><strong>Edge cases & behavior:</strong> <br>- If <code>sampleSize &gt;= totalRows</code> return all rows with <code>sampleTruncated=true</code>. <br>- On tie or deterministic collisions ensure stable tie-breaker (lexicographic rowId). <br><strong>Testing matrix:</strong> cross-host parity vectors with fixed seeds and sample fixture; large-scale stress tests for deterministic reservoir implementation. </td></tr><tr><td data-label="REG_Preview — Per-function Expert Technical Breakdown"> <strong><code>ApplyMappingSnapshot(rows, mapHash)</code> — deep deterministic mapping application</strong><br><strong>Purpose & contract:</strong> apply immutable mapping snapshot <code>mapHash</code> to rows using canonical mapping semantics and return mapped rows with <code>mappingDiagnostics[]</code>. Must not mutate map snapshot. <br><strong>Canonical mapping recipe (full):</strong> <br>1. <strong>Token normalization:</strong> normalize tokens using canonical recipe: Unicode NFKC, strip control chars, casefold, collapse internal whitespace, strip non-significant punctuation per COA policy. <br>2. <strong>mapKey construction:</strong> canonical(PayComponent) + <code>|</code> + canonical(CostCenter<code>||</code>) + <code>|</code> + canonical(effectiveFrom<code>||</code>) — this must be computed identically across runtimes. <br>3. <strong>Lookup:</strong> exact lookup in map dictionary keyed by <code>mapKey</code>. <br>4. <strong>Dedup & precedence:</strong> if multiple matches, choose lowest <code>Priority</code> then <code>mapRowId</code> lexicographic tiebreaker. <br>5. <strong>Fuzzy fallback:</strong> apply deterministic fuzzy algorithm (token overlap + normalized Levenshtein) with configured threshold; record <code>suggestionRef</code> when used. <br>6. <strong>Record diagnostics:</strong> for each row record <code>mapped=true|false</code>, <code>mapRowId</code>, <code>mappingConfidence</code>, <code>suggestionRef</code>, <code>reason</code> and <code>evidenceRef</code> for any corrections. <br><strong>Provenance & audit:</strong> if suggestions used create <code>mappingSuggestion</code> artifacts with canonicalization details and persist. <br><strong>Failure & recovery:</strong> <br>- checksum mismatch on <code>map_&lt;mapHash&gt;.json</code> → abort and surface <code>mapIntegrityError</code>. <br>- priority collision with same <code>Priority</code> and identical <code>mapKey</code> → stable tiebreaker <code>mapRowId</code> lexicographic; also emit <code>mapping.conflict</code> diagnostic for owner. <br><strong>Scaling & memory:</strong> for very large maps use disk-backed key-value store with bloom filter header for positive tests; keep small in-memory LRU cache for hot keys. <br><strong>PQ/VBA/Backend parity notes:</strong> in PQ, store canonical mapping table as a query and perform merges; in VBA prefer sorted binary-search on canonical mapping CSV to reduce memory. </td></tr><tr><td data-label="REG_Preview — Per-function Expert Technical Breakdown"> <strong><code>AggregateGroup(rows, aggKeys, scale)</code> — deterministic aggregation details</strong><br><strong>Purpose & contract:</strong> compute canonical aggregates using integer minor-units at <code>scale</code> for groups of rows and produce <code>{GroupKey, SumMinorUnits, RowCount, MinMinorUnits, MaxMinorUnits, Welford(n,mean,M2), sampleRowRefs}</code>. <br><strong>Canonical arithmetic & rounding rules:</strong> <br>1. Convert decimal amounts to <code>AmountMinorUnits = Round(decimalAmount * 10^scale)</code> using banker's rounding unless plan overrides. <br>2. Use integer-only summation for <code>SumMinorUnits</code>. <br>3. Use Welford algorithm for variance in integer domain to prevent numerical instability. <br>4. Track <code>min</code> and <code>max</code> in minor units. <br><strong>Overflow & big-numbers:</strong> if intermediate sums exceed 64-bit, switch to big-int arithmetic with explicit <code>bigInt=true</code> in aggregate metadata and emit <code>agg.bigint</code> metric. <br><strong>Sample refs per group:</strong> produce deterministic <code>sampleRowRefs</code> by seeding local group PRNG <code>groupSeed = HMAC(sampleSeed, groupKey)</code> and selecting top-M rows for triage; persist sampleRowRefs in aggregate artifact. <br><strong>Edge-cases & corrections:</strong> negative amounts treated as signed integers; ensure sign conventions are derived from <code>glIngestManifest.signConvention</code> for GL aggregates. <br><strong>Tests:</strong> invariance under row reordering, overflow simulations, rounding parity tests. </td></tr><tr><td data-label="REG_Preview — Per-function Expert Technical Breakdown"> <strong><code>FullOuterJoinAggs(payrollAgg, glAgg, keys)</code> — join contract and canonical ordering</strong><br><strong>Purpose & contract:</strong> full-outer join of payroll and GL aggregates producing deterministic union sorted lexicographically by <code>GroupKey</code>. Each <code>varianceRow</code> contains <code>GroupKey, PayrollMinorUnits, GLMinorUnits, RowCountPayroll, RowCountGL, SideFlag (PayrollOnly|GLOnly|Matched), sampleRowRefsPayroll, sampleRowRefsGL</code>. <br><strong>Key normalization & collisions:</strong> ensure both agg sets use identical canonical groupKey construction; if mismatch occurs flag <code>KEY_NORM_MISMATCH</code> with examples and recommend re-normalization. <br><strong>Multi-currency handling:</strong> include <code>Currency</code> in <code>aggKeys</code> if multi-currency; if currencies differ for the same <code>GroupKey</code> flag <code>CURRENCY_MISMATCH</code> and exclude from numeric variance until FX normalization occurs. <br><strong>Deterministic ordering:</strong> union keys sorted lexicographically, tie-break stable. <br><strong>DB pushdown pattern:</strong> for huge datasets export canonical <code>groupKey</code> expression and push join to DB; after DB returns canonical CSV, run deterministic packaging to compute preview hash. <br><strong>Tests:</strong> parity with DB pushdown vs in-memory join results on canonical fixtures. </td></tr><tr><td data-label="REG_Preview — Per-function Expert Technical Breakdown"> <strong><code>ComputeVarianceReport(joinedAggs, tolerances)</code> — materiality, decisioning & triage</strong><br><strong>Purpose & contract:</strong> compute <code>VarianceMinorUnits = PayrollMinorUnits - GLMinorUnits</code>, <code>AbsVariance</code>, <code>RelativeVariancePct</code> (null when GL=0), <code>BeyondTolerance</code> (abs threshold for GL=0 else percent threshold), <code>materialityReason</code>, <code>triagePriority</code> and <code>confidenceHint</code>. <br><strong>Policy precedence & formula:</strong> <br>1. Use <code>AbsThreshold</code> when <code>GL=0</code> to avoid noisy percent deltas. <br>2. Else compute <code>RelativeVariancePct = AbsVariance / ABS(GLMinorUnits)</code>. <br>3. <code>BeyondTolerance = IF(GL=0, AbsVariance &gt; AbsThreshold, RelativeVariancePct &gt; TolerancePct)</code> and store the exact formula used in the manifest for audit. <br><strong>Materiality scoring:</strong> combine <code>AbsVariance</code>, <code>RelativeVariancePct</code>, <code>accountSensitivityFactor</code> (policy-controlled) and <code>recentIncidence</code> to compute <code>triagePriority</code> numeric. Persist <code>triageRationale</code>. <br><strong>Edge-cases:</strong> negative GL amounts, reversal entries, or GL carry-forwards require clear sign conventions; record <code>signConvention</code> from <code>glIngestManifest</code> in variance metadata. <br><strong>Tests:</strong> boundary thresholds, per-account override tests, multi-currency exclusion tests. </td></tr><tr><td data-label="REG_Preview — Per-function Expert Technical Breakdown"> <strong><code>GenerateSuggestedJEs(varianceRow, jeTemplateSpec, historyIndex, scale, roundingMode)</code> — exhaustive suggestion engine</strong><br><strong>Purpose & contract:</strong> produce auditable JE suggestions for each variance row flagged <code>BeyondTolerance</code>. Output includes <code>suggestionId</code>, <code>varianceRowId</code>, <code>debitLines[]</code>, <code>creditLines[]</code>, <code>confidenceScore</code> (0..1), <code>confidenceBreakdown</code>, <code>rationale</code>, <code>residualAbsorbedBy</code>, <code>evidenceRefs[]</code>, <code>metadata{historySnapshotRef}</code>. Suggestions are advisory only; do not auto-apply. <br><strong>Detailed algorithm & explainability:</strong> <br>1. <strong>Attribution signal extraction:</strong> compute features: exact PayComponent match, cost center match, alias similarity, historical co-occurrence fraction, mapping stability (count of identical mapping events), recency weight (exponential decay). <br>2. <strong>Candidate counterpart ranking:</strong> produce ranked list of candidate GL accounts with scores computed from weighted features; the weight vector and model version must be persisted in suggestion metadata. <br>3. <strong>Line candidate construction:</strong> when single high-confidence counterpart exists propose two-line JE; when multiple counterparts and stable historical distribution exist propose multi-line JE apportioned by historical splits; otherwise fallback to <code>PayrollSuspense</code>. <br>4. <strong>Rounding & deterministic residual absorption:</strong> compute exact rational amounts (in minor units) for lines before rounding; round each line per <code>roundingMode</code> (default Banker's rounding); compute <code>residual = targetTotalMinorUnits - sum(roundedLines)</code>. Absorb <code>residual</code> deterministically into the line with greatest pre-rounding absolute value; tiebreak by lexicographic <code>ruleId</code>. Record <code>residualAbsorbedBy</code>. <br>5. <strong>Confidence scoring & breakdown:</strong> compute <code>confidence = clamp(0..1, w_match*matchScore + w_hist*historyScore + w_recency*recency - w_dq*dqPenalty)</code>; persist <code>confidenceBreakdown</code> with debug values. <br>6. <strong>Audit & evidence:</strong> persist the suggestion payload (full sanitized) as <code>evidenceRef:suggestion_&lt;id&gt;.json</code> and include provenance fields: <code>planId</code>, <code>previewId</code>, <code>historySnapshotRef</code>. <br><strong>Governance & approvals:</strong> suggestions touching regulated GL ranges must be annotated <code>requiresOwnerApproval</code> and cannot be auto-accepted. <br><strong>Failure & fallback:</strong> if no reasonable counterpart found create <code>PayrollSuspense</code> suggestion flagged <code>requiresFollowUp</code>. <br><strong>Tests:</strong> deterministic residual absorption parity across runtimes, confidence score unit tests, historical-snapshot provenance tests. </td></tr><tr><td data-label="REG_Preview — Per-function Expert Technical Breakdown"> <strong><code>RedactPIIForPreview(artifact, redactRules)</code> — deterministic redaction, tokenization & evidence linkage</strong><br><strong>Purpose & contract:</strong> produce redacted analyst-facing artifacts while preserving triage usefulness. Returns <code>{redactedArtifact, redactionMeta}</code>. Full sanitized artifact persisted encrypted and referenced via <code>evidenceRef</code>. UI artifacts must not contain PII. <br><strong>Redaction patterns & record tokenization rules:</strong> <br>1. <strong>Identifier tokens:</strong> deterministic salted-hash tokenization for stable analyst correlation; salt derived from <code>planSeed</code> + <code>redactionSaltId</code> stored in manifest evidence only. <br>2. <strong>Names & addresses:</strong> mask preserving length and initial characters; include token mapping in evidence only. <br>3. <strong>Dates & amounts:</strong> amounts and dates generally preservable for triage but redact or pseudonymize when combined with PII that could identify an individual. <br>4. <strong>PII detection:</strong> run pattern matching for SSN, national IDs, emails, phones; any unexpected match triggers <code>PII_LEAK_DETECTED</code>. <br><strong>Provenance & evidence:</strong> store mapping table for tokens as encrypted evidence; link via <code>redactionMeta.evidenceRef</code>. <br><strong>Failure modes:</strong> tokenization library bug or mapping leakage -> abort and escalate. <br><strong>Tests:</strong> token stability across repeated previews, inability to re-identify without evidence keys, redaction performance. </td></tr><tr><td data-label="REG_Preview — Per-function Expert Technical Breakdown"> <strong><code>PackagePreviewArtifacts(planId, artifacts, operatorId, canonicalVersion)</code> — canonical packaging</strong><br><strong>Purpose & contract:</strong> assemble redacted UI artifacts plus references to full sanitized evidence into deterministic zip package <code>preview_&lt;planId&gt;_&lt;previewHash&gt;.zip</code>. Compute per-file sha256 and <code>previewHash</code> as sha256(concat(sortedFileChecksums)). Return <code>{previewRef, previewHash, artifactChecksums}</code>. <br><strong>Canonical rules & steps:</strong> <br>1. Validate artifact filenames and sort lexicographically. <br>2. For JSON manifests, canonicalize keys and remove transient fields that must not be included in canonical hash. <br>3. Compute per-file sha256 over UTF-8 bytes. <br>4. Compute <code>previewHash = sha256(concat(sortedFileChecksums))</code>. <br>5. Create deterministic zip (normalized timestamps). <br><strong>Failure & recovery:</strong> if computed <code>previewHash</code> collides with existing artifact — extremely unlikely — record incident and include additional entropy (e.g., <code>planId</code>) and republish as new manifest (rare path; document in audit). <br><strong>Tests:</strong> cross-runtime packaging parity (same artifact set must produce identical <code>previewHash</code>). </td></tr><tr><td data-label="REG_Preview — Per-function Expert Technical Breakdown"> <strong><code>PersistPreviewManifest(previewManifest)</code> — immutability & signing</strong><br><strong>Purpose & contract:</strong> atomically persist canonical <code>previewManifest</code> to EvidenceStore and, when available, sign the manifest with system signing key. Return <code>previewManifestRef</code> and signature metadata. Manifests must be immutable; corrections create new manifests with <code>correctionOf</code>. <br><strong>Signing policy & lifecycle:</strong> <br>1. <strong>Phase 1:</strong> unsigned persist allowed but preview cannot be promoted for regulated applies until signed. <br>2. <strong>Phase 2:</strong> HSM-backed signing enforced; signed manifest includes <code>signerId</code>, <code>keyId</code>, <code>signature</code>, <code>signedTs</code>. <br>3. EvidenceStore must return <code>evidenceRef</code> and <code>checksum</code>; include both in audit. <br><strong>Failure & fallback:</strong> signing service unavailable -> persist unsigned and create signing job to sign later; for regulated runs block promotion until signing completes. <br><strong>Tests:</strong> sign/verify roundtrip, immutability enforcement, correctionOf linkage tests. </td></tr><tr><td data-label="REG_Preview — Per-function Expert Technical Breakdown"> <strong><code>EmitPreviewAudit(previewManifestRef, planId, operatorId, previewHash)</code> — audit specifics</strong><br><strong>Purpose & contract:</strong> append PII-free audit row <code>payroll.recon.preview</code> to append-only audit store with core fields: <code>timestamp</code>, <code>correlationId</code>, <code>planId</code>, <code>previewHash</code>, <code>previewManifestRef</code>, <code>operatorId</code>, <code>samplingMeta.version</code>, <code>mapHash</code>, <code>ingestChecksums[]</code>, <code>issuesSummary</code>, <code>durationMs</code>, <code>payloadHash</code>. Audit must never include raw PII. <br><strong>Audit chaining & integrity:</strong> include <code>prevHash</code> to link audit chain and optionally sign audit rotations. <br><strong>Failure handling:</strong> if audit store down enqueue durable audit to local encrypted queue and set <code>audit.persist.warning</code>. <br><strong>Tests:</strong> audit chain verification, PII scrub scan, replay audit retrieval. </td></tr><tr><td data-label="REG_Preview — Per-function Expert Technical Breakdown"> <strong><code>PreviewReplay(planSeed, previewManifestRef)</code> — reproducible replay & parity report</strong><br><strong>Purpose & contract:</strong> reproduce a historical preview run by re-executing preview logic using historical artifacts referenced in <code>previewManifestRef</code>. Compare produced <code>previewHash</code> with the stored previewHash and produce a <code>parityReport</code> that enumerates any mismatches and their probable causes. Used by CI for golden parity and by SRE in investigations. <br><strong>Replay steps:</strong> <br>1. Load <code>previewManifest</code> and referenced artifacts (mapHash, ingestChecksum, samplingMeta). <br>2. Reproduce deterministic sampling with exact <code>samplingMeta</code> and <code>planSeed</code>. <br>3. Recompute aggregates and suggestions and repackage. <br>4. Compare recomputed <code>previewHash</code> with stored hash. <br>5. Produce <code>parityReport</code> listing exact bytes that diverge, algorithm version mismatches, packaging differences, or evidence corruption. <br><strong>Common parity mismatch causes & recommendations:</strong> <br>1. Sampling algorithm version drift → upgrade/downgrade runtime sampling code or pin samplingMeta. <br>2. Canonicalization recipe drift → ensure canonicalVersion matches and check cross-runtime canonicalization tests. <br>3. Evidence corruption → verify EvidenceStore checksums and perform <code>VerifyReportParity</code> across archives. <br><strong>Tests:</strong> replay parity tests built into CI; golden regression detection. </td></tr><tr><td data-label="REG_Preview — Per-function Expert Technical Breakdown"> <strong><code>AbortPreview(correlationId, reason, operatorId)</code> — full cancellation semantics</strong><br><strong>Purpose & contract:</strong> cooperative cancellation API that sets cancellation tokens and requests workers to stop at safe points. Persist <code>abortRequest</code> and return abort ack. Worker should save partial artifacts and mark preview manifest <code>aborted:true</code> if necessary. Emit <code>preview.aborted</code> audit. <br><strong>Safe abort points:</strong> end of mapping apply, after each aggregation chunk, before packaging step. <br><strong>Failure & recovery:</strong> unresponsive worker → worker supervisor forcibly terminates and persist <code>preview.aborted.hung</code> with partial evidence. Provide operator commands to resume from last checkpoint or rerun. <br><strong>Tests:</strong> cancellation acceptance tests, staged artifact persist tests, abort audit presence. </td></tr><tr><td data-label="REG_Preview — Per-function Expert Technical Breakdown"> <strong>Cross-function integration & acceptance rules (module-level)</strong><br>1. <strong>Determinism guarantee:</strong> identical <code>planSeed</code>, <code>mapHash</code>, <code>ingestChecksum</code>, <code>samplingMeta.version</code> produce identical <code>previewHash</code> across PQ/VBA/backend. <br>2. <strong>Evidence-first policy:</strong> every artifact persisted must have <code>evidenceRef</code> and sha256 checksum. <br>3. <strong>PII policy:</strong> UI artifacts redacted; full artifacts encrypted; evidence retrieval requires approvals and chain-of-custody logging. <br>4. <strong>Signing rule:</strong> regulated runs require signed <code>previewManifest</code> before promotion to apply stages (Phase 2). <br>5. <strong>Migration gating:</strong> any change to sampling, canonicalization, rounding, or packaging requires a <code>migration_manifest</code> with golden fixtures and approvals. <br>6. <strong>CI parity:</strong> golden fixtures and <code>RegisterUnitTestHook</code> must be used in CI to reproduce run results across PQ/VBA/backend. </td></tr><tr><td data-label="REG_Preview — Per-function Expert Technical Breakdown"> <strong>Operator & SRE runbooks (executable checklists)</strong><br><strong>Preview parity mismatch triage:</strong> <br>1. Capture <code>planId</code>, <code>previewManifestRef</code>, <code>previewHash_expected</code>, <code>previewHash_produced</code>. <br>2. Run <code>PreviewReplay</code> with pinned <code>samplingMeta.version</code>. <br>3. If replay matches expected, diagnose runtime version drift (sampling/canonicalization). <br>4. If replay fails, inspect EvidenceStore checksums for corruption; revert to last good artifact snapshot and re-run. <br><strong>High unmapped ratio remediation:</strong> <br>1. Fetch <code>mapping_suggestions.csv</code>, rank by <code>Occurrences</code> and <code>ExampleAmount</code>. <br>2. Present top candidates to mapping owner; create mapping candidate manifest and run <code>RefreshMapping</code> smoke preview to estimate impact. <br>3. If impact acceptable and approvals present, hot-swap mapping snapshot. <br><strong>PII leak incident flow:</strong> <br>1. Mark artifact links private, record <code>incidentId</code> and <code>correlationId</code>. <br>2. Retrieve full sanitized evidence for compliance under <code>EvidenceAccessApprovalFlow</code>. <br>3. Produce <code>forensic_manifest</code>, copy artifacts to WORM archive, notify compliance & legal. <br>4. Remediate redaction logic and re-run affected previews; publish corrected artifacts. </td></tr><tr><td data-label="REG_Preview — Per-function Expert Technical Breakdown"> <strong>CI / Golden parity & test matrix (detailed)</strong><br><strong>Required tests for REG_Preview:</strong> <br>1. Unit: <code>ComputeSamplingSeed</code>, <code>DeterministicSample</code>, <code>AggregateGroup</code>, <code>FullOuterJoinAggs</code>, <code>ComputeVarianceReport</code>, <code>GenerateSuggestedJEs</code> rounding/residual checks. <br>2. Integration: plan->preview->package on canonical fixtures (payroll sample, GL sample, mapping snapshot). <br>3. Golden: canonical preview bundle parity across PQ/VBA/backend for regulated fixture set. <br>4. Property: reorder invariance, sampling determinism under fixed seed. <br>5. Fault injection: missing artifacts, evidence persist errors, signing service down. <br>6. Performance: preview latency microbenchmarks for target sample sizes. <br><strong>CI gating rules:</strong> golden parity failures block merges; a <code>migration_manifest</code> with sample before/after golden files and approvals required for semantic changes. Nightly parity jobs run for additional guard rails and open incidents when divergence found. </td></tr><tr><td data-label="REG_Preview — Per-function Expert Technical Breakdown"> <strong>SLOs, metrics & alerts (operational)</strong><br><strong>Recommended SLOs:</strong> <br>1. Preview median latency for <=500 rows: <2000ms. <br>2. Preview success rate: >99.5% for non-peak runs. <br>3. Parity detection latency: mean <15 minutes from scheduled parity job. <br>4. Evidence persist success rate: >99.9%. <br><strong>Key metrics to instrument:</strong> <code>preview.duration_ms</code>, <code>preview.step.select_ms</code>, <code>preview.step.mapapply_ms</code>, <code>preview.step.agg_ms</code>, <code>preview.step.join_ms</code>, <code>preview.step.suggest_ms</code>, <code>preview.error_rate</code>, <code>parity.detect.ms</code>, <code>preview.persist.latency_ms</code>. <br><strong>Alert thresholds:</strong> <br>1. Critical: <code>preview.error_rate</code> > 1% sustained for 15min for regulated workloads. <br>2. Medium: <code>preview.duration_ms</code> P95 exceeding SLO. <br>3. Parity alert: golden fixture mismatch triggers urgent investigation. </td></tr><tr><td data-label="REG_Preview — Per-function Expert Technical Breakdown"> <strong>Security, evidence access & PII controls (operational)</strong><br>1. EvidenceStore encryption, HSM-backed keys for signing manifests. <br>2. Evidence retrieval requires <code>EvidenceAccessApprovalFlow</code> with recorded <code>approvalsRef</code> and chain-of-custody entries. <br>3. PII redaction policies enforced in redaction pipeline; any redaction overrides require compliance approvals and <code>approvalsRef</code> recorded. <br>4. No long-lived credentials stored in preview pipeline; ephemeral tokens used for downstream integrations. <br>5. Audit logs append-only and signed in rotation. </td></tr><tr><td data-label="REG_Preview — Per-function Expert Technical Breakdown"> <strong>Conceptual Power Query (PQ) guidance — architecture & patterns (no code)</strong><br>1. <strong>Canonicalization-first approach:</strong> in PQ create a dedicated canonicalization query that implements the canonical token rules (NFKC, casefold, trimming, punctuation policy) and expose it as a reusable table to upstream queries. <br>2. <strong>Deterministic sampling:</strong> implement hash-ranking by computing a keyed hash over <code>rowId</code> and sorting by rank to select top N; record <code>samplingMeta</code> in a hidden sheet for parity. <br>3. <strong>Mapping application:</strong> keep the mapping snapshot as a separate query (imported from <code>map_&lt;mapHash&gt;.json</code>) and perform a stable merge on canonical keys. <br>4. <strong>Aggregation:</strong> use <code>Group By</code> with canonical key order and compute sums on minor-unit integer columns; export canonical CSV with canonical column ordering. <br>5. <strong>Packaging & checksums:</strong> export canonical CSVs and compute checksums via trusted COM helper or backend service called through safe, authenticated connector; write the <code>preview_manifest</code> back to a hidden sheet for persistence. <br>6. <strong>Performance considerations:</strong> avoid heavy transformations in UI thread; for large datasets push to backend or schedule off-UI worker jobs. <br>7. <strong>Testability:</strong> include <code>mChecksum</code> outputs for PQ queries as part of golden fixtures; CI parses exported CSVs and compares checksums. </td></tr><tr><td data-label="REG_Preview — Per-function Expert Technical Breakdown"> <strong>Conceptual DAX guidance — modeling preview results for analysts (no code)</strong><br>1. <strong>Minor-units math:</strong> store amounts as integers <code>MinorUnits</code> to avoid floating-point drift; DAX measures should operate over integer columns and present formatted decimals only in UI layers. <br>2. <strong>Variance measures:</strong> implement <code>Variance = SUM(PayrollMinorUnits) - SUM(GLMinorUnits)</code>, <code>AbsVariance = ABS(Variance)</code> and <code>RelativeVariancePct = IF(SUM(GLMinorUnits)=0, BLANK(), DIVIDE(AbsVariance, ABS(SUM(GLMinorUnits))))</code>. <br>3. <strong>BeyondTolerance measure:</strong> use plan parameters to compute <code>IF(GLAmount=0, AbsVariance &gt; AbsThreshold, RelativeVariancePct &gt; TolerancePct)</code> and surface triage buckets; persist plan thresholds in a slicer table to support scenario analysis. <br>4. <strong>Triage ranking:</strong> compute <code>triagePriority</code> combining <code>AbsVariance</code>, <code>RelativeVariancePct</code> and <code>accountSensitivityFactor</code>. <br>5. <strong>Suggestion acceptance tracking:</strong> create a measure counting accepted suggestions and compute <code>SuggestionAcceptanceRate</code> by <code>confidenceBucket</code>. <br>6. <strong>Audit traceability:</strong> include <code>planId</code> and <code>previewHash</code> in visuals metadata to facilitate drill-through to evidence. </td></tr><tr><td data-label="REG_Preview — Per-function Expert Technical Breakdown"> <strong>Extensibility & advanced patterns</strong><br>1. <strong>DB pushdown for scale:</strong> store canonicalization SQL templates to compute canonical groupKey in-database and export canonical aggregates for join; ensures parity when canonicalization SQL is kept identical to in-memory rules. <br>2. <strong>Streaming previews:</strong> for extremely large ingests, allow streaming sampling and partial aggregation with checkpointing; preview manifest must record chunk offsets and partial checksums. <br>3. <strong>Canary previews:</strong> support <code>canaryPlan</code> that runs small cohort previews on changed mapping snapshots to detect large deltas before hot-swap. <br>4. <strong>Model-based suggestions:</strong> optional ML model for suggestion ranking must be versioned and persisted in suggestion metadata with <code>modelVersion</code> and <code>explainabilityBreakdown</code>. All ML-based suggestions must include provenance and be auditable. </td></tr><tr><td data-label="REG_Preview — Per-function Expert Technical Breakdown"> <strong>Migration manifest & semantic-change governance (quick template)</strong><br>When changing sampling, canonicalization, rounding, or packaging, the <code>migration_manifest</code> must include: <code>migrationId</code>, <code>author</code>, <code>createdTs</code>, <code>changeRationale</code>, <code>affectedArtifacts[]</code> (before/after sample diffs), <code>goldenFixtures[]</code> (pre/post sha256), <code>canaryPlan</code> (planId, cohort size, KPIs), <code>rollbackPlan</code> (previous snapshots), <code>approvals[]</code> (ownerIDs & timestamps), and <code>testMatrix</code>. CI must run golden comparison and smoke preview harnesses before permitting rollouts. </td></tr><tr><td data-label="REG_Preview — Per-function Expert Technical Breakdown"> <strong>Compliance & regulator packaging (what to include)</strong><br>For regulator packages include: <code>preview_manifest</code> signed, <code>mappingSnapshot</code> signatures, <code>migration_manifest</code> (if any), golden fixtures, <code>recon_report</code> with <code>reportHash</code> and <code>chainOfCustody</code>, and <code>forensic_manifest</code> with artifact checksums. Provide human-readable deterministic reproducibility narrative mapping <code>planSeed</code> → <code>previewHash</code>. </td></tr><tr><td data-label="REG_Preview — Per-function Expert Technical Breakdown"> <strong>Forensic & incident response playbook (concise actionable steps)</strong><br>1. <strong>Detection:</strong> capture alert and <code>correlationId</code>. <br>2. <strong>Containment:</strong> disable UI artifact access and suspend applies if necessary. <br>3. <strong>Evidence collection:</strong> run <code>forensic_pack --correlation &lt;id&gt;</code> to stage audit tail, preview manifest, mapping snapshots, ingest manifests, and worker logs; compute checksums and archive to WORM. <br>4. <strong>Analysis & remediation:</strong> replay preview in isolated CI via <code>PreviewReplay</code>. <br>5. <strong>Reporting & closure:</strong> generate <code>incident_report</code> with <code>forensic_manifest</code> and regulatory package. </td></tr><tr><td data-label="REG_Preview — Per-function Expert Technical Breakdown"> <strong>Final verification checklist (10 required checks before rolling regulated change)</strong><br>1. Schema validation of preview manifest and artifacts. <br>2. Cross-runtime canonicalization parity tests for <code>rowId</code>, aggregates, and <code>previewHash</code>. <br>3. Sampling parity vector tests for seeded PRNG. <br>4. Rounding/residual absorption parity tests across PQ/VBA/backend. <br>5. Evidence persist + sign/verify roundtrip. <br>6. Plan rehydration and <code>planSeed</code> recompute equality. <br>7. Mapping snapshot integrity verification (checksum match). <br>8. PII redaction scan on sample UI artifacts. <br>9. Performance microbench for target sample sizes. <br>10. Owner approvals recorded and migration manifest present for semantic changes. </td></tr></tbody></table></div><div class="row-count">Rows: 32</div></div><script src="assets/xlsx.full.min.js?v=1758605028" defer></script>
<script src="assets/script.js?v=1759748863" defer></script>
<script src="assets/worker.js?v=1758331710" defer></script>
<script>
(function(){
  const template = "{table}_{date}";
  const userVal = "";
  const hrefPrefix = "assets";
  function formatName(tableName) {
    const date = (new Date()).toISOString().slice(0,10);
    return template.replace('{table}', tableName).replace('{date}', date).replace('{user}', userVal);
  }
  const btn = document.getElementById('exportBtn');
  if(btn) {
    btn.addEventListener('click', async function() {
      try {
        const html = document.documentElement.outerHTML;
        if(html.length > 2000000) { alert('Export refused: html too large'); return; }
        if(window.Worker) {
          const workerUrl = (function(){ try{ return hrefPrefix + '/worker.js'; }catch(e){ return null; } })();
          if(workerUrl) {
            try {
              const worker = new Worker(workerUrl);
              worker.postMessage({html: html, format: 'pdf'});
              worker.onmessage = function(e) { console.log('worker:', e.data); alert('Worker replied: '+(e.data.msg||e.data.status)); };
            } catch(err) { console.warn('Worker create failed', err); alert('Worker not available'); }
          } else { alert('Worker not available'); }
        } else { alert('Export worker not supported in this environment.'); }
      } catch(err) { console.warn('Export failed', err); alert('Export worker not available. See console for details.'); }
    });
  }
})();
window.addEventListener('load', function(){ try{ document.querySelectorAll('.table-wrapper').forEach(function(e){ e.style.opacity='1'; }); }catch(e){} });
</script>
</div>
</body>
</html>