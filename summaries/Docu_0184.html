<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='utf-8'><meta name='viewport' content='width=device-width,initial-scale=1'>
<title>Tables Viewer v2.1</title>
<style>:root{--main-header-height:56px;} .table-wrapper{opacity:0;min-height:24px;transition:opacity .12s linear}</style>
<link rel="stylesheet" href="assets/style.css?v=1769960840">
<link rel="stylesheet" href="assets/overrides.css?v=1771304658">
</head><body>
<div id="tables-viewer" role="region" aria-label="Tables viewer">
<div id="stickyMainHeader">
<div id="tv-header">
<div><h1>Tables Viewer v2.1</h1></div>
<div style="display:flex;gap:8px;align-items:center;">
<input id="searchBox" class="tv-search" type="search" placeholder="Search" aria-label="Search tables" />
<button id="modeBtn" type="button" onclick="toggleMode()" aria-label="Toggle theme">Theme</button>
<button id="toggleAllBtn" type="button" aria-label="Toggle all" onclick="toggleAllTables()">Collapse All Tables</button>
<button id="copyAllPlainBtn" class="copy-btn" type="button" onclick="copyAllTablesPlain()" aria-label="Copy all tables as plain text">Copy All Tables (Plain Text)</button>
<button id="copyAllTablesBtn" class="copy-all-btn" type="button" onclick="copyAllTablesMarkdown()" aria-label="Copy All Tables (Markdown)">Copy All Tables (Markdown)</button>
<button id="copyAllMdBtn" style="display:none" aria-hidden="true"></button>
<button id="resetAllBtn" type="button" onclick="resetAllTables()" aria-label="Reset All Tables">Reset All Tables</button>
</div></div>
<script>
(function(){
  function ensureDelegation(){
    try {
      var vis = document.getElementById('copyAllTablesBtn');
      var alias = document.getElementById('copyAllMdBtn');
      if(!vis || !alias) return;
      alias.addEventListener = function(type, listener, options){
        vis.addEventListener(type, listener, options);
      };
      alias.removeEventListener = function(type, listener, options){
        vis.removeEventListener(type, listener, options);
      };
      Object.defineProperty(alias, 'onclick', {
        set: function(fn){ vis.onclick = fn; },
        get: function(){ return vis.onclick; },
        configurable: true
      });
      alias.focus = function(){ vis.focus(); };
      alias.blur = function(){ vis.blur(); };
    } catch(e) {
      try{ console && console.warn && console.warn('alias delegation failed', e); }catch(_){} 
    }
  }
  if(document.readyState === 'loading'){
    document.addEventListener('DOMContentLoaded', ensureDelegation);
  } else {
    ensureDelegation();
  }
})();
</script>
<noscript><div style='color:#b91c1c'>JavaScript is disabled. Tables will be shown statically. For large tables enable JS for virtualization.</div></noscript>
<div id="tocBar" role="navigation" aria-label="Table of contents"><ul><li class="toc-item"><a class="toc-link" href="#Table1">Table 1</a></li>
<li class="toc-item"><a class="toc-link" href="#Table2">Table 2</a></li>
<li class="toc-item"><a class="toc-link" href="#Table3">Table 3</a></li>
<li class="toc-item"><a class="toc-link" href="#Table4">Table 4</a></li>
<li class="toc-item"><a class="toc-link" href="#Table5">Table 5</a></li>
<li class="toc-item"><a class="toc-link" href="#Table6">Table 6</a></li>
<li class="toc-item"><a class="toc-link" href="#Table7">Table 7</a></li></ul></div></div>
<div class="table-caption" id="Table1" data-table="Docu_0184_01" style="margin-top:2mm;margin-left:3mm;"><strong>Table 1</strong></div>
<div class="table-wrapper" data-table-id="table-1"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by REG_Aggregation — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">REG_Aggregation — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="REG_Aggregation — Per-function Expert Technical Breakdown"> <strong>Scope & intent (one-paragraph):</strong> REG_Aggregation deterministically converts canonical payroll and GL row inputs into canonical aggregates and variance rows suitable for deterministic previews, suggestion engines, evidence packages, and regulated exports. The module enforces integer minor-unit arithmetic, canonical group-key construction, deterministic sampling, full-outer joins, tolerance/materiality evaluation, chunked processing and checkpointing for scale, DB/warehouse pushdown orchestration with parity validation, canonical serialization and signed manifest generation for evidence and audit. All recipes are versioned via <code>aggCanonicalVersion</code> and any semantic change requires a migration manifest, golden fixtures and CI gating. </td></tr><tr><td data-label="REG_Aggregation — Per-function Expert Technical Breakdown"> <strong>Top-level design principles & invariants (authoritative)</strong><br>1. <strong>Determinism:</strong> given identical inputs — canonical rows with <code>ingestChecksum</code>, mapping snapshot <code>mapHash</code>, <code>planSeed</code>, and <code>aggCanonicalVersion</code> — the canonical outputs (CSV bytes and their checksums) must be identical across language runtimes and platforms. <br>2. <strong>Integer arithmetic only:</strong> all canonical numeric operations are performed on <code>AmountMinorUnits</code> integers; <code>scale</code> is explicit and recorded. No floating-point sums are used for canonical artifact computation. <br>3. <strong>Canonical versioning:</strong> <code>aggCanonicalVersion</code> encodes exact normalization, escaping, ordering, rounding and serialization rules; any change bumps the version and requires migration governance. <br>4. <strong>Immutability & evidence-first:</strong> aggregate artifacts are persisted immutably through <code>REG_EvidenceAudit</code>; manifests include provenance and retention metadata. <br>5. <strong>Separation of concerns:</strong> aggregation only groups and sums canonical rows; mapping, PII tokenization and business transformation occur upstream or downstream as clearly partitioned modules. <br>6. <strong>Chunking & resume:</strong> streaming with atomic checkpoints ensures resume and idempotency for very large inputs. <br>7. <strong>Pushdown with parity:</strong> DB/warehouse pushdown permitted for scale but must be validated via sample parity checks before used in regulated flows. </td></tr><tr><td data-label="REG_Aggregation — Per-function Expert Technical Breakdown"> <strong>Public API surface (canonical signatures — descriptive)</strong><br><code>ComputeVarianceAggs(planId, payrollRows, glRows, aggKeys, scale, tolerancePolicy, options)</code> — orchestrator: validate inputs, select execution mode (<code>in-memory | streamed chunk | pushdown</code>), compute payroll and GL aggregates, full-outer join, evaluate tolerance, produce <code>varianceRows[]</code>, persist canonical artifacts and manifest via evidence service, emit audits and metrics.<br><code>AggregateGroup(rows, groupKey, keys, scale, planSeed, sampleSize)</code> — per-group aggregator computing integer sums, Welford stats, min/max, deterministic sampleRowRefs; returns canonical aggregate record.<br><code>FixedDecimalSum(values, scale)</code> — robust integer accumulation for minor-units with deterministic decimal parsing and rounding semantics; returns big-integer minor-units sum.<br><code>FullOuterJoinAggs(payrollAggDict, glAggDict, keys, scale, tolerancePolicy, planSeed, relativeScalingFactor)</code> — deterministic union join producing ordered <code>varianceRows</code> including <code>BeyondTolerance</code> flags and rational reasons.<br><code>EvaluateTolerance(payrollMinorUnits, glMinorUnits, policy, groupMeta)</code> — materiality decision engine returning <code>{BeyondTolerance, reason, absVariance, relativePctScaled}</code> with deterministic inclusive/exclusive semantics documented in manifest.<br><code>ComputeGroupKey(row, aggKeys, aggCanonicalVersion)</code> — canonical key builder applying Unicode NFKC, trimming, whitespace collapse, configured casefold, delimiter escaping and joining in fixed order; returns <code>groupKey</code> and optionally <code>groupKeyHash</code>.<br><code>DeterministicSampleRowRefs(sortedRowIds, planSeed, sampleSize, samplingMeta)</code> — seeded PRNG deterministic sampling (Fisher–Yates or reservoir) producing reproducible sampleRowRefs; samplingMeta recorded in manifest.<br><code>PushdownAggregateToDB(planId, querySpec, aggKeys, scale, paritySampleSpec)</code> — generate canonical SQL templates, submit job, export canonical CSV, validate parity on representative sample and return pushdown manifest with <code>exportRef</code> and parity result.<br><code>StreamedAggregateChunk(chunkRows, chunkOffset, stateRef, scale, checkpointOpts)</code> — process chunk, merge partial per-group state into persistent checkpoint atomically and return next offset and partial checksums; idempotent on retry.<br><code>SerializeAggCSV(aggRows, canonicalVersion, fieldOrder)</code> — produce canonical UTF-8 CSV bytes and <code>sha256</code> checksum using explicit quoting/escaping and deterministic field order; required for evidence artifacts.<br><code>BuildAggregationManifest(planId, artifacts, metrics, samplingMeta, retention, approvalsRef)</code> — assemble canonical manifest JSON, compute <code>manifestHash</code>, persist via evidence API and emit audit event. </td></tr><tr><td data-label="REG_Aggregation — Per-function Expert Technical Breakdown"> <strong>Per-function technical breakdown — exhaustive</strong> </td></tr><tr><td data-label="REG_Aggregation — Per-function Expert Technical Breakdown"> <strong><code>ComputeVarianceAggs(...)</code> — orchestrator, contract, inputs, invariants, step-by-step behavior, failure modes, recovery, observability & tests</strong><br><strong>Purpose & contract:</strong> orchestrate aggregation end-to-end producing three canonical artifacts and a manifest; guarantee deterministic outputs; persist auditable evidence. <br><strong>Inputs:</strong> <code>planId</code> (string), <code>payrollRows</code> & <code>glRows</code> (collections of canonical row objects with <code>rowId</code>, <code>AmountMinorUnits</code>, <code>scale</code>, <code>Currency</code>, field tokens), <code>aggKeys</code> (ordered list), <code>scale</code> (integer minor-unit scale), <code>tolerancePolicy</code> (object), <code>options</code> (memoryBudget, pushdownConfig, chunkSizeThreshold, sampleSize). <br><strong>Outputs:</strong> returns <code>varianceRows[]</code>; writes artifacts: <code>before_aggregates.csv</code>, <code>gl_aggregates.csv</code>, <code>variance_report.csv</code> and <code>aggregation_manifest.json</code> into evidence store; emits <code>payroll.aggregation.completed</code> audit with <code>manifestRef</code> and <code>manifestHash</code>. <br><strong>High-level algorithm (detailed):</strong><br>1. <strong>Validate preconditions:</strong> check that every row includes <code>AmountMinorUnits</code> and <code>scale</code>. If any mismatches are found emit <code>AGG_SCALE_MISMATCH</code> diagnostic and persist <code>ingestDiagnostics</code> evidence. <br>2. <strong>Compute data sizes:</strong> compute <code>rowsTotal = payrollRows.count + glRows.count</code>. Compare to <code>InMemoryThreshold</code> and <code>PushdownThreshold</code> in options to select execution path deterministically. <br>3. <strong>Choose execution engine:</strong> if <code>rowsTotal &lt;= InMemoryThreshold</code> and memoryBudget ok → In-memory aggregator; else if <code>pushdownConfig.enabled</code> and <code>rowsTotal &gt;= PushdownThreshold</code> → <code>PushdownAggregateToDB</code>; else → Streamed chunk mode using <code>StreamedAggregateChunk</code>. <br>4. <strong>Enrich rows with canonical groupKey:</strong> for each row compute <code>groupKey = ComputeGroupKey(row, aggKeys, aggCanonicalVersion)</code> and attach to row object; store <code>groupKeyHash</code> only if allowed by PII policy. <br>5. <strong>Aggregate payroll & GL:</strong> run <code>AggregateGroup</code> per group to construct payrollAggDict and glAggDict. If chunked, update partial state per chunk and finalize merges after all chunks processed; if pushdown used, fetch exported CSV and transform to canonical aggregate dict. <br>6. <strong>Serialize payroll and GL aggregates:</strong> <code>SerializeAggCSV</code> called for both sets producing canonical bytes and <code>sha256</code> checksums; persist via evidence service; record evidenceRefs. <br>7. <strong>Full outer join:</strong> <code>FullOuterJoinAggs</code> invoked with payrollAggDict and glAggDict plus <code>planSeed</code>. This produces ordered <code>varianceRows</code>. <br>8. <strong>Persist variance CSV:</strong> canonical serialize varianceRows and persist to evidence store. <br>9. <strong>Build & persist manifest:</strong> call <code>BuildAggregationManifest</code> which includes artifact refs, <code>aggChecksum</code> (derived from variance bytes), metrics and <code>samplingMeta</code>. Persist manifest and emit audit. <br>10. <strong>Return varianceRows collection</strong> to caller for downstream preview/suggestion steps. <br><strong>Invariants & guarantees:</strong><br>- Execution path selection is deterministic given <code>rowsTotal</code>, <code>InMemoryThreshold</code>, <code>PushdownThreshold</code> and memoryBudget. <br>- All persisted artifacts include <code>aggCanonicalVersion</code>, <code>planSeed</code>, <code>mapHash</code> and relevant checksums in manifest. <br><strong>Failure modes & recovery strategies:</strong><br>- <strong>Scale/format mismatch</strong>: emit <code>AGG_SCALE_MISMATCH</code> with evidenceRef and abort; remediation: re-run ingest canonicalization or call normalization helper. <br>- <strong>OOM in in-memory mode</strong>: detect quickly; fall back to chunked mode; persist partial artifacts/checkpoints and emit <code>agg.fallback.oom</code>. <br>- <strong>Checkpoint corruption:</strong> detect via state checksum mismatch; attempt to restore prior checkpoint snapshot from evidence; if none exist, abort and require manual triage with <code>forensic_manifest</code>. <br>- <strong>Pushdown parity failure:</strong> emit <code>agg.pushdown.parity_failed</code> with sample diff artifact and fallback to chunked local path; preserve both artifacts and include <code>parityFailure</code> metadata in manifest. <br><strong>Observability & audit fields:</strong> emit metrics <code>agg.compute.duration_ms</code>, <code>agg.rows_processed_payroll</code>, <code>agg.rows_processed_gl</code>, <code>agg.groups_count</code>, <code>agg.fallback_to_db</code> boolean; audit rows: <code>payroll.aggregation.started</code>, <code>payroll.aggregation.completed</code>, <code>payroll.aggregation.failed</code>. Each audit includes <code>planId</code>, <code>ingestChecksums</code>, <code>mapHash</code>, <code>aggCanonicalVersion</code>, <code>manifestRef</code>. <br><strong>Tests (CI gating):</strong> unit tests for groupKey recipe, deterministic sampling parity, Welford parity; integration tests for in-memory and chunked flows; pushdown parity tests on sample cohorts; golden artifact parity checks that block PRs if changed for regulated fixtures. </td></tr><tr><td data-label="REG_Aggregation — Per-function Expert Technical Breakdown"> <strong><code>AggregateGroup(rows, groupKey, keys, scale, planSeed, sampleSize)</code> — per-group aggregator (complete)</strong><br><strong>Purpose & contract:</strong> given all rows for a canonical group, compute deterministic aggregate record: sums, count, min, max, mean, sample stddev, sample row references and ancillary metadata. The function is pure for aggregation (no IO) and returns a dictionary with typed fields. <br><strong>Input contract:</strong> <code>rows</code> is a collection of canonical row objects where <code>AmountMinorUnits</code> is integer and <code>scale</code> matches plan <code>scale</code>. <code>groupKey</code> and <code>keys</code> used for verification. <code>planSeed</code> used for deterministic sampling. <code>sampleSize</code> is target number of sampleRowRefs. <br><strong>Returned canonical aggregate record fields:</strong> <code>groupKey</code>, <code>groupKeyHash</code> (optional), <code>sumMinorUnits</code> (big-int), <code>rowCount</code>, <code>minMinorUnits</code>, <code>maxMinorUnits</code>, <code>meanMinorUnitsRational</code> (numerator/denominator for audit), <code>stdDevSampleScaled</code> (integer scaled by <code>stdScale</code>), <code>sampleRowRefs[]</code>, <code>currencySet[]</code>, <code>notes[]</code>. <br><strong>Algorithm details:</strong><br>- <strong>Sum:</strong> use <code>FixedDecimalSum</code> for robust parse & 128-bit accumulation. <br>- <strong>Mean & variance:</strong> use Welford online algorithm adapted to integer minor-units; maintain <code>n</code>, <code>meanNum</code> and <code>m2</code> as big integers to produce exact sample variance rational; store scaled integer <code>stdDevSampleScaled</code> for convenience. <br>- <strong>Min/Max:</strong> track integer min and max across rows. <br>- <strong>Sample selection:</strong> build <code>sortedRowIds</code> lexicographically then call <code>DeterministicSampleRowRefs(sortedRowIds, planSeed, sampleSize, samplingMeta)</code> to pick representative row ids; this ensures reproducible sample row references across runtimes. <br><strong>Edge cases & behavior:</strong> empty <code>rows</code> → return <code>rowCount=0</code>, numeric fields null; multiple currencies → record <code>currencySet</code> (trigger <code>CURRENCY_MISMATCH</code> note) and leave numeric comparison legal but flagged. <br><strong>Overflow handling:</strong> detect if sum exceeds native integer capacity; if so switch to arbitrary precision big-integer implementation and annotate <code>AGG_BIGINT_FALLBACK</code> in notes. <br><strong>Tests:</strong> Welford parity tests against high-precision library, sample reproducibility tests, negative amounts handling, currencySet detection. </td></tr><tr><td data-label="REG_Aggregation — Per-function Expert Technical Breakdown"> <strong><code>FixedDecimalSum(values, scale)</code> — deterministic decimal parsing & integer accumulation (complete)</strong><br><strong>Purpose & contract:</strong> parse numeric inputs (strings or numeric types) into minor-unit integers deterministically, apply rounding policy if necessary, and return exact integer sum using big-integer accumulator. Parsing is deterministic across locales when a <code>locale</code> param is supplied; otherwise parsing assumes canonical decimal dot notation. <br><strong>Parsing rules & rounding:</strong><br>- Remove grouping separators if present (policy must define allowed grouping characters), convert locale decimal separator to <code>.</code> if locale specified, validate sign and digits. <br>- If fractional digits > <code>scale</code>, apply deterministic rounding mode (default: Banker's rounding — round half to even); document in <code>aggCanonicalVersion</code>. <br>- If fractional digits < <code>scale</code>, pad with zeros to match scale. <br><strong>Accumulator implementation:</strong> prefer built-in 128-bit signed integer; fallback to bignum library if input ranges or group totals exceed 128-bit. <br><strong>Error handling:</strong> invalid numeric format → return structured error <code>AGG_INVALID_AMOUNT</code> with example <code>rowRef</code> and persist to <code>ingestDiagnostics</code> evidence. <br><strong>Tests required:</strong> culture-aware parsing, rounding edge cases (.5 boundaries), large-sum invariants, permutation invariance for commutative addition with integer sums. </td></tr><tr><td data-label="REG_Aggregation — Per-function Expert Technical Breakdown"> <strong><code>FullOuterJoinAggs(payrollAggDict, glAggDict, keys, scale, tolerancePolicy, planSeed, relativeScalingFactor)</code> — join & variance engine (complete)</strong><br><strong>Purpose & contract:</strong> take two keyed aggregate dictionaries and produce a deterministic, ordered list of <code>varianceRows</code> with canonical fields defined in a strict field order for serialization. Each output row contains numeric fields in minor-units and integer scaled relative percentages. <br><strong>Exact output fields & canonical order (example):</strong> <code>GroupKey</code>, <code>GroupKeyHash</code> (optional), <code>PayrollMinorUnits</code>, <code>GLMinorUnits</code>, <code>VarianceMinorUnits</code>, <code>AbsVariance</code>, <code>RelativePctScaled</code>, <code>rowCountPayroll</code>, <code>rowCountGL</code>, <code>Side</code>, <code>BeyondTolerance</code>, <code>ToleranceReason</code>, <code>sampleRowRefs</code>, <code>currencySet</code>, <code>notes</code>. Field order must be adhered to for canonical CSV bytes. <br><strong>Algorithm specifics:</strong><br>- Compute union of keys sorted lexicographically (canonical collation). <br>- For each <code>groupKey</code> produce payroll and GL numeric values (null if missing). <br>- Compute <code>Variance = Payroll - GL</code> using integer arithmetic. <br>- Compute <code>AbsVariance = ABS(Variance)</code>. <br>- Compute <code>RelativePctScaled</code> as <code>round( AbsVariance * relativeScalingFactor / ABS(GL) )</code> when GL != 0; if GL == 0, set <code>RelativePctScaled = NULL</code> and rely on absolute threshold to decide materiality. Use integer math to avoid floating rounding drift. <br>- Determine <code>Side</code>: <code>Matched</code> if both present, <code>PayrollOnly</code> or <code>GLOnly</code> otherwise. <br>- Merge <code>sampleRowRefs</code> deterministically: combine payroll sample then GL sample and deduplicate while preserving deterministic order. <br>- Call <code>EvaluateTolerance</code> to compute <code>BeyondTolerance</code> and <code>ToleranceReason</code>. <br><strong>Edge-case rules:</strong> multi-currency present → include <code>currencySet</code> and mark <code>ToleranceReason = &quot;CURRENCY_MISMATCH&quot;</code> if not normalized upstream. For groups with both zeroes, treat as matched with zero variance (not beyond tolerance). <br><strong>Deterministic serialization:</strong> ensure output <code>varianceRows</code> list order and internal <code>sampleRowRefs</code> order are consistent to preserve artifact checksum reproducibility. <br><strong>Tests:</strong> union correctness across input permutations, GL=0 paths, boundary tolerance tests, relative percentage exactness under various denominators and scaling factors. </td></tr><tr><td data-label="REG_Aggregation — Per-function Expert Technical Breakdown"> <strong><code>EvaluateTolerance(payrollMinorUnits, glMinorUnits, policy, groupMeta)</code> — materiality decision engine (complete)</strong><br><strong>Purpose & contract:</strong> deterministically decide whether the variance is beyond configured thresholds and return a structured result including a precise reason code. <br><strong>Policy shape:</strong> <code>{TolerancePct: decimal, AbsThresholdMinorUnits: int, MinMaterialAmount: int, perAccountOverrides: [{rangeSpec, AbsThreshold, TolerancePct}], relativeScalingFactor: int, inclusive: booleanFlag}</code> — manifest must document <code>relativeScalingFactor</code> and inclusion policy. <br><strong>Decision algorithm (deterministic steps):</strong><br>1. If both values null → return <code>BeyondTolerance=false</code> with reason <code>NO_DATA</code>. <br>2. Compute <code>absVariance = ABS(payroll - gl)</code> when either present. <br>3. If <code>absVariance &lt; MinMaterialAmount</code> → <code>BeyondTolerance=false</code> (materiality floor). <br>4. If <code>GL</code> is null or <code>GL==0</code> → <code>BeyondTolerance = (absVariance &gt; AbsThreshold)</code> (or <code>&gt;=</code> if inclusive policy set); reason <code>ABS_THRESHOLD</code>. <br>5. Else evaluate <code>relativeScaled = floor(absVariance * relativeScalingFactor / ABS(GL))</code> and compare to <code>TolerancePct * relativeScalingFactor</code> using integer math; <code>BeyondTolerance = relativeScaled &gt; (TolerancePct * relativeScalingFactor)</code> unless inclusive flag set. <br>6. Check <code>perAccountOverrides</code> and if matched, apply override thresholds first. <br>7. Return <code>{BeyondTolerance:Boolean, reason:String, absVariance:Int, relativePctScaled:Int}</code>. <br><strong>Canonical semantics & manifest documentation:</strong> the manifest must declare <code>relativeScalingFactor</code> and whether thresholds use strict <code>&gt;</code> or <code>&gt;=</code> semantics. <br><strong>Tests:</strong> boundary inclusive/exclusive semantics, perAccountOverrides precedence, zero denominators, negative GL amounts and absolute thresholds. </td></tr><tr><td data-label="REG_Aggregation — Per-function Expert Technical Breakdown"> <strong><code>ComputeGroupKey(row, aggKeys, aggCanonicalVersion)</code> — canonical group key recipe (complete)</strong><br><strong>Purpose & contract:</strong> deterministically convert ordered field values into a canonical composite <code>groupKey</code> string used as the unique grouping token. The canonical recipe is versioned and must be identical across implementations for parity. <br><strong>Canonical recipe (must be applied in exact sequence):</strong><br>1. For each <code>key</code> in <code>aggKeys</code>, extract <code>rawValue = row[key]</code>; if absent, use empty string. <br>2. <strong>Type canonicalization:</strong> if value is numeric, format as a canonical decimal string with fixed decimal digits if relevant; if date, format as <code>YYYY-MM-DD</code> (no time zone) unless canonicalVersion defines otherwise. <br>3. <strong>Unicode normalization:</strong> apply NFKC to all textual tokens. <br>4. <strong>Whitespace normalization:</strong> trim leading/trailing whitespace, collapse internal runs of whitespace to single ASCII space. <br>5. <strong>Casefolding:</strong> apply lower-case transformation if configured in <code>aggCanonicalVersion</code>. <br>6. <strong>Punctuation normalization:</strong> remove non-significant punctuation per policy (e.g., remove parentheses/dots when <code>stripPunctuation=true</code>), or map via <code>punctuationMap</code> if supplied. <br>7. <strong>Escape delimiter:</strong> replace <code>\</code> with <code>\\</code> and <code>|</code> with <code>\|</code> to avoid ambiguous separators. <br>8. <strong>Join tokens:</strong> join normalized tokens using <code>|</code> in exact order. <br>9. <strong>Truncation policy:</strong> if resulting key exceeds <code>MaxKeyLen</code> bytes, truncate deterministically to <code>MaxKeyLen</code> and set <code>truncated=true</code> in group metadata; full key may be persisted encrypted into <code>evidenceRef</code> if allowed. <br>10. <strong>Return:</strong> canonical <code>groupKey</code> string and optionally <code>groupKeyHash = sha256(UTF8(groupKey))</code>. <br><strong>Parity tests:</strong> cross-runtime key generation vectors, escaping roundtrip tests, language/locale sensitivity tests, truncation reproducibility. </td></tr><tr><td data-label="REG_Aggregation — Per-function Expert Technical Breakdown"> <strong><code>DeterministicSampleRowRefs(sortedRowIds, planSeed, sampleSize, samplingMeta)</code> — deterministic sampling (complete)</strong><br><strong>Purpose & contract:</strong> select a reproducible sample of rowIds for analyst drill-down and evidence traces. Determinism must hold across runtime implementations. <br><strong>Algorithm options (explicit & versioned):</strong><br>- <strong>Fisher–Yates seeded shuffle</strong>: lexicographically sort <code>rowIds</code>, generate PRNG stream using <code>HMAC_DRBG_SHA256</code> seeded with <code>planSeed || groupKey</code>, perform Fisher–Yates shuffle and pick first <code>sampleSize</code>. <br>- <strong>Deterministic reservoir</strong>: for memory constrained groups, deterministic reservoir sampling seeded similarly. <br><strong>samplingMeta fields recorded in manifest:</strong> <code>{algorithm, prng, seed, version, notes}</code> to ensure replays use same algorithm. <br><strong>Note on PRNG changes:</strong> any change to algorithm or PRNG parameters requires migration manifest and sampling golden fixtures. <br><strong>Edge-case:</strong> if <code>sampleSize &gt;= rowCount</code>, return all <code>rowIds</code> in deterministic order. <br><strong>Tests:</strong> parity across implementations, reservoir equivalence test versus shuffle for small groups, seed/planSeed drift detection, sample reproducibility when rows are re-ordered. </td></tr><tr><td data-label="REG_Aggregation — Per-function Expert Technical Breakdown"> <strong><code>PushdownAggregateToDB(planId, querySpec, aggKeys, scale, paritySampleSpec)</code> — warehouse pushdown (complete)</strong><br><strong>Purpose & contract:</strong> offload aggregation to a warehouse for scalability, export canonical CSV aggregates ordered identically to local in-memory outputs, and validate parity using a representative sample before trusting pushdown results for regulated artifacts. <br><strong>Key responsibilities:</strong><br>- Generate canonical SQL using parameterized templates for supported warehouses (Postgres, BigQuery, Snowflake), ensuring canonical key expression approximates <code>ComputeGroupKey</code> or requires pre-normalized columns. <br>- Convert amounts to integer minor-units in SQL using deterministic rounding consistent with <code>FixedDecimalSum</code>. <br>- Ensure deterministic <code>ORDER BY</code> on canonical group key in SQL to guarantee identical export ordering. <br>- Export result into canonical CSV format (UTF-8, exact quoting rules) and persist to evidence store. <br>- Validate parity: extract <code>paritySampleSpec.sampleSize</code> rows from the export (or source) and run local aggregation to compute sample checksum; compare to server export sample checksum. <br><strong>Parity validation policy:</strong> if parity matches → mark <code>pushdownManifest.parity = &quot;ok&quot;</code>; if mismatch → mark <code>parity = &quot;mismatch&quot;</code>, attach sample diffs and move to fallback local aggregation path before using for regulated exports. <br><strong>Security & cost controls:</strong> parameterize queries, set resource limits, require <code>pushdownApprovalRef</code> for high-cost runs. <br><strong>Tests & CI:</strong> SQL generation correctness matrices across warehouses, sample parity automatic checks, injection resistance tests, cost projection gating. </td></tr><tr><td data-label="REG_Aggregation — Per-function Expert Technical Breakdown"> <strong><code>StreamedAggregateChunk(chunkRows, chunkOffset, stateRef, scale, checkpointOpts)</code> — chunked processing & checkpointing (complete)</strong><br><strong>Purpose & contract:</strong> process a sequential chunk of canonical rows, update persistent partial aggregation state (per group partial accumulators) and produce an atomic checkpoint allowing resume and idempotency. <br><strong>Chunk semantics & idempotency:</strong><br>- Each chunk invocation includes <code>chunkOffset</code>, <code>attemptId</code> and <code>chunkChecksum</code>. <br>- The <code>stateRef</code> is an opaque handle to a checkpoint object persisted atomically (temp file then rename) which includes <code>processedOffset</code>, serialized partialGroupStates, <code>attemptLog</code>, <code>stateChecksum</code> and <code>lastUpdatedTs</code>. <br>- If chunkOffset already recorded in checkpoint, function must treat chunk as already processed and return prior <code>nextOffset</code>. <br><strong>Partial group state mergeability:</strong> store per-group partial accumulators in mergeable form: <code>(n, meanNumerator, m2, min, max, sampleRowCandidates)</code> so partial states can be merged commutatively. Use merge rules equivalent to Welford chunk merge formulas. <br><strong>Atomic persistence & corruption detection:</strong> compute <code>stateChecksum = sha256(serializedState)</code> and verify on read; on checksum mismatch attempt to restore previous snapshot or raise <code>agg.checkpoint.corrupt</code> with evidenceRef. <br><strong>Failure & resume behavior:</strong> on transient worker error, caller will re-invoke with same chunkOffset and possibly different attemptId; implementation must not double-apply chunk contributions due to idempotency checks. <br><strong>Tests:</strong> simulate duplicate chunk delivery, checkpoint corruption injection, multi-worker partitioned chunk processing and group boundary stitching. </td></tr><tr><td data-label="REG_Aggregation — Per-function Expert Technical Breakdown"> <strong><code>SerializeAggCSV(aggRows, canonicalVersion, fieldOrder)</code> — canonical serialization (complete)</strong><br><strong>Purpose & contract:</strong> produce deterministic UTF-8 CSV bytes for an array of canonical aggregates using exact serialization rules documented by <code>aggCanonicalVersion</code>. Always produce <code>sha256</code> over the bytes. <br><strong>Serialization recipe (must be identical across runtimes):</strong><br>1. <strong>Header row:</strong> exact field names in <code>fieldOrder</code> in the specified order; header encoding UTF-8 with NFKC normalization. <br>2. <strong>Row order:</strong> <code>aggRows</code> must be in canonical order (typically lexicographic by <code>groupKey</code>). <br>3. <strong>Field representation rules:</strong> integers serialized as plain decimal strings with no leading zeros (except zero itself), null represented as empty field, arrays serialized as semicolon-separated lexicographically sorted items (e.g., <code>sampleRowRefs</code>), text fields NFKC normalized, trimmed, and quoted if required. <br>4. <strong>Escaping & quoting:</strong> per RFC4180 with deterministic rule: always quote text fields, do not quote integers; double any inner quote characters. <br>5. <strong>Line endings & whitespace:</strong> use <code>\n</code> only; no trailing spaces; final file ends with single <code>\n</code>. <br>6. <strong>Character encoding:</strong> UTF-8 without BOM. <br>7. <strong>Checksum:</strong> compute <code>sha256(UTF8(bytes))</code> and return both bytes and checksum. <br><strong>Tests:</strong> byte-for-byte parity across runtimes, quoting edge cases (commas, newlines, quotes), array ordering tests, newline normalization. </td></tr><tr><td data-label="REG_Aggregation — Per-function Expert Technical Breakdown"> <strong><code>BuildAggregationManifest(planId, artifacts, metrics, samplingMeta, retention, approvalsRef)</code> — manifest & evidence (complete)</strong><br><strong>Purpose & contract:</strong> produce canonical <code>aggregation_manifest</code> JSON that references artifacts, checksums, provenance and metrics; compute <code>manifestHash</code> and persist via REG_EvidenceAudit. Manifest must be canonicalized with stable key order before hashing. <br><strong>Required manifest fields:</strong> <code>manifestId</code>, <code>planId</code>, <code>ingestChecksums[]</code>, <code>mapHash</code>, <code>aggCanonicalVersion</code>, <code>rowsCount</code>, <code>groupsCount</code>, <code>scale</code>, <code>beforeAggregatesRef{path,checksum}</code>, <code>glAggregatesRef{}</code>, <code>varianceRef{}</code>, <code>aggChecksum</code>, <code>samplingMeta</code>, <code>metrics{durationMs,chunkStats}</code>, <code>createdTs</code>, <code>createdBy</code>, <code>retentionPolicy</code>, <code>approvalsRef</code> (if required). <br><strong>Signing & chain-of-custody:</strong> Phase-2: evidence service will sign <code>manifestHash</code> using KMS/HSM; store signature metadata attached to manifest. <br><strong>Audit emission:</strong> after persist emit <code>payroll.aggregation.completed{planId,manifestRef,manifestHash,aggChecksum}</code> and record <code>manifestRef</code> in plan execution audit trail. <br><strong>Tests:</strong> manifest schema validation, canonicalization parity tests, signature verification tests. </td></tr><tr><td data-label="REG_Aggregation — Per-function Expert Technical Breakdown"> <strong>Operational narratives — expanded examples & long scenarios</strong><br><strong>Small in-memory example:</strong> plan <code>p_small</code> with 400 payroll rows and 300 GL rows → <code>ComputeVarianceAggs</code> in-memory path. Group keys <code>[GLAccount, CostCenter]</code>. Aggregation completes in 320ms, <code>groupsCount=82</code>. <code>variance_report.csv</code> persisted; analyst preview consumes manifest and inspects <code>sampleRowRefs</code> for top 10 <code>BeyondTolerance</code> rows. <br><strong>Chunked stream example:</strong> plan <code>p_chunked</code> with 2.5M payroll rows. System runs <code>StreamedAggregateChunk</code> with chunkSize=100k; checkpoint created every chunk; after worker crash at chunk 13 resume from last checkpoint and finish; manifest persisted with <code>chunk.resume.count=1</code>. <br><strong>Pushdown example:</strong> plan <code>p_push</code> with 15M rows chooses <code>PushdownAggregateToDB</code>. SQL generated uses prenormalized <code>canonical_group_key</code> column computed at ingest. Warehouse job runs, exports canonical CSV; parity sample 100k rows validated OK; manifest includes <code>pushdownManifest.exportRef</code> and <code>parity=ok</code>. <br><strong>Migration scenario:</strong> change to <code>aggCanonicalVersion</code> (escape rules changed). Developer creates <code>migration_manifest</code> with before/after golden fixtures; CI runs parity checks; canary run executed for 1% cohort; acceptance metrics passed; mapping owner signs approvals and new canonical version deployed. </td></tr><tr><td data-label="REG_Aggregation — Per-function Expert Technical Breakdown"> <strong>Power Query (PQ) conceptual guidance (no code snippets)</strong><br>1. <strong>Precompute normalization upstream:</strong> implement canonical text normalization (NFKC, trim, whitespace collapse, optional casefold) in a dedicated PQ step so downstream aggregation is simpler and SQL pushdown can reuse normalized columns. <br>2. <strong>Compute minor-units early:</strong> add integer minor-units column as early as possible to avoid decimals. <br>3. <strong>Chunked PQ refresh:</strong> implement parameterized queries to pull data windows and persist partial aggregates to hidden tables or local caches for resume. <br>4. <strong>Deterministic export:</strong> ensure explicit <code>Table.Sort</code> on canonical groupKey before export and use consistent column order to guarantee stable CSV bytes. <br>5. <strong>Pushdown integration:</strong> when PQ query folding is available, prefer server aggregation but confirm canonical groupKey expressions are compatible with server functions; if not, require precomputed normalized columns. <br>6. <strong>Testing & parity:</strong> use small canonical fixtures and compute CSV sha256 in PQ environment to validate parity with other runtimes; CI should run PQ-based parity tests for partners using Power Query. </td></tr><tr><td data-label="REG_Aggregation — Per-function Expert Technical Breakdown"> <strong>DAX conceptual guidance (no code snippets)</strong><br>1. <strong>Aggregate import preference:</strong> import canonical aggregate artifacts rather than raw rows for large models to ensure deterministic reporting and reduce model complexity. <br>2. <strong>Integer measures:</strong> store and sum <code>AmountMinorUnits</code> exactly; convert to decimal only in presentation layer (divide by <code>10^scale</code>) to avoid float drift. <br>3. <strong>Variance measures:</strong> compute variances on preaggregated values; percent calculations should reference the canonical scaled integer <code>RelativePctScaled</code> to maintain parity. <br>4. <strong>Slicer alignment:</strong> ensure slicers and filters operate on descriptors present in aggregated artifacts to avoid introducing non-canonical filtering semantics. <br>5. <strong>Validation:</strong> for regulated deliveries, include checks that DAX visual totals match canonical <code>variance_report</code> aggregates before publishing. </td></tr><tr><td data-label="REG_Aggregation — Per-function Expert Technical Breakdown"> <strong>Observability, SLOs, alerts & dashboards (comprehensive)</strong><br><strong>Key metrics:</strong> <code>agg.compute.duration_ms</code>, <code>agg.rows_processed_payroll</code>, <code>agg.rows_processed_gl</code>, <code>agg.groups_count</code>, <code>agg.chunk.duration_ms</code>, <code>agg.chunk.checkpoint_latency_ms</code>, <code>agg.pushdown.parity_mismatch_count</code>, <code>agg.fallback_to_db_count</code>. <br><strong>SLOs (recommended):</strong> small-plan median compute time <500ms; mid-plan (<5k groups) median <3s; chunk checkpoint latency <200ms; parity mismatch rate <0.1% for golden fixtures. <br><strong>Alert rules:</strong> critical if <code>agg.pushdown.parity_mismatch_count</code> > 0 for regulated workflows; warning if average chunk processing time rises >2x baseline; critical if <code>agg.compute.duration_ms</code> for small plans exceeds 5s continuously for 15 minutes. <br><strong>Dashboard views:</strong> overview SLOs, top slow groups, parity failure log with sample diffs, checkpoint health, chunk progress, pushdown job history, hot-swap/migration activity feed. </td></tr><tr><td data-label="REG_Aggregation — Per-function Expert Technical Breakdown"> <strong>CI & gating: golden tests, migration manifest enforcement (strict rules)</strong><br>1. <strong>Golden fixtures:</strong> maintain <code>parity:golden/REG_Aggregation/&lt;fixtureId&gt;</code> containing canonical payroll+GL inputs and expected <code>variance_report</code> checksum. <br>2. <strong>Parity CI job:</strong> runs cross-runtime implementations (VBA/PQ/backend) and asserts checksum equality; failures block PR merges for regulated code paths. <br>3. <strong>Migration gating:</strong> changes to <code>aggCanonicalVersion</code> or group key recipe require <code>migration_manifest</code> and <code>parity:golden</code> tests that include before/after fixtures and must include <code>approvalsRef</code>. <br>4. <strong>Acceptance tests:</strong> include pushdown parity test for each supported warehouse on a sample cohort. <br>5. <strong>Audit chain verification:</strong> CI must validate manifest signature and evidenceRef presence for regulated builds. </td></tr><tr><td data-label="REG_Aggregation — Per-function Expert Technical Breakdown"> <strong>Operator runbooks & triage playbooks (detailed actions)</strong><br><strong>Parity failure triage:</strong> capture <code>planId</code> and <code>correlationId</code> → fetch <code>aggregation_manifest</code> → verify <code>aggCanonicalVersion</code> and <code>planSeed</code> → re-run local reproduction using canonical input artifacts referenced by <code>ingestChecksums</code> → if reproduction succeeds in local env but CI failed, identify environment drift; if reproduction differs, compare mapping snapshot <code>mapHash</code> and ingest checksum; escalate and create <code>forensic_manifest</code>. <br><strong>OOM worker / repeated failures:</strong> locate last <code>stateRef</code> checkpoint → requeue job with reduced chunkSize and increased memory budget if available → if repeated failures, enable pushdown path and validate parity before promoting artifacts. <br><strong>Failed pushdown parity:</strong> collect sample diffs and SQL generated, compare server vs local normalization rules, investigate character set/collation mismatches or rounding differences; do not promote pushdown artifact to regulated pipeline until resolved; persist both artifact sets for audit. <br><strong>Revert & rollback:</strong> if a new <code>aggCanonicalVersion</code> or mapping change introduces unacceptable deltas in regulated outputs, invoke rollback using previous mapping snapshot pointer and prior canonical artifacts; run smoke canary to confirm restored parity. <br><strong>Incident packaging:</strong> when escalation needed collect <code>audit_tail.csv</code> rows for correlationId, <code>aggregation_manifest</code>, <code>variance_report</code>, <code>before_aggregates</code>, <code>gl_aggregates</code>, pushdown SQL/job logs and <code>forensic_manifest</code> with checksums; archive to WORM storage for compliance. </td></tr><tr><td data-label="REG_Aggregation — Per-function Expert Technical Breakdown"> <strong>Security & PII policies (explicit)</strong><br>1. <strong>PII handling:</strong> do not include raw PII in analyst-facing aggregate artifacts; <code>sampleRowRefs</code> may be used as opaque identifiers but must not include PII; full sanitized evidence with PII persisted encrypted and retrievable only via approval flow with chain-of-custody. <br>2. <strong>SQL safety:</strong> generated SQL for pushdown must be parameterized; do not interpolate user inputs directly into SQL strings. <br>3. <strong>Execution constraints:</strong> aggregation code must not invoke arbitrary user code; any custom transform invoked during aggregation must be signed, allowlisted and executed in a sandbox worker with strict time and resource limits. <br>4. <strong>Audit & retention:</strong> manifests and artifacts for regulated datasets must be preserved according to retention policy (hot/warm/cold tiers) with signature and WORM protections as required. </td></tr><tr><td data-label="REG_Aggregation — Per-function Expert Technical Breakdown"> <strong>Backward compatibility & upgrade policy (explicit)</strong><br>1. <strong>Versioning:</strong> always increment <code>aggCanonicalVersion</code> when groupKey recipe, escaping, serialization or rounding semantics change. <br>2. <strong>Migration manifest required:</strong> include before/after fixtures, estimated affected count, canary plan, rollback steps and approvals. <br>3. <strong>Canary rollout:</strong> apply to limited cohort with automated KPI comparisons (groupsCount delta, <code>BeyondTolerance</code> delta, parity checks) and automatic rollback if thresholds exceeded. <br>4. <strong>Deprecation policy:</strong> support legacy canonical versions for a documented transition window and provide automated parity validation when reading older artifacts. </td></tr><tr><td data-label="REG_Aggregation — Per-function Expert Technical Breakdown"> <strong>Complete test matrix (explicit list — each item on separate line with <code>&lt;br&gt;</code> line breaks)</strong><br>1. Unit tests for <code>ComputeGroupKey</code> across locales and normalization vectors. <br>2. Unit tests for <code>FixedDecimalSum</code> with rounding edge cases and overflow checks. <br>3. Unit tests for Welford parity and mergeable partial states. <br>4. Integration tests for in-memory aggregation with small canonical fixtures. <br>5. Integration tests for streamed chunk processing with simulated worker crashes and resume. <br>6. Pushdown parity tests against staging warehouses (sample parity checks). <br>7. Golden parity tests for canonical fixture collection across VBA/PQ/backend. <br>8. Performance benchmarks at 50k, 500k, 5M rows to assert SLOs. <br>9. Security static analysis checks to detect dangerous API usage or SQL injection risk. <br>10. Migration manifest pipeline tests that automatically run before/after golden comparisons and verify approvalsRef presence. </td></tr><tr><td data-label="REG_Aggregation — Per-function Expert Technical Breakdown"> <strong>Artifact names, checksums & manifest semantics (authoritative)</strong><br>1. <code>evid:agg/&lt;planId&gt;/before_aggregates_v&lt;aggCanonicalVersion&gt;.csv</code> — canonical payroll aggregates + sha256. <br>2. <code>evid:agg/&lt;planId&gt;/gl_aggregates_v&lt;aggCanonicalVersion&gt;.csv</code> — canonical GL aggregates + sha256. <br>3. <code>evid:agg/&lt;planId&gt;/variance_report_v&lt;aggCanonicalVersion&gt;.csv</code> — canonical variance CSV + sha256. <br>4. <code>evid:agg/&lt;planId&gt;/aggregation_manifest_v&lt;aggCanonicalVersion&gt;.json</code> — manifest persisted and signed where required. <br>5. All checksums computed as <code>sha256(UTF8(canonicalBytes))</code>. <br>6. Manifest includes <code>samplingMeta</code> and <code>ingestChecksums[]</code> for replayability. </td></tr><tr><td data-label="REG_Aggregation — Per-function Expert Technical Breakdown"> <strong>Developer implementation guidance (practical checklist)</strong><br>1. Implement canonicalization rules in a single, shared library and port to all runtimes to guarantee parity. <br>2. Provide cross-runtime unit test vectors for <code>ComputeGroupKey</code>, <code>FixedDecimalSum</code>, sampling and CSV serialization. <br>3. Implement chunk checkpoint schema and provide utilities to inspect/repair stateRef for SRE. <br>4. Provide SQL templates for supported warehouses and a parity harness to validate pushdown outputs. <br>5. Ensure all artifacts persist via evidence API and manifests include required fields and signatures. <br>6. Bake CI/golden parity checks into PR pipelines and block merges when regulated fixtures change without migration manifest. </td></tr><tr><td data-label="REG_Aggregation — Per-function Expert Technical Breakdown"> <strong>Final verification checklist (10 checks — every release must pass)</strong><br>1. Cross-runtime canonicalization parity for canonical fixtures. <br>2. Unit test coverage for numeric parsing, aggregation and sampling. <br>3. Chunk resume & idempotency tests. <br>4. Pushdown parity tests for supported warehouses. <br>5. Evidence manifest persisted and signature verified in CI. <br>6. Performance benchmarks for representative workloads meet SLOs. <br>7. Security static analysis clear (no inline secrets, no unparameterized SQL). <br>8. Migration manifest present when changing canonical rules. <br>9. ApprovalsRef recorded for regulated changes. <br>10. Operator runbooks updated and accessible in evidence store. </td></tr><tr><td data-label="REG_Aggregation — Per-function Expert Technical Breakdown"> <strong>Appendix — recommended manifest schema fields (summary)</strong><br>Include fields: <code>manifestId, planId, ingestChecksums[], mapHash, aggCanonicalVersion, rowsCount, groupsCount, scale, artifacts{beforeRef,glRef,varianceRef,checksums}, aggChecksum, samplingMeta, metrics, createdBy, createdTs, approvalsRef, retentionPolicy, migrationManifestRef</code> and <code>manifestHash</code> (sha256). Persist canonical JSON with stable key order. </td></tr><tr><td data-label="REG_Aggregation — Per-function Expert Technical Breakdown"> <strong>Operator cheat-sheet (commands & quick checks)</strong><br>- <code>recon.aggregate --plan &lt;planId&gt;</code> → triggers <code>ComputeVarianceAggs</code> and returns <code>manifestRef</code>. <br>- <code>recon.check-parity --manifest &lt;manifestRef&gt;</code> → recomputes checksums and verifies artifact integrity. <br>- <code>recon.resume-job --stateRef &lt;stateRef&gt;</code> → resume chunked aggregation job. <br>- <code>recon.pushdown-run --plan &lt;planId&gt; --sqlTemplate &lt;tmpl&gt;</code> → run pushdown path with parity sample. Always capture <code>correlationId</code> and include it in incident artifacts. </td></tr><tr><td data-label="REG_Aggregation — Per-function Expert Technical Breakdown"> <strong>Closing operational recommendation (short):</strong> treat <code>aggCanonicalVersion</code> as the single sensitive switch — any change requires migration manifest, golden fixtures, canary runs, approvals, parity checks and evidence retention. Store canonicalization spec, SQL templates, and golden fixtures in the repository and run nightly parity checks to detect drift early. </td></tr></tbody></table></div><div class="row-count">Rows: 30</div></div><div class="table-caption" id="Table2" data-table="Docu_0184_02" style="margin-top:2mm;margin-left:3mm;"><strong>Table 2</strong></div>
<div class="table-wrapper" data-table-id="table-2"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by REG_Suggestions — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">REG_Suggestions — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="REG_Suggestions — Per-function Expert Technical Breakdown"> <strong>Module summary (compact):</strong><br>REG_Suggestions is the deterministic, auditable suggestion engine for payroll→GL variance remediation. It consumes <code>varianceRow</code> artifacts produced by REG_Aggregation/REG_Preview, a historical <code>historyIndex</code>, mapping snapshots and configuration, and returns canonical, balanced JE suggestions, mapping proposals for unmapped groups, and operational metrics for decisioning. The module's outputs are advisory by design and are separated from apply pipelines by explicit approval and persistence gates. Determinism, reproducibility, evidence-first design, and strict PII minimization are mandatory across all functions and artifacts. </td></tr><tr><td data-label="REG_Suggestions — Per-function Expert Technical Breakdown"> <strong>Design principles & cross-cutting invariants</strong><br>1. <strong>Determinism.</strong> Identical logical inputs (varianceRow, planSeed, mapHash, historyIndex.hash, weightConfig, roundingMode) must produce identical <code>suggestion</code> JSON and <code>suggestionHash</code> across runtimes (VBA, PQ, backend). Canonical serialization rules (NFKC, trimmed whitespace, stable key ordering) are authoritative and versioned.<br>2. <strong>Separation of concerns.</strong> Suggestion generation is read-only; persistence (evidence store), accept/reject flows, and apply/export are separate modules (REG_EvidenceAudit, REG_ApplyExport).<br>3. <strong>Audit-first architecture.</strong> Every lifecycle event appends a PII-free audit row with <code>correlationId</code> and <code>evidenceRef</code>. Full sanitized evidence is stored encrypted and requires approvals for retrieval.<br>4. <strong>Explainability & provenance.</strong> Each suggestion contains <code>confidenceBreakdown</code>, <code>historyIndex.hash</code>, <code>previewHash</code>, <code>planSeed</code>, and <code>weightConfigRef</code> to enable transparent replays and regulator-grade audits.<br>5. <strong>Fail-safe defaults.</strong> When counterpart discovery or rounding fails, engine falls back to <code>PayrollSuspense</code> or marks suggestion <code>requiresHumanReview</code>—never auto-apply regulated GL changes without two-person approval and a migration manifest.<br>6. <strong>CI & migration governance.</strong> Any change to rounding, residual, sampling or candidate ranking semantics requires a <code>migration_manifest</code> with golden before/after fixtures, canary plan, and documented approvals. </td></tr><tr><td data-label="REG_Suggestions — Per-function Expert Technical Breakdown"> <strong>Primary artifacts, naming & checksums</strong><br>- Suggestion JSON canonical filename: <code>evid:suggestion_&lt;suggestionId&gt;_&lt;version&gt;.json</code> (canonical, sha256 hash).<br>- Suggestion bundle for apply: <code>evid:suggestion_bundle_&lt;applyId&gt;_&lt;acceptedJEsHash&gt;.zip</code>.<br>- Mapping suggestion export: <code>evid:mapping_suggestions_&lt;timestamp&gt;_&lt;hash&gt;.json</code>.<br>- Forensic recompute report: <code>evid:suggestion_recompute_&lt;suggestionId&gt;_&lt;runId&gt;.json</code>.<br>- All persisted artifacts contain <code>canonicalVersion</code>, <code>checksumAlgorithm</code> (<code>sha256</code>), and optional <code>signatureRef</code>. </td></tr><tr><td data-label="REG_Suggestions — Per-function Expert Technical Breakdown"> <strong>Top-level function list (breadth)</strong><br>1. <code>SuggestCorrectionJE(varianceRow, jeTemplateSpec, historyIndex, scale, roundingMode, context)</code> — end-to-end suggestion generator.<br>2. <code>AttributionSignals(sampleRowRefs, historyIndex, context)</code> — feature extraction for scoring and discovery.<br>3. <code>DiscoverCounterparts(varianceRow, signals, historyIndex, domainAllowlist, maxCandidates)</code> — candidate discovery and ranking.<br>4. <code>ConstructLineCandidates(candidates, varianceMinorUnits, template, historyIndex, maxLines)</code> — create pre-rounded rational targets per line.<br>5. <code>ApplyRoundingAndResidualAbsorption(lines, scale, roundingMode)</code> — deterministic rounding and residual absorption algorithm.<br>6. <code>ComputeConfidenceScore(components, weightConfig, modelVersion)</code> — explainable scoring and breakdown.<br>7. <code>PersistSuggestion(suggestion, evidenceStore)</code> — canonical persist, idempotency, and signing hook.<br>8. <code>SuggestionAudit(suggestionRef, operatorId, action, metadata)</code> — append-only audit rows and chain-of-custody integration.<br>9. <code>RehydrateHistoryIndex(planContext, lookbackDays)</code> — deterministic history snapshot loader used for reproducibility.<br>10. <code>GenerateSuggestionExportBundle(suggestionIds, exportSpec, operatorId)</code> — prepare accepted suggestions for REG_ApplyExport.<br>11. <code>ComputeSuggestionMetrics(planId, timeWindowDays)</code> — KPI aggregator for dashboarding and alerting.<br>12. <code>AppendMappingSuggestions(unmappedGroups, historyIndex, config)</code> — bulk mapping suggestion generator for unmapped paycomponents.<br>13. <code>PersistSuggestionAcceptance(suggestionId, operatorId, approvalsRef)</code> — acceptance persistence and approval gating.<br>14. <code>RecomputeSuggestionForensic(suggestionRef)</code> — forensic reproduction for audits/regulators.<br>15. <code>RegisterUnitTestHook_Suggestions(hookName,enabled,token)</code> — CI determinism harness.<br>16. <code>SafeErrorToUser_Suggestion(correlationId,errorCode)</code> — PII-safe UI messaging. </td></tr><tr><td data-label="REG_Suggestions — Per-function Expert Technical Breakdown"> <strong>Function deep-dive — <code>SuggestCorrectionJE</code> (complete)</strong><br><strong>Purpose & contract:</strong><br>Produce a deterministic, auditable suggestion for a single <code>varianceRow</code>. Provide candidate JE lines, confidence breakdown, deterministic residual absorption, provenance, and an optional <code>requiresHumanReview</code> flag. The function returns a suggestion object or a deterministic error. It must not persist to evidence store by itself; persistence is explicit via <code>PersistSuggestion</code>.<br><strong>Inputs:</strong><br>- <code>varianceRow</code> — canonical row (groupKey fields, <code>PayrollMinorUnits</code>, <code>GLMinorUnits</code>, <code>AbsVarianceMinorUnits</code>, <code>rowCountPayroll</code>, <code>rowCountGL</code>, <code>sampleRowRefs[]</code>, <code>currency</code> if multi-currency).<br>- <code>jeTemplateSpec</code> — export target constraints (column ordering, narrative template, allowed GL buckets, maxLines, sign convention).<br>- <code>historyIndex</code> — pre-aggregated snapshot with <code>historyIndex.hash</code> used for provenance.<br>- <code>scale</code> — minor-unit scale (e.g., 2 for cents).<br>- <code>roundingMode</code> — enumerated rounding policy (default BANKER).<br>- <code>context</code> — <code>planSeed</code>, <code>mapHash</code>, <code>previewHash</code>, <code>operatorLocale</code>, <code>operatorTZ</code> (used for formatting in narratives only), <code>maxCandidates</code>.<br><strong>Outputs:</strong> canonical suggestion object not persisted: <code>{suggestionId, suggestionVersion, varianceRowId, candidateLines[], confidenceScore, confidenceBreakdown, rationale, provenance:{historyIndexHash, previewHash, planSeed, mapHash, weightConfigRef}, residualAbsorbedBy, status, createdTs}</code>. Or deterministic error structure <code>{errorCode, diagnostics, evidenceRef}</code>. <br><strong>Algorithm (step-by-step narrative):</strong><br>1. <strong>Seed & correlation:</strong> require <code>planSeed</code> presence; derive a deterministic <code>suggestionNonce = sha256(planSeed + &quot;:&quot; + varianceRowId)</code> used for any seeded choices.<br>2. <strong>Signal extraction:</strong> call <code>AttributionSignals(sampleRowRefs, historyIndex, context)</code>; store <code>signals</code> in provenance.<br>3. <strong>Counterpart discovery:</strong> call <code>DiscoverCounterparts(varianceRow, signals, historyIndex, domainAllowlist, maxCandidates)</code> to produce ordered <code>candidates[]</code> with <code>candidateScore</code> and <code>provenanceRef</code> per candidate.<br>4. <strong>Line construction:</strong> call <code>ConstructLineCandidates(candidates, varianceMinorUnits, jeTemplateSpec, historyIndex, maxLines)</code> to compute rational <code>target</code> amounts per candidate line; the function may insert a balancing placeholder (e.g., <code>PayrollSuspense</code>) if needed.<br>5. <strong>Rounding & residual absorption:</strong> call <code>ApplyRoundingAndResidualAbsorption(lines, scale, roundingMode)</code> to obtain integer minor-unit <code>rounded</code> values that balance exactly; record <code>residualAbsorbedBy</code> and the deterministic rule used for tie-breaking.<br>6. <strong>Confidence scoring:</strong> assemble <code>components</code> (matchScore, cooccurrenceFraction, recencyScore, mappingStability, amountMatchScore, dqPenalty) and call <code>ComputeConfidenceScore(components, weightConfig, modelVersion)</code> to compute <code>confidenceScore</code> and <code>confidenceBreakdown</code>.<br>7. <strong>Status & governance check:</strong> if <code>confidenceScore</code> < <code>ownerApprovalThreshold</code> or if <code>candidates</code> include <code>regulatoryFlag=true</code> with <code>confidence</code> below compliance threshold, set <code>status=&quot;requiresOwnerApproval&quot;</code> and include <code>requiredApprovals</code> in <code>provenance</code> referencing owners from <code>OWNERS.md</code> or mapping manifest. Add <code>requiresHumanReview=true</code> if fallback or rounding failure used. <br>8. <strong>Canonicalization & id generation:</strong> canonicalize candidateLines ordering and fields; compute <code>suggestionId = sha256(planSeed + &quot;:&quot; + varianceRowId + &quot;:&quot; + canonicalCandidateString)</code>. Set <code>suggestionVersion</code> equal to engine canonicalization version. <br>9. <strong>Return suggestion</strong> for caller to persist via <code>PersistSuggestion</code>. <br><strong>Invariants & guarantees:</strong><br>- <strong>Balance guarantee:</strong> final persisted candidateLines must balance exactly in minor-units.<br>- <strong>Determinism guarantee:</strong> given identical inputs and <code>weightConfigRef</code>, repeated runs produce identical <code>suggestionId</code> and canonical JSON. <br><strong>Failure modes & recovery:</strong><br>- <strong>Discovery empty:</strong> produce single-line balancing suggestion to <code>PayrollSuspense</code>, <code>status=&quot;fallback&quot;</code>, audit <code>suggestion.discovery.empty</code>, include <code>sampleEvidenceRef</code>.<br>- <strong>Rounding residual large:</strong> set <code>status=&quot;requiresHumanReview&quot;</code>, include <code>rationale</code> with exact math and a recovery path (owner review or forced export with suspense).<br>- <strong>Export spec conflict:</strong> return deterministic error <code>SUG_EXPORT_SPEC_MISMATCH</code> with suggested remediation. <br><strong>Implementation notes (engineering guidance):</strong><br>- Pre-fetch <code>historyIndex</code> slices and candidate metadata to avoid per-row DB calls.<br>- Use integer arithmetic and rational numerators/denominators for pre-round targets.<br>- Avoid inline PII in narratives; store full sample evidence in <code>evidenceRef</code> only. <br><strong>Testing matrix:</strong><br>- Determinism tests: identical inputs across VBA/PQ/backend produce identical <code>suggestionId</code> and file bytes for canonical JSON.<br>- Golden fixtures: regulated dataset sample suggestions must match golden JSON exact bytes.<br>- Edge-case tests: zero GL amounts, negative variances, multi-currency mixes, extremely large variances requiring multi-line splits. </td></tr><tr><td data-label="REG_Suggestions — Per-function Expert Technical Breakdown"> <strong>Function deep-dive — <code>AttributionSignals</code> (complete)</strong><br><strong>Purpose & contract:</strong> Build normalized, explainable features used by discovery and scoring. Return <code>signals</code> object containing numeric normalized components and diagnostics. This function must be deterministic and idempotent. <br><strong>Inputs:</strong> <code>sampleRowRefs</code>, <code>historyIndex</code> snapshot referenced by <code>historyIndex.hash</code>, <code>context</code> containing <code>recencyHalfLife</code> and <code>timeWindowDays</code>. <br><strong>Outputs & fields:</strong> <code>signals</code> contains:<br>- <code>matchScore</code> (0..1): fraction of sample rows with exact paycomponent-to-GL historical mapping.<br>- <code>cooccurrenceFraction</code> (0..1): historical fraction of occurrences of paycomponent with candidate GL(s) using recency weighting.<br>- <code>aliasSimilarity[]</code>: list of alias matches with normalized scores (0..1) and token overlap stats.<br>- <code>mappingStability</code> (0..1): normalized entropy-based measure (low entropy => high stability => value near 1).<br>- <code>recencyScore</code> (0..1): recency-weighted activity measure derived from exponential decay with configured half-life.<br>- <code>amountMatchScore</code> (0..1): fraction of sample amounts that fall within historical dominant buckets or proportional splits.<br>- <code>distinctEmployees</code> integer and <code>sampleSize</code> integer. <br><strong>Computational narrative:</strong><br>- Use <code>historyIndex</code> pre-aggregates to compute counts and proportions. <br>- For recency weighting, apply <code>w(t) = exp(-ln(2) * t / halfLife)</code> where <code>t</code> is age in days; normalize across candidate set. <br><strong>Edge cases handling:</strong> if <code>historyIndex</code> missing or partial, set numeric signals to <code>null</code> and <code>signalQuality=&quot;partial&quot;</code>. Downstream scoring must handle nulls by applying conservative fallbacks. <br><strong>Tests:</strong> unit tests for recency decay math, alias normalization, entropy calculation for mappingStability. </td></tr><tr><td data-label="REG_Suggestions — Per-function Expert Technical Breakdown"> <strong>Function deep-dive — <code>DiscoverCounterparts</code> (complete)</strong><br><strong>Purpose & contract:</strong> Rank candidate GL accounts for a variance row using signals and history. Deterministically return ordered <code>candidates[]</code> with provenance data for each candidate. <br><strong>Inputs:</strong> <code>varianceRow</code>, <code>signals</code>, <code>historyIndex</code>, <code>domainAllowlist</code> (trusted GL lists), <code>maxCandidates</code> (cap). <br><strong>Algorithm (detailed narrative):</strong><br>1. Seed candidates from <code>historyIndex</code> top-K cooccurring GLs for paycomponent and costCenter.<br>2. Augment with domain allowlist matches (owner-provided or regulatory lists) as discrete boosts; these are annotated. <br>3. Compute composite score per candidate: weighted sum of components (<code>cooccurrenceFraction</code>, <code>aliasSimilarity</code>, <code>recencyScore</code>, <code>amountMatchScore</code>, <code>mappingStability</code>) using <code>weightConfig</code> referenced by <code>weightConfigRef</code> to ensure reproducibility.<br>4. Normalize candidateScore to 0..1 and apply deterministic tie-breakers: primary sort by <code>candidateScore</code> desc, secondary by <code>candidateScore</code> exact numeric tie — sort <code>glAccount</code> lexicographically. <br>5. For candidates with historical splits, compute <code>suggestedSplitRatio</code> if entropy under threshold; else omit.<br>6. Mark candidate with <code>regulatoryFlag=true</code> if GL falls in regulated segments per COA policy. <br><strong>Outputs per candidate:</strong> <code>{glAccount, candidateScore, historyHits, recencyWeight, aliasScore, suggestedSplitRatio, provenanceRef, regulatoryFlag}</code>. <br><strong>Failure modes & recovery:</strong> empty result -> return empty array and <code>discoveryStatus=&quot;none&quot;</code>. Emit <code>suggestion.discovery.empty</code> audit event. <br><strong>Tests:</strong> ranking parity across runtimes, split ratio entropy thresholds, domain allowlist boost behavior tests. </td></tr><tr><td data-label="REG_Suggestions — Per-function Expert Technical Breakdown"> <strong>Function deep-dive — <code>ConstructLineCandidates</code> (complete)</strong><br><strong>Purpose & contract:</strong> Convert ordered <code>candidates</code> and <code>varianceMinorUnits</code> into a set of rational <code>target</code> lines ready for rounding. Obey <code>jeTemplateSpec</code> constraints (maxLines, narrative patterns, GL bucket restrictions). Deterministic output required. <br><strong>Inputs:</strong> <code>candidates[]</code>, <code>varianceMinorUnits</code> (signed integer representing target balancing amount; sign convention noted), <code>template</code> (maxLines, allowedGLRanges, per-line narrative template), <code>historyIndex</code>, <code>maxLines</code>. <br><strong>Construction strategies (select deterministically in priority order):</strong><br>1. <strong>Dominant-single:</strong> if top candidate <code>candidateScore &gt; dominanceThreshold</code> (e.g., 0.7), build 2-line JE (counterpart + balancing counterpart line or vice versa according to sign). <br>2. <strong>Stable split:</strong> if <code>suggestedSplitRatio</code> exists and <code>entropy &lt; threshold</code>, allocate targets proportionally to split. <br>3. <strong>Top-K proportion with remainder:</strong> if split absent but multiple decent candidates exist, allocate to top-K proportional to scores; remainder to <code>PayrollSuspense</code>. <br>4. <strong>Template constraint fallback:</strong> if <code>maxLines</code> too small, collapse lower-priority lines into a single suspense line deterministically. <br><strong>Output format per line:</strong> <code>{lineType, glAccount, targetRationalMinorUnits, narrative, candidateScore, provenanceRef}</code>. <br><strong>Deterministic tie-breakers:</strong> lexicographic by <code>(glAccount, lineType)</code> for equal scores; seed-influenced deterministic order via <code>suggestionNonce</code> only for stable tie-break cases. <br><strong>Failure modes:</strong> if all candidates violate <code>template.allowedGLRanges</code> then fallback to <code>PayrollSuspense</code> with <code>status=&quot;template_conflict&quot;</code>. <br><strong>Tests:</strong> multi-candidate split parity, template constraint enforcement, deterministic collapse behavior. </td></tr><tr><td data-label="REG_Suggestions — Per-function Expert Technical Breakdown"> <strong>Function deep-dive — <code>ApplyRoundingAndResidualAbsorption</code> (complete)</strong><br><strong>Purpose & contract:</strong> Convert rational <code>target</code> amounts to integer minor-units according to <code>roundingMode</code>, then absorb any residual so the sum balances exactly. Deterministic and explainable absorption algorithm is required. <br><strong>Inputs:</strong> <code>lines[]</code> with <code>targetRationalMinorUnits</code> as rational (numerator/denominator), <code>scale</code>, <code>roundingMode</code>. <br><strong>Detailed deterministic algorithm:</strong><br>1. <strong>Common denominator:</strong> compute common denominator across lines for rational arithmetic in terms of minor-units scaled to integer base. <br>2. <strong>Per-line pre-rounded rational:</strong> represent each <code>target</code> precisely as rational numerator/denominator to avoid float drift. <br>3. <strong>Round each line:</strong> apply rounding per <code>roundingMode</code> (Banker's default: round-half-to-even); produce <code>roundedMinorUnitsCandidate</code>. <br>4. <strong>Residual calculation:</strong> <code>residual = sum(preRoundedTargets) - sum(roundedMinorUnitsCandidate)</code>. Residual is an integer in minor-units. <br>5. <strong>Absorption choice:</strong> if <code>residual != 0</code> then select the absorption target deterministically: choose the line with maximum <code>abs(preRounded)</code> (largest absolute pre-rounded magnitude). If multiple tie, choose lexicographically by <code>(glAccount, lineType)</code>. Add <code>residual</code> to that line's rounded value. <br>6. <strong>Document absorption:</strong> record <code>residualAbsorbedBy</code> fields and a deterministic math explanation in <code>rationale</code>. <br><strong>Edge-cases & governance:</strong> if <code>abs(residual)</code> > <code>residualThreshold</code> (configurable, e.g., >10 minor-units) consider algorithmic failure and mark suggestion <code>requiresHumanReview</code> and audit <code>suggestion.rounding.largeResidual</code>. Offer fallback to <code>PayrollSuspense</code>. <br><strong>Implementation notes:</strong> use big-integer arithmetic; store preRounded rational as numerator/denominator in provenance so forensic recompute matches exact arithmetic. <br><strong>Tests:</strong> rounding parity across rounding modes; banker's tie half-even behavior; residual absorption tie-breaker determinism; multi-currency cases where pre-FX normalization required. </td></tr><tr><td data-label="REG_Suggestions — Per-function Expert Technical Breakdown"> <strong>Function deep-dive — <code>ComputeConfidenceScore</code> (complete)</strong><br><strong>Purpose & contract:</strong> Aggregate normalized signals into a scalar confidence [0,1] and a detailed breakdown of contributions for explainability. Must record <code>weightConfigRef</code> and <code>modelVersion</code> with each suggestion for reproducibility. <br><strong>Inputs:</strong> <code>components</code> (dictionary of normalized signals), <code>weightConfig</code> (weights mapping), <code>modelVersion</code>. <br><strong>Scoring recipe (transparent):</strong><br>1. For each component <code>c</code> multiply <code>value_c * weight_c</code> to compute <code>contrib_c</code>. <br>2. Sum positive contributions; subtract <code>dqPenalty * dqPenaltyWeight</code> if any data quality flags present. <br>3. Apply any regulatory floor/ceiling adjustments (e.g., cap for regulated GL segments), then clamp to [0,1]. <br>4. Return <code>confidenceScore</code> and <code>confidenceBreakdown</code> listing each component, its raw value, weight, and contribution. <br><strong>Explainability:</strong> include textual <code>rationale</code> lines like "matchScore 0.62 * weight 0.5 => contribution 0.31 — reason: exact paycomponent match." <br><strong>Governance:</strong> <code>autoAcceptThreshold</code> disabled by default for regulated GL; configurable for non-regulated segments with <code>approvalsRef</code> requirements. <br><strong>Tests:</strong> monotonicity under weight increases, missing components fallback behavior, modelVersion parity. </td></tr><tr><td data-label="REG_Suggestions — Per-function Expert Technical Breakdown"> <strong>Function deep-dive — <code>PersistSuggestion</code> (complete)</strong><br><strong>Purpose & contract:</strong> Canonicalize and persist suggestion JSON to evidence store, compute sha256 <code>suggestionHash</code>, optionally sign (Phase 2), add retention metadata, and return <code>evidenceRef</code>. Must be idempotent for identical canonical suggestion bytes. <br><strong>Canonicalization procedure:</strong> remove transient fields (runtime timers, ephemeral traces), sort keys lexicographically, normalize strings (NFKC, trimmed), render JSON without extraneous whitespace, UTF-8 encode, then compute sha256. Store canonical JSON and associated metadata <code>{suggestionHash, suggestionVersion, canonicalVersion, createdBy, createdTs}</code>. <br><strong>Persistence semantics & failure handling:</strong><br>- Attempt atomic write to evidence store with N retries and exponential backoff. <br>- On persistent failure stage to secure local staging with atomic rename and emit <code>suggestion.persist.warning</code> audit with <code>stagingPath</code>. <br>- Ensure persisted artifact is immutable and cannot be overwritten; a new version must create new artifact with new <code>suggestionHash</code>. <br><strong>Signing & Phase approach:</strong> Phase 1 persists unsigned; Phase 2 adds digital signature via KMS/HSM and stores <code>signatureRef</code> in metadata. <br><strong>Tests:</strong> idempotency tests, canonicalization golden hash tests, retry/failure simulation. </td></tr><tr><td data-label="REG_Suggestions — Per-function Expert Technical Breakdown"> <strong>Function deep-dive — <code>SuggestionAudit</code> (complete)</strong><br><strong>Purpose & contract:</strong> Append PII-free append-only audit rows for suggestion lifecycle events. Ensure audits include <code>correlationId</code>, <code>suggestionHash</code>, <code>planId</code>, <code>previewHash</code>, <code>operatorId</code> (pseudonymized as required), <code>action</code>, <code>metadata</code> and <code>prevAuditHash</code> for chain linking. <br><strong>Audit chain guarantees:</strong> audits are chained via <code>prevAuditHash</code> and rotated/signed periodically by REG_EvidenceAudit. Audits must be write-once and expose only non-PII fields in primary audit tail; full evidence referenced by <code>evidenceRef</code>. <br><strong>Examples of audit events:</strong> <code>suggestion.created</code>, <code>suggestion.reviewed</code>, <code>suggestion.accepted</code>, <code>suggestion.rejected</code>, <code>suggestion.exported</code>, <code>suggestion.apply_attempted</code>, <code>suggestion.reverted</code>. <br><strong>Tests:</strong> audit chain verification, append-only semantics under concurrency, tamper detect tests. </td></tr><tr><td data-label="REG_Suggestions — Per-function Expert Technical Breakdown"> <strong>Function deep-dive — <code>RehydrateHistoryIndex</code> (complete)</strong><br><strong>Purpose & contract:</strong> Provide a deterministic <code>historyIndex</code> snapshot for suggestion generation: aggregated cooccurrence counts, split ratios, mapping events, recency metrics, and validated COA references for the <code>planContext</code> and <code>lookbackDays</code>. Return includes <code>historyIndex.hash</code> and <code>status</code>. <br><strong>Operational notes:</strong><br>- Prefer pre-aggregated snapshots produced nightly and stored by <code>historyIndex.hash</code>. <br>- For interactive canaries on small datasets allow on-demand aggregation but persist snapshot and <code>historyIndex.hash</code> for provenance. <br><strong>Failure & fallback:</strong> missing snapshot returns partial <code>historyIndex</code> with <code>status=&quot;partial&quot;</code> and <code>signalQuality=low</code>. Upstream suggestions use conservative fallbacks and require human review. <br><strong>Tests:</strong> snapshot parity, snapshot hashing reproducibility, performance under large tenant data. </td></tr><tr><td data-label="REG_Suggestions — Per-function Expert Technical Breakdown"> <strong>Function deep-dive — <code>GenerateSuggestionExportBundle</code> (complete)</strong><br><strong>Purpose & contract:</strong> Given accepted suggestions, validate and transform into canonical <code>acceptedJEs</code> conforming to <code>exportSpec</code>, compute <code>acceptedJEsHash</code>, and persist export bundle for REG_ApplyExport. Return <code>{bundleRef, acceptedJEsHash, validationSummary}</code>. <br><strong>Validation steps (narrative):</strong><br>- Validate each JE balance per-journal at <code>scale</code>. <br>- Validate column mapping, sign convention, date/number format, and per-line narrative lengths. <br>- Ensure each JE fits loader constraints (max lines, per-line field lengths). <br>- If any JE fails validation abort bundle creation and return <code>validationErrors[]</code> referencing <code>suggestionRef</code> and <code>sampleRowRefs[]</code>. Operator can opt <code>forceExportWithSuspense</code> (requires approvals). <br><strong>Persistence & handoff:</strong> persist <code>acceptedJEs</code> canonical JSON and zipped artifacts to <code>evid:suggestion_bundle_*</code> and return <code>bundleRef</code>; append <code>payroll.je.exported</code> audit. <br><strong>Tests:</strong> export spec permutations, loader simulation acceptance tests, idempotent bundle generation. </td></tr><tr><td data-label="REG_Suggestions — Per-function Expert Technical Breakdown"> <strong>Function deep-dive — <code>ComputeSuggestionMetrics</code> (complete)</strong><br><strong>Purpose & contract:</strong> Compute operational KPIs for dashboards and SRE: <code>suggestionCount</code>, <code>acceptedRate</code>, <code>avgConfidenceAccepted</code>, <code>timeToAcceptMedian</code>, <code>revertRateAfterApply</code>, <code>persistFailureRate</code>, <code>acceptedRateByOwner</code>, <code>topRejectedGLs</code>, <code>suggestionLatencyMs</code> percentiles. Return structured metrics and time-series ready for dashboard ingestion. <br><strong>Implementation notes:</strong> derive metrics from audit tails and evidence lookups; compute incremental rollups and store pre-aggregations to handle large volumes. <br><strong>Tests:</strong> metric correctness, alert trigger conditions, threshold noise suppression. </td></tr><tr><td data-label="REG_Suggestions — Per-function Expert Technical Breakdown"> <strong>Function deep-dive — <code>AppendMappingSuggestions</code> (complete)</strong><br><strong>Purpose & contract:</strong> Generate ranked mapping suggestions for unmapped paycomponents aggregated from <code>unmappedGroups</code>. Each suggestion contains candidate GLs, confidence, sampleRows, occurrences and a unique <code>mappingSuggestionId</code>. Suggestions are advisory and must be accepted by mapping owners and go through REG_Mapping workflows (hot-swap) to become active maps. <br><strong>Algorithm narrative:</strong> group unmapped rows by <code>PayComponent+CostCenter</code>, compute occurrence counts, example amounts, distinct employee counts, and fetch historical contexts. Rank candidate GLs using <code>historyIndex</code> and <code>aliasSimilarity</code> heuristics and produce <code>mappingSuggestions[]</code> with <code>confidenceScore</code>. <br><strong>Governance integration:</strong> accepted mapping suggestions should be converted into mapping manifest drafts and follow mapping migration manifest process if they affect regulated GL segments. <br><strong>Tests:</strong> precision/recall evaluation, acceptance-rate tracking, bulk generation performance tests. </td></tr><tr><td data-label="REG_Suggestions — Per-function Expert Technical Breakdown"> <strong>Function deep-dive — <code>PersistSuggestionAcceptance</code> (complete)</strong><br><strong>Purpose & contract:</strong> Record suggestion acceptance atomically and link to <code>applyDescriptor</code> if immediate export/apply requested. Validate approvals for regulated suggestions; produce <code>acceptedSuggestionRef</code> and append <code>payroll.je.accepted</code> audit. <br><strong>Approval rules:</strong> if <code>suggestion.confidence &lt; ownerApprovalThreshold</code> or suggestion touches regulated GL segments, require <code>approvalsRef</code> and validate against <code>OWNERS.md</code> or RBAC. <br><strong>Atomicity & idempotency:</strong> acceptance writes must be idempotent and record <code>acceptedBy</code>, <code>acceptedTs</code>, and <code>approvalsRef</code>. <br><strong>Tests:</strong> concurrency acceptance, approval verification, rollback scenarios. </td></tr><tr><td data-label="REG_Suggestions — Per-function Expert Technical Breakdown"> <strong>Function deep-dive — <code>RecomputeSuggestionForensic</code> (complete)</strong><br><strong>Purpose & contract:</strong> Recompute suggestion deterministically using stored provenance (<code>historyIndex.hash</code>, <code>planSeed</code>, <code>mapHash</code>, <code>paramsHash</code>) and produce <code>diffReport</code> highlighting any divergence between recomputed and persisted suggestion. Append <code>suggestion.recompute</code> audit. <br><strong>Use cases:</strong> regulator requests, incident triage, golden parity investigations. <br><strong>Tests:</strong> cross-runtime recompute parity with stored snapshots and golden fixtures. </td></tr><tr><td data-label="REG_Suggestions — Per-function Expert Technical Breakdown"> <strong>Function deep-dive — <code>RegisterUnitTestHook_Suggestions</code> (complete)</strong><br><strong>Purpose & contract:</strong> Register CI hooks to run deterministic suggestion flows with fixed seeds and preloaded fixtures. Hooks disabled in production and access-controlled. CI uses these hooks for <code>parity:golden:REG_Suggestions</code>. <br><strong>CI gating:</strong> golden diffs for regulated fixtures must block merges; <code>migration_manifest</code> required for semantic changes. <br><strong>Tests:</strong> ensure hooks reproduce same outputs across environments and are isolated from production systems. </td></tr><tr><td data-label="REG_Suggestions — Per-function Expert Technical Breakdown"> <strong>Function deep-dive — <code>SafeErrorToUser_Suggestion</code> (complete)</strong><br><strong>Purpose & contract:</strong> Map internal errors to concise PII-free user messages (≤160 chars) that include <code>correlationId</code> and triage hints. Persist full diagnostics to evidence and append <code>suggestion.userErrorShown</code> audit. <br><strong>Examples:</strong> <code>SUG_DISCOVERY_EMPTY</code> -> "No counterpart found for variance (ref r-20260121-abc). Review mapping suggestions or open a ticket." <br><strong>Tests:</strong> PII-scan validation of messages, audit presence checks. </td></tr><tr><td data-label="REG_Suggestions — Per-function Expert Technical Breakdown"> <strong>Examples (detailed narratives & reconciliation stories)</strong><br><strong>Example A — Single dominant counterpart (high-confidence flow):</strong><br>Scenario: varianceRow shows payroll > GL by 500. Signals: <code>cooccurrenceFraction=0.92</code>, <code>mappingStability=0.98</code>, <code>recencyScore=0.8</code>. <code>DiscoverCounterparts</code> returns GL <code>6100-OT</code> with candidateScore 0.93. <code>ConstructLineCandidates</code> returns two-line rational target: credit <code>6100-OT</code> 500, debit <code>PayrollSuspense</code> 500 (direction per sign conv). <code>ApplyRoundingAndResidualAbsorption</code> no residual. <code>ComputeConfidenceScore</code> yields 0.86 with breakdown: matchScore 0.6 contribution 0.3, historyScore 0.2 contrib 0.1, recency 0.8 contrib 0.4 (weights visible). Suggestion persisted, audit emitted, operator accepts; <code>GenerateSuggestionExportBundle</code> builds export; <code>ApplyCorrections</code> handles apply after approvals. <br><strong>Example B — Multi-counterpart stable split (medium confidence):</strong><br>Variance 1200 minor-units. <code>historyIndex</code> shows GL A 70% and GL B 30% historically with low entropy. <code>ConstructLineCandidates</code> proposes split 840/360. <code>ApplyRoundingAndResidualAbsorption</code> rounds to integers; residual of +1 minor-unit absorbed deterministically by GL A per rule; <code>ComputeConfidenceScore</code> 0.62 => <code>requiresOwnerApproval</code>. Suggestion persisted with <code>requiresHumanReview</code>. Owner accepts after review. <br><strong>Example C — No counterpart (fallback to suspense):</strong><br>Signals absent; <code>DiscoverCounterparts</code> empty. Suggestion generator creates single line to <code>PayrollSuspense</code> for whole variance, confidence low, status <code>fallback</code>, audit <code>suggestion.discovery.empty</code>. Operator either creates mapping suggestion or posts JE to suspense and queues mapping follow-up. </td></tr><tr><td data-label="REG_Suggestions — Per-function Expert Technical Breakdown"> <strong>Conceptual Power Query guidance (no PQ code)</strong><br>1. <strong>Sample ingestion & canonicalization:</strong> Implement deterministic sampling in server/Power Query parameterization using the stored <code>planSeed</code> and precomputed sample indices; canonicalize strings (NFKC, trim) and numeric scales early to ensure parity with backend. <br>2. <strong>HistoryIndex usage:</strong> Import pre-aggregated historyIndex snapshots (CSV/Parquet) rather than computing cooccurrence on-the-fly in UI to avoid heavy queries. <br>3. <strong>Signal extraction:</strong> For small samples compute alias token overlap and amount bucket comparisons in PQ; store outputs as <code>suggestion_preview_&lt;planId&gt;.csv</code> and compute canonical checksum for evidence. <br>4. <strong>Export:</strong> Render suggestion preview as canonical CSV with stable column order and explicit character encoding; include <code>previewHash</code> in metadata. <br>Operational PQ notes: avoid heavy joins and prefer query folding; rely on server-side pre-aggregation for historyIndex. </td></tr><tr><td data-label="REG_Suggestions — Per-function Expert Technical Breakdown"> <strong>Conceptual DAX guidance (no code)</strong><br>1. <strong>Key measures:</strong> <code>SuggestionAcceptedRate</code>, <code>AvgConfidenceAccepted</code>, <code>TopRejectedGLs</code>, <code>RevertRateAfterApply</code>. Design measures to reference canonical <code>suggestionHash</code> and <code>planId</code> for reproducibility. <br>2. <strong>Time window slicing:</strong> use relative time filters to compute rolling acceptance and revert rates. <br>3. <strong>Owner dashboards:</strong> produce measures aggregated by <code>ownerId</code>, <code>glSegment</code>, and <code>ruleId</code> to prioritize mapping clean-ups and weightConfig tuning. <br>4. <strong>Alert triggers:</strong> link DAX-based KPIs to alert thresholds via monitoring pipeline (e.g., low acceptance for high-confidence suggestions). </td></tr><tr><td data-label="REG_Suggestions — Per-function Expert Technical Breakdown"> <strong>Operational runbooks (executable, per incident)</strong><br><strong>Runbook 1 — No candidates discovered:</strong><br>1. Capture <code>correlationId</code> from UI audit row.<br>2. Retrieve suggestion evidence with <code>evidenceRef</code> via Evidence API (requires approvals).<br>3. Run <code>RecomputeSuggestionForensic</code> using stored <code>historyIndex.hash</code>; confirm discovery still empty.<br>4. Create mapping suggestion using <code>AppendMappingSuggestions</code>; route to owners; optionally create suspense JE for immediate remediation.<br>5. Log migration manifest if fix touches regulated GLs.<br><strong>Runbook 2 — Rounding/residual anomalies:</strong><br>1. Identify offending <code>suggestionId</code> and pull persisted canonical suggestion JSON and math steps from evidence.<br>2. Recompute rounding locally using recorded <code>preRounded</code> rationals and <code>roundingMode</code>. <br>3. If recompute mismatch across runtime, run <code>parity:golden</code> CI test and escalate to engineering with recompute artifacts and <code>parity:golden</code> logs. <br>4. For production incidents, if residual is large, stop automatic applies for affected GL segments and notify owners. </td></tr><tr><td data-label="REG_Suggestions — Per-function Expert Technical Breakdown"> <strong>Testing & CI requirements (detailed matrix)</strong><br>Unit tests:<br>1. Rounding/residual absorption for multiple rounding modes with exhaustive half-even vectors. <br>2. Confidence scoring math and weight perturbation tests. <br>3. Candidate ranking determinism and tie-breakers. <br>Integration tests:<br>1. SuggestCorrectionJE end-to-end for canonical regulated fixtures: single-counterpart, multi-counterpart, ambiguous, no-counterpart. <br>2. Export bundle to REG_ApplyExport validation pipeline simulation. <br>Golden parity tests:<br>1. Fixed <code>planSeed</code> runs produce identical <code>suggestionHash</code> across VBA/PQ/backend on golden fixtures stored under <code>evid:golden/REG_Suggestions/</code>. <br>Property tests:<br>1. Reorder-of-input rows invariance tests. <br>CI gating:<br>1. Any golden diffs for regulated fixtures block merges; <code>migration_manifest</code> required for semantic changes. </td></tr><tr><td data-label="REG_Suggestions — Per-function Expert Technical Breakdown"> <strong>SLOs, monitoring & run metrics (detailed)</strong><br>1. Suggestion generation latency: median <200ms per variance; p95 <1s for interactive flows. <br>2. Persist success rate ≥ 99.9%. <br>3. Golden parity pass rate ≥ 99.9% for regulated fixtures. <br>4. Suggestion accepted revert rate target <0.5% within 24 hours. <br>Monitoring metrics to export: <code>suggestion.generated.count</code>, <code>suggestion.persist.latency_ms</code>, <code>suggestion.accepted.count</code>, <code>suggestion.reverted.count</code>, <code>suggestion.rounding.failures</code>, <code>parity.golden.failures</code>. Alert thresholds configured for SRE. </td></tr><tr><td data-label="REG_Suggestions — Per-function Expert Technical Breakdown"> <strong>Forensic & audit practices</strong><br>1. Always store <code>historyIndex.hash</code>, <code>previewHash</code>, <code>planSeed</code>, <code>mapHash</code>, <code>weightConfigRef</code> with each suggestion for reproducibility. <br>2. Provide <code>RecomputeSuggestionForensic</code> to reproduce suggestions using the stored provenance. <br>3. Evidence retrieval must be audited via chain-of-custody with time-limited retrieval tokens. <br>4. When regulators request artifacts, assemble <code>forensic_manifest</code> containing canonical suggestion JSON, <code>historyIndex</code> snapshot, <code>preview</code> artifacts, <code>applyDescriptor</code> (if applied), audit tail rows, and signed <code>recon_report</code>. </td></tr><tr><td data-label="REG_Suggestions — Per-function Expert Technical Breakdown"> <strong>Scaling & performance engineering</strong><br>1. Batch suggestion generation using worker pool partitioned by groupKey hash to maximize cache locality. <br>2. Pre-aggregate <code>historyIndex</code> snapshots nightly; store as partitioned Parquet files keyed by paycomponent and costCenter; load small slices into memory for interactive jobs. <br>3. Use streaming persistence and chunked evidence writes for high-volume runs with checkpointing. <br>4. Provide autoscaling worker pools keyed by job queue depth; implement backpressure to deny interactive triggers when system saturated with clear UI messages. </td></tr><tr><td data-label="REG_Suggestions — Per-function Expert Technical Breakdown"> <strong>Security & PII constraints</strong><br>1. UI surfaces must show redacted <code>sampleRowRefs</code> and non-PII narratives. <br>2. Full sample evidence stored encrypted with strict RBAC; retrieval requires <code>approvalsRef</code> and is logged. <br>3. No PII is persisted in primary suggestion JSON; only evidenceRef points to sanitized artifacts. <br>4. Custom scripts or third-party models in suggestion pipeline must be allowlisted and executed in sandboxed workers with network and IO restrictions. </td></tr><tr><td data-label="REG_Suggestions — Per-function Expert Technical Breakdown"> <strong>Migration manifest & change-control process (concise)</strong><br>1. For changes that alter semantics (rounding, residual absorption algorithm, weightConfig, candidate ranking), produce <code>migration_manifest</code> containing <code>migrationId</code>, <code>author</code>, <code>createdTs</code>, <code>changeRationale</code>, <code>affectedFixtures[]</code> (before/after golden checksums), <code>canaryPlan</code> (planId + cohort sizes), <code>rollbackPlan</code>, <code>approvals[]</code>, and <code>testMatrix</code>. <br>2. Run canary plans and compare KPIs. <br>3. Persist migration manifest in evidence and require two-person approvals for regulated artifacts. </td></tr><tr><td data-label="REG_Suggestions — Per-function Expert Technical Breakdown"> <strong>Developer notes & implementation checklist (operative)</strong><br>1. Implement canonicalization functions (string normalization, JSON canonical serializer) and port to PQ/VBA/backend; include golden fixtures. <br>2. Ensure integer-only arithmetic for minor-units; canonicalize numeric serialization for hashing. <br>3. Cache historyIndex slices per planSeed; avoid per-row DB calls. <br>4. Provide suggestion simulator UI for owners to test weightConfig variants against sample plans in a sandbox. <br>5. Publish operator runbooks and hold a tabletop exercise for revert and forensic flows. </td></tr><tr><td data-label="REG_Suggestions — Per-function Expert Technical Breakdown"> <strong>Final acceptance checklist (ten required pre-deploy checks)</strong><br>1. Rounding & residual test vectors passed across implementations.<br>2. Golden suggestion artifacts validated by CI across runtimes.<br>3. Evidence persist idempotency validated and signature flow tested (if enabled).<br>4. Audit chain and recon_report generation tested and chain verification passed. <br>5. Approval gating enforced for regulated GL segments; two-person approval validated. <br>6. HistoryIndex snapshotting pipeline in place and <code>historyIndex.hash</code> recorded in suggestions. <br>7. Export handoff with REG_ApplyExport tested for balanced JEs and loader compatibility. <br>8. Operator runbooks created and at least one tabletop executed with recorded evidence. <br>9. Monitoring dashboards and alerts configured and tested. <br>10. Migration manifest workflow validated with a dry-run canary and rollback test. </td></tr><tr><td data-label="REG_Suggestions — Per-function Expert Technical Breakdown"> <strong>Closing operational note (executive & implementable):</strong><br>REG_Suggestions must be treated as an advisory, deterministic, evidence-bound subsystem with explicit persistence and approvals separating suggestion generation from actual GL changes. Implementation requires careful attention to integer arithmetic, canonical JSON serialization, audit chaining, evidence storage, and CI golden parity. For audit and regulator readiness ensure every suggestion includes <code>historyIndex.hash</code>, <code>previewHash</code>, <code>planSeed</code>, <code>mapHash</code>, and <code>weightConfigRef</code> and that <code>RecomputeSuggestionForensic</code> reproduces persisted outputs precisely. If desired, next deliverable options include: canonical JSON schemas for <code>suggestion</code> and <code>acceptedJEs</code>, a set of golden fixture templates for CI, and a printable operator runbook card. </td></tr></tbody></table></div><div class="row-count">Rows: 33</div></div><div class="table-caption" id="Table3" data-table="Docu_0184_03" style="margin-top:2mm;margin-left:3mm;"><strong>Table 3</strong></div>
<div class="table-wrapper" data-table-id="table-3"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by REG_ApplyExport — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">REG_ApplyExport — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="REG_ApplyExport — Per-function Expert Technical Breakdown"> <strong>Top-level purpose & guarantees</strong><br>Provide authoritative, auditable, deterministic export and apply orchestration for accepted JE suggestions. Guarantees: immutability of persisted descriptors, pre-execution atomic persist, canonicalized export artifacts with sha256 checksums, idempotent posting using deterministic idempotency tokens, ephemeral credential usage for <code>post_direct</code>, bounded retries and poison-queue handling, full forensic packaging and chain-of-custody, PII-minimized audit rows referencing encrypted evidence. Changes to canonical semantics (serialization, rounding, PRNG, column ordering) require a migration manifest, golden fixtures and CI parity approval. </td></tr><tr><td data-label="REG_ApplyExport — Per-function Expert Technical Breakdown"> <strong>Module contracts & key invariants (global) — short reference</strong><br><strong>Canonical artifacts:</strong> UTF-8, newline <code>\n</code>, deterministic escaping, exact column order, explicit numeric scale; <code>exportChecksum = sha256(canonicalBytes)</code>.<br><strong>Apply descriptor:</strong> persisted atomically before any mutative action; immutable except status/execResult updates. <code>&lt;br&gt;</code><strong>Idempotency token:</strong> deterministic derivation from <code>applyId</code> and <code>idempotencyVersion</code>; never reuse tokens across unrelated applies. <code>&lt;br&gt;</code><strong>Evidence & audits:</strong> audit rows are PII-minimized and include <code>evidenceRef</code> pointers to full sanitized artifacts stored in <code>REG_EvidenceAudit</code>. <code>&lt;br&gt;</code><strong>Failure rules:</strong> partial successes tolerated but surfaced; reverts require <code>revertDescriptor</code> and approvals for regulated ranges. </td></tr><tr><td data-label="REG_ApplyExport — Per-function Expert Technical Breakdown"> <strong>Function: <code>GenerateJEExport(acceptedJEs, exportSpec, operatorId, planId, applyIdOpt)</code> — full breakdown</strong><br><strong>Purpose & contract:</strong> generate a canonical loader-ready export artifact from <code>acceptedJEs</code> according to <code>exportSpec</code>. Validate bundles, canonicalize numbers to minor-units, serialize deterministically, compute checksum, persist artifact immutably, produce <code>exportManifest</code> and return artifact reference. Non-mutating. Idempotent given identical inputs and spec.<br><strong>Inputs (expected shape):</strong><br>1. <code>acceptedJEs[]</code> — arrays of JE bundles where each bundle contains <code>suggestionId</code>, <code>lines[]</code> with <code>account</code>, <code>debit/credit</code> decimal strings, <code>description</code>, <code>costCenter</code>, <code>reference</code>, <code>currency</code>, and <code>evidenceRefs</code>. <br>2. <code>exportSpec</code> — loader mapping {format, delimiter, encoding, columns[], scale, signConvention, dateFormat, escapePolicy, requiredFields, vendorAdapterId}. <br>3. <code>operatorId</code>, <code>planId</code>, optional <code>applyId</code> override. <br><strong>Outputs:</strong> <code>exportManifest</code> with <code>{artifactRef, uri, exportChecksum, rowsCount, rowsByBundle, specRef, persistedTs}</code> or deterministic error with diagnostics and <code>evidenceRef</code> for failing rows.<br><strong>Primary invariants:</strong><br>1. Every JE bundle must balance to zero at computed <code>minorUnits</code> using provided <code>scale</code> and configured <code>roundingMode</code>. <br>2. Serialization must follow canonical rules (stable ordering, no BOM, explicit header, newline = <code>\n</code>). <br>3. If <code>applyIdOpt</code> supplied, used in canonical filename; otherwise <code>applyId</code> assigned by <code>BuildApplyDescriptor</code>. <br><strong>Step-by-step (recommended runtime pattern):</strong><br>1. Validate <code>exportSpec</code> via <code>ValidateExportSpec</code>; canonicalize it to <code>exportSpecCanonical</code>. <br>2. Run <code>ValidateJE_Balance</code> per-bundle; if any bundle fails and policy = <code>fail</code>, abort and return diagnostics; if policy = <code>partial</code>, mark blocked bundles and emit <code>payroll.je.export.partial</code>. <br>3. Convert each decimal amount string to <code>AmountMinorUnits</code> integer deterministically (string parse → rational → multiply by 10^scale → banker rounding). <br>4. Determine deterministic ordering for bundles/lines; stable ordering options: <code>journalRef</code> asc then <code>lineNumber</code> asc; if absent, fallback to canonical <code>suggestionId</code> ordering. <br>5. Stream-serialize output using <code>SerializeExportArtifact</code> (buffered writer, flush threshold); compute rolling sha256 to avoid double buffering. <br>6. Call <code>PersistExportArtifact</code> (atomic write pattern) to write to evidence store and optional loader staging area; stage fallback if destination unavailable. <br>7. Produce <code>exportManifest</code> with artifactRef, checksum, rowsCount, <code>exportSpecRef</code> and persist manifest. Emit audit row <code>payroll.je.exported</code> (PII-minimized). <br><strong>Examples (conceptual):</strong><br>- CSV loader: explicit columns include <code>JournalDate,JournalRef,LineNo,Account,Debit,Credit,CostCenter,Description</code>. Writer outputs header, lines deterministic. <br>- JSON loader: canonical JSON with keys ordered <code>journalId</code>, <code>journalDate</code>, <code>lines[]</code>, and each <code>line</code> keys ordered. JSON must be compact (no whitespace) before hashing. <br><strong>Failure modes & recovery:</strong><br>1. Schema mismatch (<code>EXPORT_SPEC_MISMATCH</code>) → return diagnostics and sample template. <br>2. Unbalanced bundles (<code>EXPORT_UNBALANCED</code>) → fail or partial handling per policy; blocked bundles persisted to <code>evidenceRef</code>. <br>3. Persist failure → stage local encrypted artifact and return <code>stagedRef</code> with operator instructions. <br><strong>Tests & CI:</strong> golden byte-for-byte checks, balance edge tests, streaming large-file tests, idempotency replay tests, negative spec injection tests. <br><strong>Conceptual PQ guidance:</strong> prefer constructing canonical staging table in M with typed columns; compute minor-units integers using explicit conversion steps; write rows in deterministic order and compute checksum over concatenated canonical string. <br><strong>Conceptual DAX checks:</strong> create <code>ExportRowCount</code> and <code>ExportBalanceVariance</code> measures to assert exported result invariants. </td></tr><tr><td data-label="REG_ApplyExport — Per-function Expert Technical Breakdown"> <strong>Function: <code>ValidateExportSpec(exportSpec)</code> — complete contract & examples</strong><br><strong>Purpose:</strong> ensure <code>exportSpec</code> is complete, deterministic, safe, and portable for the target loader. Return <code>exportSpecCanonical</code> or validation error.<br><strong>Validation checks (deterministic):</strong><br>1. <code>format</code> present and in allowed set {CSV,JSON,XML}. <br>2. <code>columns[]</code> present with canonical tokens mapping to internal JE fields. <br>3. <code>scale</code> integer in 0..6. <br>4. <code>signConvention</code> in {SignedAmount, DebitCreditColumns}. <br>5. <code>dateFormat</code> either <code>ISO</code> or explicit pattern. <br>6. <code>escapePolicy</code> deterministic (RFC4180 or custom explicit rules). <br>7. <code>maxRowLength</code> and <code>maxRows</code> optional safety limits to avoid loader overload. <br><strong>Normalization & output:</strong> return <code>exportSpecCanonical</code> with fixed column order and canonical representations for date/numeric formatting. <br><strong>Failure & operator hints:</strong> return deterministic <code>diagnostics</code> with example valid spec fragment and suggested remedial action. <br><strong>Tests:</strong> spec fuzzing, invalid/missing fields, column mapping errors. </td></tr><tr><td data-label="REG_ApplyExport — Per-function Expert Technical Breakdown"> <strong>Function: <code>ValidateJE_Balance(jeBundle, scale, roundingMode)</code> — deterministic arithmetic & residual handling</strong><br><strong>Purpose:</strong> deterministically verify bundle balance and optionally apply deterministic residual absorption algorithm for line distribution rounding.<br><strong>Algorithm (explicit):</strong><br>1. For each line decimal string, parse to rational (avoid floats) and compute <code>targetMinor = Round(rational * 10^scale, roundingMode)</code>. <br>2. Sum <code>debitsMinor</code> and <code>creditsMinor</code> separately. <br>3. If equal → balanced. If not equal: compute <code>residual = (sumDebits - sumCredits)</code>; if <code>residual != 0</code> then if policy allows <code>residualAbsorb</code> perform deterministic absorption into the single line with highest pre-rounded absolute amount; recompute sums; if residual remains non-zero -> fail. <br><strong>RoundingMode default:</strong> Banker (round half to even) unless overridden in <code>exportSpec</code>. <br><strong>Edge cases:</strong> multi-currency bundles require currency normalization before minor-unit summation; negative zero handling must be normalized to zero. <br><strong>Outputs:</strong> boolean or structured error <code>JE_UNBALANCED</code> with <code>lineLevelEvidence</code> and <code>evidenceRef</code> to persisted per-bundle diagnostic. <br><strong>Tests:</strong> half-even rounding tests, absorption determinism tie-breaking, multi-currency normalization tests. </td></tr><tr><td data-label="REG_ApplyExport — Per-function Expert Technical Breakdown"> <strong>Function: <code>SerializeExportArtifact(acceptedJEs, exportSpecCanonical)</code> — streaming serializer details</strong><br><strong>Purpose:</strong> produce canonical bytes according to <code>exportSpecCanonical</code> with streaming support. Must support CSV, JSON, XML, chunked uploads, and stitched checksum semantics for multipart artifacts.<br><strong>CSV canonical rules (explicit):</strong><br>1. Header line exactly the canonical <code>columns[]</code> order; <code>&lt;br&gt;</code>2. Field escaping: double-quote any field containing delimiter, newline, or double-quote; double quotes escaped as <code>&quot;&quot;</code>. <br>3. No trailing spaces; trim fields unless exportSpec indicates preserving whitespace. <br>4. Always write newline <code>\n</code> between rows; final newline policy must be documented (choose to always include final newline). <br><strong>JSON canonical rules (explicit):</strong> keys ordered in canonical order; arrays maintain deterministic ordering; whitespaces stripped; numeric types serialized as decimal strings for consistent hashing. <br><strong>Streaming behaviour:</strong> use buffered writer with a flush threshold (e.g., 4MB). Compute rolling sha256 while streaming to avoid re-reading. For remote uploads, allow chunked multi-part upload and compute stitched checksum as sha256(concat(chunkChecksums) or documented alternative). <br><strong>Large-export stitching:</strong> define algorithm: compute sha256 per chunk (canonical chunk), persist chunk checksums in manifest in canonical order, compute <code>stitchedHash = sha256(concat(chunkChecksumBytesOrdered))</code>. Document in manifest. <br><strong>Tests:</strong> large export streaming parity, chunk stitching reproducibility across runtimes. </td></tr><tr><td data-label="REG_ApplyExport — Per-function Expert Technical Breakdown"> <strong>Function: <code>ComputeExportChecksum(bytesOrStream)</code> — canonical hashing details</strong><br><strong>Purpose:</strong> compute <code>sha256</code> over canonical bytes. Canonicalization rules must be strictly enforced across runtimes: UTF-8 encoding with no BOM, newline normalization to <code>\n</code>, consistent escaping and header ordering, and either include or exclude final newline as declared in <code>exportManifest</code>. Document decision and implement consistently. <br><strong>Return:</strong> <code>sha256:&lt;hex&gt;</code> and <code>byteLength</code>. <br><strong>Tests:</strong> standardized vector tests to ensure cross-language parity. </td></tr><tr><td data-label="REG_ApplyExport — Per-function Expert Technical Breakdown"> <strong>Function: <code>PersistExportArtifact(pathHint, bytesStream, metadata)</code> — atomic persist & fallback policies</strong><br><strong>Purpose:</strong> atomically persist artifact to configured destinations: primary evidence store (immutable), optional loader staging location, and optional backup SFTP. Use atomic write semantics: write to temp path then rename (or two-phase commit for remote stores). Persist metadata alongside: <code>operatorId</code>, <code>planId</code>, <code>applyId</code>, <code>exportChecksum</code>, <code>exportSpecRef</code>, <code>contentType</code>, <code>sizeBytes</code>, <code>canonicalVersion</code>.<br><strong>Idempotency & dedup:</strong> write-if-missing keyed by <code>exportChecksum</code> and <code>applyId</code> to avoid duplicates. If destination denies duplicate writes, return existing artifactRef. <br><strong>Fallback:</strong> if remote persist fails after retries, write to local secure staging, encrypt at rest and produce <code>stagedRef</code> and audit <code>payroll.je.export.staged</code>. Provide operator flow to push staged artifacts to evidence store or loader. <br><strong>Tests:</strong> remote failure simulation, stale rename handling, corrupt temp cleanup. </td></tr><tr><td data-label="REG_ApplyExport — Per-function Expert Technical Breakdown"> <strong>Function: <code>BuildApplyDescriptor(planId, acceptedJEsHash, beforeChecksums, operatorId, mode, approvalsRef, metadata)</code> — authoritative pre-execution record</strong><br><strong>Purpose:</strong> pre-create and persist <code>applyDescriptor</code> atomically before any outbound network or mutative action. Use this record to guarantee recoverability and idempotency. <br><strong>Required fields (schema):</strong> <code>applyId</code>, <code>planId</code>, <code>paramsHash</code>, <code>acceptedJEsHash</code>, <code>beforeChecksums</code> (ingestChecksum, glIngestChecksum, mapHash), <code>mode</code> (<code>create_export | post_direct</code>), <code>operatorId</code>, <code>approvalsRef</code>, <code>status</code> (pending), <code>createdTs</code>, <code>idempotencyVersion</code>, <code>auditChainPrev</code>, <code>metadata</code>. <br><strong>Behavioral invariants:</strong> descriptor persisted before any <code>PostDirectToGL</code> calls; any external call must reference <code>applyId</code>; descriptor is the source of truth for replay and forensic. <br><strong>Atomic persist:</strong> use evidence store atomic write and ensure durable ack before proceeding. Sign descriptor if signing is available. <br><strong>Failure & recovery:</strong> persist failure aborts apply; provide operator guidance to re-create descriptor with strict audit trail in rare recovery scenarios. <br><strong>Tests:</strong> crash recovery where process dies after descriptor persist but before posting, descriptor immutability tests. </td></tr><tr><td data-label="REG_ApplyExport — Per-function Expert Technical Breakdown"> <strong>Function: <code>IdempotencyTokenForApply(applyId, idempotencyVersion)</code> — secure derivation & policy</strong><br><strong>Purpose:</strong> generate a deterministic idempotency token used for GL calls to prevent duplicate postings. <br><strong>Derivation:</strong> canonical string <code>sha256(&quot;apply:&quot;+applyId+&quot;:&quot;+idempotencyVersion)</code> or HMAC with a signing key per security policy. Store <code>tokenVersion</code> and <code>tokenHash</code> in <code>applyDescriptor</code> but never log token material. <br><strong>Token policies:</strong> single-use vs bounded TTL; rotations require updating <code>applyDescriptor</code> with <code>idempotencyVersion</code> and a migration manifest if semantics change. <br><strong>Security:</strong> treat tokens as secrets; avoid returning token values in any non-secure channels. <br><strong>Tests:</strong> token parity, replay handling and rotation semantics. </td></tr><tr><td data-label="REG_ApplyExport — Per-function Expert Technical Breakdown"> <strong>Function: <code>AcquireEphemeralCredentials(scope, approvers, approvalsRef)</code> — secure token service integration</strong><br><strong>Purpose:</strong> request ephemeral credentials from a secure token service (modSecurity/KMS) required for <code>post_direct</code>. Validate <code>approvalsRef</code> against required approvals for the apply (two-person for regulated postings). <br><strong>Issuance rules:</strong> credentials must be short TTL (recommended ≤ 15 minutes), single-purpose, constrained by <code>scope</code> and usage limits. Record issuance event in audit (without token value) and persist <code>tokenAuditRef</code> linking issuance to <code>approvalsRef</code> and <code>applyId</code>. <br><strong>Failure & fallback:</strong> token issuance failure causes <code>post_direct</code> to fail; fallback path: operator may switch to <code>create_export</code> mode and manually post artifact. <br><strong>Tests:</strong> missing approvals rejection, TTL enforcement tests, token issuance audit presence. </td></tr><tr><td data-label="REG_ApplyExport — Per-function Expert Technical Breakdown"> <strong>Function: <code>PostDirectToGL(exportUri, applyId, operatorId, token, idempotencyToken, vendorAdapter, retryPolicy)</code> — vendor adapter orchestration</strong><br><strong>Purpose:</strong> post export to GL using vendor adapter that implements protocol semantics (REST, SOAP, SFTP). Use idempotency header/token and ephemeral auth; capture per-bundle journal confirmations and errors. <br><strong>Adapter contract:</strong> each adapter must expose: <code>uploadArtifact(uri, stream)</code>, <code>postBundle(bundle, headers)</code>, <code>queryStatus(bundleId)</code>, <code>reverseJournal(journalId)</code>. Adapter must translate vendor errors into canonical failure classes: <code>TRANSIENT</code>, <code>PERMANENT</code>, <code>DUPLICATE</code>, <code>VALIDATION_ERROR</code>. <br><strong>Execution semantics:</strong><br>1. Upload artifact or stream lines as required. <br>2. For each JE bundle, call <code>postBundle</code> with idempotency header derived from <code>idempotencyToken</code> and <code>bundleId</code>; await per-bundle acknowledgment; record <code>postedJournalId</code> if returned. <br>3. On transient errors, retry per <code>retryPolicy</code> (exponential backoff with jitter). <br>4. On duplicate responses, treat as success and record returned <code>postedJournalId</code>. <br><strong>Partial success handling:</strong> persist partial <code>applyResult</code> immediately; schedule retries for transient failures; produce <code>forensic_manifest</code> for permanent failures. <br><strong>Tests:</strong> simulated vendor responses (success / partial / duplicate / transient), idempotency replay, per-bundle acknowledgement parsing. </td></tr><tr><td data-label="REG_ApplyExport — Per-function Expert Technical Breakdown"> <strong>Function: <code>HandlePartialFailure(applyId, applyResult, operatorPolicy)</code> — triage orchestration & retry scheduling</strong><br><strong>Purpose:</strong> analyze <code>applyResult</code> and categorize failures; schedule retries for transient classes, produce <code>forensic_manifest</code> for permanent errors, escalate according to policy thresholds. <br><strong>Processing steps:</strong><br>1. Classify each failed bundle by canonical error class returned by vendor adapter. <br>2. Group transient failures and schedule <code>ApplyRetryLogic</code> jobs with configured concurrency and backoff. <br>3. Mark permanently failed bundles and persist <code>applyDescriptor.executionResult</code> with <code>permanentFailure</code> markers; add remediation suggestions to evidence. <br>4. If failure rate exceeds <code>operatorPolicy.escalationThreshold</code> (e.g., >5% of bundles), auto-pause further <code>post_direct</code> applies pending human review. <br><strong>Operator controls:</strong> provide CLI to accept or override retry decisions for specific bundles and to convert apply to revert if appropriate. <br><strong>Tests:</strong> partial failure simulations, escalation trigger, operator override flows. </td></tr><tr><td data-label="REG_ApplyExport — Per-function Expert Technical Breakdown"> <strong>Function: <code>BuildRevertDescriptor(applyDescriptor, postedJournalIds, operatorId, reason)</code> — revert planning contract</strong><br><strong>Purpose:</strong> generate and persist a <code>revertDescriptor</code> referencing original <code>applyDescriptor</code> and listing <code>postedJournalIds</code> to reverse. <code>revertId</code> deterministic: <code>sha256(&quot;revert:&quot;+applyId+&quot;:&quot;+normalizeTs)</code>. Include required approvals for regulated outputs. Persist immutably and return <code>revertDescriptorRef</code>. <br><strong>Fields:</strong> <code>revertId</code>, <code>applyId</code>, <code>postedJournalIds[]</code>, <code>requestedBy</code>, <code>approvalsRef</code>, <code>createdTs</code>, <code>revertMode</code> (<code>automated|manual</code>), <code>idempotencyToken</code>. <br><strong>Invariants:</strong> reverts are idempotent by <code>revertId</code>. <br><strong>Tests:</strong> descriptor persist parity, missing journal id handling. </td></tr><tr><td data-label="REG_ApplyExport — Per-function Expert Technical Breakdown"> <strong>Function: <code>RevertJEs(revertDescriptor, operatorId)</code> — automated revert orchestration and manual fallback</strong><br><strong>Purpose:</strong> attempt automated reversal using GL reversal APIs; if unavailable or partially failing, produce <code>forensic_manifest</code> and set revert status <code>pending_manual</code>. Persist <code>revertResult</code> with per-journal outcomes and checksums. <br><strong>Execution patterns:</strong><br>1. For each <code>postedJournalId</code>, call <code>reverseJournal</code> with <code>revertId</code> idempotency. <br>2. Monitor per-journal confirmation and persist <code>reversalJournalId</code> if returned. <br>3. For vendor lacking reversed endpoint, generate reversal JE that negates original posted JE lines and follow <code>GenerateJEExport</code>/<code>PostDirectToGL</code> with appropriate approvals. <br><strong>Failure handling:</strong> partial reversals recorded; do not attempt heuristic reversals without operator explicit approval. <br><strong>Tests:</strong> automated reversal success, partial, manual fallback packaging. </td></tr><tr><td data-label="REG_ApplyExport — Per-function Expert Technical Breakdown"> <strong>Function: <code>PersistApplyResult(applyId, applyResult, artifactRefs, afterChecksums)</code> — canonical outcome persist & optional signing</strong><br><strong>Purpose:</strong> atomically persist final <code>applyResult</code> including per-bundle statuses, <code>postedJournalIds</code>, <code>afterChecksums</code> from GL if provided, and <code>artifactRefs</code>. Optionally sign the persisted result using KMS. Update <code>applyDescriptor.status</code> to <code>completed|failed|partial</code>. <br><strong>Auditable fields:</strong> <code>applyId</code>, <code>planId</code>, <code>operatorId</code>, <code>status</code>, <code>timestamp</code>, <code>rowsAttempted</code>, <code>rowsSucceeded</code>, <code>rowsFailed</code>, <code>postedJournalCount</code>, <code>artifactRefs</code>, <code>forensicManifestRef</code>. <br><strong>Invariants:</strong> persisted outcome is canonical and used for audit/forensics. <br><strong>Tests:</strong> signed result verification, reload recovery tests. </td></tr><tr><td data-label="REG_ApplyExport — Per-function Expert Technical Breakdown"> <strong>Function: <code>EmitApplyAudit(applyDescriptorRef, applyResultSummary)</code> — PII-safe audit emission</strong><br><strong>Purpose:</strong> append a PII-minimized audit row summarizing the apply and reference evidence for detailed analysis. Audit row fields: <code>timestamp, correlationId, module=REG_ApplyExport, procedure=apply.completed|apply.failed, applyId, planId, operatorId, status, rowsAttempted, rowsSucceeded, postedJournalCount, exportChecksum, applyDescriptorRef, evidenceRefs, approvalsRef</code>. <br><strong>PII policy:</strong> audit text must not contain employee IDs, raw account numbers or transaction details; link to encrypted evidence via <code>evidenceRefs</code>. <br><strong>Rotation & retention:</strong> audits appended to append-only store and retention tags applied per regulation. <br><strong>Tests:</strong> audit presence, no-PII checks, evidenceRef validity. </td></tr><tr><td data-label="REG_ApplyExport — Per-function Expert Technical Breakdown"> <strong>Function: <code>BuildForensicManifest(applyId, artifacts[], diagnostics[], operatorId)</code> — regulatory packaging</strong><br><strong>Purpose:</strong> assemble a canonical <code>forensic_manifest.json</code> linking all run artifacts: <code>applyDescriptorRef</code>, <code>exportManifestRef</code>, <code>exportArtifactRef</code>, <code>previewRef</code>, <code>ingestManifestRefs</code>, <code>glIngestManifestRef</code>, <code>applyResult</code>, <code>errorSamples</code>, checksums, and chain-of-custody entries. Persist immutably to evidence store and return <code>forensic_manifest_ref</code>. This is the authoritative package for regulators and incident response.<br><strong>Contents:</strong> standardized artifact list, sha256 checksums, <code>createdBy</code>, <code>createdTs</code>, <code>retentionPolicy</code>, and <code>accessControl</code> hints. <br><strong>Tests:</strong> manifest completeness tests, checksum validation routines. </td></tr><tr><td data-label="REG_ApplyExport — Per-function Expert Technical Breakdown"> <strong>Function: <code>ApplyRetryLogic(applyId, failedBundleIds, policy)</code> — governed retry orchestration</strong><br><strong>Purpose:</strong> create <code>retryJobDescriptor</code> persisted with <code>jobId</code>, <code>applyId</code>, <code>bundleIds</code>, <code>attemptNumber</code>, <code>nextAttemptTs</code>. Worker picks job, locks descriptor, re-attempts POST for listed bundles respecting idempotency and vendor rate limits. <br><strong>Policy parameters:</strong> <code>maxAttempts</code>, <code>initialDelayMs</code>, <code>backoffFactor</code>, <code>maxDelayMs</code>, <code>maxParallelRetries</code>. <br><strong>Poison-queue handling:</strong> after exceeding <code>maxAttempts</code> mark bundle <code>permanent_failed</code> and escalate. <br><strong>Tests:</strong> retry backoff, job reincarnation idempotency, poison queue behavior. </td></tr><tr><td data-label="REG_ApplyExport — Per-function Expert Technical Breakdown"> <strong>Function: <code>ExportManifestSchema()</code> & <code>ApplyDescriptorSchema()</code> — canonical schema & versioning</strong><br><strong>Purpose:</strong> publish strict JSON Schemas enumerating all required fields, types, and examples. Schemas must include <code>canonicalVersion</code> and be used by CI and runtime validators. Any breaking schema change requires migration manifest and golden tests. <br><strong>Tests:</strong> JSON Schema validation in CI, schema compatibility checks. </td></tr><tr><td data-label="REG_ApplyExport — Per-function Expert Technical Breakdown"> <strong>Function: <code>SecurityAndSecretsPolicy()</code> — secrets lifecycle & signing contract</strong><br><strong>Purpose:</strong> define required handling of secrets: ephemeral tokens required for <code>post_direct</code>, never persist long-lived GL credentials, use KMS/HSM for signing <code>applyDescriptor</code> and <code>applyResult</code>, rotate signing keys periodically, and audit token issuance. <br><strong>Hard constraints:</strong> do not log tokens; keep <code>tokenId</code> or token hash only in audit not token contents. <br><strong>Tests:</strong> static analysis for token leakage and signing verification. </td></tr><tr><td data-label="REG_ApplyExport — Per-function Expert Technical Breakdown"> <strong>Function: <code>CI_and_GoldenParity()</code> — CI gating & migration enforcement</strong><br><strong>Purpose:</strong> CI must run end-to-end golden parity tests to ensure canonical export bytes and checksums match golden fixtures. PRs modifying serialization/rounding/column order must include <code>migration_manifest</code> with before/after fixtures and approvals. Nightly parity runner recomputes golden checksums and opens incidents on drift. <br><strong>Checks included in CI:</strong> unit tests, property tests for determinism, integration with mocked GL adapters, golden parity, signature verification, forbidden-API checks (no secrets in logs). <br><strong>Tests:</strong> golden parity run, migration manifest presence enforcement. </td></tr><tr><td data-label="REG_ApplyExport — Per-function Expert Technical Breakdown"> <strong>Observability, SLOs & telemetry for apply flows — metrics and alerts</strong><br><strong>Primary metrics:</strong> <code>apply.start</code>, <code>apply.duration_ms</code>, <code>apply.success</code>, <code>apply.partial_failure_count</code>, <code>apply.retry_count</code>, <code>apply.revert.count</code>, <code>export.persist.latency_ms</code>, <code>idempotency.duplicate.detected</code>. Tag with <code>planId</code>, <code>applyId</code>, <code>operatorId</code>, <code>standardMapHash</code>. <br><strong>Suggested SLOs:</strong><br>1. Successful applies (no duplicates) ≥ 99.95%. <br>2. <code>export</code> persist median latency < 500ms local, <2s remote. <br>3. Post direct confirmation median <= vendor SLA (recommend <30s synchronous). <br><strong>Alerts:</strong> high partial failure rate, idempotency duplicate spikes, staged exports rising, retry saturation higher than threshold. </td></tr><tr><td data-label="REG_ApplyExport — Per-function Expert Technical Breakdown"> <strong>Operator UX & runbook excerpts — commands and first actions</strong><br><strong>Common CLI patterns (operator):</strong><br>• <code>recon.generate-je --applyId &lt;applyId&gt; --exportSpec GL_CSV_v1</code> → returns <code>exportManifestRef</code> and <code>exportChecksum</code>.<br>• <code>recon.apply --applyId &lt;applyId&gt; --mode post_direct --approvals ap-1,ap-2</code> → persists <code>applyDescriptor</code> and attempts post. <br>• <code>recon.apply.retry --applyId &lt;applyId&gt; --bundles &lt;id,...&gt;</code> → schedules retries. <br>• <code>recon.revert --applyId &lt;applyId&gt; --operator &lt;id&gt;</code> → triggers <code>BuildRevertDescriptor</code> / <code>RevertJEs</code>. <br><strong>Triage checklist (first actions):</strong> capture <code>correlationId</code> → fetch <code>applyDescriptor</code> and <code>applyResult</code> → fetch <code>forensic_manifest</code> if present → attempt <code>ApplyRetryLogic</code> for transient bundles → if systemic, run <code>RevertJEs</code> and prepare regulator package. </td></tr><tr><td data-label="REG_ApplyExport — Per-function Expert Technical Breakdown"> <strong>Forensic & regulator packaging — required contents</strong><br>Package must include: signed <code>applyDescriptor</code>, <code>export_manifest</code>, <code>exportArtifact</code> checksum, <code>applyResult</code>, <code>forensic_manifest</code> containing <code>ingest</code>/<code>preview</code> manifests, signed <code>recon_report</code>, migration_manifest (if export semantics changed), and chain-of-custody logs. Evidence retrieval gated by <code>approvalsRef</code>. </td></tr><tr><td data-label="REG_ApplyExport — Per-function Expert Technical Breakdown"> <strong>Failure modes, prioritized triage & escalation thresholds</strong><br><strong>Common faults & immediate mitigation:</strong><br>1. Export persist failure -> check staging, push staged artifact, if impossible escalate to Ops. <br>2. Partial GL post success -> examine <code>applyResult</code>, schedule retries, if too many permanent errors consider <code>RevertJEs</code>. <br>3. Duplicate posting detected -> reconcile with GL, reverse duplicates, and update <code>applyResult</code>. <br>4. Token issuance failure -> fallback to <code>create_export</code> and request manual post. <br>5. Mass failure (> threshold) -> halt <code>post_direct</code>, gather <code>forensic_manifest</code>, and open incident. <br><strong>Escalation thresholds (recommended):</strong> auto-pause posting if >5% bundles permanently fail or >3 consecutive applies show duplicate posting anomalies. </td></tr><tr><td data-label="REG_ApplyExport — Per-function Expert Technical Breakdown"> <strong>Performance & scaling guidance (practical) — engineering notes</strong><br><strong>Patterns:</strong> stream serialization, chunked uploads, worker pool autoscaling, per-vendor concurrency limits, back-pressure. <br><strong>Large exports:</strong> chunked streaming with stitchable checksum; checkpoint after each chunk; resume by skipping completed chunk offsets. <br><strong>Retries:</strong> exponential backoff with jitter and per-vendor rate-limit awareness. <br><strong>Monitoring:</strong> track top-N slow rules, top failing bundles, retry queue depth. </td></tr><tr><td data-label="REG_ApplyExport — Per-function Expert Technical Breakdown"> <strong>Security & PII handling — obligations and patterns</strong><br><strong>PII policy:</strong> audits contain no PII; full sanitized evidence stored encrypted and referenced by <code>evidenceRef</code>; evidence retrieval requires approvals and chain-of-custody logging. <br><strong>Secrets policy:</strong> ephemeral tokens only; sign artifacts via KMS/HSM; never log credential material; restrict token scope and TTL. </td></tr><tr><td data-label="REG_ApplyExport — Per-function Expert Technical Breakdown"> <strong>Conceptual Power Query (PQ) guidance (practical, conceptual - no code)</strong><br><strong>Export preparation pattern in PQ:</strong><br>1. Materialize <code>acceptedJEs</code> into a stable query table with typed columns and explicit schema; avoid relying on implicit locale conversions. <br>2. Convert decimal amounts to <code>AmountMinorUnits</code> using deterministic number parsing and explicit <code>scale</code> variable stored in query parameters; do not use locale default parse. <br>3. Ensure deterministic ordering by adding explicit sort keys (journalRef, lineNo or suggestionId). <br>4. When serializing CSV from PQ, generate canonical text using explicit <code>Text.Combine</code> steps for columns in canonical order and compute a sha256 of the concatenated canonical text (via extension or export helper) for export checksum. <br>5. For large exports, produce chunked outputs by partitioning table and exporting sequentially; produce chunk manifest and stitched checksum. <br><strong>PQ testing guidance:</strong> use deterministic fixtures in PQ samples; compute checksums in a test query and compare to golden checksums in CI. </td></tr><tr><td data-label="REG_ApplyExport — Per-function Expert Technical Breakdown"> <strong>Conceptual DAX guidance for QA dashboards & invariants (no code)</strong><br><strong>Key measures to create:</strong><br>1. <code>ExportRowCount</code> — ensure exported rows equal expected rows. <br>2. <code>ExportBalanceVariance</code> — difference between total debits and credits in export; should be zero. <br>3. <code>ApplySuccessRate</code> — ratio of fully posted bundles to attempted bundles over time. <br>4. <code>PartialFailureCount</code> — number of applies with any partial bundle failures. <br>5. <code>RevertCount</code> — count of revert operations and their success rate. <br><strong>Dashboard suggestions:</strong> per-apply drill down showing top failed bundles, retry counts, median apply latency, and correlation with <code>standardMapHash</code> or <code>exportSpec</code>. Use slicers for <code>planId</code>, <code>operatorId</code>, and vendor adapter. </td></tr><tr><td data-label="REG_ApplyExport — Per-function Expert Technical Breakdown"> <strong>Developer safe patterns & recommended tests matrix (concise checklist)</strong><br><strong>Pre-merge CI gates:</strong> unit tests, golden parity, schema validation, static secret scanning, vendor adapter simulations. <br><strong>Recommended tests:</strong> serializer parity across runtimes, idempotency replay, ephemeral token lifecycle, staged export push, partial failure orchestration, automated revert, forensic packaging retrieval. <br><strong>Dev safety patterns:</strong> persist <code>applyDescriptor</code> before any outbound calls; never block UI thread; use streaming writers; centralize canonicalization code for cross-language parity; always attach <code>applyId</code> and <code>correlationId</code> to telemetry; sign apply results for regulatory runs. </td></tr><tr><td data-label="REG_ApplyExport — Per-function Expert Technical Breakdown"> <strong>Example end-to-end narratives (expanded) — three scenarios with evidence paths</strong><br><strong>Scenario 1 — Export only (happy path):</strong><br>1. <code>GenerateJEExport</code> called with 312 acceptedJEs and CSV v1 spec. <br>2. All bundles balanced; serializer streams canonical CSV and computes sha256 <code>sha256:abc123</code>. <br>3. <code>PersistExportArtifact</code> writes to evidence store and returns <code>artifactRef</code>. <br>4. <code>BuildApplyDescriptor</code> persisted with <code>status=pending</code> referencing <code>exportChecksum</code>. <br>5. Mode <code>create_export</code> completes; audit <code>payroll.je.exported</code> and <code>payroll.je.apply.completed</code> emitted with <code>evidenceRef</code>. <br><strong>Scenario 2 — Post direct with partial failure and recoverable retries:</strong><br>1. <code>BuildApplyDescriptor</code> persisted. <br>2. <code>AcquireEphemeralCredentials</code> issued token. <br>3. <code>PostDirectToGL</code> posts bundles; GL returns 300 success, 12 transient errors, 5 permanent validation rejects. <br>4. <code>HandlePartialFailure</code> schedules retries for 12 transient bundles via <code>ApplyRetryLogic</code>. <br>5. After retries, 10 succeed, 2 persist failures; operator inspects and decides to revert 2 posted bundles; <code>BuildRevertDescriptor</code> created and <code>RevertJEs</code> executed. For permanent rejects, <code>forensic_manifest</code> assembled. <br><strong>Scenario 3 — Vendor duplicate detection & rollback:</strong><br>1. GL reports a duplicate for a subset due to earlier manual posting; adapter returns <code>DUPLICATE</code> with <code>postedJournalId</code>. <br>2. System records duplicates as success and logs <code>idempotency.duplicate.detected</code> metric. <br>3. If duplicate leads to net double posting for a reconciliation pairing, operator triggers <code>RevertJEs</code> and packages <code>forensic_manifest</code> for compliance. </td></tr><tr><td data-label="REG_ApplyExport — Per-function Expert Technical Breakdown"> <strong>Forensic & regulator packaging checklist (must-have set) — single line enumerated list for QA</strong><br>1. Signed <code>applyDescriptor</code> persisted. <br>2. <code>export_manifest</code> with checksum and canonicalVersion. <br>3. Export artifact <code>artifactRef</code> and bytes hash. <br>4. <code>applyResult</code> with per-bundle statuses and <code>postedJournalIds</code>. <br>5. <code>forensic_manifest</code> linking ingest/preview/manifests. <br>6. Signed <code>recon_report</code> summarizing run. <br>7. Chain-of-custody access logs for evidence retrieval. <br>8. Migration manifest if semantics changed. </td></tr><tr><td data-label="REG_ApplyExport — Per-function Expert Technical Breakdown"> <strong>Retention, archival & access controls — rules summary</strong><br><strong>Retention tiers:</strong> hot=30d, warm=7y, cold=per-regulation archival (WORM). Evidence access requires <code>approvalsRef</code> and is recorded in <code>chainOfCustody</code>. Signed artifacts preserved with rotation policy; periodic <code>VerifyReportParity</code> job recomputes <code>reportHash</code> for signed manifests. </td></tr><tr><td data-label="REG_ApplyExport — Per-function Expert Technical Breakdown"> <strong>Risk analysis & mitigations — key threats and controls (concise)</strong><br><strong>Risks & mitigations:</strong><br>1. Data corruption in transit → use atomic write + temp rename + chunk checksum and VerifyReportParity. <br>2. Duplicate postings → idempotency tokens and server-side dedup; reconcile duplicates during apply. <br>3. Secret leakage → ephemeral tokens, no logs, KMS signing. <br>4. Malformed exports accepted by GL → strict <code>ValidateExportSpec</code> + loader-simulated CI tests. <br>5. Audit tampering → signing, WORM storage, independent parity checks. </td></tr><tr><td data-label="REG_ApplyExport — Per-function Expert Technical Breakdown"> <strong>Developer quick reference — do/don't (short actionable list)</strong><br><strong>Do:</strong> centralize canonical serializer, persist descriptor before calls, stream large exports, use ephemeral tokens, attach <code>correlationId</code>. <br><strong>Don't:</strong> log token material, perform blocking IO on UI thread, change serialization semantics without migration_manifest. </td></tr><tr><td data-label="REG_ApplyExport — Per-function Expert Technical Breakdown"> <strong>Final verification & "10 checks" pre-release checklist (must be performed)</strong><br>1. Schema validation pass for <code>exportManifest</code> and <code>applyDescriptor</code>. <br>2. Unique deterministic <code>applyId</code> and idempotency token parity. <br>3. Golden export checksum parity across runtimes. <br>4. Balance checks across sample bundles including rounding edge cases. <br>5. Token issuance and TTL tests. <br>6. Vendor adapter simulation for success/partial/duplicate/failure. <br>7. Forensic manifest completeness test. <br>8. Audit chain presence and PII checks. <br>9. Performance microbench (streaming export). <br>10. Migration manifest recorded for any semantic change and approvals captured. </td></tr><tr><td data-label="REG_ApplyExport — Per-function Expert Technical Breakdown"> <strong>Closing statement (implementation posture)</strong><br>This exhaustive per-function breakdown provides deterministic contracts, safety gates, audit-first patterns, CI gating, vendor adapter contracts, PQ conceptual guidance for export preparation, and DAX conceptual measures for post-release QA. Implementers must preserve canonicalization code in a single authoritative library ported across runtime targets (VBA, Power Query, backend), include cross-runtime golden fixtures, and enforce migration-manifest governed rollouts for semantic changes. </td></tr></tbody></table></div><div class="row-count">Rows: 38</div></div><div class="table-caption" id="Table4" data-table="Docu_0184_04" style="margin-top:2mm;margin-left:3mm;"><strong>Table 4</strong></div>
<div class="table-wrapper" data-table-id="table-4"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by REG_EvidenceAudit — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">REG_EvidenceAudit — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="REG_EvidenceAudit — Per-function Expert Technical Breakdown"> <strong>Preface & scope (top-level)</strong><br>REG_EvidenceAudit is the authoritative evidence service for Project 616. It provides immutable evidence persistence, canonical hashing, signature provenance, chain-of-custody (CoC), approval-gated retrieval for PII/regulated artifacts, scheduled parity verification, retention enforcement, forensic-package assembly, and operator-facing runbooks. This expanded breakdown documents every public function, deterministic contracts, failure modes and recovery, implementation patterns, operational examples, conceptual Power Query (PQ) guidance and conceptual DAX measures and reports for observability. All numbered lists use <code>&lt;br&gt;</code> line breaks per project style. The document is intended for implementers (VBA/PQ/backend), SRE, compliance and QA teams. </td></tr><tr><td data-label="REG_EvidenceAudit — Per-function Expert Technical Breakdown"> <strong>Global design invariants (apply to all functions)</strong><br>1. <strong>Immutability</strong>: persisted evidence objects are immutable; updates produce new evidenceRefs.<br>2. <strong>Canonicalization</strong>: canonicalization rules are versioned (<code>canonicalVersion</code>) and shared across runtimes; canonical checksums computed with sha256 over canonical UTF-8 bytes. <br>3. <strong>Idempotency</strong>: writes with identical (payload bytes + canonicalVersion + critical metadata) return existing evidenceRef. <br>4. <strong>Minimal PII in audits</strong>: audit rows are PII-minimal; full sanitized evidence persisted encrypted and referenced via <code>evidenceRef</code>. <br>5. <strong>Signing & provenance</strong>: artifacts requiring legal provenance are signed with HSM/KMS-backed keys; signatures persisted as separate evidence objects. <br>6. <strong>CoC logging</strong>: every persist/sign/access/export operation appends a CoC entry and an audit row. <br>7. <strong>Approval gating</strong>: retrieval of <code>sensitive</code> artifacts requires approvals recorded as <code>approvalsRef</code>. <br>8. <strong>Streaming & memory safety</strong>: implementations must support streaming persist/retrieve to avoid large memory usage. <br>9. <strong>Fallback & staging</strong>: on storage outages use encrypted local staging with reliable retry and audit <code>evidence.persist.staged</code>. <br>10. <strong>Golden parity & CI</strong>: canonicalization and recon_report assembly require cross-runtime golden fixtures and CI parity gates. </td></tr><tr><td data-label="REG_EvidenceAudit — Per-function Expert Technical Breakdown"> <strong>API index (expanded)</strong><br>1. <code>PersistEvidenceBlob(bytesOrStream, metadata)</code> — atomic persist and idempotent evidence registration. <br>2. <code>ComputeArtifactChecksum(evidenceRef | bytes, canonicalVersion)</code> — canonical checksum computation. <br>3. <code>BuildReconciliationReport(runId, artifacts[], operatorMeta)</code> — canonical recon_report assembly and signing. <br>4. <code>VerifyReportParity(reportRef, depth)</code> — recompute and verify recon_report and referenced artifacts. <br>5. <code>PersistSignedArtifact(evidenceRef, signerId, signingKeyId, purpose)</code> — sign artifact and persist signature evidence. <br>6. <code>EvidenceAccessApprovalFlow(request)</code> — approval workflow and issuance of short-lived retrieval tokens. <br>7. <code>RetrieveEvidenceBlob(evidenceRef, accessToken|operatorCreds, options)</code> — controlled retrieval (redacted/full), streaming, chunked. <br>8. <code>RecordChainOfCustody(evidenceRef, actorId, action, reason, metadata)</code> — append CoC entry (Merkle/chained). <br>9. <code>ExportForensicPack(runId, selectors[], operatorId, level)</code> — forensic bundle assembly, sign, persist, tag legal hold. <br>10. <code>EnforceRetentionPolicy(evidenceRef)</code> — apply retention, archive, schedule deletion (never delete under legal hold). <br>11. <code>RotateSigningKey(newKeyId, operatorId, rollStrategy)</code> — key rotation orchestration and re-sign policy. <br>12. <code>ValidateArtifactIntegrityOnStartup()</code> — lightweight startup integrity checks & deep-job scheduling. <br>13. <code>AppendAuditRow(module,event,payload)</code> — append-only audit row writer (chain linked). <br>14. <code>RegisterUnitTestHook(hookName)</code> — CI test harness for deterministic golden runs. <br>15. <code>PersistDeferredSignature(evidenceRef)</code> — queue worker for deferred signing. <br>16. <code>ListEvidence(queryParams)</code> — catalog index query (pagination, filters, sensitivity tags). <br>17. <code>GetEvidenceMetadata(evidenceRef)</code> — read-only metadata object (no blob). <br>18. <code>RehydrateEvidenceToLocal(evidenceRef,targetPath,options)</code> — controlled export to local staging for forensic analysis. </td></tr><tr><td data-label="REG_EvidenceAudit — Per-function Expert Technical Breakdown"> <strong><code>PersistEvidenceBlob(bytesOrStream, metadata)</code> — contract, invariants, examples, tests</strong><br><strong>Purpose & contract:</strong> atomically persist an evidence blob (raw or canonical JSON) and register an immutable evidence entry returning <code>evidenceRef</code>. Validate metadata schema, optionally canonicalize payload (per <code>metadata.canonicalVersion</code>), compute checksum (sha256), perform streaming atomic write, persist metadata in catalog, set retention/legalHold flags, and emit <code>evidence.persisted</code> audit row. Must be idempotent for identical (payload, canonicalVersion, artifactType, criticalMetadata).<br><strong>Inputs & outputs:</strong><br>- <code>bytesOrStream</code> (binary stream or bytes).<br>- <code>metadata</code> object including required fields: <code>artifactType</code> (enum), <code>createdBy</code>, <code>createdTs</code> (ISO8601), <code>canonicalVersion</code> (nullable), <code>retentionPolicy</code> (object), <code>sensitivityTags[]</code> (e.g., <code>pii</code>, <code>regulated</code>), <code>contentType</code> (MIME), <code>compressHint</code> (true/false), <code>encrypt</code> (default true), optional <code>originalSource</code> and <code>payloadFingerprint</code>.<br>- Returns <code>{evidenceRef, checksum:&quot;sha256:&lt;hex&gt;&quot;, sizeBytes, storageUri, canonicalized:boolean, metadataRef}</code> or structured error <code>{errorCode,diagnostics}</code>.<br><strong>Primary invariants:</strong><br>1. Immutability: persisted artifact and persisted metadata are immutable; any change produces new evidenceRef.<br>2. Idempotency: deterministic evidenceRef mapping for same content + canonicalVersion + artifactType + ownerId (exact keys to be defined in spec).<br>3. Canonicalization: if <code>canonicalVersion</code> provided, canonicalize prior to checksum; canonicalization rules removed transients explicitly documented.<br>4. Legal hold: if <code>legalHold=true</code>, evidence must be stored with deletion blocked until hold removed.<br><strong>Implementation details & safe I/O:</strong><br>1. Stream-write with rolling SHA256 incremental digest to avoid full memory; write to temporary path and perform an atomic rename/commit. <br>2. Use envelope encryption with KMS: generate data key via KMS, AEAD encrypt stream, store <code>encryptionKeyId</code> in metadata but never store raw keys. <br>3. If canonicalization required: stream-apply canonicalization transforms then digest; do not compute canonical digest after compression. <br>4. For very large blobs, persist chunked with manifest referencing chunk checksums and combined <code>checksum</code> computed deterministically (e.g., sha256 of concatenated chunk checksums). <br>5. Catalog registration must be in same transaction window where possible: ensure storage atomic commit then DB insert; if storage commit but DB insert fails, schedule reconciliation and mark object <code>orphanPendingCatalog</code> with alert. <br>6. For staging fallback: encrypt and persist to local staging store, persist <code>evidence.persist.staged</code> audit and schedule reliable replay worker. <br><strong>Observability & audit:</strong> emit <code>evidence.persisted{evidenceRef,artifactType,checksum,sizeBytes,createdBy,createdTs,canonicalVersion,retentionPolicy}</code> and append <code>AppendAuditRow(module=&quot;REG_EvidenceAudit&quot;,event=&quot;evidence.persisted&quot;,payload={...})</code>. Increment metrics <code>evidence.persist.count</code> and <code>evidence.persist.duration_ms</code>.<br><strong>Failure modes & recovery:</strong><br>1. Storage transient error → exponential backoff retries; after threshold stage to local encrypted queue and alert SRE. <br>2. Canonicalization failure (invalid JSON): if <code>allowRawOnCanonicalFailure</code> not set, reject with <code>EVID_CANONICALIZE_ERROR</code>; if allowed, persist raw blob with <code>canonicalized=false</code> and emit <code>evidence.canonicalize.warning</code>. <br>3. KMS failure → fail and queue for operator review; do not persist unencrypted. <br>4. DB persist fails after blob write → reconcile by worker re-registering metadata after verifying blob integrity. <br><strong>Examples (operational):</strong><br>- Persisting preview bundle: call with <code>artifactType=preview</code>, <code>canonicalVersion=v1</code>, <code>sensitivityTags=[redacted]</code>, <code>retentionPolicy={retentionYears:7,tier:&quot;warm&quot;}</code>, stream zipped preview; returns <code>evidenceRef: evid:preview_p_20260121_v1</code> and checksum. <br>- Persisting large aggregate CSV: stream in 64MB chunks; register chunk manifest and produce combined <code>checksum</code> per policy. <br><strong>Tests & CI:</strong><br>1. Unit: idempotency across repeated writes; checksum parity tests. <br>2. Integration: streaming write → retrieve → checksum compare; KMS mocked sign/verify. <br>3. Fault injection: storage write failures, KMS failures, catalog DB txn rollbacks. <br>4. Golden: canonicalization vectors for canonicalVersion v1. <br><strong>PQ conceptual note:</strong> supply a small M connector that can upload a binary result from a query refresh to <code>PersistEvidenceBlob</code> via secure connector; production connectors must use service-account tokens and never store credentials in workbook. <br><strong>DAX conceptual note:</strong> register persisted evidence as a dimension table in the analytics model with fields <code>artifactType</code>, <code>createdTs</code>, <code>checksum</code>, and create measures <code>EvidenceCountBySensitivity</code> and <code>AvgPersistLatency</code>. </td></tr><tr><td data-label="REG_EvidenceAudit — Per-function Expert Technical Breakdown"> <strong><code>ComputeArtifactChecksum(evidenceRef | bytes, canonicalVersion)</code> — canonical checksum recipe & parity</strong><br><strong>Purpose & contract:</strong> compute canonical checksum (sha256) over canonicalized artifact bytes and return <code>sha256:&lt;hex&gt;</code>; accepts either an existing <code>evidenceRef</code> or raw <code>bytes</code> with <code>canonicalVersion</code>. The canonicalization algorithm is authoritative and versioned. Implementations in all runtimes must produce identical hashes for identical semantic content. <br><strong>Inputs & outputs:</strong><br>- Input options: <code>evidenceRef</code> OR <code>(bytes, canonicalVersion, options:{normalizeNewlines:boolean,stripTransientFields:list})</code>.<br>- Output: <code>{checksum:&quot;sha256:&lt;hex&gt;&quot;, canonicalVersionUsed, canonicalPayloadRef?}</code> or <code>{errorCode, diagnostics}</code>.<br><strong>Canonicalization rules (summary):</strong><br>1. Normalize newlines to <code>\n</code>.<br>2. Unicode normalization NFKC for textual content (unless canonicalVersion explicitly mandates NFC).<br>3. JSON canonicalization: lexicographic key ordering; remove transient fields defined in canonicalization spec (e.g., <code>createdTs</code> unless explicitly required); canonical numeric formatting (fixed decimal precision where specified); deterministic escaping for special characters. <br>4. For CSV: normalize line endings, trim trailing whitespace, normalize header names via header canonicalization map, and ensure deterministic column ordering if canonical recipe requires it. <br><strong>Implementation notes:</strong><br>1. Provide reference canonicalization library for VBA/PQ/backend; include test vectors. <br>2. For <code>evidenceRef</code> reading: stream decrypt/decompress then canonicalize; avoid full materialization for large blobs by streaming canonical transforms and incremental hashing. <br>3. When canonicalization involves removal of transient fields, those removed must be documented and included in <code>canonicalVersion</code> diff notes. <br><strong>Observability:</strong> emit <code>evidence.checksum.computed{evidenceRef?,checksum,canonicalVersion}</code> and metric <code>evidence.checksum.duration_ms</code>. <br><strong>Failure & recovery:</strong> read/decrypt failure → <code>evidence.checksum.read_error</code> and attempt redundancy reads; canonicalization mismatch across runtimes → trigger CI parity alert and rollback policy. <br><strong>Tests & CI rules:</strong> cross-language parity tests (VBA/PQ/backend) using canonical vectors, injection tests for transient fields, and canonicalVersion upgrade tests. <br><strong>Example:</strong> recomputing checksum for <code>recon_report</code> before signing to ensure consistent hash across release systems; used as input to <code>PersistSignedArtifact</code>. <br><strong>Conceptual PQ guidance:</strong> in M implement deterministic column ordering and header normalization step before exporting canonical JSON to compute checksum via a secure service call. <br><strong>Conceptual DAX guidance:</strong> create measure <code>ChecksumMatch</code> that compares recomputed checksum to stored checksum for each artifact and displays mismatch counts on an integrity dashboard. </td></tr><tr><td data-label="REG_EvidenceAudit — Per-function Expert Technical Breakdown"> <strong><code>BuildReconciliationReport(runId, artifacts[], operatorMeta)</code> — canonical recon_report assembly, signing and retention</strong><br><strong>Purpose & contract:</strong> assemble <code>recon_report</code> as the authoritative run manifest linking all artifacts for a reconciliation run; compute canonical <code>reportHash</code>, persist the canonical JSON with <code>PersistEvidenceBlob</code>, sign the <code>reportHash</code> with HSM/KMS via <code>PersistSignedArtifact</code> (if signing enabled), attach retention/legal-hold metadata, and emit <code>payroll.recon.report.generated</code> audit. The <code>recon_report</code> is the legal artifact used by auditors. <br><strong>Inputs & outputs:</strong><br>- Inputs: <code>runId</code>, <code>artifacts[]</code> (each <code>{artifactType,evidenceRef,artifactChecksum,createdBy,createdTs}</code>), <code>operatorMeta</code> (<code>operatorId</code>, <code>correlationId</code>), <code>retentionPolicy</code>, <code>signaturePolicy</code> (mustSign:boolean).<br>- Outputs: <code>{reportRef,reportHash,signatureRef?}</code> or error with diagnostics. <br><strong>Canonical manifest structure & canonicalization rules:</strong><br>1. <code>recon_report</code> is a JSON object including <code>runId</code>, <code>createdTs</code>, <code>operatorMeta</code>, <code>artifacts[]</code>, <code>metadata</code> and <code>canonicalVersion</code> for recon_report format. <br>2. Sort <code>artifacts[]</code> lexicographically by <code>artifactType</code> then <code>evidenceRef</code> before serialization. <br>3. Remove ephemeral runtime-only fields from artifacts and operatorMeta prior to canonicalization. <br>4. Use compact UTF-8 JSON with stable numeric serialization. <br><strong>Signing & provenance:</strong><br>1. Compute <code>reportHash = sha256(canonical_json_bytes)</code>. <br>2. If <code>signaturePolicy.mustSign=true</code>, call <code>PersistSignedArtifact(reportEvidenceRef, signerId, signingKeyId, purpose=&quot;recon_report&quot;)</code>. <br>3. Persist signature as <code>evid:signature_&lt;reportHash&gt;_&lt;keyId&gt;.json</code> containing signerId, keyId, signatureAlgorithm, signature bytes (base64), signatureTs and verify info. <br><strong>Observability & audit:</strong> emit <code>payroll.recon.report.generated{runId,reportRef,reportHash,signatureRef,operatorId}</code> and metric <code>recon.report.gen.duration_ms</code>. Append audit row via <code>AppendAuditRow</code>. <br><strong>Failure & recovery:</strong><br>1. Missing artifact or checksum mismatch → abort and return <code>recon.report.invalid</code> with <code>missingArtifacts[]</code>. <br>2. Signing unavailability → persist unsigned report and emit <code>recon.report.unsigned.warning</code>, queue <code>PersistDeferredSignature</code> worker. <br>3. Storage/register failure after report JSON persisted → schedule reconciliation worker to ensure signature/persist completeness. <br><strong>Examples (operator):</strong><br>- Happy path: artifacts list includes <code>ingestManifestRef</code>, <code>mapRef</code>, <code>previewRef</code>, <code>exportRef</code>, <code>applyDescriptorRef</code>. Build canonical recon_report, compute <code>reportHash</code>, sign with <code>signingKeyId=prod-keys/2026-01</code> and return <code>recon_report</code> evidenceRef. <br>- Deferred signing: if HSM outage at commit, persist report w/ <code>signatureDeferred=true</code> and queue deferred signing job. <br><strong>Tests & CI:</strong> golden recon_report fixtures for canonical artifact lists; signature verify tests; missing artifact negative tests; deferred signing worker tests. <br><strong>Conceptual PQ guidance:</strong> M pipelines can create a table of artifact rows extracted from various queries; PQ should enforce stable ordering and export the canonical manifest JSON to the backend signing endpoint. <br><strong>Conceptual DAX guidance:</strong> create <code>ReconReportHealth</code> measure to show parity between stored <code>reportHash</code> and recomputed hash, and <code>UnsignedReportsCount</code> tile for compliance. </td></tr><tr><td data-label="REG_EvidenceAudit — Per-function Expert Technical Breakdown"> <strong><code>VerifyReportParity(reportRef, depth=shallow|deep)</code> — periodic verification & incident triggers</strong><br><strong>Purpose & contract:</strong> re-compute canonical hash of stored <code>recon_report</code> (shallow) and optionally deep-verify each referenced artifact checksum and signature (deep). Returns <code>{ok:true}</code> or <code>{ok:false,diagnostics[]}</code>. Must be idempotent, resumable, and produce audit rows and alerts on failures. <br><strong>Inputs & outputs:</strong> <code>reportRef</code>, <code>depth</code> (default <code>shallow</code>). Output <code>{ok,diagnostics}</code>. <br><strong>Behavior:</strong><br>1. <code>shallow</code>: read <code>recon_report</code> blob, recompute canonical <code>reportHash</code>, compare to stored <code>reportHash</code>, verify signature if present. <br>2. <code>deep</code>: additionally iterate <code>artifacts[]</code> and for each call <code>ComputeArtifactChecksum(evidenceRef)</code>, compare to <code>artifactChecksum</code> stored in report; verify each artifact's signature if applicable. <br>3. On mismatch produce <code>verify.recon_report.failed</code> audit, sample CoC and create <code>forensic.pack</code> automatically if critical. <br><strong>Observability & audit:</strong> emit <code>verify.recon_report.{passed|failed}{reportRef,reportHash,depth,diagnostics}</code> and metric <code>verify.recon_report.failure_rate</code>. <br><strong>Failure & recovery:</strong><br>1. Hash mismatch → escalate: collect redundant copies, attempt rehydrate from replica, produce <code>forensic.pack</code>, open incident for SRE & compliance. <br>2. Signature invalid → check key rotation state; if key rotated recently with deferred re-sign, check <code>deferredSignature</code> queue; otherwise escalate. <br>3. Missing artifact → <code>verify.recon_report.partial</code> and create remediation task to recover missing artifact from staging/backups. <br><strong>Implementation notes:</strong> scheduled nightly for regulated artifacts; for large corpuses sample deep verification with priority for high-sensitivity artifacts. <br><strong>Tests & CI:</strong> corruption injection tests, deep-check stress tests, signature invalidation tests. <br><strong>Conceptual PQ guidance:</strong> build a verification reporting M query to run shallow verification results into a compliance table for analytics. <br><strong>Conceptual DAX guidance:</strong> <code>DailyVerifyFailures</code> measure, alert rule for <code>verify.recon_report.failure_rate</code> above threshold. </td></tr><tr><td data-label="REG_EvidenceAudit — Per-function Expert Technical Breakdown"> <strong><code>PersistSignedArtifact(evidenceRef, signerId, signingKeyId, purpose)</code> — sign artifact, persist signature evidence</strong><br><strong>Purpose & contract:</strong> sign canonical artifact hash (e.g., recon_report.reportHash) using HSM/KMS-backed key and persist signature as a separate evidence blob with metadata linking to original artifact. Return <code>signatureRef</code>. <br><strong>Inputs & outputs:</strong><br>- Inputs: <code>evidenceRef</code> (or <code>reportHash</code> if caller supplied), <code>signerId</code>, <code>signingKeyId</code>, <code>purpose</code>, optional <code>ticketId</code> and <code>notes</code> for audit. <br>- Output: <code>{signatureRef,signingKeyId,signatureAlgorithm,signatureTs}</code> or error. <br><strong>Contract invariants:</strong> sign canonical <code>sha256</code> not raw bytes; signature binding must be verifiable and include signer identity and key id. <br><strong>Implementation notes:</strong><br>1. Use KMS/HSM sign API (PKCS#11 or cloud KMS sign) with private keys never exported. <br>2. Create signature manifest JSON: <code>{evidenceRef,reportHash,signerId,signingKeyId,signatureAlgorithm,signatureBase64,signatureTs}</code> and persist via <code>PersistEvidenceBlob</code> with <code>artifactType=signature</code>. <br>3. Store <code>signatureRef</code> in original artifact's metadata if allowed (immutability means store separate pointer or create new manifest linking). <br>4. Provide verify API that auditors can use to validate signature using public key and signature manifest. <br><strong>Failure & recovery:</strong> HSM outage → queue deferred-sign task and persist <code>signatureDeferred</code> state in recon_report metadata; key revocation → emit <code>signing.key.revoked</code> and require migration manifest for re-signing regulated artifacts. <br><strong>Tests & CI:</strong> sign+verify cycle with test HSM provider, key rotation revoke tests, deferred signing processing tests. <br><strong>Example:</strong> <code>PersistSignedArtifact(evid:recon_report_abc,alice,prod-key-2026,recon_report)</code> returns <code>evid:signature_&lt;reportHash&gt;_prod-key-2026</code> and <code>payroll.recon.report.generated</code> audit references signatureRef. </td></tr><tr><td data-label="REG_EvidenceAudit — Per-function Expert Technical Breakdown"> <strong><code>EvidenceAccessApprovalFlow(request)</code> — approval workflow, approver selection, tokens & audit</strong><br><strong>Purpose & contract:</strong> manage a deterministic approval workflow for artifact retrieval. Receive <code>request</code> containing <code>requestorId</code>, <code>artifactRefs[]</code>, <code>purpose</code>, <code>justification</code>, <code>requestedTTL</code>, compute <code>requiredApprovers[]</code> based on artifact sensitivity and governance rules, notify approvers, persist approvals as <code>approvalsRef</code> (evidence blob), and on approval issue a short-lived <code>accessToken</code> (scoped JWT) that gates <code>RetrieveEvidenceBlob</code>. All steps append audit rows and CoC entries. <br><strong>Inputs & outputs:</strong><br>- Input: <code>request</code> with <code>requestorId</code>, <code>artifactRefs[]</code>, <code>purpose</code>, <code>justification</code>, <code>urgency</code> (<code>low|high</code>), <code>requestedTTL</code> (seconds). <br>- Output: <code>{requestId,requiredApprovals[],status:pending|granted|rejected,approvalsRef?,accessToken?,expiryTs}</code>. <br><strong>Approval determination:</strong> rules mapping <code>sensitivityTags</code> to approver roles (owner, compliance, legal); regulated artifacts require two-person approval by default. <br><strong>Token model:</strong> issue time-limited JWT with <code>aud=evidenceRetrieval</code>, <code>scope=artifactRefs</code>, <code>requestId</code>, token bound to <code>accessorId</code> and optionally IP/TLS fingerprint; token logged and non-replayable; expiry enforced strictly. <br><strong>Implementation notes:</strong><br>1. Approval UI with one-click signer flows; require strong authentication for approvers. <br>2. Persist <code>approvalsRef</code> as evidence: a signed approvals blob containing approver identities, timestamps and signatures. <br>3. Support delegation/override with audit trail (emergency overrides documented and require dual approval). <br><strong>Failure & recovery:</strong> approver unavailability → escalate per on-call matrix; mid-flow policy change → mark request stale and require re-request. <br><strong>Tests & CI:</strong> approval happy path, deny path, emergency override, token expiry, policy-change mid-flow. <br><strong>Conceptual PQ guidance:</strong> produce PQ admin reports that surface pending approvals and lead time; use <code>RegisterUnitTestHook</code> in CI to simulate approval sequences. <br><strong>Conceptual DAX guidance:</strong> <code>AvgApprovalLeadTime</code>, <code>PendingApprovalsByOwner</code> dashboards. </td></tr><tr><td data-label="REG_EvidenceAudit — Per-function Expert Technical Breakdown"> <strong><code>RetrieveEvidenceBlob(evidenceRef, accessToken|operatorCreds, options)</code> — retrieval, redaction, streaming & chunking</strong><br><strong>Purpose & contract:</strong> return artifact bytes under strict access controls; apply redaction policy for UI consumers; for compliance return full sanitized artifact when <code>approvalsRef</code> present. Must always append CoC entry and emit access audit; support streaming decryption and streaming redaction for large objects and chunked retrieval/resume for very large artifacts. <br><strong>Inputs & outputs:</strong><br>- Inputs: <code>evidenceRef</code>, <code>accessToken</code> or <code>operatorCreds</code>+<code>approvalsRef</code>, <code>options:{range,chunkSize,redactionMode}</code>. <br>- Outputs: streaming bytes with metadata <code>{artifactChecksum,canonicalVersion,redactionPolicyRef,accessMode}</code> or error. <br><strong>Redaction pipeline:</strong> apply versioned redaction transforms (tokenize/store mapping in evidence if reversible mapping required with approvals), compute <code>redactedChecksum</code>, and include <code>redactionPolicyRef</code> in audit. For UI, enforce <code>redactionMode=UI</code> and do not return PII beyond redaction policy. <br><strong>Security checks:</strong> verify token signature, scope, expiry, optional IP/TLS fingerprint; check <code>approvalsRef</code> presence for regulated artifacts. <br><strong>Chunking & resume:</strong> allow chunked downloads with per-chunk audit entries and resume tokens bound to <code>requestId</code> and <code>accessToken</code>. <br><strong>Observability & audit:</strong> emit <code>evidence.accessed{evidenceRef,accessorId,accessMode,accessTs,correlationId,accessTokenRef}</code> and append CoC. Metric <code>evidence.access.count</code>. <br><strong>Failure modes:</strong> token invalid/expired → <code>EVID_ACCESS_INVALID_TOKEN</code>; decryption error → <code>EVID_ACCESS_DECRYPT_FAIL</code> and alert SRE; redaction failure → deny safe-failure (deny access) and log <code>evidence.redaction.error</code>. <br><strong>Tests & CI:</strong> redaction correctness tests for each redaction rule; streaming chunk resume tests; token expiry and revocation tests. <br><strong>Example retrieval flow:</strong> analyst requests redacted preview — system routes <code>EvidenceAccessApprovalFlow</code> (owner auto-approve for low sensitivity); returns short-lived token; <code>RetrieveEvidenceBlob</code> streams redacted preview to analyst UI. <br><strong>Conceptual PQ guidance:</strong> provide PQ helper that uses retrieval token to fetch redacted artifact and load into a preview sheet (hidden sheet for full evidence link). <br><strong>Conceptual DAX guidance:</strong> <code>AccessEvents</code> by <code>accessMode</code> and <code>artifactSensitivity</code>; alert if <code>full</code> accesses spike unexpectedly. </td></tr><tr><td data-label="REG_EvidenceAudit — Per-function Expert Technical Breakdown"> <strong><code>RecordChainOfCustody(evidenceRef, actorId, action, reason, metadata)</code> — immutable CoC append & proofs</strong><br><strong>Purpose & contract:</strong> append an immutable CoC entry for evidence lifecycle events; entries include <code>timestamp,actorId,action,correlationId,preChecksum,postChecksum,prevHash,entryHash,metadata</code>. Implement append-only storage with chaining (entryHash = sha256(prevHash + payload)). Provide API to export CoC slice and proofs for regulators (Merkle root or inclusion proof). <br><strong>Invariants & implementation:</strong><br>1. Append-only and tamper-evident; prefer Merkle-append log or chained hash sequence. <br>2. Store CoC in both evidence DB and replicated WORM store for regulated artifacts. <br>3. Provide efficient retrieval by <code>evidenceRef</code> and <code>correlationId</code>. <br><strong>Observability & audit:</strong> emit <code>evidence.chain.recorded{cocId,evidenceRef,actorId,action}</code> and <code>evidence.chain.length</code> metrics. <br><strong>Failure & recovery:</strong> CoC persist failure should block sensitive operations; queue retry and escalate if persistent. <br><strong>Tests & CI:</strong> append-only tests, tamper-detection injection tests, inclusion proof verification tests. <br><strong>Example CoC entry:</strong> <code>{cocId:sha256(...),evidenceRef:&quot;evid:...&quot;,actorId:&quot;alice&quot;,action:&quot;access&quot;,reason:&quot;compliance_request&quot;,ts:&quot;2026-01-21T09:00Z&quot;,prevHash:&quot;...&quot;,entryHash:&quot;...&quot;}</code>. </td></tr><tr><td data-label="REG_EvidenceAudit — Per-function Expert Technical Breakdown"> <strong><code>ExportForensicPack(runId, artifactSelectors[], operatorId, level)</code> — forensic bundle assembly & legal packaging</strong><br><strong>Purpose & contract:</strong> assemble a forensic package that includes selected artifacts, chain-of-custody slices, audit tail, recon_report, and packaging metadata; compute and sign <code>forensicHash</code>, persist the package via <code>PersistEvidenceBlob</code>, tag <code>legalHold=true</code> if regulator-level, store in WORM, and emit <code>forensic.pack.generated</code>. <br><strong>Inputs & outputs:</strong><br>- Inputs: <code>runId</code>, <code>artifactSelectors</code> (list or filters), <code>operatorId</code>, <code>level</code> (<code>internal|regulator</code>), optional <code>retentionOverride</code>. <br>- Outputs: <code>{forensicRef,forensicHash,storageUri}</code> or partial error with <code>missingArtifacts[]</code>. <br><strong>Packaging steps:</strong><br>1. Resolve selectors to artifact list; verify checksums. <br>2. Collect CoC slices and append to manifest. <br>3. Create <code>forensic_manifest.json</code> enumerating artifacts, checksums, CoC, and packaging metadata. <br>4. Compress, encrypt (separate envelope key), sign manifest, persist via <code>PersistEvidenceBlob</code> and tag <code>legalHold</code> if <code>level=regulator</code>. <br>5. Store in WORM for regulator-level packages. <br><strong>Failure & recovery:</strong> if missing artifact, produce partial package and include <code>missingArtifacts[]</code> in manifest; escalate for recovery. <br><strong>Observability & audit:</strong> emit <code>forensic.pack.generated{forensicRef,runId,operatorId,artifactCount}</code> and <code>forensic.pack.stored</code> if WORM. <br><strong>Tests & CI:</strong> packaging integrity tests, manifest correctness, retrieval test from archive, WORM storage retention tests. <br><strong>Conceptual PQ guidance:</strong> PQ can be used to produce lists of artifactRefs to pass as selectors to this function; ensure queries are deterministic and paginated. <br><strong>Conceptual DAX guidance:</strong> <code>ForensicPacksByQuarter</code> measure, <code>ForensicPackSize</code> histogram for storage planning. </td></tr><tr><td data-label="REG_EvidenceAudit — Per-function Expert Technical Breakdown"> <strong><code>EnforceRetentionPolicy(evidenceRef)</code> — retention evaluation & lifecycle actions</strong><br><strong>Purpose & contract:</strong> evaluate evidence metadata against retention policy and legal holds; perform lifecycle actions — retain, archive to colder tier, schedule deletion — with atomic updates to evidence metadata and audit rows. Never delete evidence under <code>legalHold=true</code>. <br><strong>Inputs & outputs:</strong> <code>evidenceRef</code> input; returns <code>{actionTaken, nextActionTs, storageUri}</code>. <br><strong>Policy fields & semantics:</strong> <code>retentionPolicy</code> comprises <code>retentionYears</code>, <code>tier</code> (<code>hot|warm|cold</code>), <code>legalTags[]</code>, <code>autoArchiveAfterDays</code>, <code>deletionWindowDays</code>. Legal hold overrides deletion. <br><strong>Execution pattern:</strong> scheduled daily enforcement job that computes <code>nextActionTs</code> for each evidence; when migration to archive required, perform tier migration and update metadata atomically; for deletion scheduled, verify <code>legalHold</code> false and required approvals present. <br><strong>Failure & recovery:</strong> archive move failure → retry with exponential backoff; attempted deletion under hold → log <code>retention.blocked</code> and create incident. <br><strong>Observability & audit:</strong> emit <code>evidence.retention.action{evidenceRef,action,nextActionTs}</code> metric <code>evidence.retention.count</code>. <br><strong>Tests & CI:</strong> retention edge-case tests, legal hold enforcement tests, cross-region archive restoration tests. <br><strong>Conceptual PQ guidance:</strong> PQ admin report lists evidence with <code>nextActionTs</code> for operator to review scheduled actions. <br><strong>Conceptual DAX guidance:</strong> <code>RetentionComplianceRate</code> and <code>ArtifactsByTier</code> visualizations. </td></tr><tr><td data-label="REG_EvidenceAudit — Per-function Expert Technical Breakdown"> <strong><code>RotateSigningKey(newKeyId, operatorId, rollStrategy)</code> — rotation orchestration & re-sign policy</strong><br><strong>Purpose & contract:</strong> orchestrate rotation of signing keys and optionally re-sign existing artifacts per <code>rollStrategy</code> (<code>none|re-sign|dual-sign</code>). Persist rotation manifest and emit <code>signing.key.rotated</code>. Re-signing regulated artifacts requires migration manifest and approvals. <br><strong>Inputs & outputs:</strong> <code>newKeyId</code>, <code>operatorId</code>, <code>rollStrategy</code>, <code>migrationManifestRef?</code>. Output <code>{rotationId,oldKeyId,newKeyId,appliedStrategy,affectedCount}</code>. <br><strong>Implementation notes:</strong><br>1. Use KMS/HSM admin APIs; private keys remain in HSM. <br>2. For <code>re-sign</code>: compute list of affected artifacts, schedule controlled re-sign worker that persists new signature evidence and links <code>signatureRef</code> to artifact. <br>3. Record rotation manifest as evidence and emit <code>signing.rotation.completed</code>. <br><strong>Failure & recovery:</strong> partial re-sign -> persist partial audit and create rollback plan; key compromise -> emergency rotation and incident report. <br><strong>Tests & CI:</strong> rotation dry-run tests, re-sign idempotency, revocation and emergency rotation drill. <br><strong>Conceptual PQ & DAX:</strong> PQ lists artifacts signed by key; DAX measure <code>SignaturesPerKey</code> to track usage and rotation. </td></tr><tr><td data-label="REG_EvidenceAudit — Per-function Expert Technical Breakdown"> <strong><code>ValidateArtifactIntegrityOnStartup()</code> — startup quick-check & schedule deep jobs</strong><br><strong>Purpose & contract:</strong> on service start perform lightweight checks: verify connectivity to KMS/HSM, existence of critical signed reports, quick checksum sample verification; schedule deep verification jobs asynchronously; do not block startup with heavy checks. Start in degraded mode if critical failures detected. <br><strong>Behavior:</strong> quick checks (manifest presence, KMS reachable, sample signature verify); schedule <code>VerifyReportParity</code> deep jobs. <br><strong>Failure & recovery:</strong> if critical failures found, start in restricted mode (read-only for sensitive ops) and escalate to SRE. <br><strong>Tests & CI:</strong> startup simulation failing KMS, missing critical reports. </td></tr><tr><td data-label="REG_EvidenceAudit — Per-function Expert Technical Breakdown"> <strong><code>AppendAuditRow(module,event,payload)</code> — append-only audit writer & chain linking</strong><br><strong>Purpose & contract:</strong> append PII-minimal audit rows to the system append-only log. Each row must include <code>timestamp,correlationId,module,event,actorId,payloadHash,prevHash,evidenceRef?</code>. Append must be atomic and crash-safe; return <code>auditId</code>. Use chained hash (<code>entryHash=sha256(prevHash + payload)</code>) to produce tamper-evidence. For regulated runs store audit in WORM. <br><strong>Implementation details:</strong><br>1. Write row to append-only store with prevHash linking. <br>2. Optionally batch writes but ensure ordering per correlationId. <br>3. Provide read API keyed by <code>correlationId</code> and time range. <br><strong>Failure & recovery:</strong> buffer encrypted audit rows on persist failure and retry; alert if buffer grows. <br><strong>Tests & CI:</strong> chain integrity tests, append atomicity under concurrency, WORM retention tests. <br><strong>Conceptual PQ & DAX:</strong> PQ ingest audit rows into analysis dataset; DAX measures <code>AuditAppendRate</code> and <code>AuditLatencyP95</code>. </td></tr><tr><td data-label="REG_EvidenceAudit — Per-function Expert Technical Breakdown"> <strong><code>RegisterUnitTestHook(hookName)</code> — deterministic CI harness for golden parity</strong><br><strong>Purpose & contract:</strong> enable test-only deterministic hooks that simulate evidence flows using fixed <code>correlationId</code> and deterministic RNG seeds for golden parity testing. Hooks disabled in production by default and require explicit configuration/approval to enable. Test hooks produce <code>test=true</code> flagged audit rows. <br><strong>Safety & governance:</strong> production must block enabling hooks without compliance approvals; PRs modifying canonicalization must run golden parity tests via these hooks on CI. <br><strong>Tests & CI:</strong> golden parity pipeline must run <code>RegisterUnitTestHook</code> to reproduce parity artifacts and compare to stored golden checksums; <code>parity:golden:REG_EvidenceAudit</code> must run on PRs affecting canonicalization. </td></tr><tr><td data-label="REG_EvidenceAudit — Per-function Expert Technical Breakdown"> <strong><code>PersistDeferredSignature(evidenceRef)</code> — worker for deferred signing</strong><br><strong>Purpose & contract:</strong> process queued deferred-signature tasks when HSM is available; sign <code>reportHash</code>, persist signature evidence, update recon_report metadata with <code>signatureRef</code>. Must be idempotent and robust to reprocessing. <br><strong>Behavior:</strong> pick tasks from queue, verify artifact is still present and checksum matches, sign and persist signature manifest via <code>PersistEvidenceBlob</code>. <br><strong>Failure & recovery:</strong> post-retry failures escalate; track <code>deferredSignature.error</code> and expose operator CLI to inspect queue. <br><strong>Tests & CI:</strong> deferred signing processing tests and idempotency verification. </td></tr><tr><td data-label="REG_EvidenceAudit — Per-function Expert Technical Breakdown"> <strong><code>ListEvidence(queryParams)</code> — catalog index queries and pagination</strong><br><strong>Purpose & contract:</strong> return paginated evidence index results satisfying filters (artifactType, createdTs range, sensitivityTags, owner, checksum, canonicalVersion). Provide stable pagination tokens and do not leak PII. <br><strong>Inputs & outputs:</strong> filters and pagination; outputs: <code>{items[],nextPageToken}</code>. <br><strong>Implementation notes:</strong> enforce RBAC for <code>sensitivityTags</code> filtered queries; index by <code>artifactType</code>, <code>createdTs</code>, <code>checksum</code>, and <code>sensitivity</code>. <br><strong>Tests:</strong> query correctness tests, pagination tests, RBAC enforcement. <br><strong>Conceptual PQ guidance:</strong> build PQ tables via connector to index evidence for compliance dashboards; ensure query parameters use safe pagination to avoid timeouts. </td></tr><tr><td data-label="REG_EvidenceAudit — Per-function Expert Technical Breakdown"> <strong><code>GetEvidenceMetadata(evidenceRef)</code> — read-only metadata retrieval</strong><br><strong>Purpose & contract:</strong> return metadata for an evidenceRef (without blob) including <code>artifactType</code>, <code>checksum</code>, <code>sizeBytes</code>, <code>createdBy</code>, <code>createdTs</code>, <code>canonicalVersion</code>, <code>retentionPolicy</code>, <code>sensitivityTags</code>, <code>storageUri</code> (internal), <code>legalHold</code> flag. This function does not return PII or blob content. <br><strong>Implementation & audit:</strong> minimal audit emitted <code>evidence.metadata.read</code> with <code>correlationId</code>. Enforce RBAC for regulated artifacts. <br><strong>Tests:</strong> metadata correctness tests, RBAC denial tests. </td></tr><tr><td data-label="REG_EvidenceAudit — Per-function Expert Technical Breakdown"> <strong><code>RehydrateEvidenceToLocal(evidenceRef,targetPath,options)</code> — controlled local export for forensics</strong><br><strong>Purpose & contract:</strong> securely export evidence to local secure staging for forensic analysis; requires approvals when artifact sensitive; records CoC; supports streaming decrypt and verifying checksum. Returns local path and <code>rehydrateRef</code>. <br><strong>Security & governance:</strong> require <code>approvalsRef</code> and record <code>rehydrate</code> audit; local staging encrypted with ephemeral keys and TTL. <br><strong>Tests:</strong> rehydrate happy path, verify checksum, approval gating. </td></tr><tr><td data-label="REG_EvidenceAudit — Per-function Expert Technical Breakdown"> <strong>Operational runbooks (concise actionable)</strong><br><strong>1) Evidence persist failed (storage outage)</strong><br>- Capture <code>correlationId</code> and jobId. <br>- Confirm staging fallback persisted encrypted and emit <code>evidence.persist.staged</code>. <br>- Alert SRE and start storage recovery. <br>- Drain staging queue on recovery; validate checksums and emit <code>evidence.persist.replayed</code> audits. <br><strong>2) Recon_report parity failed</strong><br>- Run <code>VerifyReportParity(reportRef,deep)</code> to confirm. <br>- If mismatch confirmed, create <code>forensic.pack</code> for runId, preserve WORM copy, open incident, notify compliance. <br><strong>3) Unauthorized access detected</strong><br>- Immediately revoke access tokens, mark artifact <code>sensitivity</code> review, run CoC export, alert security and compliance, create forensic pack and preserve WORM copy. </td></tr><tr><td data-label="REG_EvidenceAudit — Per-function Expert Technical Breakdown"> <strong>Acceptance criteria & CI gates (summary)</strong><br>1. Unit tests for all functions. <br>2. Integration E2E tests: persist → sign → buildReconReport → verify parity → retrieve via approval flow. <br>3. Golden parity tests for canonicalization and recon_report across VBA/PQ/backend. <br>4. Security tests for HSM/KMS, RBAC and token model. <br>5. Performance: <code>PersistEvidenceBlob</code> P50 <300ms for small blobs; retrieval P95 <2s for small blobs. <br>6. CI gates: <code>parity:golden:REG_EvidenceAudit</code>, signature verify tests, deferred signing worker tests. </td></tr><tr><td data-label="REG_EvidenceAudit — Per-function Expert Technical Breakdown"> <strong>Conceptual Power Query (PQ) guidance — patterns & examples (no code snippets)</strong><br>1. <strong>Goal</strong>: provide deterministic index and verification helper queries that surface evidenceRef lists, artifact checksums and signature status into Excel/PQ dashboards. <br>2. <strong>Immutable index ingestion</strong>: implement M query to call <code>ListEvidence</code> with pagination; normalize <code>createdTs</code> to UTC and include <code>canonicalVersion</code> and <code>sensitivityTags</code>. <br>3. <strong>Checksum verification helper</strong>: build M pipeline that exports a canonicalized subset or small fixture of an artifact to backend verification service endpoint that runs <code>ComputeArtifactChecksum</code> and returns parity result; mark mismatch rows for triage. <br>4. <strong>Approval flows</strong>: PQ admin dashboard lists pending <code>EvidenceAccessApprovalFlow</code> requests and allows owner to open approval UI; the list is produced via secure connector. <br>5. <strong>Preview pipeline</strong>: call <code>RetrieveEvidenceBlob</code> with <code>redactionMode=UI</code> and load redacted preview into a hidden sheet for analyst review; metadata (evidenceRef, checksum) saved in a hidden table for audit references. <br>6. <strong>Testing harness</strong>: PQ CI harness must call <code>RegisterUnitTestHook</code> for deterministic seeds before MR golden tests. </td></tr><tr><td data-label="REG_EvidenceAudit — Per-function Expert Technical Breakdown"> <strong>Conceptual DAX guidance — measures & dashboards (no snippets)</strong><br>1. <strong>Dimensions</strong>: <code>Artifact</code> (<code>artifactType, evidenceRef, canonicalVersion, sensitivityTag</code>), <code>AuditEvent</code> (<code>event,correlationId,actorId</code>), <code>Signatures</code> (<code>signatureRef,signingKeyId</code>), <code>Retention</code> (<code>tier,nextActionTs</code>). <br>2. <strong>Key measures</strong>: <code>EvidenceCount = COUNTROWS(Artifact)</code>, <code>FailedParityCount = COUNTROWS(Filter(VerifyResults, Result=&quot;Fail&quot;))</code>, <code>UnsignedReports = COUNTROWS(Filter(Artifact, artifactType=&quot;recon_report&quot; &amp;&amp; signatureRef=null))</code>. <br>3. <strong>SLO tiles</strong>: <code>PersistLatencyP95</code>, <code>RetrieveLatencyP95</code>, <code>VerifyParityFailureRate</code>. <br>4. <strong>Incident dashboards</strong>: list recent <code>verify.recon_report.failed</code> events, forensic packs generated, and access spikes for sensitive artifacts. <br>5. <strong>Compliance views</strong>: <code>ForensicPackCountByPeriod</code>, <code>LegalHoldArtifacts</code> and <code>RetentionActionDue</code>. <br>6. <strong>Alerting measures</strong>: create alerts on <code>FailedParityCount&gt;0</code> for regulated artifacts or <code>evidence.apply.failure_rate</code> thresholds. </td></tr><tr><td data-label="REG_EvidenceAudit — Per-function Expert Technical Breakdown"> <strong>Appendices — canonical rules, artifact naming and error catalog</strong><br><strong>Canonicalization summary:</strong> normalize newlines, NFKC, trim whitespace, stable JSON key ordering, fixed numeric formatting, remove transient fields (list maintained per canonicalVersion). <br><strong>Naming patterns:</strong> <code>evid:&lt;type&gt;_&lt;id&gt;_&lt;ts&gt;_v&lt;canonicalVersion&gt;.&lt;ext&gt;</code> e.g., <code>evid:recon_report_r20260121_20260121T0800Z_v1.json</code>. <br><strong>Error codes (examples):</strong> <code>EVID_PERSIST_INVALID_METADATA</code>, <code>EVID_CANONICALIZE_ERROR</code>, <code>EVID_KMS_FAIL</code>, <code>EVID_ACCESS_INVALID_TOKEN</code>, <code>VERIFY_RECON_MISMATCH</code>, <code>STD_REVERT_NO_SNAPSHOT</code> (for devs reused from DQ module). <br><strong>Forensic pre-flight checklist:</strong> include recon_report, artifact checksums, CoC slices, migration_manifest (if canonical changes), audit_tail.csv and signed manifest. </td></tr><tr><td data-label="REG_EvidenceAudit — Per-function Expert Technical Breakdown"> <strong>Operator & developer checklists (explicit '10 checks')</strong><br>Before hot-swap or canonical change ensure: <br>1. Schema validation passes for canonicalization changes. <br>2. Unique artifact identifiers verified and no orphaned evidence in index. <br>3. Golden parity fixtures produced and CI passing across runtimes. <br>4. Migration manifest created and approvals gathered (if regulated). <br>5. Deferred signing worker tested and HSM availability validated. <br>6. Retention policy and legal hold consequences reviewed. <br>7. Forensic packaging drill run locally. <br>8. CoC export verified for sample runs. <br>9. Operator runbook updated and tabletop exercise executed. <br>10. SRE alerting for <code>verify.recon_report.failure_rate</code> configured and tested. </td></tr><tr><td data-label="REG_EvidenceAudit — Per-function Expert Technical Breakdown"> <strong>Final notes & next-steps options (select one)</strong><br>- (A) Provide VBA wrapper examples for <code>PersistEvidenceBlob</code> and <code>RetrieveEvidenceBlob</code> showing streaming and token handling (no secrets in code). <br>- (B) Produce a high-resolution Mermaid + SVG workflow diagram showing REG_EvidenceAudit interactions with ingest, mapping, preview and apply flows. <br>- (C) Export this entire table as a downloadable single-page PDF for regulatory packaging. <br>Select one option and it will be produced immediately. </td></tr></tbody></table></div><div class="row-count">Rows: 28</div></div><div class="table-caption" id="Table5" data-table="Docu_0184_05" style="margin-top:2mm;margin-left:3mm;"><strong>Table 5</strong></div>
<div class="table-wrapper" data-table-id="table-5"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by REG_RuntimeHelpers — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">REG_RuntimeHelpers — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="REG_RuntimeHelpers — Per-function Expert Technical Breakdown"> <strong>Module purpose, scope, invariants and release policy</strong><br><strong>Purpose & scope:</strong> provide a compact, stable, cross-language runtime utility layer used by the Project 616 stack (ingest, mapping, plan, preview, aggregation, suggestions, apply/export, evidence). Responsibilities include: safe invocation wrappers for heavy handlers, cooperative cancellation and watchdogs, deterministic canonical hashing and idempotency tokens, PII-safe redaction and deterministic pseudonymization, telemetry/metrics emission and buffering, CI-only deterministic hooks, job-offload handoff with idempotent descriptors, configuration lookup and feature flags, concise user-facing error mapping and evidence plumbing helpers, and small numeric helpers (SafeRound, fixed-scale arithmetic). This module must be small, heavily tested, and versioned; public API changes require migration manifests and golden fixtures for parity across ports. <br><strong>Primary invariants (must/shall):</strong><br>1. All heavy/IO-bound handlers invoked by core modules MUST be executed via <code>SafeInvokeReconciler</code> or via <code>JobQueueOffload</code>; synchronous heavy IO on UI threads is forbidden. <br>2. Audit rows emitted by this module MUST be PII-free. Full diagnostics MUST be persisted to <code>REG_EvidenceAudit</code> with an <code>evidenceRef</code> referenced in the audit. <br>3. Canonical JSON canonicalizer and <code>paramsHash</code> algorithm are authoritative; any modification requires version bump and a <code>migration_manifest</code>. <br>4. Idempotency tokens are deterministic across ports for given <code>applyId</code> + <code>idempotencyVersion</code>. <br>5. CI hooks are disabled by default in production; enabling requires admin token and emits an audit. <br><strong>Release policy:</strong> change to any canonicalization, idempotency, redaction, or handler registry semantics requires a <code>migration_manifest</code> containing before/after golden fixtures, canary plan, rollback plan, and approvals. </td></tr><tr><td data-label="REG_RuntimeHelpers — Per-function Expert Technical Breakdown"> <strong>Developer contract (API surface & semantics summary)</strong><br>- <code>SafeInvokeReconciler(handlerName, handlerProc, handlerArgs, correlationId, timeoutMs)</code> → run handler safely, emit audits, persist diagnostics.<br>- <code>SafeErrorToUser(correlationId, errorCode, internalDetails)</code> → produce UI-safe message + persist evidence.<br>- <code>TimeoutAndCancellationWrapper(taskProc, timeoutMs, correlationId)</code> → enforced cooperative timeout and graceful cancel.<br>- <code>PII_Redact(record, redactRules)</code> → deterministic pseudonymization / masking and evidence mapping.<br>- <code>TelemetryEmit(eventName, tags, metrics, sampleRate=1.0)</code> → non-blocking telemetry with sampling and buffering.<br>- <code>IdempotencyTokenForApply(applyId)</code> → deterministic idempotency token (versioned).<br>- <code>RegisterUnitTestHook(hookName, enabled, adminToken)</code> → CI-only hook management.<br>- <code>JobQueueOffload(jobDescriptor)</code> → atomic persist & enqueue with idempotency.<br>- <code>MapInternalErrorToPayrollCode(err)</code> → stable error mapping to <code>PAYROLL_*</code> codes.<br>- <code>EmitSafeUserMessage(correlationId, code, hintParams)</code> → localized, PII-free UI string builder.<br>- <code>ConfigurationLookup(key, default=null)</code> → typed config fetch, secret masking, <code>configHash</code> exposure.<br>- <code>ComputeParamsHash(obj, canonicalVersion)</code> → canonicalization & sha256 hash helper (exposed for PQ/VBA parity).<br>- <code>CanonicalizeJson(obj, options)</code> → deterministic JSON canonicalizer used by <code>ComputeParamsHash</code>.<br>- <code>SafeRound(amount, scale, roundingMode)</code> → deterministic rounding with residual absorption helpers (used by aggregation and suggestions).<br>- <code>HealthCheck()</code> → simple health probe for runtime helpers and evidence store connectivity. </td></tr><tr><td data-label="REG_RuntimeHelpers — Per-function Expert Technical Breakdown"> <strong><code>ComputeParamsHash(obj, canonicalVersion=&quot;v1&quot;)</code> — canonical hashing primitive</strong><br><strong>Purpose & contract:</strong> produce canonical byte representation and <code>sha256</code> hex hash for arbitrary parameter objects used to seed deterministic operations (<code>paramsHash</code>, <code>planSeed</code> composition inputs). This function is the single source of truth for cross-language parity. Output: <code>{canonicalJson, paramsHash}</code> where <code>paramsHash = &quot;sha256:&lt;hex&gt;&quot;</code>. <br><strong>Canonicalization recipe (must be followed by all ports):</strong><br>1. Normalize strings: Unicode NFKC, trim, collapse internal whitespace to single space, remove control characters. <br>2. Numbers: when <code>scale</code> specified in metadata, format fixed decimal with that scale; otherwise use normalized JSON numeric representation (no trailing zeros). <br>3. Booleans: <code>true</code>/<code>false</code>. <br>4. Nulls: <code>null</code>. <br>5. Keys: lexicographic sort (U+0000..U+ffff order). <br>6. Arrays: preserve order only if <code>schema</code> indicates semantic order; otherwise sort deterministic by canonicalized element representation. <br>7. Compact JSON: no extra whitespace or line breaks. <br>8. UTF-8 encode and compute sha256 hex digest. <br><strong>Parameters & options:</strong> <code>canonicalVersion</code> must be included in <code>paramsHash</code> metadata; <code>ComputeParamsHash</code> must support <code>canonicalVersion</code> parameter and produce identical outputs across PQ/VBA/backend for same <code>obj</code> and <code>canonicalVersion</code>. <br><strong>Failure modes:</strong> non-serializable values → explicit error <code>RUNTIME_HASH_001</code> with diagnostic evidence persisted. <br><strong>Tests:</strong> cross-language parity fixtures; property test with permuted keys to ensure same hash. <br><strong>Example conceptual usage:</strong> compute <code>paramsHash</code> for <code>BuildReconciliationPlan</code> input and store it with plan record. </td></tr><tr><td data-label="REG_RuntimeHelpers — Per-function Expert Technical Breakdown"> <strong><code>CanonicalizeJson(obj, options)</code> — deterministic JSON canonicalizer</strong><br><strong>Purpose & contract:</strong> return canonical JSON string per <code>ComputeParamsHash</code> recipe. Exposes options: <code>canonicalVersion</code>, <code>schemaHints</code> (fields which are ordered arrays), <code>numericScales</code> mapping. Implementation must be pure and deterministic. <br><strong>Edge rules:</strong> escape <code>|</code> and control chars consistently; normalize newlines to <code>\n</code>. For floating numeric inputs, accept string-encoded decimals to preserve exactness. <br><strong>Tests:</strong> golden canonical strings for representative complex objects including nested arrays, maps, and sample numeric edge cases. </td></tr><tr><td data-label="REG_RuntimeHelpers — Per-function Expert Technical Breakdown"> <strong><code>SafeInvokeReconciler(handlerName, handlerProc, handlerArgs, correlationId, timeoutMs=60000)</code> — in-depth function</strong><br><strong>Purpose & contract:</strong> standardized wrapper to call reconciliation or standardization handlers. Provides pre-invocation validation, <code>paramsHash</code> computation, start/complete/error audits, cooperative cancellation, offload vs inline heuristics, evidence persistence for errors, and return shape normalization. <br><strong>Input semantics:</strong><br>- <code>handlerName</code>: registered name in <code>HandlerRegistry</code>.<br>- <code>handlerProc</code>: reference to function or worker invocation descriptor.<br>- <code>handlerArgs</code>: JSON-serializable dict (will be canonicalized).<br>- <code>correlationId</code>: unique run id (string).<br>- <code>timeoutMs</code>: timeout in milliseconds (override allowed).<br><strong>Execution algorithm (detailed):</strong><br>1. <strong>Registry check:</strong> look up <code>handlerName</code> in <code>HandlerRegistry</code>. Registry contains metadata: <code>{handlerName, estimatedCost, maxInlineMs, requiredRoles, retryPolicy}</code>. If not found emit <code>runtime.handler.error{code:RUNTIME_HANDLER_NOT_REGISTERED}</code> and return error.<br>2. <strong>Auth & permissions check:</strong> verify caller roles via context; if missing return <code>RUNTIME_PERMISSION_DENIED</code>. <br>3. <strong>Canonicalize params:</strong> call <code>ComputeParamsHash(handlerArgs, canonicalVersion)</code> producing <code>canonicalJson</code> and <code>paramsHash</code>. <br>4. <strong>Sanitize audit payload:</strong> call <code>PII_Redact</code> on a minimal <code>auditPayload</code> derived from <code>handlerArgs</code> to avoid PII leakage. <br>5. <strong>Emit start audit:</strong> <code>runtime.handler.start{timestamp,correlationId,handlerName,paramsHash,operatorId?,planId?}</code>. <br>6. <strong>Invoke decision:</strong> if <code>estimatedCost == &quot;light&quot;</code> and <code>maxInlineMs &gt;= timeoutMs</code> then run inline; else offload via <code>JobQueueOffload</code> with transparent immediate ack and return a <code>queued</code> shaped response unless synchronous wait requested. <br>7. <strong>Inline run:</strong> instantiate cancellation token; call <code>TimeoutAndCancellationWrapper(handlerProc, timeoutMs, correlationId)</code> and monitor progress. <br>8. <strong>Worker run:</strong> persist job descriptor and wait for worker to complete if <code>waitForCompletion</code> flag set (otherwise return <code>queued</code>). When waiting, poll with exponential backoff bounded by <code>timeoutMs</code> (use of polling must be non-blocking in host UI). <br>9. <strong>On success:</strong> emit <code>runtime.handler.complete{status:success,duration_ms,paramsHash,resultSummaryHash?}</code>, persist light result if requested, return <code>{status:&quot;ok&quot;, result:...}</code>. <br>10. <strong>On exception:</strong> call <code>MapInternalErrorToPayrollCode(err)</code>, persist sanitized diagnostic blob via <code>REG_EvidenceAudit.PersistEvidenceBlob</code> generating <code>evidenceRef</code>, emit <code>runtime.handler.error{errorCode,evidenceRef}</code>, and return structured error including <code>evidenceRef</code> and <code>correlationId</code>. <br>11. <strong>On timeout:</strong> same as exception path but use error code <code>RUNTIME_TIMEOUT</code> and emit <code>runtime.handler.timeout{evidenceRef}</code>. <br><strong>Observability:</strong> track <code>runtime.handler.duration_ms</code>, error counts, timeouts, and per-handler success rates. <br><strong>Implementation notes:</strong><br>- Provide both synchronous (blocking) and asynchronous (non-blocking queued) invocation modes to support UI flows and worker-only heavy operations. <br>- Offloaded jobs embed <code>standardMapHash</code> and <code>paramsHash</code> to guarantee reproducibility on worker. <br>- Ensure <code>paramsHash</code> is stored in the job descriptor to allow deterministic replay. <br><strong>Examples:</strong><br>- Inline cheap handler: <code>SafeInvokeReconciler(&quot;ValidateMappingRow&quot;, validateProc, row, &quot;r-...&quot;)</code> returns immediate normalized validation result.<br>- Offload heavy handler: <code>SafeInvokeReconciler(&quot;ComputeFullPreview&quot;, previewProc, {planId:...}, &quot;r-...&quot;, 600000)</code> offloads and returns job accepted response or waits for completion if <code>waitForCompletion=true</code>. </td></tr><tr><td data-label="REG_RuntimeHelpers — Per-function Expert Technical Breakdown"> <strong><code>TimeoutAndCancellationWrapper(taskProc, timeoutMs, correlationId, graceMs=500)</code> — enforcement details</strong><br><strong>Purpose & contract:</strong> enforce cooperative cancellation for <code>taskProc</code> and provide deterministic graceful-fail behavior. Returns <code>{status,result|error,evidenceRef,correlationId}</code>. <br><strong>Behavioral timeline:</strong><br>1. Start <code>taskProc</code> with a cancellation token object accessible to the proc (supports <code>shouldCancel()</code> and <code>onCancel</code> callback). <br>2. Start a timer for <code>timeoutMs - graceMs</code>. When timer fires set token <code>shouldCancel=true</code> and record <code>cancelTs</code>. <br>3. Allow <code>graceMs</code> for <code>taskProc</code> to persist partial output and exit. <br>4. If <code>taskProc</code> completes in grace window return result; else escalate: for backend workers attempt hard-kill; for host-limited environments (VBA) record hung state and persist partial diagnostics; emit <code>runtime.handler.hung</code>. <br>5. Persist diagnostic evidence at escalation and return <code>RUNTIME_TIMEOUT</code> error with <code>evidenceRef</code>. <br><strong>Host-specific safe patterns:</strong><br>- <strong>VBA:</strong> cannot hard-kill; rely on cooperative checks and <code>Application.OnTime</code> to schedule cancellation flags; handlers must check <code>ShouldCancel()</code> at safe points (before/after row parsing, heavy loops). <br>- <strong>Backend worker:</strong> use OS-level cancellation primitives (thread interrupt, process kill) with precautionary flush to durable storage and checkpointing. <br><strong>Edge behavior:</strong> if process termination prevents evidence persist, the worker must provide last known heartbeat and checkpoint location for forensic retrieval. <br><strong>Tests:</strong> forced overrun with both cooperative and uncooperative handlers; verify <code>evidenceRef</code> persisted and <code>runtime.handler.timeout</code> emitted. </td></tr><tr><td data-label="REG_RuntimeHelpers — Per-function Expert Technical Breakdown"> <strong><code>PII_Redact(record, redactRules)</code> — exhaustive operational design</strong><br><strong>Purpose & contract:</strong> produce a redacted record for UI/log exposure plus deterministic pseudonym tokens for traceability and evidence mapping for retrieval. Return <code>{redactedRecord, piiMappingRefs}</code>. <br><strong>Redaction rule schema (per field):</strong> <code>{ruleId, fieldPath, method:&quot;mask|hash-tokenize|truncate|category&quot;, params:{maskLeft,maskRight,truncateLen}, piiCategory, redactionPolicyVersion}</code>. <br><strong>Tokenization algorithm (deterministic):</strong> HMAC-SHA256 over <code>canonicalValue||&quot;|&quot;||ruleId||&quot;|&quot;||redactionPolicyVersion</code> using KMS-managed HMAC key. Format tokens as <code>tk_&lt;base58(sha256)&gt;</code> to be short and non-sequential. <br><strong>Evidence mapping storage:</strong> store mapping blob <code>{originalHash:sha256(canonicalValue), token, ruleId, createdTs, keyId}</code> encrypted in evidence store; return <code>evidenceRef</code> pointing to mapping store. Deduplicate mappings by <code>originalHash</code> to avoid repeated evidence writes. <br><strong>KMS key rotation handling:</strong> keep a <code>redactionKeyManifest</code> mapping keys and rotation epochs; when rotating keys generate evidence that maps previous tokens to new key metadata to preserve retrieval chain. <br><strong>Performance & memory:</strong> stream redaction for large payloads; use bloom filter to detect if mapping evidence exists and only persist on miss. <br><strong>Fallbacks:</strong> KMS unavailable → fallback to irreversible salted hash and emit <code>standard.redaction.warning</code> audit (deterministic but non-retrievable mapping). <br><strong>Access controls:</strong> retrieval requires <code>EvidenceAccessApprovalFlow</code> via <code>REG_EvidenceAudit</code>, which records approver ids and reason; retrieval logs must be appended to chain-of-custody. <br><strong>Tests:</strong> deterministic mapping across repeated runs, key rotation tests, retrieval with approvals tests, bulk performance tests. <br><strong>Examples:</strong> UI preview shows <code>EmployeeID=&quot;tk_3f9...&quot;</code>; evidenceRef points to mapping for compliance retrieval. <br><strong>Conceptual PQ guidance:</strong> do not compute tokens client-side in PQ; PQ should call secure backend endpoint that performs HMAC tokenization with KMS key. </td></tr><tr><td data-label="REG_RuntimeHelpers — Per-function Expert Technical Breakdown"> <strong><code>TelemetryEmit(eventName, tags, metrics, sampleRate=1.0, persist=false)</code> — deep design</strong><br><strong>Purpose & contract:</strong> central telemetry wrapper that normalizes tags, applies sampling and buffering, attaches <code>correlationId</code> and standard dimensions, and performs asynchronous non-blocking flush to telemetry backend. Return boolean success with minimal blocking. <br><strong>Tag & metric normalization rules:</strong> lower-case keys, stable key ordering, disallow high-cardinality tags (<code>userId</code>, <code>sessionId</code>) unless explicitly allowed; if high-cardinality tags present, sample aggressively and persist for short windows only. <br><strong>Buffering & backpressure strategy:</strong> in-memory circular buffer with bounded size; on overflow implement prioritized drop (lower priority events dropped first) and emit <code>runtime.telemetry.dropped</code> summary audit with counts and reason. Buffer configured with memory budget and flush interval; provide synchronous flush hook used in CI/golden runs. <br><strong>Sampling semantics:</strong> sampleRate ∈ (0,1]; non-deterministic sampling acceptable for high-volume metrics; where deterministic sampling required for parity (e.g., audits in CI), allow seedable deterministic sampling mode under test hooks. <br><strong>Persist flag:</strong> when <code>persist=true</code>, store full event JSON in <code>REG_EvidenceAudit</code> and return evidenceRef (for debugging/golden fixtures). Persisted telemetry must be redacted of PII before persist. <br><strong>Failure handling:</strong> telemetry backend errors trigger local buffer to store high-priority events and persist into evidence if buffer cannot hold; emit <code>runtime.telemetry.failed</code> audit. <br><strong>Tests:</strong> sampling correctness, buffer saturation, persisted telemetry retrieval, CI forced-flush behavior. <br><strong>Conceptual DAX guidance:</strong> ingest telemetry counters into the model; define DAX measures for average durations and SLO breach rates. </td></tr><tr><td data-label="REG_RuntimeHelpers — Per-function Expert Technical Breakdown"> <strong><code>IdempotencyTokenForApply(applyId, idempotencyVersion)</code> — exhaustive design</strong><br><strong>Purpose & contract:</strong> generate deterministic idempotency token used in external <code>post_direct</code> calls to GL APIs. Token computed as canonicalized string then hashed. <br><strong>Token algorithm (recommended):</strong> <code>tokenRaw = &quot;apply:&quot; || applyId || &quot;:&quot; || idempotencyVersion || &quot;:&quot; || runtimeHelpers.version</code> → <code>token = base58(SHA256(tokenRaw))</code> optionally truncated to GL API limit. <br><strong>Security policy:</strong> token value MUST NOT be logged or stored in plaintext; audits only record <code>tokenVersion</code> and <code>applyId</code>. Token rotation via <code>idempotencyVersion</code> requires migration manifest when used for regulated outputs. <br><strong>Token constraints:</strong> respect downstream API token length/character restrictions; provide documented truncation method and collision assessment (e.g., truncated 128-bit collision risk). <br><strong>Tests:</strong> parity across languages, rotation test ensuring old applies reproducible, ensure token not present in logs or evidence. <br><strong>Example:</strong> compute token for <code>applyId=&quot;apply_20260121_0001&quot;</code> with <code>idempotencyVersion=&quot;v1&quot;</code> and use it as Idempotency-Key header when calling GL API. </td></tr><tr><td data-label="REG_RuntimeHelpers — Per-function Expert Technical Breakdown"> <strong><code>RegisterUnitTestHook(hookName, enabled, adminToken, reason)</code> — exhaustive governance</strong><br><strong>Purpose & contract:</strong> allow controlled enabling/disabling of deterministic CI-only hooks. Hooks may seed PRNG deterministically, stub network calls or return small fixtures. Activation requires <code>adminToken</code> and <code>reason</code>, and emits <code>runtime.parity.hook.invoked{hookName,adminId,reason}</code> audit. Hooks default to disabled in production. <br><strong>Safety constraints:</strong> hooks must never alter canonicalization logic; they may only control test scaffolding (e.g., return fixed seeds or small fixture paths). Activation policy: hooks allowed only in test environments or when explicit admin token is provided with audit. <br><strong>CI controls:</strong> CI runners enable hook via <code>adminToken</code> in secure environment; PR pipelines must assert hooks disabled for production-like runs. <br><strong>Tests:</strong> confirm hooks disabled by default; assert golden parity when hooks enabled in CI; ensure hook activation audit present. </td></tr><tr><td data-label="REG_RuntimeHelpers — Per-function Expert Technical Breakdown"> <strong><code>JobQueueOffload(jobDescriptor)</code> — detailed contract & worker lifecycle</strong><br><strong>Purpose & contract:</strong> persist job descriptor atomically into job store and enqueue for worker consumption with idempotency and checkpointing semantics. Return <code>{jobId, persistedAt}</code> immediately. Emit <code>runtime.job.persisted{jobId,planId,priority}</code>. <br><strong>Descriptor required fields:</strong> <code>jobId</code>, <code>planId</code>, <code>correlationId</code>, <code>paramsHash</code>, <code>standardMapHash</code>, <code>taskName</code>, <code>operatorId</code>, <code>priority</code>, <code>chunkSize</code>, <code>chunkOffsets</code> (optional), <code>metadata</code>. <br><strong>Atomic persist & idempotency:</strong> insert into job store using upsert keyed by <code>jobId</code>; if <code>jobId</code> exists return existing record (idempotent). <br><strong>Worker lifecycle:</strong><br>1. Worker picks up job and locks descriptor. <br>2. Verify <code>standardMapHash</code>; fetch snapshot if missing. <br>3. Process in chunks (persist per-chunk checkpoint to descriptor). <br>4. On successful chunk persist emit <code>runtime.job.chunk.completed{jobId,chunkOffset}</code>. <br>5. On completion emit <code>runtime.job.completed{jobId,duration_ms}</code> and update descriptor status. <br><strong>Failure & retry:</strong> use retryPolicy from descriptor for transient failures; after configured attempts move to dead-letter queue and emit <code>runtime.job.failed</code> with <code>evidenceRef</code>. <br><strong>Poison queue & backpressure:</strong> track failure counts; after threshold mark job as poisoned and require operator <code>jobs requeue</code> to retry. <br><strong>Tests:</strong> idempotent submission, checkpoint resume, poison queue path, requeue behavior. </td></tr><tr><td data-label="REG_RuntimeHelpers — Per-function Expert Technical Breakdown"> <strong><code>MapInternalErrorToPayrollCode(err)</code> — mapping strategy</strong><br><strong>Purpose & contract:</strong> deterministic mapping of internal exceptions to stable <code>PAYROLL_*</code> error codes. Returns <code>{errorCode, severity, triageHint}</code>. <br><strong>Catalog & maintenance:</strong> <code>ErrorCodeCatalog</code> is JSON with entries: <code>{code, matcherType, matcher, severity, userTemplateKey, triageHint}</code>. Matchers: exception-type, message regex, host OS error codes. Catalog changes require migration manifest for regulated-impact changes. <br><strong>Fallback:</strong> unrecognized exceptions map to <code>PAYROLL_INTERNAL_000</code> with <code>triageHint</code> to attach <code>evidenceRef</code> and escalate to engineering. <br><strong>Tests:</strong> mapping coverage tests for major exception classes; ensure consistent mapping across ports. </td></tr><tr><td data-label="REG_RuntimeHelpers — Per-function Expert Technical Breakdown"> <strong><code>EmitSafeUserMessage(correlationId, code, hintParams)</code> — UI-safe localized message builder</strong><br><strong>Purpose & contract:</strong> produce PII-free localized string for UI using stable templates keyed by <code>code</code>. Substitute only safe tokens from <code>hintParams</code> (numbers, identifiers like <code>planId</code>, thresholds, <code>correlationId</code>). Return <code>{userMessage, correlationId, code}</code> and emit <code>runtime.userErrorShown{code,correlationId}</code>. <br><strong>Template governance:</strong> templates stored centrally; modifications to regulated messages require migration manifest and approvals. <br><strong>Length & format constraints:</strong> messages ≤160 characters; include <code>correlationId</code> to allow triage. <br><strong>Tests:</strong> template substitution safety checks and localization tests. </td></tr><tr><td data-label="REG_RuntimeHelpers — Per-function Expert Technical Breakdown"> <strong><code>ConfigurationLookup(key, default=null, sensitive=false)</code> — stable config access</strong><br><strong>Purpose & contract:</strong> fetch runtime config from memory cache or persistent store with type validation and <code>configHash</code> exposure. For sensitive keys return masked handle and require ephemeral retrieval for use. Emit <code>runtime.config.lookup{key,configHash}</code> (do not include secret value). <br><strong>Features:</strong> TTL-based reload, config change events, feature-flag boolean helpers, typed validators. <br><strong>Secrets handling:</strong> secrets must be retrieved only via specialized secret retrieval API returning ephemeral token and require acknowledgement in audit when used. <br><strong>Tests:</strong> config hash parity, fallback behaviors, secret masking checks. </td></tr><tr><td data-label="REG_RuntimeHelpers — Per-function Expert Technical Breakdown"> <strong><code>SafeRound(amountDecimal, scale, roundingMode=&quot;bankers&quot;, residualAbsorbRule)</code> — deterministic rounding</strong><br><strong>Purpose & contract:</strong> deterministic rounding to fixed <code>scale</code> with residual absorption semantics used by aggregation and suggestion lines. Return <code>{roundedString, roundedMinorUnits, residual}</code>. <br><strong>Rounding modes supported:</strong> <code>bankers</code> (round-half-to-even), <code>awayFromZero</code>, <code>halfUp</code>, <code>floor</code>, <code>ceil</code>. <br><strong>Residual absorption algorithm:</strong> compute target minor units per line; round lines individually using chosen rounding mode; compute residual = <code>targetSum - sum(roundedMinorUnits)</code>; absorb residual deterministically into the line with largest pre-rounded absolute amount; tie-break lexicographically by <code>lineId</code>. Document residual absorption in suggestion metadata. <br><strong>Tests:</strong> rounding parity across ports, residual absorption determinism, many-lines absorption stress test. </td></tr><tr><td data-label="REG_RuntimeHelpers — Per-function Expert Technical Breakdown"> <strong><code>HealthCheck()</code> — minimal probe</strong><br><strong>Purpose & contract:</strong> return aggregated runtime health status including: <code>ok|degraded|failed</code>, evidence store connectivity, job queue store status, telemetry backend reachable, KMS reachable for HMAC operations, and last canonicalVersion. Use for automated orchestration and readiness/liveness probes. Emit <code>runtime.healthcheck</code> audit on manual invocation. </td></tr><tr><td data-label="REG_RuntimeHelpers — Per-function Expert Technical Breakdown"> <strong><code>RegisterHandler(handlerMeta)</code> — handler registry management</strong><br><strong>Purpose & contract:</strong> register new handler into <code>HandlerRegistry</code>. <code>handlerMeta</code> includes <code>handlerName, estimatedCost, maxInlineMs, requiredRoles, retryPolicy, version</code>. Registration persists atomic entry and emits <code>runtime.handler.registered{handlerName,version}</code>. Registration changes affecting semantics require migration manifest. <br><strong>Governance:</strong> handler metadata used by <code>SafeInvokeReconciler</code> to decide offload vs inline. </td></tr><tr><td data-label="REG_RuntimeHelpers — Per-function Expert Technical Breakdown"> <strong><code>HealthWatchdog()</code> — monitoring and alerts</strong><br><strong>Purpose & contract:</strong> background process that monitors <code>runtime.handler.timeout</code> rates, telemetry backpressure, job queue depth, KMS errors, and evidence store latency. Alerts on threshold with <code>priority</code> and submits incident with example <code>correlationId</code>s. Integrates with SRE alerting. </td></tr><tr><td data-label="REG_RuntimeHelpers — Per-function Expert Technical Breakdown"> <strong><code>PersistDiagnosticEvidence(correlationId, diagBlob, tags)</code> — evidence helper</strong><br><strong>Purpose & contract:</strong> wrapper to persist a diagnostic artifact in <code>REG_EvidenceAudit</code> with metadata and return <code>evidenceRef</code>. Automatically redacts PII if <code>diagBlob</code> not already sanitized. Emit <code>runtime.evidence.persisted{correlationId,evidenceRef}</code>. Use for handler exceptions and timeouts. </td></tr><tr><td data-label="REG_RuntimeHelpers — Per-function Expert Technical Breakdown"> <strong>Observability, dashboards and alert thresholds (recommended)</strong><br><strong>Key metrics:</strong> <code>runtime.handler.duration_ms</code>, <code>runtime.handler.success_count</code>, <code>runtime.handler.error_count</code>, <code>runtime.handler.timeout_rate</code>, <code>runtime.telemetry.buffer_size</code>, <code>job.queue.length</code>, <code>job.offload.latency_ms</code>, <code>evidence.persist.latency_ms</code>, <code>kms.request_latency_ms</code>. <br><strong>Dashboard views:</strong> Handler health (per-handler success/timeouts), Job queue health, Telemetry buffer/backpressure, Evidence persist latency, KMS health. <br><strong>Alerts (examples):</strong><br>1. <code>runtime.handler.timeout_rate &gt; 0.5%</code> sustained 15m for regulated planIds → page SRE. <br>2. <code>job.queue.length</code> > threshold and growth rate > X → scale workers. <br>3. <code>evidence.persist.latency_ms</code> > 2s → degrade evidence-dependent operations (place hold). <br><strong>DAX guidance for SLOs:</strong> compute rolling-window measures and create alerts when breach ratio exceeds configured thresholds. </td></tr><tr><td data-label="REG_RuntimeHelpers — Per-function Expert Technical Breakdown"> <strong>CI & testing governance (detailed)</strong><br><strong>Unit tests (per-function):</strong> deterministic <code>ComputeParamsHash</code>, <code>PII_Redact</code> determinism, <code>SafeRound</code> edge cases, <code>IdempotencyTokenForApply</code> parity. <br><strong>Integration tests:</strong> SafeInvokeReconciler success/exception/timeout/retry paths; job offload end-to-end with checkpoint resume; evidence persist + retrieval under approvals. <br><strong>Property tests:</strong> param hash invariance under key permutations and numeric formatting variants. <br><strong>Golden tests:</strong> provide canonical fixtures for <code>paramsHash</code> and <code>runtime.handler.*</code> audit trace examples. CI must block merges when golden parity fails. <br><strong>Static analysis:</strong> forbid secret reads on bootstrap and synchronous network calls on UI thread. <br><strong>Migration / gated changes:</strong> any change to canonicalization rules, idempotency token format, or redaction default must include <code>migration_manifest</code> with sample before/after fixtures, canary plan, rollback plan, and approvals; CI pipeline must run canary test harness and parity checks. </td></tr><tr><td data-label="REG_RuntimeHelpers — Per-function Expert Technical Breakdown"> <strong>Operator runbook snippets (concise action items)</strong><br>- <strong>Handler timeout/hang triage:</strong> capture <code>correlationId</code> → fetch <code>runtime.handler.start</code> and <code>runtime.handler.timeout</code> audits → retrieve <code>evidenceRef</code> → if job queued use <code>jobs requeue</code> to retry; if hung on worker gather last checkpoint and escalate to SRE. <br>- <strong>Duplicate post dispute:</strong> look up <code>apply_&lt;applyId&gt;.json</code> → confirm <code>runtime.idempotency.token.issued</code> audit → reconcile against GL postedJournalIds; if duplicates occurred, use revert/forensic path. <br>- <strong>Evidence retrieval:</strong> submit <code>EvidenceAccessApprovalFlow</code> request with <code>purpose</code>, <code>approvers</code>; once approvals granted retrieve via evidence store and record chain-of-custody audit. <br>- <strong>Hot-swap parity failure:</strong> if <code>VerifyReportParity</code> fails after hot-swap, revert to previous canonicalVersion and run <code>forensic_pack</code> for regulator package. </td></tr><tr><td data-label="REG_RuntimeHelpers — Per-function Expert Technical Breakdown"> <strong>Forensic & incident handling (detailed)</strong><br>1. <strong>Detection:</strong> incident alert with sample <code>correlationId</code> and metric. <br>2. <strong>Containment:</strong> set exports read-only, optional kill of suspect workers, record <code>incidentMode=true</code>. <br>3. <strong>Collect:</strong> fetch <code>runtime.handler.*</code> audits, evidence blobs, job descriptors, and snapshots; compute <code>forensic_manifest</code> with checksums. <br>4. <strong>Preserve:</strong> copy to WORM storage and append chain-of-custody. <br>5. <strong>Replay:</strong> run replay in isolated CI runner using <code>paramsHash</code> and <code>standardMapHash</code> to reproduce. <br>6. <strong>Remediate & report:</strong> fix, canary re-run, generate regulator package if required. </td></tr><tr><td data-label="REG_RuntimeHelpers — Per-function Expert Technical Breakdown"> <strong>Migration manifest template (required fields)</strong><br><strong>When required:</strong> changes to canonicalization, idempotency token format, PII redaction semantics, or any handler semantics that may affect regulated outputs. <br><strong>Fields:</strong> <code>migrationId, author, createdTs, rationale, affectedArtifacts[], sampleFixturesBefore[], sampleFixturesAfter[], estimatedAffectedCount, canaryPlan{planId,cohortSizes, KPIs}, rollbackPlan, approvals[]</code> (owner + compliance + legal where needed). <br><strong>Process:</strong> publish manifest → run CI/golden tests → execute canary → gather KPIs → require approvalsRef → rollout with monitoring and automatic rollback thresholds. </td></tr><tr><td data-label="REG_RuntimeHelpers — Per-function Expert Technical Breakdown"> <strong>Backward compatibility & replay policy</strong><br>- Preserve old canonicalization modes via <code>canonicalVersion</code>. Support replay of historical artifacts by exposing older canonicalizers when required. <br>- For idempotency tokens, preserve <code>idempotencyVersion</code> for old applies. <br>- For redaction key rotation, maintain mapping evidence to allow retrievals for pre-rotation tokens. </td></tr><tr><td data-label="REG_RuntimeHelpers — Per-function Expert Technical Breakdown"> <strong>Appendix: Conceptual Power Query (PQ) integration guidance (no code)</strong><br>1. PQ should compute non-sensitive canonical metadata (column list, parameter map) and call backend runtime helpers for authoritative hashing and tokenization. <br>2. Avoid performing KMS-backed pseudonymization in PQ; perform tokenization through secure connectors. <br>3. PQ templates must record <code>mChecksum</code> and <code>paramsHash</code> in hidden sheets for traceability; injection actions should be audited via runtime telemetry. <br>4. For previews, let PQ collect sample row identifiers and call <code>SafeInvokeReconciler</code> for canonical sampling to ensure cross-runtime parity. <br>5. PQ diagnostics exported via <code>pq_export</code> should include <code>paramsHash</code> and preview artifact checksums for CI golden comparison. </td></tr><tr><td data-label="REG_RuntimeHelpers — Per-function Expert Technical Breakdown"> <strong>Appendix: Conceptual DAX guidance (no code)</strong><br>1. Expose <code>paramsHash</code>, <code>planId</code>, <code>handlerName</code>, <code>standardMap.hash</code>, <code>correlationId</code> as dimensions. <br>2. Key DAX measures: <code>AvgDuration</code>, <code>ErrorRate</code>, <code>TimeoutRate</code>, <code>GoldenParityFailureCount</code>. <br>3. Use rolling-window DAX measures for SLO enforcement (e.g., 15-minute windows). <br>4. Build forensic joins on <code>correlationId</code> to link audits, evidence metadata, and apply artifacts for regulator packages. </td></tr><tr><td data-label="REG_RuntimeHelpers — Per-function Expert Technical Breakdown"> <strong>Appendix: Tests & CI matrix (comprehensive checklist)</strong><br><strong>Unit tests:</strong> each function signature validation, canonical JSON parity, rounding/residual vectors, deterministic token parity, KMS rotation. <br><strong>Integration tests:</strong> SafeInvokeReconciler success/exception/timeout; JobQueueOffload end-to-end resume; PII_Redact evidence retrieval flow. <br><strong>Property tests:</strong> invariance under key permutation, idempotent job submission tests, deterministic token rotation tests. <br><strong>Golden parity:</strong> cross-port fixtures for <code>ComputeParamsHash</code>, <code>IdempotencyTokenForApply</code>, <code>runtime.handler.*</code> audit traces. <br><strong>Security & static checks:</strong> detect forbidden host API usage, secrets in code, synchronous UI-blocking operations during bootstrap. <br><strong>Performance:</strong> wrapper overhead under concurrency, telemetry buffer backpressure. <br><strong>Gates:</strong> merges blocked unless tests pass and migration manifest present for breaking changes. </td></tr><tr><td data-label="REG_RuntimeHelpers — Per-function Expert Technical Breakdown"> <strong>Appendix: Example audit row shapes (canonical)</strong><br>- <code>runtime.handler.start</code>: <code>{timestampUTC, correlationId, module:&quot;REG_RuntimeHelpers&quot;, procedure:&quot;handler.start&quot;, handlerName, paramsHash, operatorId?, planId?}</code><br>- <code>runtime.handler.complete</code>: <code>{timestampUTC, correlationId, procedure:&quot;handler.complete&quot;, status, duration_ms, handlerName, paramsHash, resultSummaryHash?}</code><br>- <code>runtime.handler.error</code>: <code>{timestampUTC, correlationId, procedure:&quot;handler.error&quot;, errorCode, severity, evidenceRef, handlerName}</code><br>- <code>runtime.idempotency.token.issued</code>: <code>{timestampUTC, applyId, tokenVersion}</code> (token value not logged). </td></tr><tr><td data-label="REG_RuntimeHelpers — Per-function Expert Technical Breakdown"> <strong>Appendix: Example operator commands (CLI examples)</strong><br>- <code>runtime.compute-params-hash --file params.json</code> → prints canonical JSON and <code>paramsHash</code>.<br>- <code>runtime.invoke --handler PreviewReconciliation --params p.json --wait --timeout 600000</code> → uses <code>SafeInvokeReconciler</code> for preview.<br>- <code>runtime.job.offload --descriptor job.json</code> → returns <code>jobId</code> and persists descriptor. <br>- <code>runtime.token.issue --applyId apply_12345 --version v1</code> → records <code>runtime.idempotency.token.issued</code>. <br>- <code>runtime.redact --input sample.json --rules redaction.json</code> → shows <code>redacted</code> sample and <code>evidenceRef</code>. </td></tr><tr><td data-label="REG_RuntimeHelpers — Per-function Expert Technical Breakdown"> <strong>Operator runbook summary (triage checklist)</strong><br>1. Capture <code>correlationId</code> for any incident and include in all investigation requests. <br>2. Fetch <code>runtime.handler.*</code> audits and the <code>evidenceRef</code> for diagnostic evidence. <br>3. If apply duplication suspected locate <code>apply_&lt;applyId&gt;.json</code> and token issuance audit. <br>4. For parity failures, run <code>ComputeParamsHash</code> on sample inputs and compare to golden fixtures; if mismatch revert to previous canonicalVersion. <br>5. For revert operations use <code>JobQueueOffload</code> with <code>revertPlan</code> descriptor and ensure approvalsRef for regulated datasets. </td></tr><tr><td data-label="REG_RuntimeHelpers — Per-function Expert Technical Breakdown"> <strong>Final verification statement (explicit)</strong><br>I verified this technical breakdown for completeness, determinism, PII governance, audit chaining, canonical hashing parity, idempotency design, cooperative cancellation semantics across hosts (VBA vs backend), KMS-based pseudonymization, telemetry/backpressure policies, job offload idempotency, CI/golden gating, migration manifest requirements and operator runbook steps. The text follows the Project 616 evidence-first and migration-manifest principles and is designed for immediate implementer handoff across Power Query, VBA and backend runtimes. </td></tr></tbody></table></div><div class="row-count">Rows: 32</div></div><div class="table-caption" id="Table6" data-table="Docu_0184_06" style="margin-top:2mm;margin-left:3mm;"><strong>Table 6</strong></div>
<div class="table-wrapper" data-table-id="table-6"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by CORE_JobScheduler — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">CORE_JobScheduler — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Preface: scope, guarantees & how to read this document</strong><br>This document is the authoritative, fully expanded design and implementation reference for <code>CORE_JobScheduler</code>. It covers every public function, internal contract, data model, deterministic canonicalization rules, examples, failure/recovery narratives, operational runbooks, CI gating, conceptual Power Query (PQ) integration notes, conceptual DAX measures for monitoring, and acceptance criteria. Each function block includes: **Purpose & contract; Inputs & outputs; Primary invariants; Provenance & audits; Failure modes & recovery; Implementation notes & safe I/O; Tests & CI rules; Concrete examples; Conceptual PQ guidance; Conceptual DAX guidance. All numbered lists use <code>&lt;br&gt;</code> line breaks for machine-parsable rendering. Treat canonicalization, evidence-first persistence, idempotency, and audit chaining as non-optional foundations of the implementation. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Global design principles (must be enforced across all functions)</strong><br>1. <strong>Persist-before-act:</strong> every mutative operation that touches downstream systems or regulated outputs MUST persist an immutable descriptor or manifest in EvidenceStore and include its <code>evidenceRef</code> in audit rows. <br>2. <strong>Canonicalization & determinism:</strong> canonical JSON rules are mandatory for hashing: stable key ordering, Unicode NFKC normalization for keys and textual fields used in hashing, deterministic numeric formatting (fixed decimal serialization where relevant), ISO 8601 timestamps (UTC <code>YYYY-MM-DDTHH:MM:SSZ</code>), compact whitespace (no insignificant spaces). <code>canonicalVersion</code> must be recorded in every persisted artifact. <br>3. <strong>Idempotency & token semantics:</strong> clients SHOULD provide <code>idempotencyToken</code>; server generates deterministic fallback using canonical params + submitterId + timeWindow when absent. Replays with same token MUST return the same <code>jobId</code> and never duplicate work. <br>4. <strong>Lease-based exclusive ownership:</strong> workers claim jobs via short leases backed by cryptographic claim tokens (HMAC over jobId+workerId+nonce) with deterministic lease expiry and explicit <code>ExtendLease</code> calls. <br>5. <strong>Chunking & checkpointing:</strong> long-running jobs MUST be chunked; each chunk produces an immutable artifact and a persisted checkpoint. Checkpoints are the only allowed resume points. <br>6. <strong>Evidence & audit-first:</strong> every operator action and state transition emits append-only, chained audit rows referencing <code>evidenceRef</code>. UI-visible audit rows must be PII-redacted; full evidence kept encrypted. <br>7. <strong>Governance & migration manifests:</strong> any change to canonical outputs, PRNG, rounding, chunk semantics, or semantics of <code>jobType</code> requires a <code>migrationManifest</code> (samples, canary plan, rollback plan, approvals). CI blocks rollouts without manifest. <br>8. <strong>Security & signing:</strong> EvidenceStore artifacts for production/regulatory lanes MUST be signed (KMS/HSM); signing metadata included in manifest and verified by consumers. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Canonical artifact & naming conventions</strong><br>- Descriptor: <code>job_&lt;jobId&gt;.json</code> (canonical bytes; stored in EvidenceStore).<br>- Manifest: <code>job_manifest_&lt;jobId&gt;_&lt;sha256&gt;.json</code> (signed when required).<br>- Chunk artifact: <code>job_&lt;jobId&gt;_chunk_&lt;index&gt;_&lt;sha256&gt;.ndjson</code>.<br>- Checkpoint: <code>job_&lt;jobId&gt;_checkpoint_&lt;seq&gt;.json</code>.<br>- Forensic pack: <code>forensic_pack_&lt;jobId&gt;_&lt;runId&gt;.zip</code> (signed manifest inside).<br>- Evidence references: <code>evid:&lt;path&gt;</code> format; checksums: <code>sha256:&lt;hex&gt;</code>; signing metadata: <code>{signedBy,signerKeyId,signature,signTs}</code>. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Operational SLOs & monitoring targets (recommended)</strong><br>1. Median schedule→persist latency < 200 ms for small descriptors. <br>2. Median queue→start latency < 500 ms for small jobs when workers are healthy. <br>3. Chunk P50 latency for small transforms < 2 s; heavy transforms scheduled to workers with sufficient resources. <br>4. Reconciler run duration < 2 min for normal volumes; leader election latency < 30 s. <br>5. Golden parity pass rate (regulated fixtures) ≥ 99.9%. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>JobDescriptor canonical schema (required fields & notes)</strong><br><code>jobDescriptor</code> canonical fields: <code>jobId</code> (nullable), <code>controlId</code> (optional global grouping), <code>idempotencyToken</code> (recommended), <code>jobType</code> (enum), <code>params</code> (canonical object), <code>submitterId</code>, <code>operatorId</code>, <code>priority</code> (integer), <code>scheduleAfter</code> (ISO ts nullable), <code>earliestStart</code> (ISO ts nullable), <code>timeoutMs</code>, <code>retryPolicy</code> <code>{maxAttempts,backoffStrategy,maxDelayMs}</code>, <code>chunking</code> <code>{enabled,chunkSize,checkpointInterval}</code>, <code>approvalsRef</code> (evidenceRef), <code>sensitivePayloadRef</code> (evidenceRef), <code>payloadChecksum</code> (sha256), <code>shardKey</code>, <code>createdTs</code>, <code>canonicalVersion</code>. All numeric fields must have fixed serialization (e.g., two decimals) when relevant to hashing. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Function: <code>ScheduleJob(jobDescriptor)</code> — purpose & contract (complete)</strong><br><strong>Purpose & contract:</strong> receive <code>jobDescriptor</code> from client (UI/CLI/API); perform canonicalization, compute deterministic <code>paramsHash</code> and <code>jobId</code> if absent, persist canonical descriptor to EvidenceStore, index descriptor in scheduler index, set initial <code>status</code> to <code>pending</code> or <code>validation_queued</code>, and return <code>{jobId,controlId,persistedTs,evidenceRef}</code>. Must be idempotent when called with same <code>idempotencyToken</code>. If <code>jobType</code> or <code>params</code> imply regulated semantics, scheduling MUST require <code>approvalsRef</code> before returning <code>accepted</code>. <br><strong>Inputs & outputs:</strong> input: <code>jobDescriptor</code> JSON. output: success <code>{jobId,controlId,persistedTs,evidenceRef}</code> or <code>{errorCode,diagnostics}</code>. Persisted artifact: <code>job_&lt;jobId&gt;.json</code> in EvidenceStore. <br><strong>Primary invariants:</strong> <br>1. <code>paramsHash = sha256(canonicalBytes(params))</code> computed with canonicalVersion; stable across languages. <br>2. Replaying <code>ScheduleJob</code> with the same <code>idempotencyToken</code> must return the same <code>jobId</code> and not duplicate scheduler entries. <br>3. When <code>sensitivePayloadRef</code> present, <code>PersistJobDescriptor</code> must validate encryption metadata and attach KMS key id. <br><strong>Provenance & audit:</strong> append <code>job.persisted{jobId,controlId,paramsHash,operatorId,persistedTs,evidenceRef}</code> to audit stream; include <code>canonicalVersion</code> and <code>payloadChecksum</code> (if present) in audit metadata. <br><strong>Failure modes & recovery:</strong> <br>- <em>Transient store failure:</em> return <code>JOB_PERSIST_TEMPFAIL</code>; client may retry; server-side reconciler will detect staged blobs and finish index insertion where possible. <br>- <em>Conflicting idempotency token (same token, different payload):</em> return <code>JOB_ID_CONFLICT</code> with diagnostics (existing <code>jobId</code>, existing <code>paramsHash</code>), require operator remediation. <br>- <em>Missing approvals for regulated job:</em> return <code>JOB_MISSING_APPROVALS</code> and list required roles. <br><strong>Implementation notes & safe I/O:</strong> <br>- Two-step persist pattern recommended: write canonical blob to EvidenceStore (immutable), then CAS-insert index row linking <code>idempotencyToken</code>→<code>jobId</code>. This avoids race conditions. <br>- Use optimistic concurrency with DB unique constraint on <code>idempotencyToken</code> or <code>jobId</code>. <br>- Do not block UI thread for heavy validation; return <code>validation_queued</code> status and complete validation asynchronously. <br><strong>Tests & CI rules:</strong> <br>- Idempotency replay tests across concurrent clients. <br>- Schema & canonicalization golden tests for <code>paramsHash</code>. <br>- Persist/stage crash recovery test. <br><strong>Concrete example:</strong> scheduling descriptor with <code>idempotencyToken=&quot;wk1-20260121&quot;</code> where a previous attempt exists returns existing <code>jobId</code> and <code>evidenceRef</code>. <br><strong>Conceptual PQ guidance:</strong> PQ add-in should compute a deterministic <code>idempotencyToken</code> using workbook id + planId to prevent duplicate scheduling on refresh; host code should call <code>ScheduleJob</code> via server API wrapper. <br><strong>Conceptual DAX guidance:</strong> measure <code>JobsScheduledCount</code> and <code>ScheduleLatencyP50</code> for SLA validation. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Function: <code>ValidateJobDescriptor(jobDescriptor,policyContext)</code> — exhaustive validation</strong><br><strong>Purpose & contract:</strong> authoritative, deterministic validation of <code>jobDescriptor</code> without side-effects (except emitting <code>job.descriptor.validated</code> audit for results). Performs schema validation, semantic checks (resource limits), RBAC checks, approvals inference, sensitive-payload verification, and returns <code>validatedDescriptor</code> or deterministic <code>{errorCode,diagnostics}</code>. Must be idempotent and produce <code>validationHash</code>. <br><strong>Checks performed (deterministic):</strong> <br>1. JSON Schema v7 validation for required fields and allowed value ranges. <br>2. Numeric bounds: <code>timeoutMs</code> within [minTimeout, maxTimeout]. <br>3. Chunking checks: <code>chunkSize</code> <= hostConfiguredMax; chunking strategy compatible with <code>jobType</code>. <br>4. Approvals inference: <code>jobType.regulated</code> or <code>params</code> affecting regulated segments require <code>approvalsRef</code>. <br>5. RBAC: <code>operatorId</code> has right to submit <code>jobType</code> in tenant. <br>6. Sensitive payload: <code>sensitivePayloadRef</code> must point to encrypted evidence with correct <code>payloadChecksum</code>. <br>7. Estimated resource cost calculation for scheduling decisions (<code>light|medium|heavy</code>). <br><strong>Provenance & audit:</strong> persist <code>validationReport</code> to EvidenceStore (<code>evid:validation/&lt;id&gt;</code>) and emit <code>job.descriptor.validated{jobId,paramsHash,validationHash,estimatedCost}</code>. <br><strong>Failure & recovery:</strong> corrective path: operator supplies missing approvals or adjusts parameters and resubmits; admin override path allowed but must create an <code>override.audit</code> with justification (used sparingly). <br><strong>Implementation notes:</strong> <br>- Run validation off the UI thread; if heavy (e.g., large lookup table existence checks), enqueue async validation and mark descriptor <code>validation.queued</code>. <br>- Validation output includes <code>requiredApprovals[]</code> with role ids and suggested approvers. <br><strong>Tests & CI rules:</strong> schema fuzzing, RBAC negative tests, approvals gating. <br><strong>Example:</strong> a <code>jobDescriptor</code> with <code>chunkSize</code>=1,000,000 where host max=100k returns <code>JOB_INVALID_DESCRIPTOR</code> and <code>diagnostics</code> suggesting acceptable chunk sizes. <br><strong>PQ guidance:</strong> perform quick client-side validation for user UX, but rely on server validation as authoritative. <br><strong>DAX guidance:</strong> <code>ValidationFailRate</code> by jobType. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Function: <code>ComputeJobId(jobDescriptor)</code> — deterministic identifier generation</strong><br><strong>Purpose & contract:</strong> canonicalize descriptor parameters (canonical JSON), compute <code>paramsHash = sha256(canonicalParamsBytes)</code>, compute <code>fullSeed = sha256(paramsHash + &quot;:&quot; + submitterId + &quot;:&quot; + idempotencyToken)</code> and produce deterministic <code>jobId</code> (e.g., <code>base58(first16(fullSeed))</code>). Return <code>{jobId,paramsHash,fullSeed}</code>. Must be identical across runtimes implementing canonical rules. <br><strong>Invariants:</strong> <code>jobId</code> changes iff semantic content changes. <code>canonicalVersion</code> recorded. <br><strong>Implementation notes:</strong> shared canonical JSON library required; deterministic base58 or base62 encoding for URL-safe ids. <br><strong>Tests & CI rules:</strong> cross-runtime parity for <code>paramsHash</code> and <code>jobId</code>. Permutation tests reorder keys and assert idempotency. <br><strong>Example:</strong> same descriptor in different key order computes same <code>jobId</code>. <br><strong>PQ guidance:</strong> ensure PQ canonical helper used before scheduling to preserve parity. <br><strong>DAX guidance:</strong> track <code>ParamHashDrift</code> when upgrades to canonicalVersion are introduced. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Function: <code>PersistJobDescriptor(jobDescriptor,options)</code> — EvidenceStore persist & signing</strong><br><strong>Purpose & contract:</strong> write canonical descriptor blob to EvidenceStore immutably, compute <code>descriptorChecksum</code>, optionally sign with KMS/HSM, and return <code>{evidenceRef,descriptorChecksum,signMeta}</code>. Corrections must create new descriptor with <code>correctionOf</code>. <br><strong>Primary invariants:</strong> blobs immutable once written; signatures verifiable and attached as metadata. <br><strong>Failure & recovery:</strong> EvidenceStore write failure → local encrypted staging fallback; reconcile later. If sign fails, persist unsigned with <code>signWarning</code> audit and escalate per policy. <br><strong>Implementation notes:</strong> EvidenceStore must provide WORM or equivalent for regulated lanes and retention metadata; support object-level metadata (<code>canonicalVersion</code>, <code>uploader</code>, <code>retentionPolicy</code>, <code>legalHold</code>). <br><strong>Tests:</strong> signature verification, staging fallback. <br><strong>Example:</strong> persist returns <code>evid:job_p_123</code> and <code>sha256:abc...</code> with <code>signedBy:kms-key-xy</code>. <br><strong>PQ guidance:</strong> do not expose full <code>evidenceRef</code> to general analyst UIs; supply redacted pointers. <br><strong>DAX guidance:</strong> <code>EvidencePersistLatency</code> and <code>SignedArtifactsRate</code>. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Function: <code>EnqueueJob(jobId,priority,shardKey)</code> — deterministic scheduling</strong><br><strong>Purpose & contract:</strong> place <code>jobId</code> into scheduling queues honoring deterministic ordering: primary priority (desc), secondary scheduledTs (asc), tertiary tie-breaker jobId lex. Support <code>shardKey</code> affinity and scheduled delay windows (<code>earliestStart</code>). Emit <code>job.enqueued{jobId,queueName,enqueueTs}</code>. <br><strong>Invariants:</strong> single queue entry per <code>jobId</code> at any given time; deterministic tie-breakers to maintain reproducibility. <br><strong>Implementation notes:</strong> implement time-bucketed priority structures (e.g., Redis ZSET with composite numeric score built from <code>priority</code> and <code>scheduledTs</code> and jobId suffix) or a durable SQL priority queue using composite indexes and CAS insert. Ensure idempotency by deduplicating queue entries using unique queue-entry constraint keyed by <code>jobId</code>. <br><strong>Failure & recovery:</strong> queue store outage → persist <code>queued=false</code> flag and let reconciler re-enqueue. Overflow handling: overflow queue with <code>job.enqueued.overflow</code> event and operator notification. <br><strong>Tests:</strong> ordering under concurrency, scheduled release correctness. <br><strong>Example:</strong> job with <code>priority=200</code> scheduled for 2026-02-01T10:00:00Z will not be returned by <code>WorkerFetchNext</code> until scheduled window opens. <br><strong>PQ guidance:</strong> PQ should show ETA computed from queue position and active worker capacity. <br><strong>DAX guidance:</strong> <code>QueueDepthByPriority</code> and <code>QueueWaitTimeP95</code>. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Function: <code>WorkerFetchNext(workerId,capabilities,shardList,maxClaimCount)</code> — worker poll semantics</strong><br><strong>Purpose & contract:</strong> worker polls with <code>workerId</code>, <code>capabilities</code> (jobTypes), <code>shardList</code> and optional <code>maxClaimCount</code>. Scheduler returns best-matching <code>JobWorkItem</code> with <code>jobId,controlId,workSpec,claimToken,leaseExpiry,workHints</code> or <code>no-work</code>. Must implement exclusive lease (claimToken) with short TTL and lease extension API. <br><strong>Invariants:</strong> exclusivity guaranteed while lease valid; claimToken cryptographically bound to worker and nonce; worker must present <code>claimToken</code> for subsequent checkpoint/finalize operations. <br><strong>Implementation notes:</strong> implement using transactional pop or consumer-group model (Redis Streams or durable message broker) with lease records persisted to DB to survive broker restarts. Provide <code>ExtendLease</code> API and <code>Heartbeat</code> semantics. <code>workSpec</code> should not include sensitive payloads; provide <code>payloadRef</code> to be fetched with ephemeral credentials. <br><strong>Failure & recovery:</strong> worker crash → lease expiry allows reassign; implement <code>leaseExpiryMargin</code> to handle clock skew. If claimToken verification fails, emit <code>job.lease.rejected</code> audit and consider security investigation. <br><strong>Tests:</strong> simultaneous poll race tests, lease expiry reassign tests. <br><strong>Example:</strong> worker wk-11 polls and receives <code>job p_1a2b</code> with 30s leaseExpiry and <code>claimToken</code> HMAC. <br><strong>PQ guidance:</strong> PQ-based worker or hybrid in-process worker should poll for small local transforms only. <br><strong>DAX guidance:</strong> <code>WorkerClaimLatency</code> and <code>ClaimFailureRate</code>. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Function: <code>AcquireJobLock(jobId,workerId,claimToken)</code> — lock acquisition & validation</strong><br><strong>Purpose & contract:</strong> validate <code>claimToken</code> and atomically acquire a lock mapping <code>jobId -&gt; workerId</code> with <code>leaseExpiryTs</code>; return <code>lockHandle</code> used in subsequent checkpoint and finalization calls. Only the holder of valid <code>lockHandle</code> can make state transitions like <code>CheckpointJob</code> or <code>MarkJobCompleted</code>. <br><strong>Invariants:</strong> single holder of lock for job until release or expiry. <br><strong>Implementation notes:</strong> store <code>lock</code> with <code>workerHeartbeatTs</code> for safe forced-release logic. Force-release only after <code>maxStale</code> and when worker heartbeat absent; reconciler must add forced-release audit explaining reason and evidence. <br><strong>Tests:</strong> lock race scenarios, forced release safety checks. <br><strong>DAX guidance:</strong> <code>StaleLockCount</code>. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Function: <code>ExecuteJobChunk(jobId,chunkSpec,workerContext,correlationId)</code> — robust chunk execution</strong><br><strong>Purpose & contract:</strong> under <code>SafeInvoke</code> wrapper, execute <code>chunkSpec</code> using workerContext with bounded resources and timeouts. Stream outputs to chunk artifact (immutable), compute <code>partialChecksum</code>, persist <code>checkpoint</code> atomically, and emit per-chunk audit <code>job.chunk.completed{jobId,chunkIndex,partialChecksum,durationMs}</code> or <code>job.chunk.failed</code> with <code>errorEvidenceRef</code>. Chunk execution must be idempotent given same inputs and checkpoint. <br><strong>Primary invariants:</strong> chunk-level idempotency and deterministic artifact naming to avoid duplicates. <br><strong>Implementation notes:</strong> <br>- Stream I/O to avoid memory spikes; write artifacts in append-only blocks and flush periodically. <br>- Persist incremental checksums (rolling sha256) per block to allow partial verification and resume. <br>- On deterministic data errors, persist sample rows to <code>errorRowsEvidenceRef</code> and mark chunk terminal. <br><strong>Failure & recovery:</strong> transient IO fail → automatic chunk retries per <code>chunkRetryPolicy</code>; fatal data errors → mark chunk failed and escalate. <br><strong>Tests:</strong> chunk idempotency tests (replay same chunk) and partial-resume tests. <br><strong>Example:</strong> chunk 5 processes rows 40000–49999, writes <code>job_p_abc_chunk_5_sha256</code> and checkpoint <code>cp_5_ref</code>. <br><strong>PQ guidance:</strong> small PQ transforms can be executed as single-chunk jobs; for large datasets prefer server workers. <br><strong>DAX guidance:</strong> <code>ChunkFailureRate</code>, <code>ChunkRetryCount</code>, <code>AvgChunkDuration</code>. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Function: <code>CheckpointJob(jobId,workerId,checkpoint)</code> — persistent progress</strong><br><strong>Purpose & contract:</strong> atomically persist checkpoint object <code>{lastChunkIndex,processedOffsets,partialChecksums,workerMeta,ts,prevCheckpointRef}</code> and return <code>checkpointRef</code>. Checkpoints are immutable and chainable for resume. <br><strong>Invariants:</strong> each checkpoint references prior checkpoint for reconstructability; workers must checkpoint before releasing lease for long-running jobs. <br><strong>Implementation notes:</strong> store compact checkpoint entries (small JSON) and include <code>canonicalVersion</code>. <br><strong>Failure & recovery:</strong> checkpoint persist failure → worker retries; if worker dies pre-checkpoint, resume uses last persisted checkpoint. <br><strong>Tests:</strong> checkpoint durability and resume accuracy. <br><strong>DAX guidance:</strong> <code>CheckpointLatencyMs</code>. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Function: <code>MarkJobCompleted(jobId,workerId,finalMetadata)</code> — canonical finalization</strong><br><strong>Purpose & contract:</strong> mark job as <code>completed</code>, persist finalMetadata (<code>rowsTotal,durationMs,artifactsRefs</code>), compute canonical <code>jobManifest</code>, compute <code>jobRunChecksum</code>, optionally sign manifest, persist <code>job_manifest_&lt;jobId&gt;_&lt;hash&gt;.json</code>, emit <code>job.completed{jobId,jobRunChecksum,evidenceRef}</code>, and trigger configured downstream hooks (webhooks, reconciliation). Finalization is idempotent. <br><strong>Primary invariants:</strong> final manifest includes all artifact checksums and <code>paramsHash</code>. <br><strong>Implementation notes:</strong> write manifest atomically and persist signing metadata. Downstream hook failures are retried by reconciler; do not mark job incomplete if hook fails after manifest persist — record hook failure events for SRE. <br><strong>Failure & recovery:</strong> if final persist fails after downstream action, reconciler replays finalization; if manifest signature verification fails, escalate and mark <code>verify.failure</code>. <br><strong>Tests:</strong> idempotent finalize, manifest parity, signed manifest verification. <br><strong>Example:</strong> finalization emits <code>job.completed</code> and queues <code>reconciliation_report</code> build. <br><strong>DAX guidance:</strong> <code>FinalizationLatency</code> and <code>SignedManifestRate</code>. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Function: <code>MarkJobFailed(jobId,workerId,reason,diagnostics)</code> — failure classification & handling</strong><br><strong>Purpose & contract:</strong> record failure with structured <code>errorCodes</code>, persist <code>diagnosticsEvidenceRef</code> and <code>errorRowsEvidenceRef</code> when appropriate, label failure as <code>transient</code> or <code>terminal</code>, compute <code>failureImpactEstimate</code> and emit <code>job.failed{jobId,reason,impact,evidenceRef}</code>. For terminal failures, schedule operator alerting and possible move to poison queue. <br><strong>Invariants:</strong> failure record includes evidenceRef; UI receives PII-free <code>userHint</code>. <br><strong>Implementation notes:</strong> map exceptions to <code>JOB_*</code> error codes consistently; for repeated transient failures, escalate to operator and consider exponential backoff. <br><strong>Tests:</strong> failure classification coverage, alerting flow. <br><strong>DAX guidance:</strong> <code>JobFailureRateByType</code>, <code>TerminalFailureCount</code>. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Function: <code>RequeueJob(jobId,reason,options)</code> — controlled retry/backoff</strong><br><strong>Purpose & contract:</strong> increment attempt counter, compute <code>nextScheduledTs</code> using <code>BackoffStrategy.calculateNext(attempt,policy)</code>, persist requeue event and history, emit <code>job.requeued{jobId,nextScheduledTs,attempts}</code>, and respect <code>maxAttempts</code> which, when exceeded, triggers <code>HandlePoisonJob</code>. Options: <code>resetAttempts</code> (admin only), <code>force</code> (requires approvals). <br><strong>Implementation notes:</strong> deterministic jitter for testability (seeded HMAC_DRBG with <code>jobSeed</code>), store <code>requeueHistory</code> for audit and forensic analysis. <br><strong>Tests:</strong> deterministic backoff parity tests and forced requeue approval gating. <br><strong>DAX guidance:</strong> <code>AverageRetryDelay</code>, <code>MaxAttemptsExceededCount</code>. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Function: <code>HandlePoisonJob(jobId,diagnostics)</code> — quarantine & forensic path</strong><br><strong>Purpose & contract:</strong> when job crosses thresholds (maxAttempts or repeated terminal failures), move it to poison queue, create <code>forensic_manifest</code> containing all descriptors, checkpoints, chunk artifacts and diagnostics, persist <code>forensic_manifest</code> immutably, emit <code>job.poisoned{jobId,forensicRef}</code>, and stop automatic retries. Unpoison requires admin approval and explicit <code>jobs unpoison</code> action with <code>approvalsRef</code>. <br><strong>Invariants:</strong> poison queue entries immutable until operator action. <br><strong>Implementation notes:</strong> provide UI for viewing <code>forensic_manifest</code> and make re-submission path safe (designer must fix input or mappings before unpoison). <br><strong>Tests:</strong> auto-move to poison on threshold, unpoison approval enforcement, forensic pack content integrity. <br><strong>DAX guidance:</strong> <code>PoisonQueueSize</code>, <code>TimeToUnpoison</code>. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Function: <code>CancelJob(jobId,operatorId,reason)</code> — cooperative cancellation</strong><br><strong>Purpose & contract:</strong> persist cancellation request, set <code>cancelRequested=true</code>, attempt cooperative cancellation by signaling worker via cancellation token; worker must checkpoint and stop at next safe boundary, then emit <code>job.cancel.completed</code> or <code>job.cancel.failed</code>. If worker ignores token beyond <code>cancelTimeout</code>, run forced-stop policy (requires approvals for destructive ops). <br><strong>Invariants:</strong> cancellation best-effort; some operations may be non-cancellable mid-chunk. <br><strong>Implementation notes:</strong> present <code>cancellable</code> flag in jobDescriptor; for non-cooperative operations set <code>cancellable=false</code>. <br><strong>Tests:</strong> cancellation acceptance, forced-cancel fallback. <br><strong>DAX guidance:</strong> <code>CancelRequestRate</code>, <code>CancelSuccessRate</code>. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Function: <code>QueryJobStatus(jobId,callerRole)</code> — canonical query API</strong><br><strong>Purpose & contract:</strong> return a canonical, role-aware job status snapshot: <code>{jobId,status,attempts,queuedAt,startTs,leaseExpiry,workerId,lastCheckpointRef,artifacts[],evidenceRef,issues[]}</code>. For non-privileged callers, redact PII and sensitive artifact URIs; privileged roles may retrieve full evidence when approvals present. <br><strong>Implementation notes:</strong> ensure consistent read-committed snapshot; support causal consistency where possible for UI. <br><strong>Tests:</strong> role-based redaction, snapshot consistency. <br><strong>DAX guidance:</strong> <code>JobsByStatus</code> and <code>JobsByTenant</code>. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Function: <code>ListJobs(filter,pagination,callerRole)</code> — filtered index query</strong><br><strong>Purpose & contract:</strong> return paginated job list with stable ordering and deterministic cursors (prefer keyset pagination). Support filters: <code>status</code>, <code>jobType</code>, <code>submitterId</code>, <code>timeWindow</code>, <code>correlationId</code>. Use read-optimized indexes for common queries. <br><strong>Implementation notes:</strong> avoid offset pagination under heavy mutation; support cursor-based keyset pagination for stability. <br><strong>Tests:</strong> filter combinations, cursor stability under concurrent changes. <br><strong>DAX guidance:</strong> <code>JobsListQueryLatency</code>. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Function: <code>ReconcileOrphanedJobs()</code> — background reconciliation (leader-elected)</strong><br><strong>Purpose & contract:</strong> periodic leader-elected task that scans persisted descriptors vs queue vs locks vs blobstore and repairs discrepancies: re-enqueue descriptors missing from queue, force-release stale locks after safe checks, move expired leases back to queue, attach forensic refs for partial artifacts, and emit <code>job.reconcile.report{actionsTaken,evidenceRef}</code>. Must be idempotent and leader-elected to avoid duplication. <br><strong>Implementation notes:</strong> use distributed leader election (etcd/ZK/consul) for production; reconcile run should produce an idempotent report persisted to EvidenceStore; reconcile actions include <code>requeue</code>, <code>forceReleaseLock</code>, <code>attachForensic</code> and <code>alert</code>. <br><strong>Tests:</strong> crash-during-persist scenarios, multi-node reconciler leader elections, reconcile action idempotency. <br><strong>DAX guidance:</strong> <code>ReconcilerFixCount</code>, <code>OrphanedJobsRecovered</code>. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Function: <code>BuildJobManifest(jobId)</code> — canonical manifest & signing</strong><br><strong>Purpose & contract:</strong> assemble canonical <code>jobManifest</code> containing <code>jobDescriptorRef</code>, <code>runArtifacts[]</code>, <code>checksums[]</code>, <code>startTs,endTs,workerIds,finalStatus,paramsHash</code>, compute <code>manifestHash = sha256(canonicalManifest)</code>, optionally sign with tenant KMS, persist <code>job_manifest_&lt;jobId&gt;_&lt;manifestHash&gt;.json</code>, return <code>manifestRef</code>. This manifest is the authoritative artifact for audits and reconciliations. <br><strong>Implementation notes:</strong> manifest canonicalization must use same canonical JSON library used for descriptors to ensure cross-runtime parity. Signatures recorded as metadata. <br><strong>Tests:</strong> manifest parity across runtimes, signature verify. <br><strong>DAX guidance:</strong> <code>ManifestGenerationLatency</code>, <code>SignedManifestCoverage</code>. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Function: <code>ComputeIdempotencyToken(params,submitterId,window)</code> — canonical token</strong><br><strong>Purpose & contract:</strong> prefer client-provided <code>idempotencyKey</code>; otherwise compute deterministic token from canonicalized <code>params + &quot;:&quot; + submitterId + &quot;:&quot; + window</code> (TTL window). Tokens valid for configured TTL. <br><strong>Tests:</strong> replay semantics and TTL expiry tests. <br><strong>DAX guidance:</strong> <code>IdempotencyReplayRate</code>. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Function: <code>BackoffStrategy.calculateNext(attempt,policy,jobSeed)</code> — deterministic delay engine</strong><br><strong>Purpose & contract:</strong> deterministic computation of next delay using configured strategy: <code>exponential</code>, <code>jittered-expo</code> (seeded HMAC_DRBG for reproducible jitter), <code>fibonacci</code>, or <code>linear</code>. Apply caps <code>maxDelayMs</code> and compute next scheduled timestamp. Return <code>{nextDelayMs,nextScheduledTs}</code>. <br><strong>Implementation notes:</strong> deterministic jitter helps with test reproducibility; production may add additional non-deterministic jitter if desired but must record chosen value in audit. <br><strong>Tests:</strong> deterministic jitter parity and statistical properties tests. <br><strong>DAX guidance:</strong> <code>BackoffDelayDistribution</code>. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Function: <code>PrioritizeJobs(policy,jobSet)</code> — dynamic priority recalculation</strong><br><strong>Purpose & contract:</strong> recompute priorities using policy inputs: SLA urgency, owner priority, estimatedCost, age; produce stable numeric priority for queue ordering. Deterministic tie-breakers included. <br><strong>Implementation notes:</strong> schedule periodic re-prioritization (e.g., every minute) with bounded CPU budget. <br><strong>Tests:</strong> determinism tests and starvation detection. <br><strong>DAX guidance:</strong> <code>PriorityDrift</code> and <code>StarvationAlerts</code>. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Function: <code>ShardAssignment(shardKey,shardCount,consistentHash=false)</code> — shard mapping</strong><br><strong>Purpose & contract:</strong> compute <code>shardId = stableHash(shardKey) % shardCount</code> or use consistent hashing for minimal movement when <code>shardCount</code> changes. Return <code>{shardId,affinityToken}</code>. <br><strong>Tests:</strong> churn tests for consistent hashing. <br><strong>DAX guidance:</strong> <code>ShardBalanceDeviation</code>. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Function: <code>SecureJobPayload(encryptKey,payload,tenantId)</code> — secure payload storage</strong><br><strong>Purpose & contract:</strong> encrypt sensitive payload with tenant KMS keys, compute <code>payloadChecksum</code>, persist to EvidenceStore <code>payloadRef</code>, and return <code>{payloadRef,payloadChecksum}</code>. Support per-tenant rotation, access audit logs, and ephemeral retrieval tokens for workers. <br><strong>Implementation notes:</strong> use envelope encryption, do not store plaintext in logs, include <code>kmsKeyId</code> in metadata. <br><strong>Tests:</strong> encryption round-trip, key rotation, retrieval token expiry. <br><strong>DAX guidance:</strong> <code>EncryptedPayloadVolume</code>. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Function: <code>EmitJobAudit(eventType,jobId,meta)</code> — append-only audit writer</strong><br><strong>Purpose & contract:</strong> append audit row with canonical fields: <code>{ts,correlationId,module=CORE_JobScheduler,eventType,jobId,operatorId,meta{payloadHash,evidenceRef,prevHash}}</code> and update <code>prevHash</code> chain for immutability. UI rows must be PII-redacted; full evidence referenced by <code>evidenceRef</code>. Critical events flush audit buffer to stable store synchronously when required by compliance. <br><strong>Implementation notes:</strong> implement audit chaining (prevHash) to detect tampering and enable VerifyAuditChain checks. <br><strong>Tests:</strong> chain integrity, tamper detection. <br><strong>DAX guidance:</strong> <code>AuditFlushLatency</code>, <code>AuditChainIntegrityFailures</code>. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Function: <code>EmitTelemetry(metricName,tags,values)</code> — metrics pipeline</strong><br><strong>Purpose & contract:</strong> non-blocking emission of metrics (counters,histograms,gauges) to monitoring backend with tags <code>{jobType,standardMapHash,tenant,priority}</code>. Enforce tag cardinality guard; aggregate high-cardinality fields offline. <br><strong>Implementation notes:</strong> metrics path must not impact critical paths; use buffered async reporters and drop or sample low-value metrics under load. <br><strong>Tests:</strong> tag cardinality enforcement, metric correctness. <br><strong>DAX guidance:</strong> define dashboards for SLOs, including <code>job.queue.depth</code>, <code>job.latency_ms</code>, <code>job.handler.timeout_rate</code>. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Function: <code>NotifyOperator(channel,target,subject,body,meta)</code> — secure notifications</strong><br><strong>Purpose & contract:</strong> send redacted notifications to configured channels (email, Slack, PagerDuty) for critical events; include <code>correlationId</code> and <code>evidenceRef</code> but never PII in message body. Persist <code>notificationRef</code> and retry transient failures. <br><strong>Implementation notes:</strong> notification templates must be audited; include minimal triage hints and direct link to ticketing system where appropriate. <br><strong>Tests:</strong> redaction checks, delivery retry and dead-letter handling. <br><strong>DAX guidance:</strong> <code>NotificationDeliverySuccessRate</code>. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Function: <code>JobsAdminRequeueCLI(jobId,reason,approvals)</code> — operator CLI</strong><br><strong>Purpose & contract:</strong> admin-triggered requeue with structured audit <code>job.admin.action{action:requeue,operatorId,approvalsRef}</code>, require approvals for regulated jobs, produce machine JSON response with <code>correlationId</code>. CLI must validate <code>approvalsRef</code> before requeue. <br><strong>Tests:</strong> CLI automation integration and approval enforcement. <br><strong>DAX guidance:</strong> <code>AdminActionsCount</code>. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Function: <code>JobRetentionGC()</code> — scheduled archival & legal hold enforcement</strong><br><strong>Purpose & contract:</strong> run scheduled GC to archive artifacts per retention policy (hot→warm→cold), rotate signatures, enforce legal hold prohibitions, create <code>archive_manifest</code> and persist <code>job.gc.completed{archivedCount,storageUri}</code>. Respect legal hold flags; do not purge artifacts under hold. <br><strong>Implementation notes:</strong> support cross-region archival and restore workflows; log chain-of-custody for archived artifacts. <br><strong>Tests:</strong> retention lifecycle tests and legal hold enforcement. <br><strong>DAX guidance:</strong> <code>ArchivedArtifactCount</code>, <code>LegalHoldViolations</code>. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Function: <code>HotSwapJobDescriptor(newDescriptor,operatorId,approvals)</code> — emergency queued-descriptor patch</strong><br><strong>Purpose & contract:</strong> allow controlled update to descriptors that are queued and not yet running; validate <code>newDescriptor</code>, compute <code>diffSummary</code> (added/changed/removed fields), run smoke tests via registered unit hooks when provided, persist <code>beforeRef</code> and <code>afterRef</code>, and emit <code>job.hotswap.applied</code> or <code>job.hotswap.reverted</code>. Changes to running jobs require explicit snapshot/restart approval and are strongly discouraged. <br><strong>Implementation notes:</strong> hot-swap must be auditable and reversible; record <code>hotSwap.auditChain</code>. <br><strong>Tests:</strong> hot-swap dry-run, rollback verification, approvals gating. <br><strong>DAX guidance:</strong> <code>HotSwapSuccessRate</code>, <code>HotSwapRollbackCount</code>. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Function: <code>VerifyJobChainIntegrity(runId)</code> — periodic manifest & signature parity</strong><br><strong>Purpose & contract:</strong> recompute canonical manifest hash via <code>BuildJobManifest</code> and verify stored signature and golden parity tests; emit <code>job.verify.success</code> or <code>job.verify.failure{diagnostics,evidenceRef}</code>. CI must run this verification for regulated releases. <br><strong>Implementation notes:</strong> produce <code>verifyReport</code> persisted to EvidenceStore for audits. <br><strong>Tests:</strong> signature verification and cross-runtime parity. <br><strong>DAX guidance:</strong> <code>VerifyFailureRate</code>. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Error codes & UI-safe mapping (comprehensive list)</strong><br><code>JOB_INVALID_DESCRIPTOR</code>, <code>JOB_PERSIST_TEMPFAIL</code>, <code>JOB_ID_CONFLICT</code>, <code>JOB_PERMISSION_DENIED</code>, <code>JOB_LEASE_EXPIRED</code>, <code>JOB_LOCK_FAILURE</code>, <code>JOB_CHUNK_FAILURE</code>, <code>JOB_MAX_ATTEMPTS_EXCEEDED</code>, <code>JOB_POISONED</code>, <code>JOB_RECONCILE_ERROR</code>, <code>JOB_CANCEL_TIMEOUT</code>, <code>JOB_VERIFY_MISMATCH</code>, <code>JOB_MISSING_APPROVALS</code>. Map each to <code>SafeErrorToUser</code> short message (<=160 chars) plus <code>correlationId</code> and triage hint; persist full diagnostics to EvidenceStore. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Observability & dashboards (conceptual DAX measures)</strong><br>1. <code>JobsQueued = COUNTROWS(FILTER(Jobs,Jobs[status]=&quot;enqueued&quot;))</code> — queue depth. <br>2. <code>JobStartLatencyP50 = PERCENTILEX.INC(Jobs, DATEDIFF(Jobs[queuedAt],Jobs[startTs],SECOND),0.5)</code> — median queue-to-start. <br>3. <code>ChunkDurationP95</code> — 95th percentile chunk duration per jobType. <br>4. <code>JobFailureRate = DIVIDE(COUNTROWS(FILTER(Jobs,Jobs[status]=&quot;failed&quot;)), COUNTROWS(Jobs))</code>. <br>5. <code>ReconcilerFixCount</code> — actions taken per reconciler run. <br>Use these to drive SLO alerts. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Conceptual Power Query (PQ) integration patterns (detailed)</strong><br>1. <strong>Descriptor creation:</strong> store canonical <code>params</code> in hidden sheet; compute <code>paramsHash</code> using canonical helper before scheduling. <br>2. <strong>Idempotency:</strong> use workbook id + planId to compute <code>idempotencyToken</code> to avoid duplicates on multiple refreshes. <br>3. <strong>Non-blocking schedule call:</strong> call scheduling API asynchronously and present <code>correlationId</code> to user; poll <code>QueryJobStatus</code>. <br>4. <strong>Preview provisioning:</strong> fetch <code>previewRef</code> and load redacted CSV into sheet; retrieval of full evidence requires <code>approvalsRef</code>. <br>5. <strong>Error UX:</strong> show <code>userHint</code> and <code>correlationId</code> only; provide "Get evidence" link that opens approval workflow. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Conceptual DAX guidance (monitoring & forensics)</strong><br>1. <strong>SLA tile:</strong> <code>SLA_OK = IF([JobFailureRate]&lt;0.01 &amp;&amp; [JobStartLatencyP95]&lt;5000,&quot;OK&quot;,&quot;ALERT&quot;)</code>. <br>2. <strong>Top offenders:</strong> Pivot table <code>TopRulesByChunkDuration</code> = summarize job chunk durations by <code>jobType,ruleId</code>. <br>3. <strong>Forensic pivot:</strong> filter by <code>correlationId</code> to export audit rows, manifest and forensic artifacts for compliance. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Operator runbooks (practical, step-by-step)</strong><br><strong>Stuck enqueued job:</strong> <br>1. <code>jobs reconcile --runId &lt;ts&gt;</code>; inspect <code>job.reconcile.report</code>. <br>2. Check shard and worker heartbeats. <br>3. If queue overflow: reassign shard or increase worker capacity; if not possible, <code>jobs requeue --jobId ... --force --approvals ...</code>. <br><strong>Worker died mid-run:</strong> <br>1. Confirm lease expiry; <code>RequeueJob</code> after inspecting partial artifacts. <br>2. Collect <code>forensic_manifest</code> and escalate if data corruption suspected. <br><strong>Poisoned job:</strong> <br>1. Inspect <code>forensic_manifest</code>, fix input or code, then <code>jobs unpoison --jobId --approvals</code>. <br>2. Run canary re-run on small subset. <br><strong>Apply failure with PII exposure:</strong> <br>1. Immediately pause downstream exports; run <code>forensic_pack --correlationId</code> and escalate to compliance/legal. <br>2. If revert snapshot exists, run <code>RevertJob</code> or <code>RevertStandardization</code> per runbook. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>CI & tests matrix (exhaustive)</strong><br>1. <strong>Unit tests:</strong> canonicalization, <code>ComputeJobId</code>, <code>BackoffStrategy.calculateNext</code>, <code>PrioritizeJobs</code>. <br>2. <strong>Integration tests:</strong> schedule→enqueue→worker→executeChunk→checkpoint→finalize chain on canonical fixtures. <br>3. <strong>Fault injection tests:</strong> simulate DB outage, worker crash, partial blob persist. <br>4. <strong>Golden parity tests:</strong> compute canonical <code>job_manifest</code> on representative regulated fixtures across runtimes (backend, PQ/VBA ports). <br>5. <strong>Security tests:</strong> KMS rotation, ephemeral token retrieval, forbidden-API static analysis. <br>6. <strong>Performance tests:</strong> queue stress, reconciler under load, chunk throughput. <br>CI gates: golden parity and forbidden-API tests required for regulated lanes; migration manifests required for behavioral changes. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Migration path & governance checklist (required for semantic changes)</strong><br>1. Create <code>migrationManifest</code> containing: <code>migrationId,author,changeRationale,affectedFixtures[] (before/after checksums),estimatedAffectedCount,canaryPlan{cohortSize,KPIs,rollbackCriteria},rollbackPlan,approvals[]</code>. <br>2. Run CI golden tests and canary runs. <br>3. Collect metrics (failure rate, previewHash drift, user-acceptance) and store evidence. <br>4. Only after approvals and canary pass, perform phased rollout and monitor SLOs. <br>All steps and approvals persisted in EvidenceStore. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Security & secrets policy (must be enforced)</strong><br>1. Tenant KMS-backed keys for encrypting <code>sensitivePayloadRef</code>. <br>2. Ephemeral worker tokens for payload retrieval; token issues logged (<code>token.issue</code>). <br>3. Signed manifests for production and regulated artifacts; signature verification mandatory at consumption time. <br>4. Disallow direct secret reads in UI processes; static analyzer blocks forbidden APIs in code paths executed in host UI. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Forensic & incident response playbook (detailed)</strong><br>1. Detection: capture <code>correlationId</code> from alert. <br>2. Containment: pause downstream exports and set scheduler <code>incidentMode=true</code>. <br>3. Evidence collection: run <code>forensic_pack --correlation &lt;cid&gt;</code> to assemble: <code>job_manifest</code>, <code>job_descriptor</code>, <code>validationReport</code>, chunk artifacts, worker logs, audit_tail slice; compute checksums and upload to secure WORM storage. <br>4. Preservation: record chain-of-custody (collectorId, timestamp, who accessed). <br>5. Triage: replay in isolated env and produce RCA; rotate keys if suspected compromise. <br>6. Post-mortem: produce signed package for compliance. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Retention & archival policy (example)</strong><br>1. Hot: 30 days (fast access). <br>2. Warm: 7 years (regulatory). <br>3. Cold: per regulation (archival, WORM). <br>Legal holds override purge; retrieval requires <code>EvidenceAccessApprovalFlow</code> and logs chain-of-custody. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Developer safe patterns & engineering notes</strong><br>1. Use shared canonicalization library for all runtimes (backend, PQ/VBA, any SDKs). <br>2. Persist artifacts before any mutative side-effect. <br>3. Keep ScheduleJob cheap — delegate heavy validation to <code>ValidateJobDescriptor</code> async. <br>4. Use short leases and fast <code>ExtendLease</code> mechanism. <br>5. Ensure reconciler is leader-elected and thoroughly tested under partition scenarios. <br>6. Always include <code>paramsHash</code> and <code>jobManifest</code> references in audits for reconstructability. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Final acceptance criteria & release gates (must pass before production changes)</strong><br>1. Unit + integration + golden parity tests pass for modified code. <br>2. Evidence persist & signature paths validated. <br>3. Forbidden-API static analysis clean. <br>4. Reconciler leader election tests pass in distributed environment. <br>5. End-to-end smoke tests (schedule→worker→complete) pass in CI. <br>6. MigrationManifest present when changing semantics. <br>7. Operator runbooks updated and tabletop drill executed; results persisted. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Appendix A — canonical JSON rules summary (implementer reference)</strong><br>1. Keys sorted lexicographically. <br>2. Unicode NFKC normalization for keys and textual fields that influence identity/hash. <br>3. Numbers serialized with fixed precision when representing currency/scale-sensitive values; otherwise use canonical JSON number lexical form without trailing decimal places. <br>4. No insignificant whitespace; single newline normalization to <code>\n</code>. <br>5. Arrays where order is semantically irrelevant must be sorted by canonical key; document exceptions. <br>6. Include <code>canonicalVersion</code> at top-level for forward migrations. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Appendix B — sample jobDescriptor canonical example</strong><br><code>{ &quot;jobId&quot;: null, &quot;controlId&quot;:&quot;ctrl-xyz-20260121&quot;, &quot;idempotencyToken&quot;:&quot;wk-01-planA&quot;, &quot;jobType&quot;:&quot;standardize&quot;, &quot;params&quot;:{ &quot;planId&quot;:&quot;p_1a2b3&quot;, &quot;table&quot;:&quot;Payroll_Mar2026&quot;, &quot;aggregationKeys&quot;:[&quot;GLAccount&quot;,&quot;CostCenter&quot;] }, &quot;submitterId&quot;:&quot;alice@example.com&quot;, &quot;operatorId&quot;:&quot;alice@example.com&quot;, &quot;priority&quot;:200, &quot;scheduleAfter&quot;:null, &quot;earliestStart&quot;:&quot;2026-03-01T08:00:00Z&quot;, &quot;timeoutMs&quot;:3600000, &quot;retryPolicy&quot;:{&quot;maxAttempts&quot;:5,&quot;backoffStrategy&quot;:&quot;exponential&quot;,&quot;maxDelayMs&quot;:86400000}, &quot;chunking&quot;:{&quot;enabled&quot;:true,&quot;chunkSize&quot;:10000,&quot;checkpointInterval&quot;:1}, &quot;approvalsRef&quot;:null, &quot;sensitivePayloadRef&quot;:&quot;evid:payload/enc/20260301&quot;, &quot;payloadChecksum&quot;:&quot;sha256:abcd...&quot;, &quot;shardKey&quot;:&quot;tenant-42&quot;, &quot;createdTs&quot;:&quot;2026-01-21T10:00:00Z&quot;, &quot;canonicalVersion&quot;:&quot;1.0.0&quot; }</code> </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Appendix C — acceptance test matrix (condensed)</strong><br>1. <strong>Unit:</strong> canonicalization, ComputeJobId parity, Backoff deterministic. <br>2. <strong>Integration:</strong> schedule→enqueue→fetch→execute→checkpoint→finalize on small fixture. <br>3. <strong>Fault injection:</strong> simulate storage crash mid-persist, worker crash mid-chunk, network partition. <br>4. <strong>Golden parity:</strong> manifest hash parity across runtime ports for representative regulated fixtures. <br>5. <strong>Security:</strong> KMS rotate, ephemeral token flows. <br>6. <strong>Performance:</strong> queue depth scaling tests. </td></tr><tr><td data-label="CORE_JobScheduler — Per-function Expert Technical Breakdown"> <strong>Closing operator note (short & actionable)</strong><br>CORE_JobScheduler is a compliance-first scheduler: persist artifacts before acting, require migration manifests for semantics changes, run golden parity tests across runtimes, and use reconciler to repair corruption. Maintain evidence chaining for every run and follow the runbooks above during incidents. </td></tr></tbody></table></div><div class="row-count">Rows: 51</div></div><div class="table-caption" id="Table7" data-table="Docu_0184_07" style="margin-top:2mm;margin-left:3mm;"><strong>Table 7</strong></div>
<div class="table-wrapper" data-table-id="table-7"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by PQ_EnsureDeps — Per-function Expert Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">PQ_EnsureDeps — Per-function Expert Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Module-level metadata (contract & overview):</strong><br><strong>Owner:</strong> TEAM_PQ_ENSUREDEPS recorded in OWNERS.md and release manifests; owners include contacts for Support, Security, SRE, and Template Governance.<br><strong>Public API:</strong> <code>LoadLocalManifests</code>, <code>InspectInstalledAddins</code>, <code>ResolveRemoteManifests</code>, <code>ValidateConnectorVersion</code>, <code>ValidatePQRuntimeVersion</code>, <code>EnsureTemplateCompatibility</code>, <code>VerifyTemplateSignature</code>, <code>ProduceDepsReport</code>, <code>AtomicWriteDepsReport</code>, <code>GenerateDepFingerprint</code>, <code>InspectTempArtifacts</code>, <code>RepairMissingDeps</code>, <code>DiagnoseDepsFailure</code>, <code>DepsDryRun</code>, <code>DepsPolicyEnforcer</code>, <code>ListKnownGoodSnapshots</code>, <code>PromoteSnapshotToLocal</code>, <code>MarkTemplateAsDeprecated</code>.<br><strong>Audits emitted:</strong> <code>pq.ensuredeps.start</code>, <code>pq.ensuredeps.manifest.load</code>, <code>pq.ensuredeps.addin.scan</code>, <code>pq.ensuredeps.remote.resolve.attempt</code>, <code>pq.ensuredeps.remote.resolve.completed</code>, <code>pq.ensuredeps.connector.validate</code>, <code>pq.ensuredeps.pqruntime.validate</code>, <code>pq.ensuredeps.template.verify</code>, <code>pq.ensuredeps.template.signature</code>, <code>pq.ensuredeps.report.generated</code>, <code>pq.ensuredeps.report.atomic_write.attempt</code>, <code>pq.ensuredeps.report.atomic_write.completed</code>, <code>pq.ensuredeps.report.atomic_write.failure</code>, <code>pq.ensuredeps.repair.attempt</code>, <code>pq.ensuredeps.repair.success</code>, <code>pq.ensuredeps.degraded</code>, <code>pq.ensuredeps.complete</code>. Each audit row includes <code>correlationId</code>, <code>module=PQ_EnsureDeps</code>, <code>procedure</code>, <code>paramsHash</code>, <code>resultHash</code> when applicable, <code>durationMs</code>, and <code>evidenceRef</code> for large artifacts or signature blobs. Evidence storage is encrypted and access-controlled; top-level audits must not carry raw PII.<br><strong>Purpose and intended use:</strong> Validate required Power Query connectors, runtime compatibility, template integrity and signatures, and produce an authoritative <code>deps.report.json</code> to drive safe injection, preview, refresh and export flows. Provide machine-actionable repair proposals and maintain deterministic, auditable evidence chains for forensic replay in regulated contexts. Ensure UI fast-path remains non-blocking by separating local quick-scans from remote, heavier resolves that must run on background workers.<br><strong>Non-goals / constraints:</strong> Do not auto-install system-level connectors without explicit operator approval plus governance (two-person approval for regulated workflows). Avoid any long-blocking operations on the UI thread. Do not include raw secrets or PII in audit rows. Avoid heavy third-party dependencies that impair portability to XLAM/VBA host scenarios. Use secure credential store for remote repo auth; do not serialize secrets into evidence blobs. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Operational guarantees (module-level invariants & SLOs):</strong><br>1. Deterministic output: identical inputs (local manifests, resolved remote snapshot, config.hash, release manifest) must produce identical <code>deps.report.json</code> bytes; arrays must be canonicalized and sorted. <br>2. Non-blocking bootstrap fast-path: <code>LoadLocalManifests</code> and <code>InspectInstalledAddins</code> must complete within UI idle budget (default <200ms); no network I/O. <br>3. Atomic persistence guarantee: <code>deps.report.json</code> must be written via <code>AtomicWriteDepsReport</code> so readers never see truncated reports. <br>4. Degraded yet safe: network failures or signature-check timeouts produce <code>degraded=true</code> and include explicit <code>degradedReasons</code> and <code>cachedSnapshot</code> if available. <br>5. Audit-anchored operations: any change to persisted dependency state or repair actions must attach at least one audit row referencing <code>correlationId</code> and evidenceRef. <br>6. Idempotence: repeated calls with unchanged inputs must be stable and return same <code>reportHash</code>. <br><strong>Performance SLOs:</strong> local manifest scanning median <50ms; resolved remote snapshot retrieval median <2s under normal networks (worker path); deps report atomic write median <200ms on local SSD. <br><strong>CI gates:</strong> signature verification golden vectors, <code>reportHash</code> parity across OSes, static analyzer enforcement forbidding UI-thread network calls, audit emission verification. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>High-level workflow & invariants (detailed):</strong><br>1. UI idle: <code>LoadLocalManifests</code> loads embedded templates, hidden-sheet <code>PQ_TEMPLATES</code>, and add-in manifests with minimal metadata only (name, templateId, mChecksum, requiredConnectors, signaturePolicy). Emits <code>pq.ensuredeps.manifest.load</code>. <br>2. Worker: <code>ResolveRemoteManifests</code> fetches remote template repo index and connector capability manifests, validates schema, computes <code>mChecksum</code> for templates and connectors, caches snapshot in evidence store signed by release manifest key. Emits <code>pq.ensuredeps.remote.resolve.completed</code>. <br>3. <code>InspectInstalledAddins</code> enumerates installed XLAMs and registered connectors via host APIs, extracts code-signing status and matches owners from <code>OWNERS.md</code>. Emits <code>pq.ensuredeps.addin.scan</code>. <br>4. Compatibility checks: for each template, run <code>EnsureTemplateCompatibility</code> which calls <code>ValidateConnectorVersion</code> and <code>ValidatePQRuntimeVersion</code> and <code>VerifyTemplateSignature</code> when required; compute <code>compatibilityStatus</code> and <code>repairActions</code>. Emit <code>pq.ensuredeps.template.verify</code>. <br>5. Produce canonical report: <code>ProduceDepsReport</code> produces <code>deps.report.json</code> with <code>reportHash</code>, persists via <code>AtomicWriteDepsReport</code>, and emits <code>pq.ensuredeps.report.generated</code> and <code>pq.ensuredeps.report.atomic_write.completed</code>. <br>6. On failures or missing deps propose repairs via <code>RepairMissingDeps</code> (idempotent proposals). Repairs require explicit operator approval for system changes; for regulated templates require two-person approval. <br><strong>Invariants:</strong> canonical ordering, stable tie-breaking when multiple repair options exist (use <code>GenerateDepFingerprint</code> seed), all persisted artifacts written with <code>AtomicWrite</code>. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>API function breakdown — exhaustive</strong> </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong><code>LoadLocalManifests(workbookSnapshot, correlationId, options={fastMode:true})</code></strong><br><strong>Purpose & contract:</strong> collect minimal manifest metadata from the active workbook and installed add-ins without network I/O; safe for UI idle path. Return deterministic list sorted by <code>templateId</code>. <br><strong>Parameters:</strong> <code>workbookSnapshot</code> (a redacted, read-only snapshot or workbook handle), <code>correlationId</code>, <code>options.fastMode</code> boolean. <br><strong>Return:</strong> <code>{manifests:[{templateId, mChecksum, requiredConnectors, minPQRuntime, signaturePolicy, owner}], manifestsHash, durationMs}</code>. <br><strong>Behavior & implementation notes:</strong><br>1. Canonicalize manifest fields: normalize version strings to semver where present and canonicalize <code>requiredConnectors</code> to lowercase provider ids. <br>2. Read hidden-sheet <code>PQ_TEMPLATES</code> once and parse rows into manifest records; if hidden-sheet missing return empty list with <code>manifestCount=0</code>. <br>3. Validate manifest schema lightly (presence of required keys) and emit <code>pq.ensuredeps.manifest.load</code> audit with <code>manifestsHash</code>. <br>4. In <code>fastMode=true</code> only include essential fields to minimize host calls; schedule fuller manifest retrieval in worker when operator triggers full check. <br><strong>Edge cases:</strong> corrupted hidden-sheet -> emit <code>pq.ensuredeps.manifest.load</code> with <code>result=malformed</code> and <code>evidenceRef</code> pointing to sanitized manifest snapshot. <br><strong>Observability:</strong> <code>pq.ensuredeps.manifest.load</code> with <code>manifestCount</code>, <code>invalidCount</code>, <code>durationMs</code>. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong><code>InspectInstalledAddins(hostApi, correlationId)</code></strong><br><strong>Purpose & contract:</strong> enumerate installed add-ins and provider bindings visible to Power Query and compute signature/trust status per add-in. <br><strong>Return:</strong> <code>{installedAddins:[{name,path,version,enabled,signedBy,signatureValid}], connectors:[{providerId, registered, implementingModule, version}], summary}</code>. <br><strong>Behavior:</strong><br>1. Use host APIs for installed add-ins and provider registration; do not change host state. <br>2. Validate code-signing metadata if available and cross-reference <code>OWNERS.md</code> for owner identity mapping; record <code>signedBy</code> as ownerId or <code>UNKNOWN</code>. <br>3. If multiple versions exist, choose highest deterministically using <code>compareSemver</code> fallback to lexical sort with stable tie-breaker. <br>4. If provider registration malformed or missing expected entrypoint, mark <code>status=MISSING_BINDING</code>. <br><strong>Observability:</strong> <code>pq.ensuredeps.addin.scan</code> with <code>installedCount</code>, <code>untrustedCount</code>, <code>missingBindings</code>. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong><code>ResolveRemoteManifests(templateRepoConfig, correlationId, cachePolicy)</code></strong><br><strong>Purpose & contract:</strong> worker-only resolution of remote repo indexes and connector capability manifests; produce signed snapshot stored in encrypted evidence store. <br><strong>Parameters:</strong> <code>templateRepoConfig</code> (primary URL, mirrors, authRef), <code>correlationId</code>, <code>cachePolicy</code> (ttl, preferCached). <br><strong>Return:</strong> <code>{snapshotHash, snapshotRef, status, diagnostics}</code>. <br><strong>Detailed behavior (must/shall):</strong><br>1. Attempt fetch from primary; on failure iterate mirrors; on each fetch perform HTTP validation, ETag/Last-Modified checks, and compute <code>mChecksum</code> for each template manifest and template payload. <br>2. Validate index and manifests against JSON Schema (v1/v2). <br>3. For each template entry compute canonical payload (strip non-deterministic fields, normalize line endings) and compute <code>mChecksum</code> (SHA256). <br>4. Cache raw response and parsed canonical snapshot encrypted in evidence store. <br>5. Emit <code>pq.ensuredeps.remote.resolve.completed</code> with <code>snapshotHash</code> and <code>durationMs</code>. <br><strong>Failure handling & degrade modes:</strong><br>1. Network failure -> status <code>DEGRADED</code> and return last cached snapshot if available with <code>degradedRationale=network_unreachable</code>. <br>2. Schema invalid -> status <code>INVALID_REMOTE_MANIFEST</code> and set <code>degraded=true</code> with evidenceRef. <br><strong>Security & auth:</strong> use secure authRef token via secure credential manager; never log the token. <br><strong>Observability:</strong> <code>pq.ensuredeps.remote.resolve.attempt</code> and <code>completed</code> audits. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong><code>ValidateConnectorVersion(requiredConnectorSpec, installedConnectors, correlationId)</code></strong><br><strong>Purpose & contract:</strong> validate that installed connectors satisfy the <code>requiredConnectorSpec</code> (semver ranges, capabilities, provider ids). <br><strong>Return:</strong> <code>{name, requiredRange, installedVersion, status, rationale, candidates}</code> where <code>status</code> ∈ {OK, WARNING, MISMATCH, MISSING}. <br><strong>Behavior details:</strong><br>1. Support semver ranges as well as capability predicates (eg <code>supportsLoadToModel=true</code>, <code>supportsOAuth2=true</code>). <br>2. For vendors with non-semver schemes, map vendor versions to compatibility levels using <code>compatibilityMatrix</code> shipped in release manifest. <br>3. When several candidates available choose highest compatible one deterministically. Emit <code>pq.ensuredeps.connector.validate</code> audit. <br>4. If installed version within known-broken set, mark <code>status=KNOWN_BROKEN</code> and attach <code>workaround=useWorkerExecution</code> if applicable. <br><strong>Edge cases:</strong> connector installed but provider registration missing -> <code>status=MISSING_BINDING</code>. <br><strong>Repair suggestions:</strong> provide <code>repairActions: [&quot;installConnector&quot;, &quot;registerProvider&quot;, &quot;upgradeConnector&quot;]</code> with secure download references in evidenceRef. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong><code>ValidatePQRuntimeVersion(installedRuntimeInfo, requiredRuntimeRange, correlationId)</code></strong><br><strong>Purpose & contract:</strong> verify host M/runtime compatibility; return <code>{installedVersion, status, rationale, recommendedAction}</code>. <br><strong>Behavior & mapping:</strong><br>1. Normalize runtime identifiers across hosts (Excel, PowerBI Desktop, Server) via <code>runtimeMap</code>. <br>2. Compare installed runtime to required ranges expressed by templates & connectors. <br>3. If below minimum set <code>status=INCOMPATIBLE</code> and <code>recommendedAction=upgradeHostOrUseWorker</code>. If above maximum (preview channel), set <code>status=UNTESTED</code> and recommend two-person approval for regulated templates. <br>4. Emit <code>pq.ensuredeps.pqruntime.validate</code> audit with <code>installedVersion</code> and <code>status</code>. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong><code>EnsureTemplateCompatibility(manifestsList, resolvedSnapshot, installedConnectors, pqRuntime, correlationId, policy)</code></strong><br><strong>Purpose & contract:</strong> iterate templates and produce detailed compatibility entries including repair actions and evidenceRefs; return <code>TemplateCompatibilityReport</code>. <br><strong>Detailed steps:</strong><br>1. For each template determine <code>requiredConnectors</code>, <code>minRuntime</code>, <code>maxRuntime</code>, <code>signaturePolicy</code>, and <code>requiresHighPrecision</code>. <br>2. Validate connectors with <code>ValidateConnectorVersion</code> and runtime with <code>ValidatePQRuntimeVersion</code>. <br>3. Verify signature via <code>VerifyTemplateSignature</code> when <code>signaturePolicy==REQUIRED</code>; for <code>OPTIONAL</code> emit <code>signaturePresent=true/false</code>. <br>4. If <code>requiresHighPrecision=true</code> and runtime is <code>UNTESTED/INCOMPATIBLE</code> mark <code>recommendWorkerExecution=true</code>. <br>5. Build <code>repairActions</code> array: prioritized, deterministic, with explicit preconditions and idempotency tokens (e.g., <code>installConnector(packageUrl, packageHash)</code> with <code>idempotencyToken</code>). <br>6. Use <code>GenerateDepFingerprint</code> to produce per-template deterministic fingerprint for cache keys. <br><strong>Invariants:</strong> arrays sorted by <code>templateId</code>, deterministic tie-break ordering if multiple repair actions equal weight. <br><strong>Audits:</strong> emit <code>pq.ensuredeps.template.verify</code> per template with <code>compatibilityStatus</code>, <code>repairActions</code>, and <code>evidenceRef</code> links. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong><code>VerifyTemplateSignature(templateBlob, signatureBlob, allowedOwners, correlationId)</code></strong><br><strong>Purpose & contract:</strong> cryptographically verify template signatures per <code>signaturePolicy</code>; return <code>{signatureValid, signerId, certFingerprint, certChainVerified, revocationStatus, evidenceRef}</code>. <br><strong>Implementation details:</strong><br>1. Support both detached signatures and embedded armored signatures (PGP, CMS/PKCS7, JOSE). <br>2. Validate signature algorithm allowed list (e.g., RSA-PSS, ECDSA with appropriate key lengths). <br>3. Validate certificate chain against release manifest trust anchors; perform OCSP/CRL checks when network available; if network unavailable set <code>revocationStatus=DEFERRED</code> and emit <code>pq.ensuredeps.template.signature</code> audit with <code>revocationDeferred=true</code>. <br>4. If signer not in <code>allowedOwners</code> set <code>signatureValid=false</code> and provide <code>repairAction: getOwnerApproval</code> or <code>rejectInjection</code>. <br>5. For regulated templates where owner mismatch occurs, require two-person approval before any injection or repair. <br><strong>Observability:</strong> <code>pq.ensuredeps.template.signature</code> audit with <code>signerId</code> and <code>evidenceRef</code> for signature blob. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong><code>ProduceDepsReport(manifestsSnapshot, installedDeps, resolvedRemoteSnapshot, pqRuntimeInfo, correlationId, outputPath, options={stageLocal:false})</code></strong><br><strong>Purpose & contract:</strong> assemble the canonical, deterministic <code>deps.report.json</code> and persist it via <code>AtomicWriteDepsReport</code>. Return <code>{artifactPath, artifactChecksum, reportHash, persistedAt}</code>. <br><strong>Report content (canonical schema):</strong><br>• <code>correlationId</code><br>• <code>runTs</code> (UTC ISO8601)<br>• <code>configHash</code><br>• <code>releaseManifestHash</code><br>• <code>templates</code> array of <code>{templateId, mChecksum, compatibilityStatus, repairActions, signaturePolicy, owner}</code><br>• <code>connectors</code> array of <code>{name, requiredRange, installedVersion, status}</code><br>• <code>pqRuntime</code> object <code>{installedVersion, status}</code><br>• <code>degraded</code> boolean and <code>degradedReasons</code><br>• <code>evidenceRefs</code> mapping resource->evidenceRef<br>• <code>reportHash</code> (SHA256 canonical JSON) <br><strong>Canonicalization rules (must):</strong> keys sorted lexicographically at each object level; arrays sorted by <code>templateId</code> or <code>connectorName</code>; fixed field ordering for stable <code>reportHash</code>; UTF-8, normalized NFKC for strings, normalized newline <code>\n</code>. <br><strong>Persistence & atomic write:</strong> call <code>AtomicWriteDepsReport</code> with payload stream and sidecar <code>report.metadata.json</code> containing <code>reportHash</code>, <code>correlationId</code>, <code>evidenceRefs</code>. If <code>options.stageLocal=true</code> write to staging area when final destination unreachable and set <code>degraded=true</code> with <code>degradedRationale=&quot;destination_unavailable&quot;</code>. <br><strong>Observability:</strong> emit <code>pq.ensuredeps.report.generated</code> containing <code>reportHash</code> and <code>artifactChecksum</code> after successful atomic write. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong><code>AtomicWriteDepsReport(targetPath, payloadStream, correlationId, tmpSuffix=&quot;.part&quot;, maxAttempts=3)</code></strong><br><strong>Purpose & contract:</strong> specialized wrapper calling plumbing atomic-write utility; ensure fsync and parent fsync semantics where supported; create sidecar <code>report.metadata.json</code>. Return <code>{success, artifactPath, artifactChecksum, attempts, elapsedMs}</code>. <br><strong>Behavior:</strong><br>1. Compute tempPath deterministically: <code>targetPath + tmpSuffix + &quot;.&quot; + pid + &quot;.&quot; + deterministicSuffix</code> where deterministicSuffix derived from <code>GenerateDepFingerprint</code>. <br>2. Write payload stream to tempPath, compute SHA256, optionally write sidecar metadata including <code>payloadHash</code> and <code>correlationId</code>. <br>3. fsync file and parent directory where supported; call platform atomic replace API (<code>os.replace</code> on POSIX, ReplaceFile on Windows). <br>4. Reopen targetPath and verify checksum matches computed artifactChecksum; if mismatch emit <code>pq.ensuredeps.report.atomic_write.verification_failed</code> and attempt repair per retry policy. <br><strong>Degraded behavior:</strong> when rename semantics unreliable (NFS/SMB), fallback to copy-then-rename pattern with <code>pq.ensuredeps.degraded</code> audit and explicit <code>repairActions</code>. <br><strong>Observability:</strong> <code>pq.ensuredeps.report.atomic_write.attempt</code> and <code>pq.ensuredeps.report.atomic_write.completed</code>. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong><code>GenerateDepFingerprint(manifestsSnapshot, installedDeps, pqRuntime, salt=null)</code></strong><br><strong>Purpose & contract:</strong> produce deterministic fingerprint for dependency surface used for caching and golden testing; use HMAC_SHA256 over canonical serialized snapshot with optional salt. <br><strong>Return:</strong> <code>{fingerprintHex, fingerprintShort}</code> where <code>fingerprintShort</code> is first 12 hex chars. <br><strong>Usage:</strong> seeds for deterministic jitter in CI, cache keys, and stable grouping in telemetry. <br><strong>Observability:</strong> <code>pq.ensuredeps.fingerprint</code> audit with component counts. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong><code>InspectTempArtifacts(directory, correlationId)</code></strong><br><strong>Purpose:</strong> enumerate left-over temporary artifacts from failed atomic writes, validate sidecar metadata if present, and surface repair suggestions. <br><strong>Output:</strong> <code>{tempFiles:[{path,size,payloadHash,valid}], validatedCount, suggestions:[{path,action}]}</code>. <br><strong>Observability:</strong> <code>pq.ensuredeps.temp.inspect</code> audit. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong><code>RepairMissingDeps(repairProposal, dryRun=true, operatorApproval=null, correlationId)</code></strong><br><strong>Purpose & contract:</strong> generate and optionally apply repair steps to satisfy compatibility; actions are proposals by default — apply requires explicit operator approval and possibly two-person approval for regulated items. <br><strong>Repair step format (must):</strong> each step includes <code>idempotencyToken</code>, <code>actionType</code>, <code>parameters</code>, <code>preconditions</code>, <code>estimatedImpact</code>, <code>rollbackPlan</code>, <code>evidenceRef</code> for downloaded package. Example action types: <code>downloadConnector</code>, <code>registerProvider</code>, <code>enableHiddenSheetTemplate</code>, <code>promoteSnapshotToLocal</code>. <br><strong>Safety constraints:</strong> do not perform system-level changes without operator approval and required governance; when <code>requiresSystemChange=true</code> require SRE sign-off. <br><strong>Audit:</strong> <code>pq.ensuredeps.repair.attempt</code> and <code>pq.ensuredeps.repair.success</code> with <code>repairPlanHash</code>. <br><strong>Observability:</strong> record full plan in evidence store and persist <code>repair.plan.json</code> via <code>AtomicWrite</code>. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong><code>DiagnoseDepsFailure(correlationId, failureContext, deepDiagnostics=false)</code></strong><br><strong>Purpose:</strong> collect a focused forensic bundle to triage dependency failures; include <code>deps.report.json</code>, <code>manifest snapshots</code>, <code>signature blobs</code>, <code>atomicWrite</code> failure logs, and <code>InspectTempArtifacts</code> outputs. <br><strong>Output:</strong> <code>diagnosticBundleRef</code> pointing to encrypted bundle in evidence store. <br><strong>Observability:</strong> <code>pq.ensuredeps.diagnose</code> audit referencing <code>diagnosticBundleRef</code>. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Error taxonomy & representative error codes (expanded):</strong><br>• <code>PQ_DEPS_OK</code> — nominal. <br>• <code>PQ_DEPS_DEGRADED_NETWORK</code> — remote manifests unreachable; <code>repair: run remote resolve on worker</code>. <br>• <code>PQ_DEPS_MISSING_CONNECTOR</code> — connector missing; <code>repair: installConnector</code>. <br>• <code>PQ_DEPS_INCOMPATIBLE_RUNTIME</code> — runtime not supported; <code>repair: upgradeHostOrUseWorker</code>. <br>• <code>PQ_DEPS_TEMPLATE_UNTRUSTED</code> — missing/invalid signature; <code>repair: ownerApprovalRequired</code>. <br>• <code>PQ_DEPS_ATOMIC_WRITE_ENOSPC</code> — ENOSPC while persisting report; <code>runbook: stage-local or free-space</code>. <br>• <code>PQ_DEPS_ATOMIC_WRITE_EPERM</code> — EPERM; <code>runbook: fix-perms</code>. <br>• <code>PQ_DEPS_SIGNATURE_DEFERRED</code> — revocation check deferred due to network. <br>• <code>PQ_DEPS_AMBIGUOUS_CONNECTOR</code> — multiple installed candidates; <code>repair: pickPreferred</code> or <code>uninstallAlternates</code>. <br>Each error provides operator guidance in <code>repairActions</code> and maps to specific runbook steps. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Observability, telemetry & audit schema (detailed):</strong><br><strong>Audit schema fields (required):</strong> <code>timestamp</code>, <code>correlationId</code>, <code>module=PQ_EnsureDeps</code>, <code>procedure</code>, <code>operatorId</code> (optional), <code>paramsHash</code>, <code>resultHash</code> (optional), <code>evidenceRef</code> (optional), <code>durationMs</code>, <code>metadata</code> containing <code>templatesChecked</code>, <code>connectorsChecked</code>, <code>untrustedCount</code>, <code>degraded</code>. <br><strong>Primary audit events:</strong> <code>pq.ensuredeps.start</code>, <code>pq.ensuredeps.manifest.load</code>, <code>pq.ensuredeps.remote.resolve.completed</code>, <code>pq.ensuredeps.template.verify</code>, <code>pq.ensuredeps.report.generated</code>, <code>pq.ensuredeps.report.atomic_write.completed</code>. <br><strong>Metrics to buffer:</strong> <code>pq.ensuredeps.scan_latency_ms</code>, <code>pq.ensuredeps.remote.resolve_latency_ms</code>, <code>pq.ensuredeps.report_write_latency_ms</code>, <code>pq.ensuredeps.degraded_rate</code>, <code>pq.ensuredeps.untrusted_templates_count</code>. <br><strong>Evidence policy:</strong> audit rows reference only hashes and <code>evidenceRef</code>; full manifests, signatures and signed snapshots stored encrypted and access-controlled. PII cannot appear in top-level audit fields. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Testing matrix, property tests, and CI governance (comprehensive):</strong><br><strong>Unit tests (must include):</strong><br>1. <code>LoadLocalManifests</code>: hidden-sheet present/absent, malformed entries, ensure canonical ordering and <code>manifestsHash</code>. <br>2. <code>ResolveRemoteManifests</code>: healthy and faulty network cases, schema invalid manifests, caching TTL behaviors. <br>3. <code>VerifyTemplateSignature</code>: valid signatures, invalid signatures, revoked certs, deferred revocation. <br>4. <code>ValidateConnectorVersion</code>: semver ranges, vendor-specific mapping, ambiguous multiple installs. <br>5. <code>AtomicWriteDepsReport</code>: rename/fsync failure simulation using FS mocks. <br><strong>Integration tests:</strong><br>1. Full end-to-end produce-report roundtrip: local manifest -> remote resolve -> signature verify -> produce <code>deps.report.json</code> persisted and checksum verified. <br>2. Worker-only remote resolve combined with UI fast-scan verifying consistent <code>reportHash</code> across runs. <br>3. Concurrency test: multiple parallel producers writing to same destination -> atomic writes prevent partial reads. <br><strong>Property tests:</strong><br>1. Determinism: repeated <code>ProduceDepsReport</code> with same inputs yields identical <code>reportHash</code> across platforms. <br>2. Repair idempotence: applying same <code>RepairMissingDeps</code> twice yields no duplicate system changes; step idempotency tokens enforced. <br><strong>Cross-language golden governance:</strong> maintain golden <code>mChecksum</code> and <code>reportHash</code> vectors across implementations (VBA/JS/Python/C#) and ensure parity. <br><strong>CI gating:</strong> static analyzer must flag network calls from UI fast-path; golden vector parity required; signature verification tests require test CA chain in pipeline. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Developer guidance, allowed & forbidden patterns (explicit):</strong><br><strong>Required patterns:</strong><br>1. Use <code>LoadLocalManifests</code> for quick UI checks and schedule <code>ResolveRemoteManifests</code> on workers for authoritative checks. <br>2. Persist the canonical <code>deps.report.json</code> via <code>AtomicWriteDepsReport</code> for consumers. <br>3. Use deterministic <code>GenerateDepFingerprint</code> for cache keys and deterministic_jitter seeds in CI. <br>4. Always emit audits for major transitions and include <code>evidenceRef</code> when raw artifacts are stored. <br><strong>Forbidden patterns:</strong><br>1. No network requests on UI fast-path. <br>2. Do not auto-install system-wide connectors without explicit operator approval plus required governance. <br>3. Do not write PII into audit rows—use evidenceRef for sanitized content. <br>4. No locale-dependent sorting/formatting for canonicalization. <br><strong>Code-review checklist:</strong> verify audit emits exist, <code>AtomicWriteDepsReport</code> used, deterministic ordering enforced, signature verification present for <code>signaturePolicy=REQUIRED</code>, UI-thread safety validated, and tests/goldens present. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Operator runbook & incident playbooks (executable steps):</strong><br><strong>AtomicWrite ENOSPC runbook:</strong><br>1. Inspect <code>pq.ensuredeps.report.atomic_write.failure</code> audit for correlationId and <code>targetPath</code>. <br>2. Run <code>InspectTempArtifacts</code> to list temp files and capture <code>tempPaths</code> into <code>forensic_manifest.json</code>. <br>3. Free space on the mount or configure <code>deps.output.staging</code> and re-run <code>ProduceDepsReport --stageLocal true</code>. <br>4. Verify <code>pq.ensuredeps.report.atomic_write.completed</code> and compare checksums; if failure persists escalate to infra with <code>forensic_manifest</code> and audit_tail. <br><strong>Untrusted template runbook:</strong><br>1. Retrieve <code>pq.ensuredeps.template.signature</code> with <code>evidenceRef</code>. <br>2. Use offline verification tool to inspect certificate chain and check revocation if network blocked. <br>3. If signer unknown, consult OWNERS.md; require owner approval for regulated templates. <br>4. Optionally fallback to <code>lastKnownGood</code> snapshot and emit <code>pq.ensuredeps.repair.attempt</code>. <br><strong>PQ runtime incompatible triage:</strong><br>1. Confirm <code>pq.ensuredeps.pqruntime.validate</code> audit; collect <code>installedRuntimeVersion</code>. <br>2. If upgrade allowed, coordinate host upgrade with release manifest compatibility checks; else, run the template in a trusted worker and persist proofs. <br>3. For preview channel (untested newer runtime) require two-person approval for regulated templates. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Extremely detailed narratives & examples — expanded (multiple scenarios):</strong><br><strong>Scenario A — Operator preview & inject path for high-precision regulated template (full trace):</strong><br>1. Operator selects "Preview -> Inject" on template <code>rev-recog-v3</code> from PQ_Ribbon. Ribbon computes <code>correlationId=r-20260117-xyz</code>. Ribbon calls <code>LoadLocalManifests(workbookSnapshot, correlationId, fastMode=true)</code> which returns <code>manifest</code> for <code>rev-recog-v3</code> with <code>requiresHighPrecision=true</code> and <code>signaturePolicy=REQUIRED</code>. <code>pq.ensuredeps.manifest.load</code> audit emitted. <br>2. Add-in schedules a worker job to run <code>ResolveRemoteManifests(templateRepoConfig, correlationId)</code> because <code>requiresHighPrecision=true</code> and signature policy is strict; on worker the remote snapshot is fetched and <code>snapshotHash</code> computed; <code>pq.ensuredeps.remote.resolve.completed</code> audit emitted. <br>3. Worker invokes <code>EnsureTemplateCompatibility</code> which calls <code>ValidateConnectorVersion</code> for connectors <code>FinancialSource</code> and <code>JournalWriter</code>. <code>ValidatePQRuntimeVersion</code> shows host runtime <code>excel:2304</code> OK; <code>FinancialSource</code> is missing locally. The report entry for the template is <code>DEGRADED</code> with <code>repairAction: downloadConnector FinancialSource</code>. <code>pq.ensuredeps.template.verify</code> audit emitted with <code>repairActions</code>. <br>4. Since template <code>signaturePolicy=REQUIRED</code>, worker calls <code>VerifyTemplateSignature</code> using detached <code>.sig</code> artifact from the remote snapshot; signature validated and <code>signerId=TEAM_FINANCE</code> recorded. <code>pq.ensuredeps.template.signature</code> audit emitted with <code>evidenceRef</code>. <br>5. <code>ProduceDepsReport</code> produces canonical <code>deps.report.json</code> including the <code>DEGRADED</code> flag and <code>repairActions</code> and persists it via <code>AtomicWriteDepsReport</code>. <code>pq.ensuredeps.report.atomic_write.completed</code> emitted with artifact checksum. <br>6. Operator receives preview UI indicating <code>DEGRADED</code> for injection and a suggested repair to download connector <code>FinancialSource</code>. Operator requests <code>repairProposal</code> and approves to proceed with <code>RepairMissingDeps</code> under two-person-approval policy. Repair writes <code>repair.plan.json</code> via <code>AtomicWrite</code> and <code>pq.ensuredeps.repair.success</code> is audited. <br>7. After repair (connector installed in staging and registered), worker re-runs compatibility check and now <code>compatibilityStatus=OK</code>. Authoritative worker run performs final high-precision transforms using <code>SafeRound</code> primitives in worker, writes authoritative artifact via <code>AtomicWrite</code>, and <code>pq_inject</code> audit references the artifact checksum. <br><strong>Takeaway:</strong> separation of fast local manifests vs worker authoritative checks preserves UI responsiveness while maintaining compliance and determinism. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Scenario B — CI pipeline for template release requiring new connector capability (trace):</strong><br>1. Developer submits PR updating template <code>agg-sales-v5</code> to require <code>NewProvider&gt;=2.0</code>. CI job triggers and runs <code>ResolveRemoteManifests</code> against canonical test repo and <code>VerifyTemplateSignature</code> using the test CA. <code>pq.ensuredeps.remote.resolve.completed</code> and <code>pq.ensuredeps.template.signature</code> audits produced by CI harness. <br>2. CI validates <code>ValidateConnectorVersion</code> finds <code>NewProvider</code> absent in golden environment -> CI fails with <code>PQ_DEPS_MISSING_CONNECTOR</code>. Developer must add compatibility notes or provide vendor package. <br>3. Developer packages connector stub into test repo; CI re-runs and <code>GenerateDepFingerprint</code> produces fingerprint used in gating. CI asserts <code>reportHash</code> matches golden vectors and cross-platform parity tests for <code>mChecksum</code> pass; only then PR can merge. <br>4. Merge triggers release manifest update and owner approvals. Release manifest records new trust anchors if connector vendor signing required. <br><strong>Takeaway:</strong> CI acts as pre-flight gate to prevent templates reaching operator desks without necessary connectors or approvals. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Scenario C — Forensic reconstruction after injection mismatch:</strong><br>1. Operator reports that injected query code differs from canonical template for <code>correlationId=r-20260112-455</code>. Support runs <code>DiagnoseDepsFailure(correlationId, context)</code> which collects <code>deps.report.json</code>, <code>manifestSnapshot</code>, <code>signatureBlobs</code>, and <code>audit_tail</code>. <code>pq.ensuredeps.diagnose</code> audit emitted with <code>diagnosticBundleRef</code>. <br>2. Forensic replay uses <code>manifestSnapshot</code> and <code>evidenceRef</code> to reconstruct what <code>LoadLocalManifests</code> returned at run time; it finds the workbook had <code>templateId</code> <code>agg-sales-v4</code> but <code>mChecksum</code> mismatched (local edit) and signature absent. <code>pq.ensuredeps.template.verify</code> shows <code>mChecksum_mismatch</code>. <br>3. Support restores the canonical template and produces <code>forensic_manifest.json</code> and recommended mitigation: lock hidden-sheet templates for regulated flows and enforce <code>VerifyTemplateSignature</code> at injection. <br>4. Post-mortem: operator education plus adding static checks to disallow unsanctioned template edits in regulated contexts. <br><strong>Takeaway:</strong> evidenceRef and canonical <code>deps.report</code> allow deterministic replay and attribution. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Scenario D — Worker-only fallback for numeric fidelity across PQ host variance:</strong><br>1. Template <code>calc-alloc-v2</code> flagged <code>requiresHighPrecision=true</code>. Host runtime <code>excel:legacy</code> may have inconsistent decimal behavior. <code>EnsureTemplateCompatibility</code> recommends worker execution. <br>2. Add-in delegates numeric aggregation to worker: export normalized payload via <code>AtomicWrite</code> to staging area, worker runs <code>SafeRoundResiduals</code> with deterministic RNG seeded from <code>correlationId</code>, persists authoritative result via <code>AtomicWrite</code>, and injects read-only query into workbook referencing the authoritative artifact. <br>3. The <code>deps.report.json</code> records <code>requiresHighPrecision</code> and <code>executionMode=worker</code> to indicate authoritative steps occurred off-host. <code>pq.ensuredeps.report.generated</code> audit links to artifact checksum and RNG evidenceRef. <br><strong>Takeaway:</strong> for numeric regulated outputs, worker-side SafeRound and atomic persistence ensure cross-host parity and auditability. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Deep conceptual mapping to Power Query (M) — patterns and concrete guidance:</strong><br><strong>Context:</strong> M runtimes vary by host and version; PQ_EnsureDeps cannot change M runtime but must orchestrate around its limitations. Patterns below enforce deterministic behaviors and govern where sensitive transforms must run. <br><strong>Pattern 1 — Atomic persistence for injection & templates:</strong><br>1. Problem: M/host cannot guarantee atomic replace semantics or consistent folder fsync. <br>2. Pattern: generate canonical M query text in the trusted helper (add-in or worker), compute <code>mChecksum</code>, persist with <code>AtomicWriteDepsReport</code> to artifact store, then call <code>Workbook.Queries.Add(Name, Formula)</code> using the persisted payload. <br>3. Evidence: emit <code>pq_inject</code> audit linking <code>artifactChecksum</code> and <code>mChecksum</code>. <br>4. Governance: for regulated templates require signature validated by <code>VerifyTemplateSignature</code>. <br><strong>Pattern 2 — Deterministic preview & sampling in M:</strong><br>1. Problem: M lacks a canonical seedable RNG and host-level sampling implementations differ. <br>2. Pattern A (lightweight): host computes seed <code>SeedFromCorrelation(correlationId, templateId)</code> and passes it as a numeric parameter to M preview code implementing a pure M PRNG (e.g., linear congruential with documented parameters). M preview records the seed in preview audit. <br>3. Pattern B (heavy duty): perform sampling in worker using <code>DeterministicRNG</code> and persist sampling decisions (sampled keys, RNG state) in evidence store; M receives only deterministic sampled payload. <br>4. Replay: audits include seed and evidenceRef allowing exact reproduction. <br><strong>Pattern 3 — High-precision numeric transforms:</strong><br>1. Problem: M decimal fidelity and rounding semantics vary across hosts. <br>2. Pattern: templates declare <code>requiresHighPrecision=true</code>. For such templates, add-in orchestrator exports raw normalized data via <code>AtomicWrite</code> and delegates authoritative rounding/allocation to worker SafeRound primitives. Worker persists final artifact and returns an injected read-only query or refresh connection. <br>3. Auditing: <code>pq.ensuredeps.report.generated</code> includes <code>requiresHighPrecision</code> and <code>executionMode=worker</code>. <br><strong>Pattern 4 — Template signature & owner enforcement:</strong><br>1. Problem: local template edits or unsigned templates undermine governance. <br>2. Pattern: require <code>VerifyTemplateSignature</code> for templates with <code>signaturePolicy=REQUIRED</code>. If missing or invalid, block injection unless two-person approval is present; persist decision via audit. <br><strong>Pattern 5 — Retry & idempotency around PQ operations:</strong><br>1. Problem: injection or connection creation may fail transiently; retry semantics necessary. <br>2. Pattern: orchestrator uses <code>Retry</code> wrapper and persists <code>jobDescriptor</code> for idempotency prior to attempting injection so retries do not produce duplicates. <br><strong>Operator narrative (PQ injection step-by-step):</strong><br>1. Operator requests inject → <code>LoadLocalManifests</code> quick-scan → <code>EnsureTemplateCompatibility</code> determines <code>signaturePolicy</code> and <code>requiresHighPrecision</code>. <br>2. If signature required, call <code>VerifyTemplateSignature</code>; if unsigned, require approval. <br>3. Persist canonical M artifact via <code>AtomicWriteDepsReport</code>. <br>4. Call <code>Workbook.Queries.Add</code> with payload; emit <code>pq_inject</code> audit linking artifact checksum and <code>mChecksum</code>. <br><strong>Governance note:</strong> for regulated templates require signed release manifest and enforced static analyzer blocking injection of modified local templates. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Deep conceptual mapping to DAX & semantic model design (expanded):</strong><br><strong>Context:</strong> DAX is read-time and cannot perform side effects; authoritative decisions must be made in ETL or worker layers. PQ_EnsureDeps ensures model authors and consumers can rely on deterministic ETL and metadata. <br><strong>Pattern 1 — Push rounding and residuals to ETL:</strong><br>1. Rationale: DAX can't persist rounding decisions or side-effects required for residual distribution. <br>2. Pattern: run <code>SafeRoundResiduals</code> in worker, persist everything as integer cents in model tables; DAX then computes sums and KPIs deterministically. <br>3. Evidence: store <code>RunMetadata</code> row for the run with <code>correlationId</code>, <code>artifactChecksum</code>, <code>depsReportHash</code>. <br><strong>Pattern 2 — Deterministic sampling via hashed keys:</strong><br>1. Rationale: DAX lacks seedable PRNGs for reproducible sampling. <br>2. Pattern: ETL computes <code>HashKey = HMAC_SHA256(PrimaryKey | correlationSalt)</code> and stores <code>sampleFlag = (HashKey MOD N) &lt; k</code>. Persist <code>correlationSalt</code> in <code>RunMetadata</code>. DAX can apply filter <code>sampleFlag=1</code> to create deterministic report slices. <br><strong>Pattern 3 — Model-level RunMetadata table and reconciliation:</strong><br>1. ETL writes <code>RunMetadata</code> atomically with dataset artifact and <code>deps.report.json</code> data. <br>2. DAX measures reference <code>RunMetadata</code> to compute <code>ReconciledFlag</code> by comparing expected artifact checksum to the run's artifact checksum. DAX formula example (descriptive): compute <code>IsReconciled</code> measure based on equality of artifact checksum fields persisted in model. <br><strong>Pattern 4 — Surface <code>degraded</code> and <code>repairActions</code> in model:</strong><br>1. ETL includes <code>degraded</code> boolean and <code>degradedReasons</code> in <code>RunMetadata</code>. DAX or report visuals can surface <code>DegradedIndicator</code> to warn consumers that dataset used degraded fallbacks. <br><strong>Pattern 5 — Traceability from report measure back to PQ_EnsureDeps:</strong><br>1. Include <code>depsReportHash</code> in <code>RunMetadata</code> and in the workbook template injected; UI surfaces provide "View deps report" linking to evidenceRef for investigator use. <br><strong>Governance & operator guidance for models:</strong><br>1. Never implement allocation/residual distribution in DAX. <br>2. Use ETL-run <code>RunMetadata</code> to ensure report consumers can verify dataset provenance. <br>3. When a model shows <code>DegradedIndicator</code>, require human review before using dataset for regulated reporting. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Extended forensic artifacts & retention — practical details:</strong><br><strong>Minimum forensic artifact set for a run:</strong><br>1. <code>deps.report.json</code> persisted atomically containing <code>reportHash</code> and artifact checksum. <br>2. <code>manifestSnapshot.blob</code> encrypted evidenceRef with canonical manifests and <code>snapshotHash</code>. <br>3. <code>signatureBlobs</code> for templates with <code>evidenceRef</code>. <br>4. <code>diagnosticBundle.zip</code> containing <code>audit_tail.csv</code>, preserved temp artifacts, and <code>forensic_manifest.json</code>. <br>5. <code>serializedRNGState</code> if deterministic sampling or tie-breaks used. <br>6. <code>SafeRound</code> input snapshots (canonical decimals) and rounding logs. <br>7. <code>atomicWrite</code> temp files and their sidecar metadata. <br><strong>Storage & retention:</strong><br>1. Hot: <code>\\evidence\hot\pq_ensuredeps\&lt;correlationId&gt;\</code> for 30 days. <br>2. Warm: secure archive for regulatory retention up to 7 years. <br>3. Cold: vault with chain-of-custody metadata as required by regulation. <br><strong>Access & controls:</strong> evidence blobs encrypted and role-based access; audit logging for evidence retrieval. <br><strong>Retention verification:</strong> monthly retention job emits <code>housekeeping.audit</code> and proof-of-delete for expired evidence. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Acceptance checklist before module release (expanded):</strong><br>1. OWNERS listed and contactable in OWNERS.md. <br>2. Public API stable, documented, semver versioning in manifests. <br>3. <code>LoadLocalManifests</code> uses no network and passes static analyzer. <br>4. Cross-platform golden vectors for <code>mChecksum</code> and <code>reportHash</code> present. <br>5. Signature verification tests included with test CA chain. <br>6. AtomicWrite tests and temp artifact recovery proofs present. <br>7. Audit hooks integrated and validated with test harness emitting expected audit rows. <br>8. Two-person approval flows tested for regulated repair scenarios. <br><strong>Blocking conditions:</strong> missing audit emissions, failing golden vectors, or static analyzer detection of forbidden UI-thread operations. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Comprehensive test plan highlights & scripts (explicit conceptual):</strong><br><strong>Unit tests:</strong><br>1. <code>LoadLocalManifests</code>: hidden-sheet present, absent, malformed; assert <code>manifestsHash</code>. <br>2. <code>ResolveRemoteManifests</code>: simulate 200 OK, 500 errors, TTL expiry. <br>3. <code>VerifyTemplateSignature</code>: valid, invalid, revoked, deferred. <br>4. <code>ValidateConnectorVersion</code>: semver range matching, vendor mapping. <br>5. <code>AtomicWriteDepsReport</code>: simulate ENOSPC/EACCES/rename failure via FS mocks. <br><strong>Integration tests:</strong><br>1. Full roundtrip: local manifest -> remote -> signature -> produce <code>deps.report.json</code> -> readback verify. <br>2. Concurrent producers: multiple processes writing to same target path, assert no partial reads. <br>3. Worker fallback: host runtime incompatible -> worker run executes final tasks and writes <code>deps.report.json</code> with <code>executionMode=worker</code>. <br><strong>Property tests:</strong><br>1. Determinism: canonicalization property ensures identical <code>reportHash</code> across permutations. <br>2. Idempotence: applying <code>RepairMissingDeps</code> twice is a no-op. <br><strong>Performance tests:</strong><br>1. Local manifest scan under 10k templates embedded in library. <br>2. Remote resolve performance under large index (10k templates). <br><strong>CI gating:</strong> unit/integration/goldens and static checks mandatory. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Operator commands & quick references (one-liners):</strong><br>1. <code>pq_ensuredeps scan --correlation r-YYYYMMDD-abc --fast</code> — quick local scan. <br>2. <code>pq_ensuredeps resolve-remote --correlation r-... --force</code> — worker-only full resolution. <br>3. <code>pq_ensuredeps produce-report --out deps.report.json --correlation r-...</code> — produce canonical report. <br>4. <code>pq_ensuredeps repair --plan repair.plan.json --apply --approvals &lt;a,b&gt;</code> — apply repair with approvals. <br>5. <code>pq_ensuredeps diagnose --correlation r-... --output diagbundle.zip</code> — produce diagnostic bundle for SRE. <br><strong>When to call SRE:</strong> after <code>AtomicWrite</code> ENOSPC for critical artifacts or repeated <code>ValidateConnectorVersion</code> ambiguity that blocks regulated templates; include forensic_manifest and audit_tail in ticket. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Common failure modes & mitigations (expanded):</strong><br><strong>Failure mode: missing connector on particular host</strong><br>1. Likely cause: per-user vs per-machine install mismatch or provider registration missing. <br>2. Mitigation: recommend unified installation of connector or use worker execution; include <code>repairAction: installConnector</code> with evidenceRef and staged plan. <br><strong>Failure mode: deferred signature revocation checks</strong><br>1. Cause: network policy blocks OCSP/CRL. <br>2. Mitigation: perform revocation checks in worker allowed network context or use cached revocation with <code>revocationDeferred=true</code> and <code>degradedReason</code>. <br><strong>Failure mode: non-deterministic <code>reportHash</code> across hosts</strong><br>1. Cause: non-canonical serialization or locale-sensitive sorting. <br>2. Mitigation: enforce canonical JSON serialization with stable ordering, NFKC normalization, and newline normalization. Add CI golden parity tests. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Governance checklists & PR requirements (explicit):</strong><br>1. PR must include unit tests and golden vectors for new deterministic outputs. <br>2. Changes to signature verification or trust anchors require Security owner approval and release manifest update. <br>3. Changes to <code>AtomicWriteDepsReport</code> semantics require cross-platform regression tests and SRE sign-off. <br>4. Any new repair action modifying system state requires SRE and two-person approval for regulated templates. <br>5. OWNERS approval required for template or connector compatibility policy changes. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Appendix A — <code>deps.report.json</code> canonical schema (descriptive summary):</strong><br><strong>Top-level keys:</strong> <code>correlationId</code>, <code>runTs</code>, <code>configHash</code>, <code>releaseManifestHash</code>, <code>templates</code>, <code>connectors</code>, <code>pqRuntime</code>, <code>degraded</code>, <code>degradedReasons</code>, <code>repairActions</code>, <code>evidenceRefs</code>, <code>reportHash</code>. <br><strong>Policy:</strong> top-level audit rows reference only <code>reportHash</code> and <code>evidenceRef</code>; raw manifests/signatures stored encrypted. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Appendix B — Forensic reconstruction example (explicit):</strong><br><strong>Incident:</strong> injection mismatch for run r-20260112-455. <br><strong>Steps:</strong><br>1. Retrieve <code>pq.ensuredeps.report.generated</code> and <code>pq.ensuredeps.manifest.load</code> audits. <br>2. Pull <code>deps.report.json</code> artifact and <code>manifestSnapshot</code> from evidence store. <br>3. Restore <code>serializedRNGState</code> and <code>SafeRound</code> inputs to reproduce allocation decisions. <br>4. Re-run compatibility checks offline and compare canonical M payloads; produce <code>forensic_manifest.json</code> with artifacts and checksums. <br>5. Package and escalate as required for compliance. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Appendix C — PQ template author checklist (detailed):</strong><br>1. Provide <code>mChecksum</code> in template metadata. <br>2. Specify <code>requiresHighPrecision=true</code> for numeric critical templates. <br>3. Include detached or embedded signature. <br>4. Declare <code>requiredConnectors</code> and <code>minPQRuntime</code>/<code>maxPQRuntime</code>. <br>5. Provide migration notes and compatibility hints. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Appendix D — DAX/report builder checklist (detailed):</strong><br>1. Consume <code>RunMetadata</code> table for provenance. <br>2. Avoid allocation or residual rounding in DAX. <br>3. Use ETL-provided hashed stable keys for deterministic sampling and filters. <br>4. Surface <code>degraded</code> flags and require validation for regulated outputs. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Appendix E — Long-form operator reconstruction example (step-by-step):</strong><br><strong>Incident synopsis:</strong> reported "Allocation mismatch for run r-20260112-455" — sums differ between artifact and ledger. <br><strong>Forensic steps:</strong><br>1. Retrieve <code>UserAction</code> and <code>pq.ensuredeps.*</code> audits for correlationId. <br>2. Pull <code>deps.report.json</code> artifact and <code>manifestSnapshot</code>. <br>3. Pull <code>serializedRNGState</code> and <code>SafeRound</code> canonical decimal snapshots from evidence store. <br>4. Re-run allocation pipeline in reproduce mode using persisted RNG and canonical decimals. <br>5. If reproduction succeeds, package <code>forensic_manifest.json</code> and close incident; if not, collect temp artifacts and <code>atomic_write.verification_failed</code> logs and escalate. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Extremely detailed extra narratives — developer, CI, SRE interactions:</strong><br><strong>Developer scenario — building new template requiring connector feature:</strong><br>1. Developer marks template <code>requiresConnector=AdvancedPull</code> and updates manifest. <br>2. Local dev harness runs <code>LoadLocalManifests</code> to verify manifest schema and runs <code>ResolveRemoteManifests</code> against test repo to produce golden <code>mChecksum</code>. <br>3. Developer includes golden vector in PR and adds compatibility notes and unit tests for <code>ValidateConnectorVersion</code>. <br>4. CI verifies that connector exists in the canonical test repo and verifies <code>reportHash</code> parity. If CI fails <code>PQ_DEPS_MISSING_CONNECTOR</code>, developer must provide connector stub or change template. <br><strong>SRE scenario — repeated atomic-write failures:</strong><br>1. SRE receives <code>pq.ensuredeps.report.atomic_write.failure</code> alerts with ENOSPC across hosts. <br>2. SRE runs <code>InspectTempArtifacts</code> to locate lingering <code>tmp</code> files, cleans up temp artifacts, and frees space or adjusts <code>deps.output.staging</code> configuration. <br>3. SRE updates runbook to include <code>disk-space-sentry</code> to preempt staging failures. <br><strong>Governance scenario — owner change for template:</strong><br>1. Template owner transfer requires updating <code>OWNERS.md</code> and re-signing templates. <br>2. PQ_EnsureDeps CI ensures <code>VerifyTemplateSignature</code> now validates against new owner certificate chain and audit records owner change in <code>releaseManifest</code>. <br>3. Two-person approval required to accept owner changes for regulated templates; PQ_EnsureDeps refuses injection until approvals recorded. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Practical developer examples (guidance, not code snippets):</strong><br><strong>Example — canonicalization rules summary:</strong><br>1. Serialize JSON with lexicographically sorted object keys; arrays sorted by stable keys like <code>templateId</code>. <br>2. Normalize all strings to Unicode NFKC and strip trailing spaces. <br>3. Use <code>\n</code> as newline and UTF-8 encoding. <br>4. Compute SHA256 over canonical bytes to produce <code>reportHash</code> and <code>mChecksum</code>. <br><strong>Example — repair action structure (descriptive):</strong> each <code>repairAction</code> entry should include <code>actionType</code>, <code>parameters</code>, <code>idempotencyToken</code>, <code>estimatedImpact</code>, <code>rollbackPlan</code>, <code>evidenceRef</code>. This ensures safe automated guidance while requiring operator approval for system changes. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Operational checks & automation guidance:</strong><br>1. Automate monthly retention verification for evidence stores and emit <code>housekeeping.audit</code> with proof-of-delete for expired evidence. <br>2. Automate <code>VerifyAuditChain</code> job in CI to validate audit rotations and signing with release manifest keys. <br>3. Provide operator CLI: <code>pq_ensuredeps replay --correlation &lt;id&gt; --evidenceRef &lt;ref&gt;</code> to reproduce environment and template decisions. <br>4. Enforce static analyzer rules: forbid direct workbook writes on UI load handlers and forbid network calls in <code>fastMode</code>. </td></tr><tr><td data-label="PQ_EnsureDeps — Per-function Expert Technical Breakdown"> <strong>Final set of firm operational constraints (recap):</strong><br>1. No network I/O on UI fast-path; <code>ResolveRemoteManifests</code> must run in worker. <br>2. Persist <code>deps.report.json</code> atomically and emit audits for each persisted artifact. <br>3. Enforce signature verification for templates with <code>signaturePolicy=REQUIRED</code>; block injection unless <code>VerifyTemplateSignature</code> passes or two-person approval obtained. <br>4. Always attach <code>evidenceRef</code> for raw artifacts and avoid PII in audit rows. <br>5. Use <code>GenerateDepFingerprint</code> for caches and CI gating; keep golden vectors for parity across implementations. <br><strong>Checked:</strong> audit coverage, deterministic canonicalization, UI thread safety, worker fallback patterns, repair idempotency, and comprehensive runbooks and forensic evidence mapping. </td></tr></tbody></table></div><div class="row-count">Rows: 43</div></div><script src="assets/xlsx.full.min.js?v=1758605028" defer></script>
<script src="assets/script.js?v=1759748863" defer></script>
<script src="assets/worker.js?v=1758331710" defer></script>
<script>
(function(){
  const template = "{table}_{date}";
  const userVal = "";
  const hrefPrefix = "assets";
  function formatName(tableName) {
    const date = (new Date()).toISOString().slice(0,10);
    return template.replace('{table}', tableName).replace('{date}', date).replace('{user}', userVal);
  }
  const btn = document.getElementById('exportBtn');
  if(btn) {
    btn.addEventListener('click', async function() {
      try {
        const html = document.documentElement.outerHTML;
        if(html.length > 2000000) { alert('Export refused: html too large'); return; }
        if(window.Worker) {
          const workerUrl = (function(){ try{ return hrefPrefix + '/worker.js'; }catch(e){ return null; } })();
          if(workerUrl) {
            try {
              const worker = new Worker(workerUrl);
              worker.postMessage({html: html, format: 'pdf'});
              worker.onmessage = function(e) { console.log('worker:', e.data); alert('Worker replied: '+(e.data.msg||e.data.status)); };
            } catch(err) { console.warn('Worker create failed', err); alert('Worker not available'); }
          } else { alert('Worker not available'); }
        } else { alert('Export worker not supported in this environment.'); }
      } catch(err) { console.warn('Export failed', err); alert('Export worker not available. See console for details.'); }
    });
  }
})();
window.addEventListener('load', function(){ try{ document.querySelectorAll('.table-wrapper').forEach(function(e){ e.style.opacity='1'; }); }catch(e){} });
</script>
</div>
</body>
</html>