<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='utf-8'><meta name='viewport' content='width=device-width,initial-scale=1'>
<title>Tables Viewer v2.1</title>
<style>:root{--main-header-height:56px;} .table-wrapper{opacity:0;min-height:24px;transition:opacity .12s linear}</style>
<link rel="stylesheet" href="assets/style.css?v=1759925496">
<link rel="stylesheet" href="assets/overrides.css?v=1762101804">
</head><body>
<div id="tables-viewer" role="region" aria-label="Tables viewer">
<div id="stickyMainHeader">
<div id="tv-header">
<div><h1>Tables Viewer v2.1</h1></div>
<div style="display:flex;gap:8px;align-items:center;">
<input id="searchBox" class="tv-search" type="search" placeholder="Search" aria-label="Search tables" />
<button id="modeBtn" type="button" onclick="toggleMode()" aria-label="Toggle theme">Theme</button>
<button id="toggleAllBtn" type="button" aria-label="Toggle all" onclick="toggleAllTables()">Collapse All Tables</button>
<button id="copyAllPlainBtn" class="copy-btn" type="button" onclick="copyAllTablesPlain()" aria-label="Copy all tables as plain text">Copy All Tables (Plain Text)</button>
<button id="copyAllTablesBtn" class="copy-all-btn" type="button" onclick="copyAllTablesMarkdown()" aria-label="Copy All Tables (Markdown)">Copy All Tables (Markdown)</button>
<button id="copyAllMdBtn" style="display:none" aria-hidden="true"></button>
<button id="resetAllBtn" type="button" onclick="resetAllTables()" aria-label="Reset All Tables">Reset All Tables</button>
</div></div>
<script>
(function(){
  function ensureDelegation(){
    try {
      var vis = document.getElementById('copyAllTablesBtn');
      var alias = document.getElementById('copyAllMdBtn');
      if(!vis || !alias) return;
      // Delegate addEventListener/removeEventListener to visible button
      alias.addEventListener = function(type, listener, options){
        vis.addEventListener(type, listener, options);
      };
      alias.removeEventListener = function(type, listener, options){
        vis.removeEventListener(type, listener, options);
      };
      // Delegate onclick assignments
      Object.defineProperty(alias, 'onclick', {
        set: function(fn){ vis.onclick = fn; },
        get: function(){ return vis.onclick; },
        configurable: true
      });
      // Delegate focus/blur
      alias.focus = function(){ vis.focus(); };
      alias.blur = function(){ vis.blur(); };
      // If legacy code used direct addEventListener earlier than this script, listeners would already exist
      // on alias element; attempt to re-dispatch those by cloning them to visible button is non-trivial.
      // This approach covers the common case where legacy scripts query the alias and bind after DOM ready.
    } catch(e) {
      try{ console && console.warn && console.warn('alias delegation failed', e); }catch(_){} 
    }
  }
  if(document.readyState === 'loading'){
    document.addEventListener('DOMContentLoaded', ensureDelegation);
  } else {
    ensureDelegation();
  }
})();
</script>
<noscript><div style='color:#b91c1c'>JavaScript is disabled. Tables will be shown statically. For large tables enable JS for virtualization.</div></noscript>
<div id="tocBar" role="navigation" aria-label="Table of contents"><ul><li class="toc-item"><a class="toc-link" href="#Table1">Table 1</a></li>
<li class="toc-item"><a class="toc-link" href="#Table2">Table 2</a></li>
<li class="toc-item"><a class="toc-link" href="#Table3">Table 3</a></li>
<li class="toc-item"><a class="toc-link" href="#Table4">Table 4</a></li>
<li class="toc-item"><a class="toc-link" href="#Table5">Table 5</a></li></ul></div></div>
<div class="table-caption" id="Table1" data-table="Docu_0086_01" style="margin-top:2mm;margin-left:3mm;"><strong>Table 1 — Project Structure</strong></div>
<div class="table-wrapper" data-table-id="table-1"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by **Project Structure**"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>Project Structure</strong></div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Project Structure"> PasteToTable<br>├─ main.py<br>├─ server.py<br>├─ html_renderer.py<br>├─ utils.py<br>├─ groups.json<br>├─ render<br>│  ├─ <strong>init</strong>.py<br>│  ├─ core_compat.py<br>│  ├─ core_table.py — table-level helpers, render_table wrapper<br>│  ├─ core_assets.py — new: asset discovery, embedding, copy logic<br>│  ├─ core_io.py — new: atomic write, tmp dirs, path checks<br>│  ├─ core_utils.py — new: misc helpers (sanitize, streaming, sri_attr)<br>│  ├─ core_renderer.py<br>│  ├─ core_fragments.py<br>│  ├─ convert.py<br>│  ├─ markdown.py<br>│  ├─ sanitize.py<br>│  ├─ io_utils.py<br>│  ├─ toc_manager.py<br>│  ├─ assets.py<br>│  ├─ cli.py<br>│  └─ types.py<br>├─ assets<br>│  ├─ script.core.js<br>│  ├─ script.table.js<br>│  ├─ script.toc.js<br>│  ├─ script.save.js<br>│  ├─ script.list.js<br>│  ├─ overrides.css<br>│  ├─ style.css<br>│  ├─ utils.js<br>│  ├─ extra.js<br>│  └─ worker.js — optional: future client-side features<br>└─ templates<br>  └─ index.html<br><br>Project structure for PasteToTable:<br><code>render/</code> — core rendering modules for tables, HTML assembly, sanitization, and conversion.<br><code>assets/</code> — CSS, JS, and worker scripts for styling, interactivity, saving, and list management.<br><code>templates/</code> — contains the main HTML template for rendering outputs.<br><code>groups.json</code> — stores precomputed table groups or indexes.<br>Root scripts (<code>main.py</code>, <code>server.py</code>, <code>html_renderer.py</code>, <code>utils.py</code>) coordinate the conversion workflow and backend operations. </td></tr></tbody></table></div><div class="row-count">Rows: 1</div></div><div class="table-caption" id="Table2" data-table="Docu_0086_02" style="margin-top:2mm;margin-left:3mm;"><strong>Table 2 — Script 1 Nov 2025</strong></div>
<div class="table-wrapper" data-table-id="table-2"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by **Technical Breakdown**"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>Technical Breakdown</strong></div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown"> <strong>Batch 1 — Algorithm </strong><br>All algorithm flows are presented as ordered steps and weight maps rather than literal function syntax. Verified 10× for consistency with the supplied script. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Role / Purpose (concise)</strong><br><code>script.list.js</code> discovers saved files from multiple server endpoints, normalizes listings to a canonical shape, groups and captions files, exposes a searchable modal UI with persistent DOM rows, and sequentially fetches selected files into the PasteToTable input path while attempting safe caption injection and clipboard delegation. Defensive behavior and fallback hooks are core design goals. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Primary inputs & outputs (concrete)</strong><br>Inputs: <code>LIVE_FILES_ENDPOINTS</code>, <code>GROUPS_JSON_CANDIDATES</code>, <code>LABELS_JSON_CANDIDATES</code>, DOM elements <code>#listBtn</code>, <code>#paste</code>, <code>#status</code>, and optional runtime hooks (<code>__tv_do_convert</code>, <code>convert</code>, <code>copyTableMarkdown</code>, <code>copyTablePlain</code>). Outputs: appended modal overlay, populated <code>#paste</code> or <code>#page-area</code>, <code>window.__tv_last_source</code>, <code>window.__ptt_runtime_captions</code>, status messages, and clipboard side-effects via exposed helpers. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Top-level configuration & limits</strong><br>Key constants and intent: <code>INDEX_FETCH_TIMEOUT_MS</code> (8s) and <code>FILE_FETCH_TIMEOUT_MS</code> (12s) favor network patience. <code>MAX_LIST_ITEMS</code> (5k) limits memory. <code>MAX_POSTINGS</code> and <code>MAX_CANDIDATES</code> guard index cost. Feature flags like <code>ENABLE_CACHE_BUSTER</code> and <code>ENABLE_WORKER_SCORING</code> are present for optional behavior. Do not change weights or timeouts without test vectors. </td></tr><tr><td data-label="Technical Breakdown"> <strong>High-level control flow (stepwise)</strong><br>1. Attach a click handler to <code>#listBtn</code> at DOM ready.<br>2. On activation, present status and start three probes concurrently: (a) sequential search over <code>LIVE_FILES_ENDPOINTS</code>, (b) candidates for <code>groups.json</code>, (c) candidates for <code>labels.json</code>.<br>3. When the first valid list is found, normalize entries.<br>4. Build groups from normalized list and resolved captions.<br>5. Create modal UI and build persistent rows for groups.<br>6. User searches or expands groups.<br>7. On selection, close modal and sequentially fetch files.<br>8. Inject fetched raw text into paste area and attempt to trigger conversion hooks.<br>9. Run caption-apply retry loop until DOM produced by renderer contains caption targets or timeout. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Fetch primitives and semantics</strong><br><code>fetch</code> wrapper semantics described as: perform network request with <code>no-store</code> cache and <code>same-origin</code> credentials; if <code>AbortController</code> exists use it; enforce a timer; on timeout return a controlled failure object rather than throw; on success return response text. Use this wrapper for discovery to allow soft failures. For content fetches use a stricter wrapper that rejects on non-OK statuses so caller can record fail notes. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Discovery probing algorithm (ordered steps)</strong><br>1. Iterate endpoints in configured order.<br>2. For each endpoint perform fetch-with-timeout.<br>3. If response body empty or non-OK, continue to next endpoint.<br>4. Attempt to parse body as JSON array or object with <code>files</code> array. If parsed array found and non-empty, normalize and return.<br>5. Otherwise split body into trimmed non-empty lines. If lines exist normalize and return.<br>6. If no endpoint produced usable data return failure and show status.<br>Design note: stops at first usable source to preserve server precedence. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Normalization rules (canonicalization)</strong><br>Input shapes handled: arrays of strings, arrays of objects with varied keys, and <code>{ files: [] }</code> wrappers. For strings map to <code>{ name: basename, url: null, path: original, description: &#x27;&#x27; }</code>. For objects derive <code>name</code>, <code>url</code>, <code>path</code>, and <code>description</code> from likely keys (<code>name</code>, <code>filename</code>, <code>url</code>, <code>path</code>, <code>description</code>). Enforce <code>MAX_LIST_ITEMS</code> while iterating. Maintain raw fields so later encoding into saved URLs is possible. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Label map normalization (key expansion)</strong><br>Given label JSON the normalizer builds alternate keys: lowercase duplicates, <code>-</code>↔<code>_</code> forms, stripped-leading-zero numeric variants, padded numeric variants, and lowercased numeric variants. This ensures file basename matching finds captions under a wide variety of naming conventions. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Caption resolution (ordered attempts)</strong><br>Given a basename, derive candidate keys in an ordered list: canonical base, lowercase, dash→underscore, underscore→dash, stripped numeric suffix variants, zero-padded numeric variants, underscore substitution for spaces, and concatenated lowercase form. For each candidate check exact match in label map then lowercased variant. Return first match or empty string. Rationale: maximize hit-rate for inconsistent file naming. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Group key derivation rules</strong><br>If a path includes <code>/</code>, use first segment as group. Otherwise strip extension then split by <code>_</code> and if last token is numeric assume it's an index and drop it. If at least two tokens remain use first two tokens joined. Otherwise fallback to the base filename. Sanitize to non-empty human text. This yields stable grouping across <code>group/name</code> or <code>project_01_report</code> patterns. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Group aggregation steps</strong><br>Iterate normalized items: determine group key via above rules; create group record if missing; push file record with resolved caption; after collection produce sorted groups with <code>Ungrouped</code> last and annotate counts. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Lightweight file-level search engine (design overview)</strong><br>Purpose: fast candidate retrieval and decent relevance without external dependencies. Components: a <code>lite</code> array of small item records and an <code>inverted</code> posting map from token to list of item ids. Each <code>lite</code> item stores tokens and trigram array. Posting lists are capped. Engine exposes two operations: (a) fast candidate union for query tokens and (b) asynchronous scoring over candidate set. Keep an explicit <code>dispose</code> hook for future worker cleanup. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Tokenization and trigram extraction (rules)</strong><br>Tokenize by converting to lowercase, stripping a range of punctuation/unicode marks, replacing non-alphanumeric with spaces, collapsing whitespace, then splitting and truncating to first 50 tokens. Trigram extraction pads the input with two spaces at each end and extracts all consecutive 3-character slices into a set. Tokens capture keyword-level signals; trigrams capture fuzzy substring overlap. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Index construction sequence</strong><br>For each item compute tokens and trigram set. For each token push the item id into posting list up to <code>MAX_POSTINGS</code>. Also index caption tokens with a <code>c:</code> prefix and filename tokens with an <code>f:</code> prefix to allow field-weighted matches. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Candidate union algorithm (steps)</strong><br>1. Normalize query and produce query tokens.<br>2. For each token gather postings for token, <code>c:token</code>, and <code>f:token</code>.<br>3. Accumulate unique ids until <code>MAX_CANDIDATES</code> cap.<br>4. Return array of ids or null when nothing found. This produces a small candidate set for scoring. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Scoring model (weight map and stepwise pseudocode)</strong><br>Scoring intent: prefer precise matches then captions and filenames then token matches then fuzzy overlap. Steps to score a candidate set:<br>1. Normalize query to lower-case and extract token list and trigram set.<br>2. For each candidate item compute a numeric score as sum of components:<br>- Exact quoted phrase presence in the candidate full text yields a very large positive boost.<br>- Presence of query substring in caption adds a large boost.<br>- Presence of query substring in name adds a medium boost.<br>- Number of matched query tokens times a per-token weight adds a strong base contribution.<br>- For tokens that match word-prefix in caption add a moderate bonus per token.<br>- For tokens that match word-prefix in filename add a smaller bonus.<br>- Trigram overlap ratio scaled into a bounded bonus adds an approximate fuzzy match signal.<br>- For very short queries and low base score, compute bounded edit-distance to caption and filename and add fallback boosts inversely proportional to edit distance.<br>- Add slight normalization bonus favoring shorter full-text length to avoid ranking extremely long docs ahead of concise matches.<br>3. Sort scored candidates descending and return list.<br>Numeric constants used by the script: a large exact-phrase boost, caption weight ~220, name weight ~140, per-token weight ~28, prefix caption bonus ~18, prefix filename bonus ~10, trigram scale up to ~60, edit-distance fallback thresholds and small length bonus formula. Do not change constants without testing. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Asynchronous scoring behavior</strong><br>Scoring runs inside a promise so UI thread can yield. Candidate caps are applied upstream to bound work. The engine is designed to be trivially moved into a WebWorker later for large indexes. The engine exposes <code>lite</code> for debugging and <code>dispose</code> for future cleanup. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Modal creation and focus model (steps)</strong><br>1. Create overlay container with <code>role=dialog</code> and <code>aria-modal=true</code>.<br>2. Build sticky header with title and a close button.<br>3. Attempt to relocate any existing search input with <code>[data-ptt-search]</code> into the header.<br>4. Append content and overlay to <code>document.body</code>.<br>5. Add keydown listener for Escape to close.<br>6. Focus the first actionable element within the modal after a short timed delay to allow layout.<br>7. Close removes listeners and overlay and tries to call <code>engine.dispose</code>. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Persistent DOM row strategy (why and how)</strong><br>Rows for groups are built once at initial render and stored in <code>groupControls</code> to minimize repeated node creation on each search. Only file lists within rows are cleared and repopulated as query results change. Event listeners for toggle, header click, and load button are attached once per row and remain valid during updates. This approach reduces GC churn and improves responsiveness for repeated queries. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Search UI update flow (ordered)</strong><br>1. On input, start debounced handler.<br>2. If query empty repopulate all groups with full file lists.<br>3. Else request candidate ids from engine.<br>4. Request scored ranked results from engine.<br>5. Map scored files to their groups and compute per-group matched slices (cap per group).<br>6. Hide groups without matches and update visible groups' file lists with highlighted tokens.<br>7. Update expand/collapse button state accordingly. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Highlighting approach</strong><br>Token-based highlight: split query into tokens; for each token replace matching substrings in displayed name and caption with an inline <code>&lt;mark&gt;</code> wrapper. Escape user text first to avoid injection. This is a UI affordance and not used in matching logic. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Markdown table detector (heuristic rationale)</strong><br>Heuristic attempts to find a contiguous block of lines that resemble a markdown table by looking for characters and a separator row with dashes. Primary goal is to extract first table for copy-as-markdown fallback. This is intentionally heuristic and favors recall; consider stricter separator checks to improve precision. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Sequential file load flow (detailed steps)</strong><br>1. Validate list non-empty; show fetching status.<br>2. Build runtime caption array from provided captions or defaults while deduplicating default labels.<br>3. For each file in list:<br>a. Resolve fetch URL from file object or encoded saved path.<br>b. Fetch content using strict fetch wrapper.<br>c. On success push raw text into <code>accRaw</code>. If file has caption create a sanitized injection-ready marked section (comment + sanitized heading) and push into <code>accMarked</code>.<br>d. On error push an HTML comment fail-note into both accumulators.<br>e. Wait a small delay to avoid server stress then continue.<br>4. After loop: join marked and raw accumulators into combined document strings.<br>5. Write raw string into <code>#paste.value</code> or <code>#page-area</code>.<br>6. Set runtime captions into <code>window.__ptt_runtime_captions</code> and try to call conversion hooks in preferred order; fallback to hidden textarea approach if no hooks present.<br>7. Schedule caption-apply retry loop. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Caption apply retry loop (behavioral steps)</strong><br>1. Identify target nodes by selecting <code>.table-caption</code> nodes.<br>2. If none found select heading nodes <code>h1..h6</code> with <code>id</code> starting <code>Table</code>.<br>3. If still none return false and retry after interval.<br>4. For each found node up to runtime captions length:<br>a. Compute sanitized caption text via HTML escaping.<br>b. If caption empty and node contains default 'Table N' text remove or clear node.<br>c. Else set node content to strong-wrapped safe text.<br>5. For heading nodes set deterministic id <code>TableN</code> if missing.<br>6. On success show toast and clear progress flag.<br>7. On max retries show timeout toast and clear progress flag.<br>Default retry policy is configurable at call site and typically totals several seconds of patience to allow slow renderer processing. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Clipboard delegation and fallback logic (flow)</strong><br>1. Global click listener looks for markdown or plain copy selectors.<br>2. If force-raw flag present copy raw paste text.<br>3. If markdown button and <code>copyTableMarkdown</code> exists, delegate.<br>4. Else if plain button and <code>copyTablePlain</code> exists, delegate.<br>5. Else for markdown extract first detected markdown table block using heuristic and copy that or fallback to entire raw paste text.<br>6. Prefer <code>navigator.clipboard.writeText</code> when available. Else use hidden textarea + <code>execCommand(&#x27;copy&#x27;)</code>.<br>7. Expose two helpers for external usage and toggling behavior. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Accessibility and defensive ARIA patterns</strong><br>Modal uses <code>role=dialog</code> and keyboard close. Search input has <code>aria-label</code>. Buttons have <code>aria-pressed</code> and <code>aria-expanded</code> as appropriate. Status messages prefer <code>#status</code> with <code>role=alert</code> else use hidden <code>aria-live</code> container appended offscreen. All interactive elements are focusable and attempts are made to restore or set focus logically. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Error handling philosophy</strong><br>Discovery errors are soft-failed and probing continues. File fetch failures are recorded as inline fail-comments and do not abort the combined load. UI DOM exceptions are swallowed to avoid breaking host page. All long-running steps set human-readable status updates. This favors availability over strict failure. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Performance notes and tuning knobs</strong><br>- Keep candidate caps to bound scoring CPU.<br>- Consider parallel fetch with concurrency limits to speed many-file loads, but only enable when server capacity is known.<br>- Move scoring to WebWorker if <code>lite.length</code> exceeds a threshold to prevent main-thread jank.<br>- Replace large <code>innerHTML = &#x27;&#x27;</code> repopulate patterns with granular diffs for huge lists. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Security & sanitization warnings</strong><br>Caption sanitization is minimal (removes obvious script markers and comment terminators). If stored files or labels are untrusted, use a robust sanitizer before injecting into HTML or markdown. Avoid embedding raw file content into HTML contexts. Consider CSP and content-signing for sensitive deployments. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Testing checklist (condensed)</strong><br>Unit tests: BOM stripping, basename/extract utilities, normalization variants, label key expansion, caption resolution, group key heuristics, tokenization/trigram extraction, posting cap enforcement, cheap edit-distance fallback. Integration tests: endpoint arrays/object/lines, search ranking and ordering, modal open/close, search handling, sequential load success/fail, caption-apply success/timeout, clipboard delegation present/absent, race conditions like modal closing during fetch. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Actionable recommendations (priority)</strong><br>1. Strengthen caption sanitizer.<br>2. Add optional concurrency-limited parallel fetch with server-friendly defaults.<br>3. Offload scoring to WebWorker above a deterministic threshold.<br>4. Replace full <code>innerHTML</code> clears for file lists with document-fragment diffs for very large groups.<br>5. Add opt-in telemetry hooks for metrics. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Self-verification (10 checks applied)</strong><br>1. Matched all top-level constants to original script.<br>2. Verified fetch wrapper behavior and timeout semantics.<br>3. Reconstructed normalization and caption resolution variants.<br>4. Reproduced search engine token/trigram strategy and posting capping.<br>5. Converted scoring mechanics into stepwise weighted map without literal code.<br>6. Recreated modal DOM skeleton and persistent row strategy.<br>7. Rewrote sequential fetch and caption-apply flows as ordered steps.<br>8. Rechecked clipboard delegation fallbacks and exposed helper names.<br>9. Reviewed security and performance recommendations for immediate relevance.<br>10. Confirmed no raw code syntax or function declarations remain in this batch. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Batch 2 — Executive summary</strong><br>This batch finalizes the deep-dive without literal source code. It contains: DOM lifecycle traces, event timeline, structured algorithm steps for scoring and caption application (expressed as ordered operations and weight maps), exhaustive test vectors, unit and integration harness plans, changelog template, ops checklist, performance/security tuning knobs, and recommended incremental patches.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      </td></tr><tr><td data-label="Technical Breakdown"> <strong>Scope and intent</strong><br>Deliverable completes the audit-ready documentation needed to integrate, test, and harden script.list.merged.js. Content is practical, precise, and avoids literal code. Use it to implement tests, port logic to other languages, or produce PR diffs.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 </td></tr><tr><td data-label="Technical Breakdown"> <strong>DOM lifecycle — condensed authoritative trace</strong><br>1) Activation: user clicks list button or host calls build operation. Status text updates immediately.  <br>2) Parallel probe phase starts: discover endpoints, groups.json candidates, labels.json candidates. Each probe uses a fetch-with-timeout primitive.  <br>3) On first usable listing the script normalizes entries and builds group structures synchronously.  <br>4) Modal creation: off-DOM element creation then single append to document.body to minimize reflow. Sticky header is built and existing search input is relocated if present. Focus moved to first focusable element after short delay.  <br>5) Initial render builds per-group persistent rows and attaches per-row handlers. Rows stored in groupControls for reuse.  <br>6) User search triggers debounced candidate lookup → scoring promise → UI update path that hides non-matching groups and repopulates matched-file lists.  <br>7) On group or file load the modal closes and the sequential fetch loop begins.  <br>8) After files fetched, combined raw text is injected into paste area or page-area. Preferred converter hooks are invoked.  <br>9) Caption-apply retry loop polls for renderer-created caption or heading nodes and injects sanitized text.  <br>10) Cleanup: engine.dispose invoked, modal listeners removed, in-progress flags cleared.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     </td></tr><tr><td data-label="Technical Breakdown"> <strong>Event timeline (relative ordering)</strong><br>T+0: click/list invocation.  <br>T+1: status update and probe start.  <br>T+2..T+n: endpoint probes complete; first success returns normalized data.  <br>T+n+1: groups constructed and modal created.  <br>T+n+2: initial rows rendered.  <br>T+n+3..: search interactions and scoring promise responses.  <br>T+m: user selects group/file -> modal closed -> sequential fetch started.  <br>T+m+1..T+m+k: files fetched, combined, and written to paste area.  <br>T+m+k+1..: conversion hook invoked and caption-apply retry loop runs until success or timeout.  <br>T+end: flags cleared and resources released.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </td></tr><tr><td data-label="Technical Breakdown"> <strong>Scoring algorithm (structured step sequence and weights)</strong><br>Objective: rank file items by relevance using deterministic, explainable signals. Steps for scoring a candidate item against a query:  <br>1) Normalize query to lowercase and obtain lightweight tokens and trigram set.  <br>2) Early exact phrase match check: if the query is a quoted phrase and appears verbatim in the item's full text then add a dominant boost.  <br>3) Substring presence: if the query substring appears in the caption add a large weight; if it appears in the filename add a medium weight.  <br>4) Token-match contribution: count how many query tokens exist in the item's token set and multiply by a per-token weight to produce the primary additive score.  <br>5) Prefix bonuses: for tokens that match as word prefixes in caption add a moderate per-token bonus; for filename prefix matches add a smaller bonus.  <br>6) Trigram overlap: compute overlap ratio of query trigrams against item trigrams and scale that ratio into a bounded fuzzy-match bonus.  <br>7) Short-query edit fallback: for short queries and low-scoring items compute a bounded edit distance to caption and filename and add fallback bonuses inversely proportional to edit distance when under threshold.  <br>8) Short-document slight bias: add a small normalization bonus favoring shorter full-text to prefer concise matches.  <br>9) Final sort: sort candidates by descending score.  <br>Key numeric intents (documented for reproducible tuning): exact-phrase = very large boost; caption substring = large weight; filename substring = medium weight; per-token weight = substantial base; prefix caption bonus = moderate; prefix filename bonus = small; trigram scale = bounded moderate; edit fallback acts only when primary score low. Do not change values without test coverage. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Scoring implementation notes (performance & safety)</strong><br>- Apply candidate cap upstream to limit expensive scoring.  <br>- Run scoring asynchronously (promise/future) so UI thread can yield.  <br>- For large indexes implement worker offload; pass compact arrays and token lists to worker to avoid repeated recomputation.  <br>- Use early rejects for empty queries.  <br>- Cache recent query results with an LRU to avoid re-scoring identical queries.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             </td></tr><tr><td data-label="Technical Breakdown"> <strong>Caption-application algorithm (ordered operations)</strong><br>Goal: reliably attach user-provided captions to renderer output despite renderer latency. Steps:  <br>1) Use runtime caption array prepared during load.  <br>2) Poll DOM for caption target elements in this order of preference: nodes with class table-caption, then heading nodes h1..h6 with id pattern TableN.  <br>3) If targets found iterate up to runtime caption length:  a) For each target get desired caption text, sanitize by HTML-escaping &, <, >, and remove suspicious sequences like comment terminators and script tag boundaries;  b) If desired caption is empty and node contains default TableN pattern then remove node or clear content; else set node inner content to strong-wrapped safe text;  c) If target is heading without id assign deterministic id TableN.  <br>4) If no targets found retry after configurable interval until max retries exceeded.  <br>5) On success set progress flags and notify via toast. On max retries, clear progress and notify timeout.  <br>Configuration knobs: retry interval, max retries, initial delay.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      </td></tr><tr><td data-label="Technical Breakdown"> <strong>Sequential fetch flow (operational steps)</strong><br>1) Build runtime caption list with sanitized captions and de-duplicated defaults.  <br>2) For each file: resolve URL from explicit url or encoded saved path; call content fetch wrapper with timeout; on success push raw text and a marked section if caption present; on failure append a fail-comment placeholder.  <br>3) Introduce small inter-file delay to avoid server overload.  <br>4) After all files processed join accumulators into combinedMarked and combinedRaw.  <br>5) Insert combinedRaw into paste area or page-area.  <br>6) Call preferred conversion hook with metadata when available; otherwise place combinedMarked into hidden textarea for host fallback.  <br>7) Start caption-apply loop.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      </td></tr><tr><td data-label="Technical Breakdown"> <strong>DOM structure snapshot (hierarchy and attributes)</strong><br>Modal root: overlay role=dialog aria-modal=true. Panel contains header (sticky), and content area. Content area layout: searchWrap (data-ptt-search) includes input[type=search] and expand/collapse button. resultsWrap holds group rows. Each group row structure: header (role=button, tabindex=0) with toggle button aria-expanded, title block with nameLine and subLine, right-side load button; fileList holds file rows with fname and cap elements. Group controls stored in groupControls with fields rowWrap, toggleBtn, fileList, expanded, visible.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       </td></tr><tr><td data-label="Technical Breakdown"> <strong>Event bindings and lifecycle rules</strong><br>- Bind click on #listBtn at DOM ready.  <br>- Search input bound to debounced handler with passive option.  <br>- Per-group toggle, header click, and load button bound once at row creation.  <br>- Global document click for copy delegation.  <br>- Modal close removes keydown listener and overlay.  <br>- Engine.dispose invoked on close.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      </td></tr><tr><td data-label="Technical Breakdown"> <strong>Highlighting and sanitization policy</strong><br>- Highlighting uses token-based substring replacement wrapped in mark after escaping displayed text.  <br>- Caption injections use HTML-escaping; remove comment terminators and common script markers.  <br>- Prefer textContent assignments when possible; only set innerHTML when content was escaped to avoid injection.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </td></tr><tr><td data-label="Technical Breakdown"> <strong>Test vectors — prioritized and exhaustive</strong><br>Unit vectors (representative):  <br>- BOM removal: input with BOM yields trimmed string.  <br>- Path parsing: nested paths, names with spaces, repeated dots.  <br>- Label normalization: keys with dash/underscore, numeric padding and leading zeros.  <br>- Caption resolution: file_01 -> label map match for variants.  <br>- Group derivation: "grp/sub/file", "proj_01_report.csv".  <br>- Tokenization: remove punctuation, collapse spaces, limit tokens.  <br>- Trigrams: padding verify.  <br>- Posting cap: repeated token postings truncated to cap.  <br>Integration vectors (representative):  <br>- Endpoint returning JSON array; endpoint returning {files:[]}; endpoint returning plaintext listing.  <br>- Network timeouts and abort behavior for discovery and file fetch.  <br>- Search: exact phrase vs token matching vs fuzzy scenarios; assert top-k ordering.  <br>- Sequential load: mixed success/failure; assert combinedRaw order, fail comments present, paste value updated.  <br>- Caption application: renderer delayed insertion; ensure captions eventually applied or timeout reached.  <br>- Copy delegation: with and without renderer-level copy functions; execCommand fallback correctness.  <br>- Race: close modal while fetching and ensure no DOM exceptions.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  </td></tr><tr><td data-label="Technical Breakdown"> <strong>Test harness plan (practical recipe)</strong><br>Unit tests: Jest with jsdom. Integration: Playwright. Steps:  <br>1) Create DOM fixture with #listBtn, #paste, #status.  <br>2) Mock global fetch to return controlled responses per URL.  <br>3) Provide AbortController shim in Node environment.  <br>4) Stub navigator.clipboard and window.showToast.  <br>5) Use fake timers to control retries and delays.  <br>6) For Playwright run a small server exposing endpoints with configurable delays to test timing and UI.  <br>7) Validate metrics and toasts by spying on window.showToast.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   </td></tr><tr><td data-label="Technical Breakdown"> <strong>Concrete mock behaviors</strong><br>- fetch mock returns object with ok flag, status, and text() method resolving to simulated body.  <br>- AbortController mock exposes signal and abort method; fetch mock watches signal.aborted to simulate abort.  <br>- clipboard.writeText stub captures writes and can produce success or reject.  <br>- showToast spy captures messages.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </td></tr><tr><td data-label="Technical Breakdown"> <strong>Changelog template (operational discipline)</strong><br>For any change include: version tag, exact change summary, rationale, impacted callers, tests run, and rollback plan. Example entries: adjust timeouts, change scoring weight, add concurrency flag. Each entry must include test vectors executed and expected metric thresholds for canary.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </td></tr><tr><td data-label="Technical Breakdown"> <strong>Operational checklist before deploy</strong><br>1) Run unit suite and integration suite.  <br>2) Run performance test with representative large index.  <br>3) Canary deploy to subset and collect metrics.  <br>4) Verify CORS for groups/labels endpoints.  <br>5) Validate clipboard behavior across target browsers.  <br>6) Confirm modal CSS variable fallbacks do not break host layouts.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     </td></tr><tr><td data-label="Technical Breakdown"> <strong>Performance tuning guide (explicit knobs and effects)</strong><br>- Reduce INDEX_FETCH_TIMEOUT_MS to reduce wait on dead endpoints but risk skipping slow servers.  <br>- Increase FILE_FETCH_TIMEOUT_MS for large files.  <br>- Lower MAX_LIST_ITEMS to conserve mobile memory.  <br>- Lower MAX_POSTINGS to conserve memory at cost of recall for common tokens.  <br>- Set a threshold to move scoring into WebWorker (offload when lite length exceeds set point).  <br>- Consider limited parallel fetch with max concurrency 3–6 for servers that can handle it.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               </td></tr><tr><td data-label="Technical Breakdown"> <strong>Security and privacy considerations</strong><br>- Captions sanitized but not fully sanitized; for untrusted deployments use robust sanitizer.  <br>- Avoid injecting raw file contents into HTML without explicit sanitization.  <br>- Clipboard writes must be user-initiated; do not auto-write.  <br>- Do not send file contents in telemetry.  <br>- Recommend CSP guidance and content signing for sensitive environments.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      </td></tr><tr><td data-label="Technical Breakdown"> <strong>Recommended incremental patches (priority order)</strong><br>1) Replace quick sanitizer with vetted HTML sanitizer for caption injection.  <br>2) Add optional parallel fetch with concurrency and backoff.  <br>3) Add LRU cache for recent query results to reduce repeated scoring.  <br>4) Add optional scoring worker and a small transfer format for lite index.  <br>5) Replace full innerHTML clears for file lists with diff-based updates using document fragments.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </td></tr><tr><td data-label="Technical Breakdown"> <strong>Monitoring and metrics suggestions</strong><br>Expose optional metrics via a hook or global object: discovery latency, number of files returned, file fetch failures, caption apply success rate, scoring latency, conversion hook used. Use these in canary to detect regressions.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  </td></tr><tr><td data-label="Technical Breakdown"> <strong>Edge-case summary (concise)</strong><br>- Heuristic markdown detection may false-positive on pipe-rich text.  <br>- Missing AbortController increases reliance on timers.  <br>- Large index sizes may cause main-thread stalls without worker offload.  <br>- Caption injection may fail if renderer outputs unexpected DOM shapes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 </td></tr><tr><td data-label="Technical Breakdown"> <strong>Batch 2 self-verification (10 checks applied)</strong><br>1) Confirm no literal function declarations or code blocks present.  <br>2) Confirm all algorithmic flows expressed as ordered steps.  <br>3) Rechecked constants and behavior against prior Batch 1 values.  <br>4) Validated DOM hierarchy snapshot aligns with actual code.  <br>5) Re-validated scoring weight map and fallback rules.  <br>6) Cross-checked caption retry math and default patience.  <br>7) Reviewed test vectors for coverage completeness.  <br>8) Ensured recommended patches are actionable and low-risk.  <br>9) Re-affirmed security warnings and sanitizer priority.  <br>10) Ensured readiness for integration or export.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </td></tr><tr><td data-label="Technical Breakdown"> <strong>Deliverable actions for you</strong><br>1) Reply with “Complete” to mark this audit delivered and final.  <br>2) Reply with “Export <filename>” to receive a repo-ready markdown file.  <br>3) Reply with “Patch <changespec>” to receive a focused patch diff (no code text restriction waived for diffs).  <br>4) Reply with “Test harness” to receive Jest+Playwright scaffold files.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             </td></tr></tbody></table></div><div class="row-count">Rows: 56</div></div><div class="table-caption" id="Table3" data-table="Docu_0086_03" style="margin-top:2mm;margin-left:3mm;"><strong>Table 3 — Enhancement 1</strong></div>
<div class="table-wrapper" data-table-id="table-3"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown"> <strong>Enhancement</strong><br> Delivering an enhancement plan that benchmarks against Evernote-style search features and proposes concrete, low-risk upgrades to the file-level engine, UI, and fetch flows. Goals: higher relevance, fielded queries, robust phrase/proximity/fuzzy support, scalable performance, and safe deployment path.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     </td></tr><tr><td data-label="Technical Breakdown"> <strong>Benchmark target: Evernote search behavior (summary)</strong><br>Evernote search is notable for: fielded queries (tag:, intitle:, created:, updated:), boolean operators (AND/OR/NOT), phrase and proximity search, prefix/wildcard, fuzzy matching and synonyms, strong recency and notebook/tag boosting, and responsive UX on large note collections via efficient inverted/positional indices and selective ranking (term frequency + field boosts + recency). Use these features selectively to raise relevance while keeping client-side resource constraints in mind.                                                                                                                                                                                                                                                     </td></tr><tr><td data-label="Technical Breakdown"> <strong>Enhancement objectives (concrete)</strong><br>1. Provide fielded search support (<code>name:</code>, <code>caption:</code>, <code>path:</code>).  <br>2. Support quoted phrases and proximity-like behavior (phrase-first, then loosen).  <br>3. Add prefix/wildcard support for filename and caption tokens.  <br>4. Add simple fuzzy matching (bounded edit distance or n-gram) with configurable tolerance.  <br>5. Implement field-boosted ranking (caption > name > path > description).  <br>6. Add recency or heuristic boosting for recently-loaded lists (if metadata present).  <br>7. Keep core inverted index but add positional information for phrase/proximity.  <br>8. Provide a small query parser to handle boolean and fielded expressions.  <br>9. Offer server-side search fallback when index size or device capability exceeds thresholds. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Design constraints & tradeoffs</strong><br>- Client-side memory and CPU are limited on low-end devices.  <br>- Avoid heavy-weight full-text engines.  <br>- Prefer multi-tier approach: lightweight client index + optional server search for heavy lifting.  <br>- Keep deterministic, auditable scoring so UX changes are explainable.  <br>- Preserve existing UI and incremental rollout.                                                                                                                                                                                                                                                                                                                                                                                                                                    </td></tr><tr><td data-label="Technical Breakdown"> <strong>Proposed index schema (compact)</strong><br>Keep <code>lite</code> array but extend each item with: <code>tokens</code> (as now), <code>triArr</code> (trigrams), <code>positions</code> map <code>{ token -&gt; [positions] }</code> truncated per-token to cap postings, and <code>fieldTokens</code> sets for <code>f:name</code>, <code>c:caption</code>, <code>p:path</code>, <code>d:desc</code>. Store <code>lastModified</code> or numeric ordinal if available for recency boost. Keep posting lists inverted map keyed by token and prefixed field tokens for fast field queries. Cap per-token postings and position arrays to control memory.                                                                                                                                                                                                                                                                                                    </td></tr><tr><td data-label="Technical Breakdown"> <strong>Query model (user-facing features)</strong><br>- Fielded expressions: <code>name:report caption:&quot;annual summary&quot;</code>.  <br>- Boolean operators: <code>tag:finance AND (report OR summary) NOT draft</code>.  <br>- Phrase search: <code>&quot;annual summary&quot;</code> exact contiguous phrase.  <br>- Proximity-like fallback: <code>&quot;annual summary&quot;~3</code> semantics (prefer exact then near matches).  <br>- Prefix/wildcard in filename: <code>name:rep*</code> -> prefix search on name tokens.  <br>- Fuzzy operator: <code>summary~</code> or implicit fuzzy for short typos.  <br>- Bare token queries remain tokenized ANDed by default with configurable OR mode.                                                                                                                                                                                                                         </td></tr><tr><td data-label="Technical Breakdown"> <strong>Query parser (ordered responsibilities)</strong><br>1. Tokenize raw query into tokens, phrases, operators, field qualifiers, and modifiers.  <br>2. Build an AST: boolean nodes, fielded nodes, phrase nodes, proximity modifiers, fuzzy flags.  <br>3. Validate and normalize node values (lowercase, strip punctuation).  <br>4. Emit candidate token sets and field-specific searches for engine to handle.  <br>5. Fallback to simple token search on parse errors.                                                                                                                                                                                                                                                                                                                                                          </td></tr><tr><td data-label="Technical Breakdown"> <strong>Candidate retrieval (execution plan)</strong><br>1. From AST collect union of posting lists for query tokens and field-token variants.  <br>2. For phrase nodes consult positional <code>positions</code> maps to filter candidates with contiguous positions (exact) or positions within proximity window (near).  <br>3. For prefix tokens perform postings union for tokens that share prefix (use token prefix map or scan keys; maintain a prefix-tree/trie if item set large).  <br>4. For fuzzy tokens request fuzzy candidates from trigram overlap or bounded edit distance on small candidate pool (first pass).  <br>5. Cap overall candidate set with <code>MAX_CANDIDATES</code> then pass to scoring.                                                                                                                                    </td></tr><tr><td data-label="Technical Breakdown"> <strong>Ranking enhancements (weight map & order)</strong><br>Compute final score as combination of:<br>- Field-boosted match score: matches in caption × high boost, name × medium, path/desc × small.  <br>- Term frequency (within item) normalized by document length.  <br>- Exact phrase bonus (very large).  <br>- Proximity bonus for phrase tokens appearing close.  <br>- Trigram or fuzzy-match penalty when edit-distance used (lower than exact).  <br>- Recency boost (if timestamp available) as small additive function (e.g., exponential decay).  <br>- Popularity/usage boost (optional telemetry-based).  <br>Design note: implement as linear combination with tunable weights and normalize components to predictable ranges. Keep weights conservative to avoid single-signal dominance.                          </td></tr><tr><td data-label="Technical Breakdown"> <strong>Positional index details (practical cap rules)</strong><br>- Store positions only for tokens appearing in name and caption fields.  <br>- For each token keep up to N positions (e.g., 8) to enable phrase/proximity checks while limiting memory.  <br>- For posting lists store item id and optional field bitmask to indicate which fields the token occurs in.  <br>- For prefix/prefix-trie, maintain a small prefix map for high-frequency short prefixes only.                                                                                                                                                                                                                                                                                                                                                            </td></tr><tr><td data-label="Technical Breakdown"> <strong>Fuzzy matching strategy (cost-aware)</strong><br>1. Use trigram overlap as a cheap prefilter for fuzzy candidates.  <br>2. Apply bounded edit-distance only on candidate set below a threshold.  <br>3. Expose per-token fuzzy threshold dependent on token length (e.g., allow distance 1 for length ≤4, 2 for length ≤8).  <br>4. Penalize fuzzy matches in final score but accept them to increase recall.  <br>Rationale: edit-distance is expensive; using trigram prefilter keeps cost under control.                                                                                                                                                                                                                                                                                                                      </td></tr><tr><td data-label="Technical Breakdown"> <strong>Prefix & wildcard handling (efficient approach)</strong><br>- Implement prefix search by scanning token dictionary keys with the prefix. Use a compact sorted array or an adaptive trie. For small indexes a linear scan over unique tokens is acceptable.  <br>- Limit prefix expansion by a maximum matched-token count per prefix (e.g., 50) to avoid explosion.  <br>- Wildcard <code>*</code> allowed only at end (suffix wildcard) to simplify. Avoid full regex wildcard to limit CPU.                                                                                                                                                                                                                                                                                                                                               </td></tr><tr><td data-label="Technical Breakdown"> <strong>Field boosts and query-time scoring composition (example weights)</strong><br>Suggested starting weight scheme (relative, tune with tests): caption boost 4.0, name boost 2.5, path boost 1.0, description boost 0.8; exact phrase bonus 6.0; per-token match base 1.0; trigram overlap scaled 0..2.0; proximity multiplier 0.5–2.0; recency additive 0..1.0. Document-level TF normalization scale to 0..3.0. Keep these as config constants for A/B tuning.                                                                                                                                                                                                                                                                                                                                                                    </td></tr><tr><td data-label="Technical Breakdown"> <strong>UI changes (minimal and progressive)</strong><br>- Add small query syntax hint inside search input (placeholder), e.g., <code>Support: &quot;quoted phrase&quot;, field:name, OR, NOT</code>.  <br>- Add a toggle for "Fuzzy" and "Exact only" for non-technical users.  <br>- Display matched field badges in file rows (e.g., <code>Matched: caption</code>), and highlight phrase matches.  <br>- Add a lightweight explanation UI on first use describing fielded search.                                                                                                                                                                                                                                                                                                                                                                                    </td></tr><tr><td data-label="Technical Breakdown"> <strong>Server-side fallback & hybrid model</strong><br>When <code>lite.length</code> or device capability exceeds thresholds, prefer delegating complex parsing and scoring to a server endpoint:  <br>- Client sends parsed AST or simple tokens + field hints.  <br>- Server returns top-k ids and scores.  <br>Advantages: unlimited index size, advanced scoring (BM25/BERT later), fast client UX. Ensure server endpoint respects same-origin/CORS and is optional.                                                                                                                                                                                                                                                                                                                                                                          </td></tr><tr><td data-label="Technical Breakdown"> <strong>Migration plan & rollout (phased)</strong><br>1. Phase 0: Add query parser and fielded token support but keep existing scoring path as default. Smoke test.  <br>2. Phase 1: Add positional info and phrase/proximity filtering. Expose toggle for phrase sensitivity.  <br>3. Phase 2: Add fuzzy prefilter via trigrams and bounded edit-distance fallback.  <br>4. Phase 3: Add recency boosting and optional server-side search.  <br>At each phase run A/B tests on sample datasets and monitor metrics. Provide rollback via feature flags.                                                                                                                                                                                                                                                                                 </td></tr><tr><td data-label="Technical Breakdown"> <strong>Testing and evaluation plan (metrics & vectors)</strong><br>Measure: precision@5, recall@50, latency median/95th, CPU time for scoring, memory footprint, and perceived UI responsiveness. Test vectors: exact phrase queries, fielded queries, prefix queries, typo scenarios, heavy index sizes (1k, 10k, 50k), and device matrix (desktop, mid-phone, low-end phone). Use synthetic datasets derived from real filenames and captions to measure ranking shifts.                                                                                                                                                                                                                                                                                                                                                              </td></tr><tr><td data-label="Technical Breakdown"> <strong>Concrete unit/integration tests (examples)</strong><br>- Field parsing: <code>name:report caption:&quot;annual 2024&quot;</code> → AST fields recognized.  <br>- Phrase exactness: <code>&quot;annual summary&quot;</code> matches only when tokens contiguous given positions.  <br>- Proximity fallback: <code>&quot;annual summary&quot;~3</code> matches when tokens within window ≤3.  <br>- Prefix expansion: <code>name:rep*</code> returns items with name tokens starting <code>rep</code>.  <br>- Fuzzy: <code>summry</code> should match <code>summary</code> with lower score.  <br>- Combined: <code>caption:&quot;budget 2023&quot; OR name:budget</code> ranking shows caption matches above name-only.  <br>- Performance: scoring on 10k items returns top-50 in <200ms on mid-range device.  <br>Define expected pass thresholds and include them in CI.                                                                                       </td></tr><tr><td data-label="Technical Breakdown"> <strong>Implementation roadmap (tasks & priorities)</strong><br>High priority (P0): query parser, fielded token mapping, field-boosted scoring, phrase exact match using limited positions, simple prefix support, UI hints.  <br>Medium (P1): trigram prefilter fuzzy, proximity scoring, recency boost, matched-field badges.  <br>Low (P2): full wildcard support, server search endpoint, more advanced rankers (BM25), synonyms/thesaurus integration.                                                                                                                                                                                                                                                                                                                                                                              </td></tr><tr><td data-label="Technical Breakdown"> <strong>Operational & safety notes</strong><br>- Keep per-token and position caps to avoid memory blowouts.  <br>- Expose feature flags to disable fuzzy or phrase features if performance issues arise.  <br>- Avoid sending raw index or file contents in telemetry.  <br>- When a server-side search is introduced secure endpoints and avoid exposing internal file content without auth.                                                                                                                                                                                                                                                                                                                                                                                                                                            </td></tr><tr><td data-label="Technical Breakdown"> <strong>Fallbacks and user controls</strong><br>- Provide UI toggles: "Exact mode", "Fuzzy mode", and "Use server search".  <br>- When device is flagged low-memory disable positional storage and enable server search.  <br>- If query parser fails, fallback to token-based search as current behavior.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               </td></tr><tr><td data-label="Technical Breakdown"> <strong>Metrics to capture for tuning</strong><br>- Query distribution (token lengths, operator use).  <br>- Average candidate set size per query.  <br>- Scoring latency percentiles.  <br>- Correction rate: how often fuzzy matching yields useful results (user-accepted).  <br>- Caption-apply success after enabling new API flows.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </td></tr><tr><td data-label="Technical Breakdown"> <strong>Example scoring scenarios (explainable outcomes)</strong><br>- Query: <code>caption:&quot;Q1 report&quot; name:summary</code> → top hits with <code>caption</code> phrase exact match rank highest; name matches secondary.  <br>- Query: <code>repot</code> (typo) → fuzzy/trigram picks <code>report</code> items; user toggles off fuzzy to force exact.  <br>- Query: <code>path:invoices</code> → path hits appear but with lower boost unless caption/name contain tokens.                                                                                                                                                                                                                                                                                                                                                                                                                  </td></tr><tr><td data-label="Technical Breakdown"> <strong>Resource estimates & thresholds</strong><br>- Positional map per-item average extra memory ~ tokens_per_item × avg_positions × index_entry_size. For small items (name+caption) expect low multiplier. Recommend enabling positions only for caption and name fields to keep footprint low.  <br>- Worker offload threshold default: <code>lite.length &gt;= 250</code> (user-tunable).  <br>- Server-side fallback threshold default: <code>lite.length &gt;= 5000</code> or device memory low.                                                                                                                                                                                                                                                                                                                                                             </td></tr><tr><td data-label="Technical Breakdown"> <strong>Compatibility with existing codebase</strong><br>- Extend <code>lite</code> items and inverted map; keep existing <code>c:</code> and <code>f:</code> conventions.  <br>- Reuse existing trigram functions for fuzzy prefilter.  <br>- Add <code>queryParser</code> module that returns AST for existing search flow; keep scoring function as a pluggable strategy.  <br>- Preserve current UI and gradually expose enhanced features.                                                                                                                                                                                                                                                                                                                                                                                                                                      </td></tr><tr><td data-label="Technical Breakdown"> <strong>Risk assessment & mitigation</strong><br>- Risk: main-thread stalls from phrase/positional checks. Mitigation: run heavy checks in WebWorker or cap candidates.  <br>- Risk: fuzzy explosion on short tokens. Mitigation: increase min token length for fuzzy and cap expansions.  <br>- Risk: user confusion with complex syntax. Mitigation: UI hints and simple toggles.                                                                                                                                                                                                                                                                                                                                                                                                                                                      </td></tr><tr><td data-label="Technical Breakdown"> <strong>Acceptance criteria (for rollout)</strong><br>1. Feature parity for basic fielded and phrase queries with existing tests passing.  <br>2. Precision@5 improved or stable for benchmark queries.  <br>3. 95th percentile scoring latency within target on supported device classes.  <br>4. Memory usage within acceptable bounds for typical lists.  <br>5. No new client-side exceptions under stress tests.                                                                                                                                                                                                                                                                                                                                                                                                                    </td></tr><tr><td data-label="Technical Breakdown"> <strong>Next actionable steps for you (pick one)</strong><br>1. <strong>Implement P0</strong>: I can produce a patch diff that adds the query parser, field tokens, and field-boosted ranking (no raw code restriction waived only if you request a patch).  <br>2. <strong>Prototype</strong>: I can generate a minimal worker-based scoring prototype and test vectors.  <br>3. <strong>Server design</strong>: I can outline a small server search API and mapping to client AST.  <br>4. <strong>Run A/B plan</strong>: I can generate a test matrix and telemetry plan for canary.  <br>Reply with choice: <strong>P0</strong>, <strong>Prototype</strong>, <strong>Server</strong>, or <strong>AB</strong> and I will proceed.                                                                                                                                                                                                             </td></tr></tbody></table></div><div class="row-count">Rows: 28</div></div><div class="table-caption" id="Table4" data-table="Docu_0086_04" style="margin-top:2mm;margin-left:3mm;"><strong>Table 4 — Enhancement 2</strong></div>
<div class="table-wrapper" data-table-id="table-4"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown"> <strong>World-class search</strong><br>World-class search spans two families: classical lexical ranking (inverted index + TF/IDF/BM25 + field boosts + positional data) and semantic/dense retrieval (embeddings + ANN). For PasteToTable pick a hybrid: keep a lightweight client lexical index for instant filtering and UI, offload BM25 and dense/semantic ranking to a server (or worker) when dataset or features exceed device capacity. This yields best tradeoff of relevance, latency, and device safety.                                                                                                                                                                                                                                                                                                                       </td></tr><tr><td data-label="Technical Breakdown"> <strong>Scope of comparison</strong><br>We compare practical algorithms and systems relevant to this codebase: TF–IDF, BM25 (and BM25F), positional indices for phrase/proximity, n-gram/trigram indexing, Levenshtein/fuzzy, phonetic/stemming, learning-to-rank, lexical engines (Lucene/Elasticsearch/Solr), lightweight libraries (Whoosh, Lunr), vector/semantic search (FAISS, HNSW, Annoy, ScaNN), hybrid rankers combining lexical+dense, and retrieval-augmented models. For each we list pros, cons, resource profile, and suitability for client vs server.                                                                                                                                                                                                                                                                       </td></tr><tr><td data-label="Technical Breakdown"> <strong>TF–IDF (classic)</strong><br>Overview: term-frequency × inverse-document-frequency scoring combined with cosine similarity. Pros: simple, explainable, cheap to compute offline. Cons: weak on short queries and phrase proximity; single-term weighting brittle. Resource: small index, simple linear algebra. Suitability: server or client for small indexes; serves as baseline.                                                                                                                                                                                                                                                                                                                                                                                                                                                 </td></tr><tr><td data-label="Technical Breakdown"> <strong>BM25 (state-of-the-art lexical)</strong><br>Overview: probabilistic ranking function that improves TF–IDF with saturation and length normalization. Pros: robust relevance, field boosting via BM25F, well-understood. Cons: needs term frequencies and doc lengths, not semantic. Resource: moderate CPU for scoring; efficient with inverted index. Suitability: primary server-side ranker; for small client indexes BM25 can run in JS if candidate set small or in a WebWorker.                                                                                                                                                                                                                                                                                                                                                 </td></tr><tr><td data-label="Technical Breakdown"> <strong>BM25F (fielded BM25)</strong><br>Overview: BM25 extended to multiple fields with independent weights. Pros: maps directly to our <code>caption &gt; name &gt; path</code> requirement; boosts fields systematically. Cons: slightly more storage for per-field stats. Suitability: server-side or advanced client when positions/field tokens stored.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 </td></tr><tr><td data-label="Technical Breakdown"> <strong>Positional indices & proximity/phrase search</strong><br>Overview: store token positions to verify exact phrases and compute proximity scores. Pros: precise phrase matching and proximity boosting. Cons: positions increase index size. Suitability: essential for phrase queries; store positions for name/caption only to keep footprint manageable.                                                                                                                                                                                                                                                                                                                                                                                                                                                                             </td></tr><tr><td data-label="Technical Breakdown"> <strong>n-gram / trigram indexing (fuzzy + prefix)</strong><br>Overview: index overlapping n-grams for fuzzy substring matching and typo tolerance. Pros: good for short tokens and typo resilience; cheap fuzzy prefilter. Cons: larger index, can produce noisy matches for tiny tokens. Suitability: client-side fuzzy prefilter; combine with edit-distance or BM25 on server for final ranking.                                                                                                                                                                                                                                                                                                                                                                                                                                         </td></tr><tr><td data-label="Technical Breakdown"> <strong>Edit-distance (Levenshtein / Damerau-Levenshtein)</strong><br>Overview: exact string distance metric for typos. Pros: accurate for small/token edits. Cons: O(n*m) cost; expensive at scale. Suitability: use only on candidate set filtered by trigram overlap or token-postings; not for full-index runs on client.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </td></tr><tr><td data-label="Technical Breakdown"> <strong>Stemming / Lemmatization / Phonetic (Soundex, Metaphone)</strong><br>Overview: morphological normalization or phonetic matching. Pros: improves matching across inflections and phonetic typos. Cons: language-dependent and adds complexity. Suitability: optional for captions with natural language; avoid heavy lemmatizers on client.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           </td></tr><tr><td data-label="Technical Breakdown"> <strong>Suffix arrays / suffix trees / full-text substring indices</strong><br>Overview: support arbitrary substring/wildcard searches. Pros: powerful substring queries. Cons: heavy memory and complex to maintain. Suitability: not recommended for client; server-side only when substring search is business-critical.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  </td></tr><tr><td data-label="Technical Breakdown"> <strong>Boolean operators and query parsing</strong><br>Overview: parse expression syntax (AND/OR/NOT, field: qualifiers). Pros: expressive power; common in Evernote and Lucene. Cons: parser complexity; UX burden. Suitability: implement lightweight parser client-side; fallback to token-search on parse error.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </td></tr><tr><td data-label="Technical Breakdown"> <strong>Learning-to-Rank (LTR)</strong><br>Overview: ML models combine many features (BM25, recency, click data). Pros: superior ranking when trained. Cons: needs labeled data and infrastructure. Suitability: long-term server-side enhancement for production; not for client.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           </td></tr><tr><td data-label="Technical Breakdown"> <strong>Vector / Dense Retrieval (embeddings + ANN)</strong><br>Overview: represent text as vectors (e.g., sentence transformers). Use ANN indexes (HNSW, FAISS, Annoy, ScaNN) for scalable nearest-neighbor search. Pros: semantic matching, good for synonyms and paraphrase. Cons: heavy (embeddings + ANN storage), requires model for embeddings, can surface false positives without lexical filter. Suitability: server-side hybrid with lexical prefilter; consider if semantic relevance matters (e.g., user searches meaningfully beyond filenames and captions).                                                                                                                                                                                                                                                                  </td></tr><tr><td data-label="Technical Breakdown"> <strong>Hybrid Lexical + Dense Ranking</strong><br>Overview: combine BM25 for lexical signals and dense scores for semantics; then merge via linear combination or learning-to-rank. Pros: best of both worlds. Cons: more infrastructure and tuning. Suitability: recommended server-side for high-quality results. Keep client-side for lexical filtering and UI.                                                                                                                                                                                                                                                                                                                                                                                                                                                                          </td></tr><tr><td data-label="Technical Breakdown"> <strong>Systems: Lucene / Elasticsearch / Solr</strong><br>Overview: full-featured servers offering BM25, fielded search, phrase, proximity, analyzers, synonyms, and LTR plugins. Pros: mature, scalable, battle-tested. Cons: operational overhead, requires server deployment and CORS/auth handling. Suitability: ideal server-side backend for production-grade search.                                                                                                                                                                                                                                                                                                                                                                                                                                                                 </td></tr><tr><td data-label="Technical Breakdown"> <strong>Lightweight JS search libs (Lunr, FlexSearch, Fuse.js)</strong><br>Overview: in-browser search libraries offering TF-IDF-ish ranking (Lunr/FlexSearch) or fuzzy search (Fuse). Pros: easy to embed, no server. Cons: limited features and scaling. Suitability: good for small datasets (<1k items) when server not available. FlexSearch is fastest among JS libs; Lunr has BM25-like scoring.                                                                                                                                                                                                                                                                                                                                                                                                                                      </td></tr><tr><td data-label="Technical Breakdown"> <strong>ANN libraries & HNSW (client vs server)</strong><br>Overview: HNSW (hierarchical small-world graphs) implemented widely (nmslib, hnswlib, FAISS HNSW), provides sub-linear NN. Pros: fast nearest neighbor at scale. Cons: memory and complexity. Suitability: server-side; or edge-worker if resourceful. Not feasible on constrained browsers for large indexes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                   </td></tr><tr><td data-label="Technical Breakdown"> <strong>Operational tradeoffs & guidance</strong><br>1. <strong>Client-first constraint:</strong> keep client index small, avoid heavy per-token position arrays except for name/caption.  <br>2. <strong>Server fallback:</strong> shift BM25/BM25F, positional proximity and dense retrieval to server when index grows.  <br>3. <strong>Hybrid workflow:</strong> client produces a compact query AST, filters candidate set quickly, asks server for top-k if needed.  <br>4. <strong>Feature gating:</strong> expose toggles for fuzzy, phrase, and semantic modes; disable expensive features by default on low-end devices.                                                                                                                                                                                                                                                               </td></tr><tr><td data-label="Technical Breakdown"> <strong>Concrete enhancement recommendations (prioritized)</strong><br>1. <strong>P0 (high impact, low cost):</strong> Add BM25F-style scoring with field boosts and positional checks for exact phrase where available. Keep trigram prefilter for fuzzy fallbacks. Implement as an optional WebWorker or server endpoint.  <br>2. <strong>P1 (moderate):</strong> Add positional storage for name/caption (cap positions per token) to enable phrase/proximity and improve phrase scoring.  <br>3. <strong>P2 (medium complexity):</strong> Add trigram-based fuzzy prefilter and bounded edit-distance on candidate subset.  <br>4. <strong>P3 (long-term):</strong> Introduce hybrid dense retrieval: compute embeddings server-side and combine BM25 + dense score via linear blend or LTR.  <br>5. <strong>P4 (ops):</strong> Integrate with Elasticsearch or lightweight server for large deployments. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Example hybrid flow (recommended architecture)</strong><br>1. Client tokenizes query, applies local inverted index to produce up to N candidates.  <br>2. Client requests server search with parsed AST and candidate id hints (or just tokens) if candidate set small; else request server top-k BM25.  <br>3. Server runs BM25F ± dense ranking, returns top-k ids + scores + field match metadata.  <br>4. Client displays results immediately if server responds fast; fallback to client ranked results if server unreachable.                                                                                                                                                                                                                                                                                                  </td></tr><tr><td data-label="Technical Breakdown"> <strong>Evaluation metrics & benchmarks to use</strong><br>- Precision@k (k=5), Recall@50, NDCG@10.  <br>- Latency median and p95 for query roundtrip (target p95 < 200–300ms for good UX).  <br>- CPU time for scoring and memory footprint of index on representative devices.  <br>- User acceptance: correction rate when fuzzy enabled.  <br>Establish datasets: synthetic (large), production-like sample (filenames + captions), and human labeled relevance judgments for a small golden set.                                                                                                                                                                                                                                                                                                                                        </td></tr><tr><td data-label="Technical Breakdown"> <strong>Testing plan (practical)</strong><br>1. Unit tests for parser, tokenization, trigram prefilter and edit-distance.  <br>2. Microbenchmarks: scoring time per 100/1k/10k candidates.  <br>3. A/B test: baseline inverted engine vs BM25F vs BM25F+fuzzy vs hybrid dense+lexical on golden queries.  <br>4. Device matrix: run on desktop and low-end phone emulators to validate latency/memory.                                                                                                                                                                                                                                                                                                                                                                                                                                       </td></tr><tr><td data-label="Technical Breakdown"> <strong>Risks and mitigations</strong><br>- Risk: main-thread jank from heavy scoring. Mitigation: WebWorker offload, candidate caps, and server fallback.  <br>- Risk: false positives from dense models. Mitigation: require lexical match threshold or combine scores conservatively.  <br>- Risk: telemetry leakage. Mitigation: avoid sending document content; send only anonymized metrics.                                                                                                                                                                                                                                                                                                                                                                                                                                           </td></tr><tr><td data-label="Technical Breakdown"> <strong>Suggested roadmap with timeboxed steps</strong><br>Week 0: Implement query parser and BM25F in a worker; add field boosts.  <br>Week 1: Add trigram prefilter + edit-distance fallback.  <br>Week 2: Add positional indices for name/caption and phrase/proximity checks.  <br>Week 3: Benchmark and tune weights; run A/B tests.  <br>Week 4+: Prototype server BM25+ANN hybrid if semantic matches required.                                                                                                                                                                                                                                                                                                                                                                                                                       </td></tr><tr><td data-label="Technical Breakdown"> <strong>Final recommendation (short)</strong><br>Adopt BM25F as the core ranking signal, add trigram fuzzy prefilter and positional support for phrase queries, and implement scoring in a WebWorker or server for indexes >250 items. Only adopt dense/ANN semantic search once user needs exceed lexical limits and you can host an embeddings service.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </td></tr><tr><td data-label="Technical Breakdown"> <strong>Concrete next steps I can perform for you</strong><br>- Produce a non-code design doc for BM25F and worker integration.  <br>- Generate test vectors and tuning matrices for weight calibration.  <br>- Produce a patch diff implementing query parser + BM25F scoring in worker (requires permission to include code).  <br>Reply with <strong>“Design doc”</strong>, <strong>“Test vectors”</strong>, or <strong>“Patch”</strong> to pick next action.                                                                                                                                                                                                                                                                                                                                                                                                                    </td></tr><tr><td data-label="Technical Breakdown"> <strong>Self-verification (10× checks applied)</strong><br>1. Confirmed taxonomy of algorithms covers lexical, fuzzy, and dense approaches.  <br>2. Verified BM25F suitability for fielded search tie-in with labels/captions.  <br>3. Checked resource implications for client vs server.  <br>4. Ensured positional and trigram tradeoffs enumerated.  <br>5. Validated hybrid flow practicality and fallbacks.  <br>6. Cross-checked evaluation metrics and test plans.  <br>7. Re-run mental proof of suggested roadmap sequence.  <br>8. Verified mitigation strategies for common risks.  <br>9. Ensured recommendations align with prior conversation constraints.  <br>10. Confirmed no raw code blocks present and all algorithmic steps are described conceptually.                                                                </td></tr></tbody></table></div><div class="row-count">Rows: 27</div></div><div class="table-caption" id="Table5" data-table="Docu_0086_05" style="margin-top:2mm;margin-left:3mm;"><strong>Table 5</strong></div>
<div class="table-wrapper" data-table-id="table-5"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by **Search Refactor**"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>Search Refactor</strong></div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Search Refactor"> <strong>1) Objectives</strong>: The refactor must surgically replace only the search/index/scoring internals in <code>script.list.js</code>. It must deliver highly relevant results, handle international text and common human errors, and provide clear visual feedback (highlights) while preserving all DOM, I/O, and public API behavior. This scope reduces risk and enables quick rollback. <br> <strong>Steps & rationale:</strong> <br> 1) Make search case-insensitive to avoid duplicate logic for capitalization. <br> 2) Strip diacritics so users typing plain ASCII still find accented content. <br> 3) Support exact, prefix, and substring matching to satisfy precision and convenience searches. <br> 4) Add bounded fuzzy matching for short queries to catch typos while minimizing false positives. <br> 5) Implement additive (AND-like) multi-word scoring so multiple tokens increase confidence. <br> 6) Keep candidate sets bounded and provide optional Worker scoring to preserve UI responsiveness. <br> 7) Preserve all existing DOM hooks and public APIs to guarantee compatibility. </td></tr><tr><td data-label="Search Refactor"> <strong>2) Normalization</strong>: A consistent normalization pipeline is the foundation of robust matching and precise highlighting; it must be applied identically to documents, group names, captions, descriptions, and queries. <br> <strong>Steps & rationale:</strong> <br> 1) Apply Unicode NFD and strip <code>\p{M}</code> combining marks to remove accents while preserving base letters. <br> 2) Convert non-ASCII punctuation into single spaces so token boundaries are consistent. <br> 3) Lowercase everything to create case-insensitive comparisons. <br> 4) Trim and collapse repeated whitespace for stable token positions. <br> 5) Preserve stable token order — important for exact-phrase detection and deterministic highlights. <br> 6) Optionally expose normalized string in debug mode for QA and audits.                                                                                                                                                                                                                                                                                 </td></tr><tr><td data-label="Search Refactor"> <strong>3) Tokenization</strong>: Tokenization must produce meaningful, compact tokens and preserve token offsets where possible to support accurate highlighting. It must handle programming-style tokens too (camelCase, snake_case). <br> <strong>Steps & rationale:</strong> <br> 1) Split on whitespace and punctuation to form base tokens. <br> 2) Further split camelCase, snake_case, and dash-separated tokens to surface meaningful word parts. <br> 3) Remove empty tokens to avoid noise in the index. <br> 4) Cap tokens per document at <strong>N</strong> (suggest 40) to bound index size and CPU cost. <br> 5) Return unique tokens for inverted index efficiency, but keep token position lists in <code>full</code> for highlight offsets. <br> 6) Ensure query tokenization mirrors document tokenization exactly to avoid scoring mismatches.                                                                                                                                                                                                                                                                  </td></tr><tr><td data-label="Search Refactor"> <strong>4) Indexing</strong>: Build a compact, capped inverted index plus trigram and acronym structures. Precompute fields needed for scoring and highlighting. Design for predictable memory usage. <br> <strong>Steps & rationale:</strong> <br> 1) Precompute <code>full</code> (normalized document string) for substring/phrase checks and highlight context. <br> 2) Precompute <code>tokens</code> (unique token array) and token positions. <br> 3) Compute <code>triArr</code> trigram set for approximate matching. <br> 4) Build <code>acronym</code> from initial letters of name/caption tokens for fast shorthand lookups. <br> 5) Keep <code>name</code>, <code>caption</code>, <code>description</code>, <code>group</code> and <code>path</code> as separate fields so scoring weights can distinguish them. <br> 6) Build <code>invertedIndex</code> mapping token → posting list of doc IDs. Cap each posting list to <code>MAX_POSTINGS</code> to prevent index bloat. <br> 7) If token positions stored, persist them in postings (or in a side map) to enable exact highlight offsets without re-scanning the source.                                                                                          </td></tr><tr><td data-label="Search Refactor"> <strong>5) Candidate Selection</strong>: Efficiently retrieve a bounded candidate set for scoring; support phrase bypass, token union, and linear-scan fallback. Instrument counts for QA. <br> <strong>Steps & rationale:</strong> <br> 1) Normalize and tokenize the incoming query to produce query tokens and detect quoted phrases. <br> 2) For each query token, fetch the posting list from <code>invertedIndex</code> and union them into the candidate set; stop early when union size exceeds <code>MAX_CANDIDATES_FROM_INDEX</code>. <br> 3) If query contains quoted phrase(s), bypass the index and run exact substring checks against <code>full</code> on documents. <br> 4) If query yields no tokens (e.g., only punctuation or very short non-alphanum), fallback to a linear scan limited to the first <strong>M</strong> documents. <br> 5) Return candidate IDs plus token-position metadata needed for highlight rendering. <br> 6) Record instrumentation: candidate count, retrieval time, and whether fallback or worker was used.                                                                                                </td></tr><tr><td data-label="Search Refactor"> <strong>6) Scoring (canonical scorer)</strong>: One canonical scoring function must combine all signals in a clear, auditable, and tweakable way. <br> <strong>Steps & rationale:</strong> <br> 1) Exact-phrase found in <code>full</code> → add <code>EXACT_PHRASE_BOOST</code>. <br> 2) Caption match → <code>CAPTION_MATCH_BOOST</code>. <br> 3) Name match → <code>NAME_MATCH_BOOST</code>. <br> 4) Description match → <code>DESCRIPTION_MATCH_WEIGHT</code>. <br> 5) Token overlap → <code>TOKEN_MATCH_WEIGHT * matched token count</code>. <br> 6) Prefix token matches → <code>PREFIX_TOKEN_BOOST</code> per prefix hit. <br> 7) Trigram overlap → <code>TRIGRAM_BASE_MULT * overlap_ratio</code>. <br> 8) Short-query fuzzy → bounded Damerau-Levenshtein scoring for queries ≤6 chars. <br> 9) Length penalty → subtract <code>LENGTH_PENALTY_BASE</code> scaled for long fields. <br> 10) Group weight → small positive adjustment if document occurs in many groups. <br> 11) Produce normalized final score; optionally filter results below <code>MIN_SCORE_TO_SHOW</code>.                                                                                                                                  </td></tr><tr><td data-label="Search Refactor"> <strong>7) Concrete Scoring Weights</strong>: Top-level constants for easy tuning. <br> 1) EXACT_PHRASE_BOOST = 500 <br> 2) CAPTION_MATCH_BOOST = 220 <br> 3) NAME_MATCH_BOOST = 140 <br> 4) DESCRIPTION_MATCH_WEIGHT = 100 <br> 5) TOKEN_MATCH_WEIGHT = 28 <br> 6) PREFIX_TOKEN_BOOST = 18 <br> 7) TRIGRAM_BASE_MULT = 60 <br> 8) SHORT_FUZZY_BASE = 28 <br> 9) LENGTH_PENALTY_BASE = 8 <br> 10) ACRONYM_BOOST = 40 <br> 11) MIN_SCORE_TO_SHOW = 10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            </td></tr><tr><td data-label="Search Refactor"> <strong>8) Short Fuzzy Rules</strong>: Restrict edit-distance checks to short queries. <br> 1) Apply Damerau-Levenshtein only if query length ≤6 and distance ≤2. <br> 2) Award SHORT_FUZZY_BASE - distance<em>8 to name, SHORT_FUZZY_BASE/2 - distance</em>4 to description/caption. <br> 3) Longer queries rely on trigram overlap.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  </td></tr><tr><td data-label="Search Refactor"> <strong>9) Trigram Strategy</strong>: For medium/long queries. <br> 1) Build trigram sets for document <code>full</code> and query. <br> 2) Compute overlap ratio. <br> 3) Score contribution = TRIGRAM_BASE_MULT * overlap_ratio. <br> 4) Store matching trigrams to enable highlight rendering. <br> 5) Reduces cost vs repeated edit-distance checks.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   </td></tr><tr><td data-label="Search Refactor"> <strong>10) Acronym & Prefix Matching</strong>: <br> 1) Compute acronyms from first letters of tokens (e.g., “Mount Peak” → mp). <br> 2) Support multi-token acronyms and partial matches. <br> 3) Apply ACRONYM_BOOST and PREFIX_TOKEN_BOOST. <br> 4) Highlight matched letters in UI.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         </td></tr><tr><td data-label="Search Refactor"> <strong>11) Highlighting Logic</strong>: Every matched element must be highlighted in UI for name, caption, description, group. <br> 1) Maintain token position lists to generate precise offsets. <br> 2) Use offsets for exact-token and prefix matches. <br> 3) Use substring offsets for exact-phrase matches. <br> 4) Trigram overlap highlights contiguous matched regions. <br> 5) Acronym highlights matched letters. <br> 6) Apply consistent CSS classes for styling. <br> 7) Optionally return highlight metadata for QA.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            </td></tr><tr><td data-label="Search Refactor"> <strong>12) Limits & Defaults</strong>: Define safe, conservative defaults to ensure predictable behavior. <br> 1) MAX_CANDIDATES_FROM_INDEX = 800 — bounds candidate set. <br> 2) MAX_POSTINGS = 2000 — prevent common token from exploding memory. <br> 3) TOKEN_LIMIT_PER_DOC = 40 — bound index size. <br> 4) TOP_RESULTS = 200 — results to return to UI. <br> 5) WORKER_THRESHOLD = 1500 — use Worker only for large datasets. <br> 6) WORKER_TIMEOUT_MS = 250 — timeout for worker; fallback if exceeded. <br> 7) FALLBACK_LINEAR_SCAN_LIMIT = 200 — linear scan cap for short/non-alphanum queries. <br> 8) MIN_SCORE_TO_SHOW = 10 — filter noise. </td></tr><tr><td data-label="Search Refactor"> <strong>13) Instrumentation & Logging</strong>: Lightweight dev instrumentation for QA/perf. <br> 1) window.__ptt_search_stats = { lastQueryMs, candidatesReturned, workerUsed, workerTimeMs, indexBuildMs, postingCountsSummary }. <br> 2) Optional console.debug traces in dev, toggleable by ENABLE_SEARCH_DEBUG. <br> 3) Track histograms of candidatesReturned and queryTimeMs. <br> 4) Log only IDs, counts, durations, and flags — no document text. <br> 5) Optional sampling for production metrics.                                                                                                                                              </td></tr><tr><td data-label="Search Refactor"> <strong>14) Test Harness & Assertions</strong>: Dev-only console harness to validate behavior. <br> 1) Expose candidatesForQuery(q), scoreQuery(q), highlightForCandidate(id,q), runAssertions() on window.__ptt_search_engine. <br> 2) runAssertions() executes canonical test cases and prints pass/fail summary. <br> 3) Assertions examples: topResultIncludes(scoreQuery('peak'), expectedId), Peak/peak parity, typo tolerance, quoted phrase checks, highlights present. <br> 4) Include timing assertions: queryTimeMs ≤ threshold or workerUsed. <br> 5) API to run performance microbenchmarks.                                                  </td></tr><tr><td data-label="Search Refactor"> <strong>15) Required Test Cases</strong>: Each case must pass on dev/representative data. <br> 1) Query peak → top 3 includes a file with peak token; highlights in name/caption/description. <br> 2) Query Peak → identical results to peak. <br> 3) Query peack → fuzzy match DL ≤2; highlight shows matched fragment. <br> 4) Query "blue peak" → exact phrase only. <br> 5) Query mt pk → acronym surfaces Mount Peak with highlight. <br> 6) Query lake peak → both tokens present; higher score. <br> 7) Non-word short query (c++) → fallback linear scan; no exceptions. <br> 8) Ensure no UI exceptions; __ptt_search_stats updated.              </td></tr><tr><td data-label="Search Refactor"> <strong>16) Acceptance Criteria</strong>: Measurable QA gates. <br> 1) All test cases pass. <br> 2) Performance: 2,000-item candidate selection + scoring ≤120ms main-thread or worker used with main-thread <20ms. <br> 3) No UI/DOM regressions; highlights, expand/collapse, copy unchanged. <br> 4) Instrumentation reasonable (candidate counts ≤ MAX_CANDIDATES_FROM_INDEX). <br> 5) Memory usage reasonable.                                                                                                                                                                                                                                        </td></tr><tr><td data-label="Search Refactor"> <strong>17) Rollout & Fallback</strong>: Feature-flagged staged deployment. <br> 1) Dev branch: implement & unit-test; run harness on multiple corpora. <br> 2) Staging: ENABLE_INVERTED_INDEX/ENABLE_WORKER_SCORING/ENABLE_HIGHLIGHTING = true; small group. <br> 3) Gradual rollout: ramp 1% → 100% with monitoring. <br> 4) Quick rollback: flip flags false → instant reversion. <br> 5) Post-rollout validation: runAssertions(), sample queries, verify highlights.                                                                                                                                                                                  </td></tr><tr><td data-label="Search Refactor"> <strong>18) Monitoring & Performance Budget</strong>: Long-term monitoring & thresholds. <br> 1) P95 query time <200ms (adjust per environment). <br> 2) P95 candidatesReturned < MAX_CANDIDATES_FROM_INDEX. <br> 3) Worker failure rate <0.5%. <br> 4) Inverted index growth linear & bounded. <br> 5) Highlight rendering P95 <60ms. <br> 6) Log alerts on unusual values.                                                                                                                                                                                                                                                                               </td></tr><tr><td data-label="Search Refactor"> <strong>19) Ten-point Verification Checklist</strong>: QA gating. <br> 1) normalize() correct; diacritics stripped. <br> 2) Tokenizer tested on camelCase/snake_case/dash. <br> 3) MAX_POSTINGS respected. <br> 4) candidatesForQuery union capped at MAX_CANDIDATES_FROM_INDEX. <br> 5) Quoted phrase bypass works. <br> 6) Short fuzzy applies only ≤6 chars, DL ≤2. <br> 7) Trigram overlap validated. <br> 8) Worker used only when itemCount > WORKER_THRESHOLD; timeout triggers fallback. <br> 9) Highlight offsets correct for all fields. <br> 10) No UI regressions.                                                                             </td></tr><tr><td data-label="Search Refactor"> <strong>20) Dev Console Harness</strong>: Quick dev validation commands. <br> 1) candidatesForQuery('peak') → array ≤ MAX_CANDIDATES_FROM_INDEX with tokenPositions. <br> 2) scoreQuery('peak') → top 10 results with score & highlights. <br> 3) highlightForCandidate(expectedId,'peak') → offsets & type metadata. <br> 4) runAssertions() → prints PASS/FAIL. <br> 5) benchmark({queries:['peak','mt pk'], iterations:100}) → avg/p95 timings & worker usage.                                                                                                                                                                                          </td></tr><tr><td data-label="Search Refactor"> <strong>21) Deliverables</strong>: Artifacts produced upon “go”. <br> 1) Patched createFileSearchEngine exposing candidatesForQuery, scoreQuery, highlightForCandidate. <br> 2) Worker blob implementing scoring + highlight offsets. <br> 3) Dev-only console harness with runAssertions() & microbenchmarks. <br> 4) Eight canonical test assertions. <br> 5) README with constants & rollback instructions. <br> 6) 10-step verification log. <br> 7) Changelog & suggested git commit messages.                                                                                                                                                        </td></tr><tr><td data-label="Search Refactor"> <strong>22) Revert Instructions & Safety</strong>: Immediate rollback steps. <br> 1) Set ENABLE_INVERTED_INDEX/ENABLE_WORKER_SCORING/ENABLE_HIGHLIGHTING = false. <br> 2) Reload page/clear cache to use old code path. <br> 3) Restore index from backup if needed. <br> 4) runAssertions() to confirm prior behavior restored. <br> 5) For performance regressions, tweak worker flags or thresholds. <br> 6) Document incident and create hotfix branch if necessary.                                                                                                                                                                                   </td></tr></tbody></table></div><div class="row-count">Rows: 22</div></div><script src="assets/xlsx.full.min.js?v=1758605028" defer></script>
<script src="assets/script.js?v=1759748863" defer></script>
<script src="assets/worker.js?v=1758331710" defer></script>
<script>
(function(){
  const template = "{table}_{date}";
  const userVal = "";
  const hrefPrefix = "assets";
  function formatName(tableName) {
    const date = (new Date()).toISOString().slice(0,10);
    return template.replace('{table}', tableName).replace('{date}', date).replace('{user}', userVal);
  }
  const btn = document.getElementById('exportBtn');
  if(btn) {
    btn.addEventListener('click', async function() {
      try {
        const html = document.documentElement.outerHTML;
        if(html.length > 2000000) { alert('Export refused: html too large'); return; }
        if(window.Worker) {
          const workerUrl = (function(){ try{ return hrefPrefix + '/worker.js'; }catch(e){ return null; } })();
          if(workerUrl) {
            try {
              const worker = new Worker(workerUrl);
              worker.postMessage({html: html, format: 'pdf'});
              worker.onmessage = function(e) { console.log('worker:', e.data); alert('Worker replied: '+(e.data.msg||e.data.status)); };
            } catch(err) { console.warn('Worker create failed', err); alert('Worker not available'); }
          } else { alert('Worker not available'); }
        } else { alert('Export worker not supported in this environment.'); }
      } catch(err) { console.warn('Export failed', err); alert('Export worker not available. See console for details.'); }
    });
  }
})();
window.addEventListener('load', function(){ try{ document.querySelectorAll('.table-wrapper').forEach(function(e){ e.style.opacity='1'; }); }catch(e){} });
</script>
</div>
</body>
</html>