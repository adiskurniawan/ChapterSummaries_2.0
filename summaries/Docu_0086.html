<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='utf-8'><meta name='viewport' content='width=device-width,initial-scale=1'>
<title>Tables Viewer v2.1</title>
<style>:root{--main-header-height:56px;} .table-wrapper{opacity:0;min-height:24px;transition:opacity .12s linear}</style>
<link rel="stylesheet" href="assets/style.css?v=1769960840">
<link rel="stylesheet" href="assets/overrides.css?v=1771304614">
</head><body>
<div id="tables-viewer" role="region" aria-label="Tables viewer">
<div id="stickyMainHeader">
<div id="tv-header">
<div><h1>Tables Viewer v2.1</h1></div>
<div style="display:flex;gap:8px;align-items:center;">
<input id="searchBox" class="tv-search" type="search" placeholder="Search" aria-label="Search tables" />
<button id="modeBtn" type="button" onclick="toggleMode()" aria-label="Toggle theme">Theme</button>
<button id="toggleAllBtn" type="button" aria-label="Toggle all" onclick="toggleAllTables()">Collapse All Tables</button>
<button id="copyAllPlainBtn" class="copy-btn" type="button" onclick="copyAllTablesPlain()" aria-label="Copy all tables as plain text">Copy All Tables (Plain Text)</button>
<button id="copyAllTablesBtn" class="copy-all-btn" type="button" onclick="copyAllTablesMarkdown()" aria-label="Copy All Tables (Markdown)">Copy All Tables (Markdown)</button>
<button id="copyAllMdBtn" style="display:none" aria-hidden="true"></button>
<button id="resetAllBtn" type="button" onclick="resetAllTables()" aria-label="Reset All Tables">Reset All Tables</button>
</div></div>
<script>
(function(){
  function ensureDelegation(){
    try {
      var vis = document.getElementById('copyAllTablesBtn');
      var alias = document.getElementById('copyAllMdBtn');
      if(!vis || !alias) return;
      alias.addEventListener = function(type, listener, options){
        vis.addEventListener(type, listener, options);
      };
      alias.removeEventListener = function(type, listener, options){
        vis.removeEventListener(type, listener, options);
      };
      Object.defineProperty(alias, 'onclick', {
        set: function(fn){ vis.onclick = fn; },
        get: function(){ return vis.onclick; },
        configurable: true
      });
      alias.focus = function(){ vis.focus(); };
      alias.blur = function(){ vis.blur(); };
    } catch(e) {
      try{ console && console.warn && console.warn('alias delegation failed', e); }catch(_){} 
    }
  }
  if(document.readyState === 'loading'){
    document.addEventListener('DOMContentLoaded', ensureDelegation);
  } else {
    ensureDelegation();
  }
})();
</script>
<noscript><div style='color:#b91c1c'>JavaScript is disabled. Tables will be shown statically. For large tables enable JS for virtualization.</div></noscript>
<div id="tocBar" role="navigation" aria-label="Table of contents"><ul><li class="toc-item"><a class="toc-link" href="#Table1">Table 1</a></li>
<li class="toc-item"><a class="toc-link" href="#Table2">Table 2</a></li>
<li class="toc-item"><a class="toc-link" href="#Table3">Table 3</a></li>
<li class="toc-item"><a class="toc-link" href="#Table4">Table 4</a></li></ul></div></div>
<div class="table-caption" id="Table1" data-table="Docu_0086_01" style="margin-top:2mm;margin-left:3mm;"><strong>Table 1 • Project Structure</strong></div>
<div class="table-wrapper" data-table-id="table-1"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by **Project Structure**"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>Project Structure</strong></div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Project Structure"> PasteToTable<br>├─ main.py<br>├─ server.py<br>├─ html_renderer.py<br>├─ utils.py<br>├─ groups.json<br>├─ render<br>│  ├─ <strong>init</strong>.py<br>│  ├─ core_compat.py<br>│  ├─ core_table.py — table-level helpers, render_table wrapper<br>│  ├─ core_assets.py — new: asset discovery, embedding, copy logic<br>│  ├─ core_io.py — new: atomic write, tmp dirs, path checks<br>│  ├─ core_utils.py — new: misc helpers (sanitize, streaming, sri_attr)<br>│  ├─ core_renderer.py<br>│  ├─ core_fragments.py<br>│  ├─ convert.py<br>│  ├─ markdown.py<br>│  ├─ sanitize.py<br>│  ├─ io_utils.py<br>│  ├─ toc_manager.py<br>│  ├─ assets.py<br>│  ├─ cli.py<br>│  └─ types.py<br>├─ assets<br>│  ├─ script.core.js<br>│  ├─ script.table.js<br>│  ├─ script.toc.js<br>│  ├─ script.save.js<br>│  ├─ script.list.js<br>│  ├─ overrides.css<br>│  ├─ style.css<br>│  ├─ utils.js<br>│  ├─ extra.js<br>│  └─ worker.js — optional: future client-side features<br>└─ templates<br>  └─ index.html<br><br>Project structure for PasteToTable:<br><code>render/</code> — core rendering modules for tables, HTML assembly, sanitization, and conversion.<br><code>assets/</code> — CSS, JS, and worker scripts for styling, interactivity, saving, and list management.<br><code>templates/</code> — contains the main HTML template for rendering outputs.<br><code>groups.json</code> — stores precomputed table groups or indexes.<br>Root scripts (<code>main.py</code>, <code>server.py</code>, <code>html_renderer.py</code>, <code>utils.py</code>) coordinate the conversion workflow and backend operations. </td></tr></tbody></table></div><div class="row-count">Rows: 1</div></div><div class="table-caption" id="Table2" data-table="Docu_0086_02" style="margin-top:2mm;margin-left:3mm;"><strong>Table 2 • Script 1 Nov 2025</strong></div>
<div class="table-wrapper" data-table-id="table-2"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by **Technical Breakdown**"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>Technical Breakdown</strong></div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown"> <strong>Batch 1 — Algorithm (concise)</strong><br>All algorithm flows are presented as ordered steps and weight maps rather than literal function syntax;<br>verified for consistency with the supplied <code>script.list.js</code>.<br>This blueprint is the canonical plan I will implement when producing code. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Role / Purpose (concise)</strong><br><code>script.list.js</code> discovers saved files from configured endpoints,<br>canonicalizes records, groups items, resolves captions,<br>presents a searchable modal with persistent group rows,<br>and sequentially fetches selected files into the paste area<br>while safely attempting caption injection and delegated copy behavior.<br>Defensive fallbacks and low-risk changes are required. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Primary inputs & outputs (concrete)</strong><br>Inputs: <code>LIVE_FILES_ENDPOINTS</code>, <code>GROUPS_JSON_CANDIDATES</code>, <code>LABELS_JSON_CANDIDATES</code>, DOM <code>#listBtn</code>, <code>#paste</code>, <code>#status</code>, and runtime hooks (<code>__tv_do_convert</code>, <code>convert</code>, <code>copyTableMarkdown</code>, <code>copyTablePlain</code>).<br>Outputs: modal overlay, <code>#paste</code> or <code>#page-area</code> populated, <code>window.__tv_last_source</code>, <code>window.__ptt_runtime_captions</code>,<br>status messages, and copy-to-clipboard side effects. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Top-level configuration & limits</strong><br>Use existing constants: <code>FILE_FETCH_TIMEOUT_MS=12000</code>, <code>INDEX_FETCH_TIMEOUT_MS=8000</code>, <code>GROUPS_FETCH_TIMEOUT_MS=4000</code>, <code>LABELS_FETCH_TIMEOUT_MS=3000</code>, <code>MAX_LIST_ITEMS=5000</code>.<br>Keep <code>ENABLE_INVERTED_INDEX</code> & <code>ENABLE_WORKER_SCORING</code> flags; do not change timeouts without test vectors.<br>Candidate/posting caps guard memory and CPU. </td></tr><tr><td data-label="Technical Breakdown"> <strong>High-level control flow (stepwise)</strong><br>1) Attach click handler to <code>#listBtn</code> at DOM ready.<br>2) On click, run discovery + labels/groups loads.<br>3) Normalize list → build <code>liveFiles</code>.<br>4) Build groups and captions.<br>5) Build file-level engine and modal UI with persistent group rows.<br>6) On search input: candidate selection → scoring → render matched groups.<br>7) On group/file click: close modal → fetch files sequentially → inject paste → trigger renderer → apply captions with retry loop. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Fetch primitives & semantics</strong><br><code>fetchWithTimeout</code> (discovery): <code>no-store</code>, <code>same-origin</code>, uses <code>AbortController</code> if present,<br>returns controlled <code>{ok,status,url,body,err}</code> object on timeout/failure.<br><code>fetchFileWithTimeout</code> (per-file): rejects on non-OK so caller can record failure notes.<br>Keep semantics unchanged. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Discovery probing algorithm (ordered)</strong><br>1) Iterate <code>LIVE_FILES_ENDPOINTS</code> in order.<br>2) For each endpoint call <code>fetchWithTimeout</code>.<br>3) If body parseable as JSON array or <code>{files:[]}</code> → normalize and return.<br>4) Else split lines → normalize if non-empty.<br>5) Stop at first usable source; preserve server precedence. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Normalization rules (canonicalization)</strong><br>Handle: array of strings, array of objects, <code>{files:[]}</code> wrapper.<br>Produce canonical items <code>{ name, url, path, description }</code>.<br>For strings return <code>{name:basename, url:null, path:original, description:&#x27;&#x27;}</code>.<br>Enforce <code>MAX_LIST_ITEMS</code>.<br>Preserve raw fields for <code>encodedSavedUrlFor</code>. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Label map normalization (key expansion)</strong><br>From <code>labels.json</code> produce many lookup keys: lowercase, <code>-</code>↔<code>_</code>, stripped leading zeros,<br>numeric padded/unpadded variants, and lowercase numeric forms.<br>Store a stable map for caption resolution. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Caption resolution (ordered attempts)</strong><br>Given base name produce candidate keys: base, base.toLowerCase(), dash→underscore,<br>underscore→dash, stripped numeric suffix, zero-padded numeric variants, spaces→underscore,<br>concatenated lowercase.<br>Return first matching entry from normalized label map. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Group key derivation rules</strong><br>If path contains <code>/</code> → use first segment;<br>else strip extension and split on <code>_</code>.<br>If trailing token numeric → drop it;<br>if ≥2 tokens remain use first two as group;<br>else fallback to base name.<br>Sanitize to non-empty; default <code>&#x27;Ungrouped&#x27;</code>. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Group aggregation steps</strong><br>Iterate normalized items → compute groupKey → create group record <code>{name, files:[]}</code> → push file with <code>{name, url, path, caption}</code>.<br>After build, sort keys (Ungrouped last) and annotate counts. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Lightweight file-level search engine (design overview)</strong><br><code>createFileSearchEngine</code> should: produce <code>lite[]</code> per item (fields: idx, groupIdx, fileIndex, name, caption, path, desc, full, tokens, triArr),<br>build capped inverted index token→postings, expose <code>candidatesForQuery(q)</code>, <code>scoreFiles(q,candidates)</code> (async promise),<br><code>lite</code> for debugging, and <code>dispose()</code> hook.<br>Keep posting cap (<code>MAX_POSTINGS</code>) and candidate cap. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Tokenization & trigram extraction (rules)</strong><br>Normalization pipeline (applied identically to docs & queries): Unicode NFD → strip combining marks → replace punctuation with spaces → lowercase → collapse whitespace → trim.<br>Tokenization: split on whitespace, further split camelCase/snake/dash into subtokens,<br>dedupe tokens, cap per doc (<code>TOKEN_LIMIT_PER_DOC</code>, suggest 40).<br>Trigrams: pad with two spaces and extract all 3-char slices into a Set. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Index construction sequence</strong><br>For each lite item: compute <code>full</code> = normalized concat (group name, desc, name, caption, path), tokens (and per-field prefixed tokens <code>c:</code>, <code>f:</code>), triSet.<br>For each token push doc id into <code>inverted[token]</code> up to <code>MAX_POSTINGS</code>. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Candidate union algorithm</strong><br>1) Normalize & tokenize query (detect quoted phrase).<br>2) For each query token fetch postings for token, <code>c:token</code>, <code>f:token</code>.<br>3) Union into <code>candidates</code> until <code>MAX_CANDIDATES_FROM_INDEX</code> cap.<br>4) If quoted phrase present, perform targeted substring checks on <code>full</code> (bypass index)<br>but limit scan to union or <code>FALLBACK_LINEAR_SCAN_LIMIT</code>.<br>5) If union empty fallback to linear scan of first <code>FALLBACK_LINEAR_SCAN_LIMIT</code> docs. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Scoring model (canonical, auditable)</strong><br>Scorer combines: exact-phrase boost; caption match boost; name match boost;<br>description match weight; token overlap weight; prefix bonuses for caption/name;<br>trigram overlap; short-query fuzzy DL adjustments; small length penalty.<br>Use explicit constants and return sorted results with numeric score. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Suggested concrete weights</strong><br>EXACT_PHRASE_BOOST=500;<br>CAPTION_MATCH_BOOST=220;<br>NAME_MATCH_BOOST=140;<br>DESCRIPTION_MATCH_WEIGHT=110;<br>TOKEN_MATCH_WEIGHT=28;<br>PREFIX_TOKEN_BOOST=18;<br>TRIGRAM_BASE_MULT=60;<br>SHORT_FUZZY_BASE=28;<br>LENGTH_PENALTY_SCALE=180;<br>MIN_SCORE_TO_SHOW=10. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Short fuzzy rules (safety)</strong><br>Only for query length ≤6: compute cheap DL / bounded edit-distance;<br>accept distance ≤2; award <code>SHORT_FUZZY_BASE - distance*8</code> to name and half to caption/desc.<br>Do not allow fuzzy to dominate other signals. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Trigram strategy</strong><br>Use trigram overlap ratio for medium/long queries;<br>score contribution = <code>TRIGRAM_BASE_MULT * overlap_ratio</code>.<br>Trigrams are cheaper than many edit-distance checks. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Acronym & prefix matching</strong><br>Build acronym from token initials of <code>name</code>/<code>caption</code>;<br>if query equals or prefixes acronym apply small <code>ACRONYM_BOOST</code> (~40) and highlight matched letters.<br>Prefix matches on caption/name tokens receive <code>PREFIX_TOKEN_BOOST</code>. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Highlighting approach</strong><br>Highlight UI uses normalized tokens to find candidates and then maps matches back into the original visible text for rendering (escape HTML first).<br>Strategy: exact-phrase → substring <code>&lt;mark&gt;</code>, token matches → token-based <code>&lt;mark&gt;</code>, fuzzy → nearest substring.<br>Keep CSS classes consistent.<br>Use the existing <code>highlight()</code> helper but ensure normalization mapping aligns with engine tokens. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Modal & persistent DOM strategy</strong><br>Build the modal overlay with header, move existing search input if present,<br>create persistent <code>groupControls[]</code> rows at initial render,<br>attach event listeners once per row.<br>On search only update <code>fileList</code> children to reduce reflows and GC.<br>Keep keyboard/focus/escape handling unchanged. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Search UI update flow (practical)</strong><br>Debounced input → <code>candidatesForQuery</code> → <code>scoreFiles</code> (promise).<br>If results present: build <code>byGroup</code> map and update matching groups' <code>fileList</code> slices (cap per-group results).<br>Hide groups with no matches.<br>Update expand/collapse button state.<br>Provide fallback linear-scan in scoring if engine errors. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Load & paste sequence (unchanged behavior)</strong><br>On selection close modal → <code>loadFilesSequentialAndHideForm(list)</code> fetches each file with <code>fetchFileWithTimeout</code>,<br>composes <code>accMarked</code> and <code>accRaw</code>, sets <code>#paste</code> or <code>#page-area</code>, triggers renderer (<code>__tv_do_convert</code> or <code>convert</code>),<br>then run caption-apply retry loop that finds <code>.table-caption</code> or Hn nodes and writes runtime captions.<br>Keep delays/retries but ensure safe sanitization of captions. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Instrumentation & dev harness</strong><br>Expose <code>window.__ptt_search_stats = { lastQueryMs, candidatesReturned, workerUsed, workerTimeMs, indexBuildMs, postingCountsSummary }</code>.<br>Provide a dev-only harness behind <code>ENABLE_SEARCH_DEBUG</code> exposing <code>candidatesForQuery(q)</code>, <code>scoreQuery(q)</code>, <code>highlightForCandidate(id,q)</code>, and <code>runAssertions()</code> that runs canonical tests.<br>Keep harness disabled in production. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Required tests / acceptance checks</strong><br>Examples: case-insensitive parity, quoted-phrase exactness, short-fuzzy tolerance (≤2), caption/name weighting,<br>multi-token additive scoring, acronym match, fallback linear-scan resilience, UI no-exceptions,<br><code>__ptt_search_stats</code> present.<br>Acceptance: pass tests + no DOM regressions + performance within candidate/worker caps. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Limits & defaults (safety)</strong><br>MAX_CANDIDATES_FROM_INDEX=800;<br>MAX_POSTINGS=1200;<br>TOKEN_LIMIT_PER_DOC=40;<br>TOP_RESULTS_TO_RENDER=200;<br>WORKER_THRESHOLD=250;<br>WORKER_TIMEOUT_MS=500;<br>FALLBACK_LINEAR_SCAN_LIMIT=200;<br>MIN_SCORE_TO_SHOW=10. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Rollback & feature flags</strong><br>Changes are behind existing flags: <code>ENABLE_INVERTED_INDEX</code> and <code>ENABLE_WORKER_SCORING</code>.<br>Rollback: set those flags false and reload to revert to prior path.<br>Keep changed code isolated in small well-commented blocks to allow quick revert. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Deliverables I will produce (on request)</strong><br>1) Patched <code>script.list.js</code> with in-file change comments.<br>2) Small dev harness & <code>__ptt_search_stats</code>.<br>3) 10-point verification checklist & sample run outputs.<br>4) Short changelog and rollback steps.<br>5) Suggested git commit message. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Why this format first? (brief)</strong><br>Giving this blueprint first lets you review algorithm, weights, limits and QA checks before any code is changed — minimizing risk and aligning on acceptance criteria.<br>I will always present this or your preferred variant before producing the patch. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Batch 2 — Implementation, Testing, Rollout & Verification (single-table)</strong> </td></tr><tr><td data-label="Technical Breakdown"> <strong>1) Patch Scope (concise)</strong><br>• Make surgical edits only where needed: <code>createFileSearchEngine</code>, <code>presentGroupsOnly</code> wiring (index → lite → engine), and caption/group lookup usage (<code>buildGroupsFromLive</code>, <code>resolveCaptionFor</code>, <code>presentGroupsOnly</code>).<br>• Add normalization/tokenization helpers and instrument <code>window.__ptt_search_stats</code>.<br>• Keep other UI flows (modal, paste, convert, copy) unchanged. </td></tr><tr><td data-label="Technical Breakdown"> <strong>2) File-by-file patch plan</strong><br>Stepwise, small commits (single-file when possible):<br>• Patch A: Add <code>normalize()</code> + <code>tokenize()</code> utilities and unit tests (dev-only).<br>• Patch B: Replace engine tokenization/trigram building with normalized pipeline; add posting caps and prefix tokens <code>c:</code>/<code>f:</code>.<br>• Patch C: Fix caption/description lookup — use <code>descriptions</code> map consistently and fallback to <code>g.description</code>.<br>• Patch D: Improve <code>candidatesForQuery</code> and <code>scoreFiles</code> per canonical scorer; add <code>__ptt_search_stats</code> updates.<br>• Patch E: Add dev harness (<code>runAssertions</code>) guarded by <code>ENABLE_SEARCH_DEBUG</code>.<br>Each patch: backup comment header, original checksum comment, and small changelog entry. </td></tr><tr><td data-label="Technical Breakdown"> <strong>3) Implementation notes & safety</strong><br>• Non-destructive: do not mutate incoming <code>groups</code> or <code>liveFiles</code> objects — create normalized clones used only in the engine.<br>• Fail-safe: on any unexpected error, engine falls back to simple substring scan so UI still functions.<br>• Guard heavy work with caps (MAX_CANDIDATES_FROM_INDEX, MAX_POSTINGS).<br>• Keep network/timeouts unchanged. </td></tr><tr><td data-label="Technical Breakdown"> <strong>4) Dev harness & debug API</strong><br>Expose (dev-only if <code>ENABLE_SEARCH_DEBUG</code> true):<br>• <code>window.__ptt_search_engine.candidatesForQuery(q)</code> → array of ids.<br>• <code>window.__ptt_search_engine.scoreQuery(q)</code> → scored results with score and hit metadata.<br>• <code>window.__ptt_search_engine.highlightForCandidate(id,q)</code> → highlight metadata (field, offsets, snippet).<br>• <code>window.__ptt_search_engine.runAssertions()</code> → runs canonical test suite and prints PASS/FAIL summary. </td></tr><tr><td data-label="Technical Breakdown"> <strong>5) Canonical test suite (runnable via runAssertions)</strong><br>Core functional tests (minimal set):<br>1) Case parity: <code>peak</code> vs <code>Peak</code> → same top results.<br>2) Caption boost: file with query in caption ranks above same token in description only.<br>3) Quoted phrase: <code>&quot;blue peak&quot;</code> only matches phrase occurrences.<br>4) Short fuzzy: <code>peack</code> matches <code>peak</code> when DL ≤2.<br>5) Token AND: <code>lake peak</code> prefers items containing both tokens.<br>6) Acronym: <code>mp</code> or <code>mt pk</code> surfaces "Mount Peak".<br>7) Non-word fallback: <code>c++</code> → no crash and returns safe fallback.<br>8) Highlight validation: highlights returned for top result fields.<br>9) Stats present: <code>window.__ptt_search_stats</code> updated.<br>10) UI integrity: modal remains functional after queries (no exceptions). </td></tr><tr><td data-label="Technical Breakdown"> <strong>6) Performance microbenchmarks (dev harness)</strong><br>Commands in harness: <code>benchmark({queries:[&#x27;peak&#x27;,&#x27;lake peak&#x27;], iterations:100})</code> → prints avg/p95 ms for candidate selection and scoring. Targets: main-thread <120ms for 2000 items (or worker used). Worker path reserved for > WORKER_THRESHOLD. </td></tr><tr><td data-label="Technical Breakdown"> <strong>7) Instrumentation & telemetry surface</strong><br><code>window.__ptt_search_stats</code> object fields:<br>• <code>lastQueryMs</code> (ms), <code>candidatesReturned</code>, <code>indexBuildMs</code>, <code>workerUsed</code> (bool), <code>workerTimeMs</code>, <code>postingCountsSummary</code> (top token counts), <code>lastQuery</code> (sanitized token list).<br>Dev-only <code>ENABLE_SEARCH_DEBUG</code> toggles console traces and normalized text dumps. </td></tr><tr><td data-label="Technical Breakdown"> <strong>8) Verification checklist (10 points)</strong><br>1) Normalization identical for docs & queries (diacritics removed).<br>2) Tokenizer splits camelCase/snake_case/dash properly.<br>3) MAX_POSTINGS enforced per token.<br>4) candidatesForQuery capped ≤ MAX_CANDIDATES_FROM_INDEX.<br>5) Quoted phrase bypass functions and returns exact matches.<br>6) Short fuzzy only applied for queries ≤6 chars and DL ≤2.<br>7) Trigram overlap contributes to score for medium/long queries.<br>8) Worker used only when item count > WORKER_THRESHOLD and fallback occurs on timeout.<br>9) Highlights map normalized hits back to original text accurately (no HTML injection).<br>10) Modal/expand/collapse/load/copy flows unchanged. </td></tr><tr><td data-label="Technical Breakdown"> <strong>9) Rollout & feature-flagged deployment</strong><br>• Keep changes behind <code>ENABLE_INVERTED_INDEX</code> and <code>ENABLE_WORKER_SCORING</code>.<br>• Stages: developer → QA (runAssertions + benchmarks) → small production cohort (if telemetry available) → full rollout.<br>• Quick rollback: flip flags false and reload. </td></tr><tr><td data-label="Technical Breakdown"> <strong>10) Monitoring & thresholds</strong><br>Track: <code>p95 query time &lt; 200ms</code>, <code>p95 highlight render &lt; 60ms</code>, <code>p95 candidatesReturned &lt; MAX_CANDIDATES_FROM_INDEX</code>, <code>worker failure rate &lt; 0.5%</code> (if used).<br>Raise alerts if candidate counts or timing deviate significantly from historical baseline. </td></tr><tr><td data-label="Technical Breakdown"> <strong>11) Revert instructions (immediate)</strong><br>1) In runtime console or config set: <code>window.ENABLE_INVERTED_INDEX = false; window.ENABLE_WORKER_SCORING = false;</code><br>2) Reload page. Old code path will be used.<br>3) If further rollback required, restore previous build from git and redeploy. </td></tr><tr><td data-label="Technical Breakdown"> <strong>12) Changelog & commit guidance</strong><br>Small descriptive commits per patch. Example commit messages:<br>• <code>search(engine): normalize/tokenize and build capped inverted index</code><br>• <code>search(ui): fix group description lookup and caption resolution</code><br>• <code>search(test): add dev harness and runAssertions</code><br>Include short changelog entry describing purpose, flags added/used, and quick revert steps. </td></tr><tr><td data-label="Technical Breakdown"> <strong>13) Acceptance criteria (explicit)</strong><br>• All runAssertions tests PASS.<br>• No console exceptions in UI flows (modal open/search/expand/load/copy).<br>• Performance: 2,000 items selection+scoring ≤120ms main-thread or worker used with main-thread <20ms.<br>• Highlights correct for name/caption/description across sample corpus.<br>• <code>window.__ptt_search_stats</code> produced and reasonable. </td></tr><tr><td data-label="Technical Breakdown"> <strong>14) Deliverables I will produce on request</strong><br>• Patched <code>script.list.js</code> (annotated blocks + original checksum).<br>• Dev harness added to same file guarded by <code>ENABLE_SEARCH_DEBUG</code>.<br>• <code>runAssertions()</code> results (printed).<br>• 10-point verification log with pass/fail outputs.<br>• Changelog and suggested git commit messages. </td></tr><tr><td data-label="Technical Breakdown"> <strong>15) Quick manual QA commands (to run in browser console)</strong><br>• <code>window.__ptt_search_stats</code> → inspect last stats.<br>• <code>window.__ptt_search_engine &amp;&amp; window.__ptt_search_engine.scoreQuery(&#x27;peak&#x27;)</code> → view top results.<br>• <code>window.__ptt_search_engine &amp;&amp; window.__ptt_search_engine.highlightForCandidate(0,&#x27;peak&#x27;)</code> → view highlight metadata.<br>• <code>window.__ptt_search_engine &amp;&amp; window.__ptt_search_engine.runAssertions()</code> → run canonical tests and report. </td></tr><tr><td data-label="Technical Breakdown"> <strong>16) Safety & security notes</strong><br>• Escape all caption text before injecting into DOM; strip <code>&lt;/script&gt;</code> and <code>--&gt;</code> sequences (existing sanitization used).<br>• Do not store or log raw document bodies in telemetry; only store counts, timings, and IDs. </td></tr><tr><td data-label="Technical Breakdown"> <strong>17) Small implementation caveats</strong><br>• Mapping normalized matches back to original text is heuristic — prefer token-boundary re-scan of original text rather than naive indexOf on original if punctuation/diacritics differ.<br>• Keep postings cap conservative to avoid memory blowups on pathological corpora. </td></tr><tr><td data-label="Technical Breakdown"> <strong>18) Final verification step before merge</strong><br>1) Run runAssertions() across representative corpora.<br>2) Benchmarks: 100 queries × 3 shards (small/medium/large).<br>3) Visual check on modal: highlights, expand/collapse, load behavior, copy delegation. <br>4) Approve and merge if criteria satisfied. </td></tr><tr><td data-label="Technical Breakdown"> <strong>19) Rollback & incident playbook (short)</strong><br>1) Flip flags false → reload → confirm UI behaves as prior. <br>2) If regression persists, revert commit and redeploy previous tag. <br>3) Record incident, test vectors, and create hotfix branch. </td></tr><tr><td data-label="Technical Breakdown"> <strong>20) Why this batch matters</strong><br>Batch 1 defined the algorithm and scoring. Batch 2 turns that into safe, testable code with clear QA gates, instrumentation, and a reversible rollout plan — minimizing risk while improving search relevance and stability. </td></tr></tbody></table></div><div class="row-count">Rows: 52</div></div><div class="table-caption" id="Table3" data-table="Docu_0086_03" style="margin-top:2mm;margin-left:3mm;"><strong>Table 3 • Enhancement 1</strong></div>
<div class="table-wrapper" data-table-id="table-3"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown"> <strong>Enhancement</strong><br> Delivering an enhancement plan that benchmarks against Evernote-style search features and proposes concrete, low-risk upgrades to the file-level engine, UI, and fetch flows. Goals: higher relevance, fielded queries, robust phrase/proximity/fuzzy support, scalable performance, and safe deployment path.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     </td></tr><tr><td data-label="Technical Breakdown"> <strong>Benchmark target: Evernote search behavior (summary)</strong><br>Evernote search is notable for: fielded queries (tag:, intitle:, created:, updated:), boolean operators (AND/OR/NOT), phrase and proximity search, prefix/wildcard, fuzzy matching and synonyms, strong recency and notebook/tag boosting, and responsive UX on large note collections via efficient inverted/positional indices and selective ranking (term frequency + field boosts + recency). Use these features selectively to raise relevance while keeping client-side resource constraints in mind.                                                                                                                                                                                                                                                     </td></tr><tr><td data-label="Technical Breakdown"> <strong>Enhancement objectives (concrete)</strong><br>1. Provide fielded search support (<code>name:</code>, <code>caption:</code>, <code>path:</code>).  <br>2. Support quoted phrases and proximity-like behavior (phrase-first, then loosen).  <br>3. Add prefix/wildcard support for filename and caption tokens.  <br>4. Add simple fuzzy matching (bounded edit distance or n-gram) with configurable tolerance.  <br>5. Implement field-boosted ranking (caption > name > path > description).  <br>6. Add recency or heuristic boosting for recently-loaded lists (if metadata present).  <br>7. Keep core inverted index but add positional information for phrase/proximity.  <br>8. Provide a small query parser to handle boolean and fielded expressions.  <br>9. Offer server-side search fallback when index size or device capability exceeds thresholds. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Design constraints & tradeoffs</strong><br>- Client-side memory and CPU are limited on low-end devices.  <br>- Avoid heavy-weight full-text engines.  <br>- Prefer multi-tier approach: lightweight client index + optional server search for heavy lifting.  <br>- Keep deterministic, auditable scoring so UX changes are explainable.  <br>- Preserve existing UI and incremental rollout.                                                                                                                                                                                                                                                                                                                                                                                                                                    </td></tr><tr><td data-label="Technical Breakdown"> <strong>Proposed index schema (compact)</strong><br>Keep <code>lite</code> array but extend each item with: <code>tokens</code> (as now), <code>triArr</code> (trigrams), <code>positions</code> map <code>{ token -&gt; [positions] }</code> truncated per-token to cap postings, and <code>fieldTokens</code> sets for <code>f:name</code>, <code>c:caption</code>, <code>p:path</code>, <code>d:desc</code>. Store <code>lastModified</code> or numeric ordinal if available for recency boost. Keep posting lists inverted map keyed by token and prefixed field tokens for fast field queries. Cap per-token postings and position arrays to control memory.                                                                                                                                                                                                                                                                                                    </td></tr><tr><td data-label="Technical Breakdown"> <strong>Query model (user-facing features)</strong><br>- Fielded expressions: <code>name:report caption:&quot;annual summary&quot;</code>.  <br>- Boolean operators: <code>tag:finance AND (report OR summary) NOT draft</code>.  <br>- Phrase search: <code>&quot;annual summary&quot;</code> exact contiguous phrase.  <br>- Proximity-like fallback: <code>&quot;annual summary&quot;~3</code> semantics (prefer exact then near matches).  <br>- Prefix/wildcard in filename: <code>name:rep*</code> -> prefix search on name tokens.  <br>- Fuzzy operator: <code>summary~</code> or implicit fuzzy for short typos.  <br>- Bare token queries remain tokenized ANDed by default with configurable OR mode.                                                                                                                                                                                                                         </td></tr><tr><td data-label="Technical Breakdown"> <strong>Query parser (ordered responsibilities)</strong><br>1. Tokenize raw query into tokens, phrases, operators, field qualifiers, and modifiers.  <br>2. Build an AST: boolean nodes, fielded nodes, phrase nodes, proximity modifiers, fuzzy flags.  <br>3. Validate and normalize node values (lowercase, strip punctuation).  <br>4. Emit candidate token sets and field-specific searches for engine to handle.  <br>5. Fallback to simple token search on parse errors.                                                                                                                                                                                                                                                                                                                                                          </td></tr><tr><td data-label="Technical Breakdown"> <strong>Candidate retrieval (execution plan)</strong><br>1. From AST collect union of posting lists for query tokens and field-token variants.  <br>2. For phrase nodes consult positional <code>positions</code> maps to filter candidates with contiguous positions (exact) or positions within proximity window (near).  <br>3. For prefix tokens perform postings union for tokens that share prefix (use token prefix map or scan keys; maintain a prefix-tree/trie if item set large).  <br>4. For fuzzy tokens request fuzzy candidates from trigram overlap or bounded edit distance on small candidate pool (first pass).  <br>5. Cap overall candidate set with <code>MAX_CANDIDATES</code> then pass to scoring.                                                                                                                                    </td></tr><tr><td data-label="Technical Breakdown"> <strong>Ranking enhancements (weight map & order)</strong><br>Compute final score as combination of:<br>- Field-boosted match score: matches in caption × high boost, name × medium, path/desc × small.  <br>- Term frequency (within item) normalized by document length.  <br>- Exact phrase bonus (very large).  <br>- Proximity bonus for phrase tokens appearing close.  <br>- Trigram or fuzzy-match penalty when edit-distance used (lower than exact).  <br>- Recency boost (if timestamp available) as small additive function (e.g., exponential decay).  <br>- Popularity/usage boost (optional telemetry-based).  <br>Design note: implement as linear combination with tunable weights and normalize components to predictable ranges. Keep weights conservative to avoid single-signal dominance.                          </td></tr><tr><td data-label="Technical Breakdown"> <strong>Positional index details (practical cap rules)</strong><br>- Store positions only for tokens appearing in name and caption fields.  <br>- For each token keep up to N positions (e.g., 8) to enable phrase/proximity checks while limiting memory.  <br>- For posting lists store item id and optional field bitmask to indicate which fields the token occurs in.  <br>- For prefix/prefix-trie, maintain a small prefix map for high-frequency short prefixes only.                                                                                                                                                                                                                                                                                                                                                            </td></tr><tr><td data-label="Technical Breakdown"> <strong>Fuzzy matching strategy (cost-aware)</strong><br>1. Use trigram overlap as a cheap prefilter for fuzzy candidates.  <br>2. Apply bounded edit-distance only on candidate set below a threshold.  <br>3. Expose per-token fuzzy threshold dependent on token length (e.g., allow distance 1 for length ≤4, 2 for length ≤8).  <br>4. Penalize fuzzy matches in final score but accept them to increase recall.  <br>Rationale: edit-distance is expensive; using trigram prefilter keeps cost under control.                                                                                                                                                                                                                                                                                                                      </td></tr><tr><td data-label="Technical Breakdown"> <strong>Prefix & wildcard handling (efficient approach)</strong><br>- Implement prefix search by scanning token dictionary keys with the prefix. Use a compact sorted array or an adaptive trie. For small indexes a linear scan over unique tokens is acceptable.  <br>- Limit prefix expansion by a maximum matched-token count per prefix (e.g., 50) to avoid explosion.  <br>- Wildcard <code>*</code> allowed only at end (suffix wildcard) to simplify. Avoid full regex wildcard to limit CPU.                                                                                                                                                                                                                                                                                                                                               </td></tr><tr><td data-label="Technical Breakdown"> <strong>Field boosts and query-time scoring composition (example weights)</strong><br>Suggested starting weight scheme (relative, tune with tests): caption boost 4.0, name boost 2.5, path boost 1.0, description boost 0.8; exact phrase bonus 6.0; per-token match base 1.0; trigram overlap scaled 0..2.0; proximity multiplier 0.5–2.0; recency additive 0..1.0. Document-level TF normalization scale to 0..3.0. Keep these as config constants for A/B tuning.                                                                                                                                                                                                                                                                                                                                                                    </td></tr><tr><td data-label="Technical Breakdown"> <strong>UI changes (minimal and progressive)</strong><br>- Add small query syntax hint inside search input (placeholder), e.g., <code>Support: &quot;quoted phrase&quot;, field:name, OR, NOT</code>.  <br>- Add a toggle for "Fuzzy" and "Exact only" for non-technical users.  <br>- Display matched field badges in file rows (e.g., <code>Matched: caption</code>), and highlight phrase matches.  <br>- Add a lightweight explanation UI on first use describing fielded search.                                                                                                                                                                                                                                                                                                                                                                                    </td></tr><tr><td data-label="Technical Breakdown"> <strong>Server-side fallback & hybrid model</strong><br>When <code>lite.length</code> or device capability exceeds thresholds, prefer delegating complex parsing and scoring to a server endpoint:  <br>- Client sends parsed AST or simple tokens + field hints.  <br>- Server returns top-k ids and scores.  <br>Advantages: unlimited index size, advanced scoring (BM25/BERT later), fast client UX. Ensure server endpoint respects same-origin/CORS and is optional.                                                                                                                                                                                                                                                                                                                                                                          </td></tr><tr><td data-label="Technical Breakdown"> <strong>Migration plan & rollout (phased)</strong><br>1. Phase 0: Add query parser and fielded token support but keep existing scoring path as default. Smoke test.  <br>2. Phase 1: Add positional info and phrase/proximity filtering. Expose toggle for phrase sensitivity.  <br>3. Phase 2: Add fuzzy prefilter via trigrams and bounded edit-distance fallback.  <br>4. Phase 3: Add recency boosting and optional server-side search.  <br>At each phase run A/B tests on sample datasets and monitor metrics. Provide rollback via feature flags.                                                                                                                                                                                                                                                                                 </td></tr><tr><td data-label="Technical Breakdown"> <strong>Testing and evaluation plan (metrics & vectors)</strong><br>Measure: precision@5, recall@50, latency median/95th, CPU time for scoring, memory footprint, and perceived UI responsiveness. Test vectors: exact phrase queries, fielded queries, prefix queries, typo scenarios, heavy index sizes (1k, 10k, 50k), and device matrix (desktop, mid-phone, low-end phone). Use synthetic datasets derived from real filenames and captions to measure ranking shifts.                                                                                                                                                                                                                                                                                                                                                              </td></tr><tr><td data-label="Technical Breakdown"> <strong>Concrete unit/integration tests (examples)</strong><br>- Field parsing: <code>name:report caption:&quot;annual 2024&quot;</code> → AST fields recognized.  <br>- Phrase exactness: <code>&quot;annual summary&quot;</code> matches only when tokens contiguous given positions.  <br>- Proximity fallback: <code>&quot;annual summary&quot;~3</code> matches when tokens within window ≤3.  <br>- Prefix expansion: <code>name:rep*</code> returns items with name tokens starting <code>rep</code>.  <br>- Fuzzy: <code>summry</code> should match <code>summary</code> with lower score.  <br>- Combined: <code>caption:&quot;budget 2023&quot; OR name:budget</code> ranking shows caption matches above name-only.  <br>- Performance: scoring on 10k items returns top-50 in <200ms on mid-range device.  <br>Define expected pass thresholds and include them in CI.                                                                                       </td></tr><tr><td data-label="Technical Breakdown"> <strong>Implementation roadmap (tasks & priorities)</strong><br>High priority (P0): query parser, fielded token mapping, field-boosted scoring, phrase exact match using limited positions, simple prefix support, UI hints.  <br>Medium (P1): trigram prefilter fuzzy, proximity scoring, recency boost, matched-field badges.  <br>Low (P2): full wildcard support, server search endpoint, more advanced rankers (BM25), synonyms/thesaurus integration.                                                                                                                                                                                                                                                                                                                                                                              </td></tr><tr><td data-label="Technical Breakdown"> <strong>Operational & safety notes</strong><br>- Keep per-token and position caps to avoid memory blowouts.  <br>- Expose feature flags to disable fuzzy or phrase features if performance issues arise.  <br>- Avoid sending raw index or file contents in telemetry.  <br>- When a server-side search is introduced secure endpoints and avoid exposing internal file content without auth.                                                                                                                                                                                                                                                                                                                                                                                                                                            </td></tr><tr><td data-label="Technical Breakdown"> <strong>Fallbacks and user controls</strong><br>- Provide UI toggles: "Exact mode", "Fuzzy mode", and "Use server search".  <br>- When device is flagged low-memory disable positional storage and enable server search.  <br>- If query parser fails, fallback to token-based search as current behavior.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               </td></tr><tr><td data-label="Technical Breakdown"> <strong>Metrics to capture for tuning</strong><br>- Query distribution (token lengths, operator use).  <br>- Average candidate set size per query.  <br>- Scoring latency percentiles.  <br>- Correction rate: how often fuzzy matching yields useful results (user-accepted).  <br>- Caption-apply success after enabling new API flows.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </td></tr><tr><td data-label="Technical Breakdown"> <strong>Example scoring scenarios (explainable outcomes)</strong><br>- Query: <code>caption:&quot;Q1 report&quot; name:summary</code> → top hits with <code>caption</code> phrase exact match rank highest; name matches secondary.  <br>- Query: <code>repot</code> (typo) → fuzzy/trigram picks <code>report</code> items; user toggles off fuzzy to force exact.  <br>- Query: <code>path:invoices</code> → path hits appear but with lower boost unless caption/name contain tokens.                                                                                                                                                                                                                                                                                                                                                                                                                  </td></tr><tr><td data-label="Technical Breakdown"> <strong>Resource estimates & thresholds</strong><br>- Positional map per-item average extra memory ~ tokens_per_item × avg_positions × index_entry_size. For small items (name+caption) expect low multiplier. Recommend enabling positions only for caption and name fields to keep footprint low.  <br>- Worker offload threshold default: <code>lite.length &gt;= 250</code> (user-tunable).  <br>- Server-side fallback threshold default: <code>lite.length &gt;= 5000</code> or device memory low.                                                                                                                                                                                                                                                                                                                                                             </td></tr><tr><td data-label="Technical Breakdown"> <strong>Compatibility with existing codebase</strong><br>- Extend <code>lite</code> items and inverted map; keep existing <code>c:</code> and <code>f:</code> conventions.  <br>- Reuse existing trigram functions for fuzzy prefilter.  <br>- Add <code>queryParser</code> module that returns AST for existing search flow; keep scoring function as a pluggable strategy.  <br>- Preserve current UI and gradually expose enhanced features.                                                                                                                                                                                                                                                                                                                                                                                                                                      </td></tr><tr><td data-label="Technical Breakdown"> <strong>Risk assessment & mitigation</strong><br>- Risk: main-thread stalls from phrase/positional checks. Mitigation: run heavy checks in WebWorker or cap candidates.  <br>- Risk: fuzzy explosion on short tokens. Mitigation: increase min token length for fuzzy and cap expansions.  <br>- Risk: user confusion with complex syntax. Mitigation: UI hints and simple toggles.                                                                                                                                                                                                                                                                                                                                                                                                                                                      </td></tr><tr><td data-label="Technical Breakdown"> <strong>Acceptance criteria (for rollout)</strong><br>1. Feature parity for basic fielded and phrase queries with existing tests passing.  <br>2. Precision@5 improved or stable for benchmark queries.  <br>3. 95th percentile scoring latency within target on supported device classes.  <br>4. Memory usage within acceptable bounds for typical lists.  <br>5. No new client-side exceptions under stress tests.                                                                                                                                                                                                                                                                                                                                                                                                                    </td></tr><tr><td data-label="Technical Breakdown"> <strong>Next actionable steps for you (pick one)</strong><br>1. <strong>Implement P0</strong>: I can produce a patch diff that adds the query parser, field tokens, and field-boosted ranking (no raw code restriction waived only if you request a patch).  <br>2. <strong>Prototype</strong>: I can generate a minimal worker-based scoring prototype and test vectors.  <br>3. <strong>Server design</strong>: I can outline a small server search API and mapping to client AST.  <br>4. <strong>Run A/B plan</strong>: I can generate a test matrix and telemetry plan for canary.  <br>Reply with choice: <strong>P0</strong>, <strong>Prototype</strong>, <strong>Server</strong>, or <strong>AB</strong> and I will proceed.                                                                                                                                                                                                             </td></tr></tbody></table></div><div class="row-count">Rows: 28</div></div><div class="table-caption" id="Table4" data-table="Docu_0086_04" style="margin-top:2mm;margin-left:3mm;"><strong>Table 4 • Enhancement 2</strong></div>
<div class="table-wrapper" data-table-id="table-4"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown"> <strong>World-class search</strong><br>World-class search spans two families: classical lexical ranking (inverted index + TF/IDF/BM25 + field boosts + positional data) and semantic/dense retrieval (embeddings + ANN). For PasteToTable pick a hybrid: keep a lightweight client lexical index for instant filtering and UI, offload BM25 and dense/semantic ranking to a server (or worker) when dataset or features exceed device capacity. This yields best tradeoff of relevance, latency, and device safety.                                                                                                                                                                                                                                                                                                                       </td></tr><tr><td data-label="Technical Breakdown"> <strong>Scope of comparison</strong><br>We compare practical algorithms and systems relevant to this codebase: TF–IDF, BM25 (and BM25F), positional indices for phrase/proximity, n-gram/trigram indexing, Levenshtein/fuzzy, phonetic/stemming, learning-to-rank, lexical engines (Lucene/Elasticsearch/Solr), lightweight libraries (Whoosh, Lunr), vector/semantic search (FAISS, HNSW, Annoy, ScaNN), hybrid rankers combining lexical+dense, and retrieval-augmented models. For each we list pros, cons, resource profile, and suitability for client vs server.                                                                                                                                                                                                                                                                       </td></tr><tr><td data-label="Technical Breakdown"> <strong>TF–IDF (classic)</strong><br>Overview: term-frequency × inverse-document-frequency scoring combined with cosine similarity. Pros: simple, explainable, cheap to compute offline. Cons: weak on short queries and phrase proximity; single-term weighting brittle. Resource: small index, simple linear algebra. Suitability: server or client for small indexes; serves as baseline.                                                                                                                                                                                                                                                                                                                                                                                                                                                 </td></tr><tr><td data-label="Technical Breakdown"> <strong>BM25 (state-of-the-art lexical)</strong><br>Overview: probabilistic ranking function that improves TF–IDF with saturation and length normalization. Pros: robust relevance, field boosting via BM25F, well-understood. Cons: needs term frequencies and doc lengths, not semantic. Resource: moderate CPU for scoring; efficient with inverted index. Suitability: primary server-side ranker; for small client indexes BM25 can run in JS if candidate set small or in a WebWorker.                                                                                                                                                                                                                                                                                                                                                 </td></tr><tr><td data-label="Technical Breakdown"> <strong>BM25F (fielded BM25)</strong><br>Overview: BM25 extended to multiple fields with independent weights. Pros: maps directly to our <code>caption &gt; name &gt; path</code> requirement; boosts fields systematically. Cons: slightly more storage for per-field stats. Suitability: server-side or advanced client when positions/field tokens stored.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 </td></tr><tr><td data-label="Technical Breakdown"> <strong>Positional indices & proximity/phrase search</strong><br>Overview: store token positions to verify exact phrases and compute proximity scores. Pros: precise phrase matching and proximity boosting. Cons: positions increase index size. Suitability: essential for phrase queries; store positions for name/caption only to keep footprint manageable.                                                                                                                                                                                                                                                                                                                                                                                                                                                                             </td></tr><tr><td data-label="Technical Breakdown"> <strong>n-gram / trigram indexing (fuzzy + prefix)</strong><br>Overview: index overlapping n-grams for fuzzy substring matching and typo tolerance. Pros: good for short tokens and typo resilience; cheap fuzzy prefilter. Cons: larger index, can produce noisy matches for tiny tokens. Suitability: client-side fuzzy prefilter; combine with edit-distance or BM25 on server for final ranking.                                                                                                                                                                                                                                                                                                                                                                                                                                         </td></tr><tr><td data-label="Technical Breakdown"> <strong>Edit-distance (Levenshtein / Damerau-Levenshtein)</strong><br>Overview: exact string distance metric for typos. Pros: accurate for small/token edits. Cons: O(n*m) cost; expensive at scale. Suitability: use only on candidate set filtered by trigram overlap or token-postings; not for full-index runs on client.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </td></tr><tr><td data-label="Technical Breakdown"> <strong>Stemming / Lemmatization / Phonetic (Soundex, Metaphone)</strong><br>Overview: morphological normalization or phonetic matching. Pros: improves matching across inflections and phonetic typos. Cons: language-dependent and adds complexity. Suitability: optional for captions with natural language; avoid heavy lemmatizers on client.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           </td></tr><tr><td data-label="Technical Breakdown"> <strong>Suffix arrays / suffix trees / full-text substring indices</strong><br>Overview: support arbitrary substring/wildcard searches. Pros: powerful substring queries. Cons: heavy memory and complex to maintain. Suitability: not recommended for client; server-side only when substring search is business-critical.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  </td></tr><tr><td data-label="Technical Breakdown"> <strong>Boolean operators and query parsing</strong><br>Overview: parse expression syntax (AND/OR/NOT, field: qualifiers). Pros: expressive power; common in Evernote and Lucene. Cons: parser complexity; UX burden. Suitability: implement lightweight parser client-side; fallback to token-search on parse error.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </td></tr><tr><td data-label="Technical Breakdown"> <strong>Learning-to-Rank (LTR)</strong><br>Overview: ML models combine many features (BM25, recency, click data). Pros: superior ranking when trained. Cons: needs labeled data and infrastructure. Suitability: long-term server-side enhancement for production; not for client.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           </td></tr><tr><td data-label="Technical Breakdown"> <strong>Vector / Dense Retrieval (embeddings + ANN)</strong><br>Overview: represent text as vectors (e.g., sentence transformers). Use ANN indexes (HNSW, FAISS, Annoy, ScaNN) for scalable nearest-neighbor search. Pros: semantic matching, good for synonyms and paraphrase. Cons: heavy (embeddings + ANN storage), requires model for embeddings, can surface false positives without lexical filter. Suitability: server-side hybrid with lexical prefilter; consider if semantic relevance matters (e.g., user searches meaningfully beyond filenames and captions).                                                                                                                                                                                                                                                                  </td></tr><tr><td data-label="Technical Breakdown"> <strong>Hybrid Lexical + Dense Ranking</strong><br>Overview: combine BM25 for lexical signals and dense scores for semantics; then merge via linear combination or learning-to-rank. Pros: best of both worlds. Cons: more infrastructure and tuning. Suitability: recommended server-side for high-quality results. Keep client-side for lexical filtering and UI.                                                                                                                                                                                                                                                                                                                                                                                                                                                                          </td></tr><tr><td data-label="Technical Breakdown"> <strong>Systems: Lucene / Elasticsearch / Solr</strong><br>Overview: full-featured servers offering BM25, fielded search, phrase, proximity, analyzers, synonyms, and LTR plugins. Pros: mature, scalable, battle-tested. Cons: operational overhead, requires server deployment and CORS/auth handling. Suitability: ideal server-side backend for production-grade search.                                                                                                                                                                                                                                                                                                                                                                                                                                                                 </td></tr><tr><td data-label="Technical Breakdown"> <strong>Lightweight JS search libs (Lunr, FlexSearch, Fuse.js)</strong><br>Overview: in-browser search libraries offering TF-IDF-ish ranking (Lunr/FlexSearch) or fuzzy search (Fuse). Pros: easy to embed, no server. Cons: limited features and scaling. Suitability: good for small datasets (<1k items) when server not available. FlexSearch is fastest among JS libs; Lunr has BM25-like scoring.                                                                                                                                                                                                                                                                                                                                                                                                                                      </td></tr><tr><td data-label="Technical Breakdown"> <strong>ANN libraries & HNSW (client vs server)</strong><br>Overview: HNSW (hierarchical small-world graphs) implemented widely (nmslib, hnswlib, FAISS HNSW), provides sub-linear NN. Pros: fast nearest neighbor at scale. Cons: memory and complexity. Suitability: server-side; or edge-worker if resourceful. Not feasible on constrained browsers for large indexes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                   </td></tr><tr><td data-label="Technical Breakdown"> <strong>Operational tradeoffs & guidance</strong><br>1. <strong>Client-first constraint:</strong> keep client index small, avoid heavy per-token position arrays except for name/caption.  <br>2. <strong>Server fallback:</strong> shift BM25/BM25F, positional proximity and dense retrieval to server when index grows.  <br>3. <strong>Hybrid workflow:</strong> client produces a compact query AST, filters candidate set quickly, asks server for top-k if needed.  <br>4. <strong>Feature gating:</strong> expose toggles for fuzzy, phrase, and semantic modes; disable expensive features by default on low-end devices.                                                                                                                                                                                                                                                               </td></tr><tr><td data-label="Technical Breakdown"> <strong>Concrete enhancement recommendations (prioritized)</strong><br>1. <strong>P0 (high impact, low cost):</strong> Add BM25F-style scoring with field boosts and positional checks for exact phrase where available. Keep trigram prefilter for fuzzy fallbacks. Implement as an optional WebWorker or server endpoint.  <br>2. <strong>P1 (moderate):</strong> Add positional storage for name/caption (cap positions per token) to enable phrase/proximity and improve phrase scoring.  <br>3. <strong>P2 (medium complexity):</strong> Add trigram-based fuzzy prefilter and bounded edit-distance on candidate subset.  <br>4. <strong>P3 (long-term):</strong> Introduce hybrid dense retrieval: compute embeddings server-side and combine BM25 + dense score via linear blend or LTR.  <br>5. <strong>P4 (ops):</strong> Integrate with Elasticsearch or lightweight server for large deployments. </td></tr><tr><td data-label="Technical Breakdown"> <strong>Example hybrid flow (recommended architecture)</strong><br>1. Client tokenizes query, applies local inverted index to produce up to N candidates.  <br>2. Client requests server search with parsed AST and candidate id hints (or just tokens) if candidate set small; else request server top-k BM25.  <br>3. Server runs BM25F ± dense ranking, returns top-k ids + scores + field match metadata.  <br>4. Client displays results immediately if server responds fast; fallback to client ranked results if server unreachable.                                                                                                                                                                                                                                                                                                  </td></tr><tr><td data-label="Technical Breakdown"> <strong>Evaluation metrics & benchmarks to use</strong><br>- Precision@k (k=5), Recall@50, NDCG@10.  <br>- Latency median and p95 for query roundtrip (target p95 < 200–300ms for good UX).  <br>- CPU time for scoring and memory footprint of index on representative devices.  <br>- User acceptance: correction rate when fuzzy enabled.  <br>Establish datasets: synthetic (large), production-like sample (filenames + captions), and human labeled relevance judgments for a small golden set.                                                                                                                                                                                                                                                                                                                                        </td></tr><tr><td data-label="Technical Breakdown"> <strong>Testing plan (practical)</strong><br>1. Unit tests for parser, tokenization, trigram prefilter and edit-distance.  <br>2. Microbenchmarks: scoring time per 100/1k/10k candidates.  <br>3. A/B test: baseline inverted engine vs BM25F vs BM25F+fuzzy vs hybrid dense+lexical on golden queries.  <br>4. Device matrix: run on desktop and low-end phone emulators to validate latency/memory.                                                                                                                                                                                                                                                                                                                                                                                                                                       </td></tr><tr><td data-label="Technical Breakdown"> <strong>Risks and mitigations</strong><br>- Risk: main-thread jank from heavy scoring. Mitigation: WebWorker offload, candidate caps, and server fallback.  <br>- Risk: false positives from dense models. Mitigation: require lexical match threshold or combine scores conservatively.  <br>- Risk: telemetry leakage. Mitigation: avoid sending document content; send only anonymized metrics.                                                                                                                                                                                                                                                                                                                                                                                                                                           </td></tr><tr><td data-label="Technical Breakdown"> <strong>Suggested roadmap with timeboxed steps</strong><br>Week 0: Implement query parser and BM25F in a worker; add field boosts.  <br>Week 1: Add trigram prefilter + edit-distance fallback.  <br>Week 2: Add positional indices for name/caption and phrase/proximity checks.  <br>Week 3: Benchmark and tune weights; run A/B tests.  <br>Week 4+: Prototype server BM25+ANN hybrid if semantic matches required.                                                                                                                                                                                                                                                                                                                                                                                                                       </td></tr><tr><td data-label="Technical Breakdown"> <strong>Final recommendation (short)</strong><br>Adopt BM25F as the core ranking signal, add trigram fuzzy prefilter and positional support for phrase queries, and implement scoring in a WebWorker or server for indexes >250 items. Only adopt dense/ANN semantic search once user needs exceed lexical limits and you can host an embeddings service.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </td></tr><tr><td data-label="Technical Breakdown"> <strong>Concrete next steps I can perform for you</strong><br>- Produce a non-code design doc for BM25F and worker integration.  <br>- Generate test vectors and tuning matrices for weight calibration.  <br>- Produce a patch diff implementing query parser + BM25F scoring in worker (requires permission to include code).  <br>Reply with <strong>“Design doc”</strong>, <strong>“Test vectors”</strong>, or <strong>“Patch”</strong> to pick next action.                                                                                                                                                                                                                                                                                                                                                                                                                    </td></tr><tr><td data-label="Technical Breakdown"> <strong>Self-verification (10× checks applied)</strong><br>1. Confirmed taxonomy of algorithms covers lexical, fuzzy, and dense approaches.  <br>2. Verified BM25F suitability for fielded search tie-in with labels/captions.  <br>3. Checked resource implications for client vs server.  <br>4. Ensured positional and trigram tradeoffs enumerated.  <br>5. Validated hybrid flow practicality and fallbacks.  <br>6. Cross-checked evaluation metrics and test plans.  <br>7. Re-run mental proof of suggested roadmap sequence.  <br>8. Verified mitigation strategies for common risks.  <br>9. Ensured recommendations align with prior conversation constraints.  <br>10. Confirmed no raw code blocks present and all algorithmic steps are described conceptually.                                                                </td></tr></tbody></table></div><div class="row-count">Rows: 27</div></div><script src="assets/xlsx.full.min.js?v=1758605028" defer></script>
<script src="assets/script.js?v=1759748863" defer></script>
<script src="assets/worker.js?v=1758331710" defer></script>
<script>
(function(){
  const template = "{table}_{date}";
  const userVal = "";
  const hrefPrefix = "assets";
  function formatName(tableName) {
    const date = (new Date()).toISOString().slice(0,10);
    return template.replace('{table}', tableName).replace('{date}', date).replace('{user}', userVal);
  }
  const btn = document.getElementById('exportBtn');
  if(btn) {
    btn.addEventListener('click', async function() {
      try {
        const html = document.documentElement.outerHTML;
        if(html.length > 2000000) { alert('Export refused: html too large'); return; }
        if(window.Worker) {
          const workerUrl = (function(){ try{ return hrefPrefix + '/worker.js'; }catch(e){ return null; } })();
          if(workerUrl) {
            try {
              const worker = new Worker(workerUrl);
              worker.postMessage({html: html, format: 'pdf'});
              worker.onmessage = function(e) { console.log('worker:', e.data); alert('Worker replied: '+(e.data.msg||e.data.status)); };
            } catch(err) { console.warn('Worker create failed', err); alert('Worker not available'); }
          } else { alert('Worker not available'); }
        } else { alert('Export worker not supported in this environment.'); }
      } catch(err) { console.warn('Export failed', err); alert('Export worker not available. See console for details.'); }
    });
  }
})();
window.addEventListener('load', function(){ try{ document.querySelectorAll('.table-wrapper').forEach(function(e){ e.style.opacity='1'; }); }catch(e){} });
</script>
</div>
</body>
</html>