<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='utf-8'><meta name='viewport' content='width=device-width,initial-scale=1'>
<title>Tables Viewer v2.1</title>
<style>:root{--main-header-height:56px;} .table-wrapper{opacity:0;min-height:24px;transition:opacity .12s linear}</style>
<link rel="stylesheet" href="assets/style.css?v=1769960840">
<link rel="stylesheet" href="assets/overrides.css?v=1771304648">
</head><body>
<div id="tables-viewer" role="region" aria-label="Tables viewer">
<div id="stickyMainHeader">
<div id="tv-header">
<div><h1>Tables Viewer v2.1</h1></div>
<div style="display:flex;gap:8px;align-items:center;">
<input id="searchBox" class="tv-search" type="search" placeholder="Search" aria-label="Search tables" />
<button id="modeBtn" type="button" onclick="toggleMode()" aria-label="Toggle theme">Theme</button>
<button id="toggleAllBtn" type="button" aria-label="Toggle all" onclick="toggleAllTables()">Collapse All Tables</button>
<button id="copyAllPlainBtn" class="copy-btn" type="button" onclick="copyAllTablesPlain()" aria-label="Copy all tables as plain text">Copy All Tables (Plain Text)</button>
<button id="copyAllTablesBtn" class="copy-all-btn" type="button" onclick="copyAllTablesMarkdown()" aria-label="Copy All Tables (Markdown)">Copy All Tables (Markdown)</button>
<button id="copyAllMdBtn" style="display:none" aria-hidden="true"></button>
<button id="resetAllBtn" type="button" onclick="resetAllTables()" aria-label="Reset All Tables">Reset All Tables</button>
</div></div>
<script>
(function(){
  function ensureDelegation(){
    try {
      var vis = document.getElementById('copyAllTablesBtn');
      var alias = document.getElementById('copyAllMdBtn');
      if(!vis || !alias) return;
      alias.addEventListener = function(type, listener, options){
        vis.addEventListener(type, listener, options);
      };
      alias.removeEventListener = function(type, listener, options){
        vis.removeEventListener(type, listener, options);
      };
      Object.defineProperty(alias, 'onclick', {
        set: function(fn){ vis.onclick = fn; },
        get: function(){ return vis.onclick; },
        configurable: true
      });
      alias.focus = function(){ vis.focus(); };
      alias.blur = function(){ vis.blur(); };
    } catch(e) {
      try{ console && console.warn && console.warn('alias delegation failed', e); }catch(_){} 
    }
  }
  if(document.readyState === 'loading'){
    document.addEventListener('DOMContentLoaded', ensureDelegation);
  } else {
    ensureDelegation();
  }
})();
</script>
<noscript><div style='color:#b91c1c'>JavaScript is disabled. Tables will be shown statically. For large tables enable JS for virtualization.</div></noscript>
<div id="tocBar" role="navigation" aria-label="Table of contents"><ul><li class="toc-item"><a class="toc-link" href="#Table1">Table 1</a></li>
<li class="toc-item"><a class="toc-link" href="#Table2">Table 2</a></li>
<li class="toc-item"><a class="toc-link" href="#Table3">Table 3</a></li>
<li class="toc-item"><a class="toc-link" href="#Table4">Table 4</a></li>
<li class="toc-item"><a class="toc-link" href="#Table5">Table 5</a></li>
<li class="toc-item"><a class="toc-link" href="#Table6">Table 6</a></li>
<li class="toc-item"><a class="toc-link" href="#Table7">Table 7</a></li>
<li class="toc-item"><a class="toc-link" href="#Table8">Table 8</a></li>
<li class="toc-item"><a class="toc-link" href="#Table9">Table 9</a></li>
<li class="toc-item"><a class="toc-link" href="#Table10">Table 10</a></li>
<li class="toc-item"><a class="toc-link" href="#Table11">Table 11</a></li>
<li class="toc-item"><a class="toc-link" href="#Table12">Table 12</a></li>
<li class="toc-item"><a class="toc-link" href="#Table13">Table 13</a></li></ul></div></div>
<div class="table-caption" id="Table1" data-table="Docu_0164_01" style="margin-top:2mm;margin-left:3mm;"><strong>Table 1</strong></div>
<div class="table-wrapper" data-table-id="table-1"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown — adapter_base.py"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown — adapter_base.py</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — adapter_base.py"> <strong>File-level responsibilities</strong><br><br>This module defines the canonical adapter <em>contract</em> used by all concrete ingestion adapters (CSV, SAP, Workday, etc.). Its purpose is to: 1) provide a small, stable, well-documented base class and helper functions that codify lifecycle and error/observability contracts; 2) centralize cross-cutting concerns (idempotency, retry/backoff, secrets handling, timeouts, telemetry, tracing, and safe shutdown); and 3) offer light-weight testing fakes and explicit extension points (hooks) so concrete adapters remain thin. Keep this file pure and free of heavy SDK imports — lazy-import any vendor SDK inside concrete adapter implementations or inside guarded helper factories. Document the adapter contract exhaustively: which methods are mandatory overrides, which are optional hooks, what exceptions mean (transient vs permanent), and how the host system will call the adapter (sync vs async expectations). Unit tests should be able to instantiate the base with a <code>FakeTransport</code> and assert lifecycle semantics without network I/O. </td></tr><tr><td data-label="Technical Breakdown — adapter_base.py"> <strong><code>class AdapterBase</code> (responsibilities & invariants)</strong><br><br>Canonical abstract base for adapters. Responsibilities: declare and enforce the public API (connect/disconnect, list_sources, fetch/stream, transform/map, checkpointing), provide default implementations for common helpers (retry wrappers, health-check scaffolding, secret fetching), and host extension hooks. Invariants: methods must be idempotent where possible; <code>connect()</code> must be safe to call repeatedly; <code>disconnect()</code> must be best-effort and swallow non-fatal errors; long-running IO must expose timeouts and cancellation; all public methods must attach trace context and emit structured diagnostic events. Keep the base small — prefer <code>NotImplementedError</code> for operations adapters must implement. Document which methods are synchronous vs async (choose one model consistently — prefer <code>async</code> for IO-bound flows; if the project is synchronous, clearly document that adapters must not call <code>await</code>). </td></tr><tr><td data-label="Technical Breakdown — adapter_base.py"> <strong><code>__init__(self, config: Mapping, *, logger: Logger | None = None, metrics: Metrics | None = None, secrets: SecretProvider | None = None, dry_run: bool = False)</code></strong><br><br>Initialize internal registries and shallow-validate the configuration shape. Responsibilities: record <code>config</code> (but never store raw secrets; only SecretRefs), set <code>self.logger</code>, <code>self.metrics</code>, <code>self._secrets</code>, <code>self.dry_run</code>, initialize <code>self._connected = False</code>, <code>self._checkpoint_lock</code>, and <code>self._registered_hooks = {}</code>. Must be side-effect free (no network I/O). Validate only surface-level keys and types; heavy validation in <code>validate_config()</code>. Provide a <code>to_safe_dict()</code> helper that redacts secret references for logging. Unit tests: construct with incomplete config and assert <code>dry_run</code> behavior and that <code>to_safe_dict()</code> redacts secrets. </td></tr><tr><td data-label="Technical Breakdown — adapter_base.py"> <strong><code>validate_config(self) -&gt; None</code></strong><br><br>Perform synchronous, cheap validation of <code>config</code> semantics (required keys, types, allowed ranges). Must <em>not</em> contact external services. Should raise a clearly-typed <code>ConfigurationError</code> (subclass) for fatal misconfiguration. When <code>settings.strict</code>-like behavior exists upstream, callers will use this to gate startup. Provide helper validators for common patterns (host:port, path patterns, enum choices). Unit tests: exercise missing keys, invalid types, and boundary values. </td></tr><tr><td data-label="Technical Breakdown — adapter_base.py"> <strong><code>connect(self, *, timeout: Optional[float] = None) -&gt; None | Awaitable</code></strong><br><br>Establish any lightweight client factories or sessions but <strong>must not</strong> open heavy persistent connections unless the caller explicitly allows (document startup modes). The method should be idempotent (safe to call multiple times), set <code>self._connected = True</code> on success, and attach connection metadata to <code>self.metadata()</code> for observability. Implement a <code>timeout</code> guard (use <code>asyncio.wait_for</code> for async implementations or an external timer/circuit-breaker for sync). Errors: raise <code>TransientAdapterError</code> for retryable failures (network hiccup) and <code>FatalAdapterError</code> for non-retryable configuration/credentials problems. Tests: simulate transient and fatal failures and assert correct exception types. </td></tr><tr><td data-label="Technical Breakdown — adapter_base.py"> <strong><code>disconnect(self, *, timeout: Optional[float] = None) -&gt; None | Awaitable</code></strong><br><br>Close resources in reverse order of acquisition. Must be best-effort and swallow non-fatal exceptions; always set <code>self._connected = False</code> even if errors occur. Ensure that <code>disconnect()</code> cancels/awaits any in-process background workers started by the adapter and flushes buffers. It should never raise on shutdown; instead log errors (with redaction) and emit telemetry. Tests: verify idempotence and that <code>disconnect()</code> runs when <code>connect()</code> partially succeeded. </td></tr><tr><td data-label="Technical Breakdown — adapter_base.py"> <strong><code>open_session(self, *, ephemeral: bool = True) -&gt; ContextManager | AsyncContextManager</code></strong><br><br>Convenience context manager for connection/session-scoped operations. Responsibilities: call <code>connect()</code> on enter (if not connected), yield a session object or self, and ensure <code>disconnect()</code> on exit if <code>ephemeral</code> is true. Provide stable trace/span boundaries during session lifetime. Implementation note: prefer returning an async context manager in async codepaths. Tests: use <code>async with</code> in integration tests to verify automatic teardown. </td></tr><tr><td data-label="Technical Breakdown — adapter_base.py"> <strong><code>list_sources(self, *, filters: Optional[Mapping] = None) -&gt; Iterable[SourceDescriptor]</code></strong><br><br>Return descriptors for available data sources the adapter can ingest (files, endpoints, report IDs). Each <code>SourceDescriptor</code> should be a small dataclass <code>{id, name, type, size_hint, updated_ts, meta}</code>. Must be fast and cacheable; callers will rely on a stable <code>id</code> for dedup/manifesting. Avoid fetching large metadata; provide hooks for <code>enrich_source_descriptor()</code> if heavy checks are required. Tests: provide fakes that exercise pagination and filtering. </td></tr><tr><td data-label="Technical Breakdown — adapter_base.py"> <strong><code>fetch_records(self, source: SourceDescriptor, *, start: Optional[str]=None, limit: Optional[int]=None) -&gt; Iterator[RawRecord] | AsyncIterator</code></strong><br><br>Core ingestion primitive: stream raw records (binary lines, rows) from a source. Contract: yield items in deterministic order, include a stable position/token with each record for checkpointing, and keep per-record metadata <code>{position, received_ts, source_id}</code>. Must respect <code>limit</code>, backpressure, and caller cancellation. Error model: raise <code>TransientAdapterError</code> for recoverable IO failures (caller may retry from last checkpoint) and <code>PermanentAdapterError</code> for unrecoverable record-level errors. Implement optional chunked pulls to avoid unbounded memory use. Tests: verify ordering, checkpoint tokens, and recover on transient errors. </td></tr><tr><td data-label="Technical Breakdown — adapter_base.py"> <strong><code>stream_records(self, source: SourceDescriptor, chunk_size: int = 1000, *, stop_event: Optional[Event] = None) -&gt; Iterator[List[RawRecord]]</code></strong><br><br>Batching wrapper around <code>fetch_records</code> that yields record lists for efficiency. Responsibilities: apply memory/size guards, expose a <code>chunk_size</code> control, and emit metrics about batch size and rate. Should collect partial batches on source exhaustion or timeout. Tests: ensure consistent batch sizes and that metrics are emitted. </td></tr><tr><td data-label="Technical Breakdown — adapter_base.py"> <strong><code>parse_record(self, raw: RawRecord) -&gt; Mapping[str, Any]</code></strong><br><br>Convert a raw record into a normalized mapping suitable for <code>map_record</code>. Must be deterministic and pure (no side effects). Should raise a <code>RowParseError</code> for malformed rows (include helpful hints and scrubbed raw data). Must not leak secrets in the error message. Provide pluggable parsers for CSV, XML, JSON via strategy pattern. Unit tests should include many malformed inputs and assert structured error payloads. </td></tr><tr><td data-label="Technical Breakdown — adapter_base.py"> <strong><code>map_record(self, parsed: Mapping[str, Any]) -&gt; CanonicalRecord</code></strong><br><br>Map parsed fields into the canonical internal schema. Responsibilities: column mapping, type coercion, value normalization (dates, currencies), and minimal domain validation (required fields, basic formats). Must be pure and deterministic. Reject with <code>MappingError</code> for schema violations. Provide explicit mapping metadata <code>{source_column -&gt; target_field, transform_fn_ref}</code> for traceability and explainability. Tests: verify mappings, null handling, and edge cases (large numbers, timezones). </td></tr><tr><td data-label="Technical Breakdown — adapter_base.py"> <strong><code>transform_record(self, canonical: CanonicalRecord) -&gt; CanonicalRecord</code></strong><br><br>Optional additional transformations (enrichment, derived fields) performed after mapping but before validation/ingest. Keep implementations pure and idempotent. Do <strong>not</strong> call external services here unless behind an explicit <code>prewarming</code> / <code>connect()</code> hook; if enrichment requires network calls, implement them as async cached calls with strict per-record timeouts and circuit-breaker behavior. Tests: include deterministic fixtures and assert no side-effects. </td></tr><tr><td data-label="Technical Breakdown — adapter_base.py"> <strong><code>prepare_snapshot(self, records: Iterable[CanonicalRecord], *, manifest_meta: Optional[Mapping]=None) -&gt; SnapshotDescriptor</code></strong><br><br>Package a sequence of canonical records into a snapshot object suitable for object storage. Responsibilities: apply serialization (JSONL/Parquet), compute content hash for idempotent storage keys, and produce snapshot metadata <code>{checksum, count, byte_size, schema_version}</code>. Must expose a <code>stream_bytes()</code> for uploading without full buffering. In <code>dry_run</code> mode, return an in-memory snapshot descriptor without touching object storage. Tests: ensure checksum stability and idempotent keys across repeated calls. </td></tr><tr><td data-label="Technical Breakdown — adapter_base.py"> <strong><code>checkpoint(self, token: str) -&gt; None</code></strong><br><br>Persist progress tokens so resumed runs can continue from a safe point. Design: adapters should not implement storage themselves but call an injected <code>checkpoint_store</code> (or return tokens to caller). Checkpoint operations must be idempotent and atomic; support optimistic concurrency where available. Provide a <code>checkpoint_with_retry()</code> helper that applies the adapter's retry/backoff config. Tests: simulate concurrent checkpoint writes and assert last-writer-wins or guarded semantics as specified. </td></tr><tr><td data-label="Technical Breakdown — adapter_base.py"> <strong><code>ack(self, token: str) -&gt; None</code> and <code>nack(self, token: str, error: Exception) -&gt; None</code></strong><br><br>Optional hooks for message-oriented adapters (MQ, streaming). <code>ack()</code> marks a record or batch consumed; <code>nack()</code> requests re-delivery or dead-letter routing. Both must be safe to call even after <code>disconnect()</code>; <code>nack()</code> must attach sanitized error metadata and emit telemetry. Document how the host will act on <code>nack()</code> (retry, dead-letter). Tests: confirm semantics with a fake transport that records ack/nack calls. </td></tr><tr><td data-label="Technical Breakdown — adapter_base.py"> <strong><code>health_check(self) -&gt; Mapping[str, Any]</code></strong><br><br>Return small health envelope <code>{ok: bool, meta: {last_connect, last_error, latency_ms}}</code>. Health checks must be cheap (no heavy queries). If deep readiness checks are required, expose a separate <code>deep_health_check()</code> that callers will call only when appropriate. Always redact secrets in <code>meta</code>. Tests: assert both light and deep checks behave correctly under simulated partial failures. </td></tr><tr><td data-label="Technical Breakdown — adapter_base.py"> <strong><code>metadata(self) -&gt; Mapping[str, Any]</code></strong><br><br>Return adapter metadata used in manifests and audit (adapter_name, adapter_version, connector_type, supported_formats, capabilities: {streaming, batch, checkpointing}, applied_config_safe). This must not contain secrets. Used by <code>manifest.py</code> and exporters for reproducibility. Unit test: assert stable output and that secrets are redacted. </td></tr><tr><td data-label="Technical Breakdown — adapter_base.py"> <strong><code>configure_retry(self, policy: RetryPolicy) -&gt; None</code></strong><br><br>Install a retry/backoff policy used by wrapper helpers. The base should provide a standard <code>retry_on_transient()</code> decorator implementing exponential backoff, jitter, max-retries, and backoff cap. Retry must be applied only to idempotent operations or operations the caller has documented as safe to repeat; otherwise raise if user attempts to enable retries for non-idempotent calls. Tests: ensure retry counts, sleep behavior (mocked timing), and idempotency checks. </td></tr><tr><td data-label="Technical Breakdown — adapter_base.py"> <strong><code>fetch_secret(self, secret_ref: SecretRef) -&gt; str</code></strong><br><br>Helper that resolves secrets via the injected <code>SecretProvider</code>. It should never persist secret values on the instance; if a credential must be reused, use ephemeral token objects with short TTL and secure memory handling where feasible. Fetch operations must be performed lazily and only during <code>connect()</code> or just-in-time in <code>fetch_records()</code> when required. Tests: assert secrets are not present in <code>repr()</code> or metadata outputs. </td></tr><tr><td data-label="Technical Breakdown — adapter_base.py"> <strong><code>rotate_credentials(self) -&gt; None</code></strong><br><br>Optional hook to refresh ephemeral credentials. Must be safe to call concurrently with in-flight fetches (coordinate via lock). On rotation failure, prefer to emit a degraded health state rather than crash the process. Document the rotation strategy and TTL expectations. Tests: simulate rotation mid-run and ensure new credentials are used for subsequent calls. </td></tr><tr><td data-label="Technical Breakdown — adapter_base.py"> <strong><code>to_manifest(self) -&gt; Mapping[str, Any]</code></strong><br><br>Return a serializable manifest fragment describing adapter-specific pieces required in export manifests: supported output formats, applied adapter_version, content-hash function used, and any adapter-specific policy flags. Must be deterministic and safe to publish (no secrets). Used by <code>manifest.py</code> and exporters during bundle generation. Tests: verify stable values across runs and version bumps. </td></tr><tr><td data-label="Technical Breakdown — adapter_base.py"> <strong><code>translate_error(self, exc: Exception) -&gt; AdapterError</code></strong><br><br>Centralize mapping from raw SDK/transport exceptions into the project's typed adapter errors (<code>TransientAdapterError</code>, <code>PermanentAdapterError</code>, <code>ConfigurationError</code>, <code>RowParseError</code>, etc.). Important: always preserve original exception type and message in a non-secret <code>debug</code> field while ensuring user-visible messages are scrubbed. Tests: assert that different upstream SDK exceptions map to correct adapter categories for retry logic. </td></tr><tr><td data-label="Technical Breakdown — adapter_base.py"> <strong><code>register_hook(self, name: str, fn: Callable)</code> & <code>run_hook(self, name: str, *args, **kwargs)</code></strong><br><br>Lightweight hook registry for pre/post events (pre_fetch, post_fetch, pre_map, post_map, pre_snapshot). Hooks must run in a protected try/except; failures in non-critical hooks must not abort the main flow — instead log at warn/error and emit telemetry. Document hook guarantee: synchronous execution order and behavior under errors. Tests: register failing hooks and assert that main flow continues and that hooks are invoked in order. </td></tr><tr><td data-label="Technical Breakdown — adapter_base.py"> <strong><code>instrumentation helpers: start_span(self, name: str)</code>, <code>end_span(self, span)</code>, <code>emit_metric(self, name: str, value: Any, tags: Optional[Mapping]=None)</code></strong><br><br>Small adapters should call these helpers rather than binding directly to telemetry libraries. Helpers must attach <code>trace_id</code>, <code>adapter</code>, and <code>source_id</code> to every event and respect sampling controls. Provide a <code>dry_run</code> instrumentation sink for tests that captures events in memory for assertions. Tests: assert trace propagation and metric tags correctness. </td></tr><tr><td data-label="Technical Breakdown — adapter_base.py"> <strong><code>safe_logging(self, message: str, *, extra: Optional[Mapping] = None, level: str = &quot;info&quot;)</code></strong><br><br>Structured logging helper that applies redaction rules (configured at instantiation) before emitting. Must never log secret values; when logging raw payloads for debug, apply sampling and explicit allow-lists. Tests: ensure redaction for common secret keys and that debug logging is gated by <code>dry_run</code> or <code>settings.debug</code>. </td></tr><tr><td data-label="Technical Breakdown — adapter_base.py"> <strong><code>sniff_format(cls, sample_bytes: bytes) -&gt; str</code> (classmethod)</strong><br><br>Heuristic to detect input format (csv, xlsx, xml, json). Keep deterministic, fast, and documented for border cases. Used by generic file adapters to choose parser strategy. Tests: include sample files for each format and edge cases (malformed headers). </td></tr><tr><td data-label="Technical Breakdown — adapter_base.py"> <strong><code>close(self) -&gt; None | Awaitable</code></strong><br><br>Alias that forwards to <code>disconnect()</code>; provided for code that expects a generic <code>.close()</code> method. It must be idempotent and safe to call from finalizers. Tests: verify <code>close()</code> is no-op after <code>disconnect()</code>. </td></tr><tr><td data-label="Technical Breakdown — adapter_base.py"> <strong>Testing guidance (unit / integration / contract tests)</strong><br><br>— <strong>Unit</strong>: test each small pure function (parsers, sniffers, mapping transforms) with a wide variety of fixtures (normal, boundary, malformed). Use in-memory fakes for <code>SecretProvider</code>, <code>checkpoint_store</code>, and <code>object_store</code>. <br>— <strong>Adapter contract tests</strong>: instantiate <code>AdapterBase</code> subclass fake that implements minimally required methods and assert lifecycle (connect → fetch → checkpoint → disconnect) with simulated transient and permanent failures. <br>— <strong>Integration (fast)</strong>: use <code>open_session</code> against a local test transport (file-backed, not network) to verify end-to-end streaming and snapshot generation. <br>— <strong>Resilience</strong>: simulate credential rotation, partial connect failures, and ensure behavior under <code>strict=True</code> vs tolerant modes. </td></tr><tr><td data-label="Technical Breakdown — adapter_base.py"> <strong>Operational & security checklist</strong><br><br>Before production: ensure adapters only request the least-privilege secrets, all network calls honor per-call timeouts, connection pools are bounded, logs are redacted, metrics include adapter and source tags, and <code>dry_run</code> mode exists for safe validation. Confirm that snapshot keys are content-hash based and that checkpoint tokens are not guessable. Add runbooks describing how to manually drain and restart adapters. </td></tr><tr><td data-label="Technical Breakdown — adapter_base.py"> <strong>Implementation patterns & guardrails</strong><br><br>— <strong>Lazy imports</strong>: heavy SDKs must be imported inside <code>connect()</code> or inside concrete adapters. <br>— <strong>Idempotence</strong>: implement idempotent <code>connect</code>, <code>disconnect</code>, and <code>checkpoint</code>. <br>— <strong>Secrets</strong>: store only <code>SecretRef</code>s in <code>config</code>; call <code>fetch_secret()</code> at <code>connect()</code> and avoid persisting plaintext credentials. <br>— <strong>Observability</strong>: always attach <code>trace_id</code> to logs and metrics and expose <code>metadata()</code> for manifests. <br>— <strong>Retry discipline</strong>: only retry idempotent operations and use backoff + jitter. <br>— <strong>Fail-fast vs degraded</strong>: make the distinction explicit — configuration errors are fatal; transient network glitches should surface as degradations that the orchestrator can decide to retry. <br>— <strong>Testing hygiene</strong>: include a <code>FakeTransport</code> and <code>FakeCheckpointStore</code> shipped with the module for downstream tests. </td></tr><tr><td data-label="Technical Breakdown — adapter_base.py"> <strong>Recommended CI checks & tests to add</strong><br><br>1. <strong>Adapter contract smoke</strong>: run a simple fake adapter through the full lifecycle in CI. <br>2. <strong>Parser fuzzing</strong>: lightweight fuzzing over common input shapes to catch parser regressions. <br>3. <strong>Redaction tests</strong>: ensure no secrets leak into logs or metadata. <br>4. <strong>Idempotency</strong>: repeat connect/fetch/checkpoint flows and validate results are stable. <br>5. <strong>Timeout/resilience</strong>: assert <code>connect()</code> and <code>fetch_records()</code> honor configured timeouts and do not hang. Automate these checks in CI gating. </td></tr><tr><td data-label="Technical Breakdown — adapter_base.py"> <strong>Example reviewer checklist</strong><br><br>- Does every public method have clear sync/async semantics?<br>- Are secrets represented as references only and redacted in <code>to_safe_dict()</code> and logs?<br>- Are retry policies applied only to idempotent operations?<br>- Are timeouts and cancellation points documented and enforced?<br>- Is tracing/metrics integrated and are trace_ids propagated?<br>- Are errors mapped into the project's adapter error taxonomy?<br>- Are hooks available for testing and non-fatal instrumentation?<br>- Are heavy imports lazy and side-effect-free at import time?<br>- Are operations idempotent where callers expect them to be?<br>- Is there a <code>dry_run</code> path for safe validation without external side effects? </td></tr><tr><td data-label="Technical Breakdown — adapter_base.py"> <strong>Maintenance notes</strong><br><br>When extending the adapter contract: add small, well-documented hooks rather than growing the base API surface. Keep the base file focused on contracts and small helpers; move heavy or adapter-specific logic into new modules (e.g., <code>adapter_s3_helpers.py</code>, <code>adapter_parsers.py</code>). When changing error classes or retry semantics, bump adapter contract version and update <code>to_manifest()</code> so exported bundles remain auditable. </td></tr></tbody></table></div><div class="row-count">Rows: 34</div></div><div class="table-caption" id="Table2" data-table="Docu_0164_02" style="margin-top:2mm;margin-left:3mm;"><strong>Table 2</strong></div>
<div class="table-wrapper" data-table-id="table-2"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown — src/adapters/csv_adapter.py"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown — src/adapters/csv_adapter.py</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — src/adapters/csv_adapter.py"> <strong>File-level responsibilities</strong><br><br>This module implements the CSV <em>adapter</em> that turns incoming CSV sources (HTTP uploads, uploaded files, local paths, streams provided by other adapters) into the application's canonical ingest artifacts: a validated, normalized stream of canonical records, a persisted immutable snapshot, and a small metadata/manifest object that the rest of the pipeline (snapshot store, job queue, validators, recorder) relies on. Keep logic deterministic, testable, and side-effect-free at import time. Heavy I/O belongs behind small, well-documented helper functions and is only exercised from explicit <code>run</code>/<code>handle_*</code> entrypoints. Expose a single high-level ingress API used by HTTP handlers and CLI code; all other functions are helpers with clear invariants. Priorities: correctness of mapping, robust handling of malformed CSVs, idempotence of snapshot writing, safe handling of encodings and delimiters, small-memory streaming, observability (per-row metrics, sample diagnostics), and secure handling of uploaded content (size/line-length limits, scanning). Unit-testable pieces should be pure; integration tests exercise the streaming + snapshot + enqueue path. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/csv_adapter.py"> <strong><code>CSVAdapter(settings, mapper, validator, storage, queue, logger, progress_hook=None)</code> — constructor & public surface</strong><br><br><strong>Responsibility:</strong> create a small, dependency-injected adapter instance that coordinates parsing, mapping, validation, snapshot persistence, and job-enqueue. Constructor should only assign references and perform lightweight validation of required callables/objects; avoid network or filesystem access at init-time. Document expected minimal interfaces for <code>mapper</code> (callable or object with <code>map_row(headers, row) -&gt; dict</code>), <code>validator</code> (callable <code>validate(record) -&gt; (bool, list[errors])</code>), <code>storage</code> (factory with <code>write_snapshot(stream, meta) -&gt; SnapshotMeta</code>), and <code>queue</code> (with <code>enqueue_ingest(snapshot_meta) -&gt; job_id</code>). Provide a <code>build_test_adapter</code> helper in tests that injects fakes. Ensure the constructor supports a <code>dry_run</code> flag for unit tests (no snapshot writes, no queue ops) and <code>strict</code> semantics that influence final behavior. Tests: assert no side-effects on init, required attribute types raise on construction, <code>progress_hook</code> is optional and called with (processed, total, last_ts). </td></tr><tr><td data-label="Technical Breakdown — src/adapters/csv_adapter.py"> <strong><code>detect_encoding_and_dialect(source_bytes: bytes) -&gt; dict</code></strong><br><br><strong>Responsibility:</strong> Examines a small sample of the incoming bytes (e.g., first 64KB or less) and returns <code>{ encoding, newline, delimiter, quotechar, has_header, sample_rows }</code>. Implementation notes: prefer <code>chardet</code>/<code>charset-normalizer</code> detection inside a try/except and fall back to UTF-8 with <code>errors=&#x27;replace&#x27;</code>. Use a robust dialect sniffing strategy: sample multiple lines and test common delimiters (<code>,</code>, <code>;</code>, <code>\t</code>, <code>|</code>) while counting consistent column counts. Detect presence of a header row heuristically (field names not purely numeric, no repeated empty headers). This function must be pure from input bytes to output dict so it can be fully unit-tested. Tests: ambiguous delimiter choices, mixed newlines, BOMs, quoted newlines inside fields, and truncated samples. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/csv_adapter.py"> <strong><code>open_source(source) -&gt; ContextManager[fileobj]</code></strong><br><br><strong>Responsibility:</strong> Uniformly accept various "source" types (path-like, file-like, bytes, requests.Response-like, adapter-provided stream) and return a binary-mode file-like object that yields bytes. The context manager must normalize: apply size guard (max upload bytes), enforce content-type/header validations when available, and provide a <code>peek(n)</code> or expose the first N bytes for dialect detection. Must avoid reading entire file into memory: use an internal buffered wrapper with configurable buffer size. Security: ensure temporary files created have restrictive permissions and are removed on exit; if the source is a network stream, ensure read timeouts and rate limits. Tests: wire fake file-like objects and a mock network stream. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/csv_adapter.py"> <strong><code>stream_rows(fileobj, encoding, dialect, *, max_field_size, chunk_rows=None) -&gt; Iterator[Tuple[int, List[str], bytes]]</code></strong><br><br><strong>Responsibility:</strong> Low-level CSV streaming generator that yields <code>(row_no, raw_fields, raw_line_bytes)</code> for each parsed CSV row. Use the detected encoding to decode linewise in a streaming manner (avoid <code>.read().decode()</code>), and rely on a csv-like parser configured with the detected dialect. Must enforce <code>max_field_size</code> per field and <code>max_row_bytes</code> overall to avoid memory bombs. Preserve original raw bytes for audit/roundtrip when needed. Provide robust handling of embedded newlines, mismatched quote counts (treat as parse error at row boundary policy), and rows longer than permitted (emit a row-level error, not a process-level crash). This generator should be cancelable and respect adapter-level cancellation flags. Tests: huge fields, quoted-newline fields, mixed delimiters in the same file, and encoding errors with <code>errors=&#x27;replace&#x27;</code> vs strict mode. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/csv_adapter.py"> <strong><code>_normalize_headers(raw_headers: List[str]) -&gt; List[str]</code></strong><br><br><strong>Responsibility:</strong> Convert raw header tokens into canonical header names used by mappers/CSV schema detectors. Rules: trim whitespace, lower-case (configurable), collapse multiple spaces to single <code>_</code>, remove or map illegal characters (keep ASCII safe subset), and detect duplicate names (append <code>.1</code>, <code>.2</code> deterministically). Preserve original header order and provide a reverse-map to original names for diagnostics. Should be pure and fully unit-tested with many locale/whitespace examples. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/csv_adapter.py"> <strong><code>parse_row(raw_fields: List[str], headers: List[str]) -&gt; Dict[str,str]</code></strong><br><br><strong>Responsibility:</strong> Turn a raw token list into a dict keyed by the normalized headers. For header-less CSVs, generate positional header names (<code>col_000</code>, <code>col_001</code>) consistently. Must preserve empty strings vs missing fields; handle trailing columns (if a row is shorter, fill <code>None</code> or empty string based on settings). Do not perform type conversion here — keep values as raw strings; conversion belongs to the mapper/normalizer. Tests: rows with fewer/more fields than headers, header-less input, and rows with only delimiters. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/csv_adapter.py"> <strong><code>map_row(parsed_row: Dict[str,str]) -&gt; MappingResult</code></strong><br><br><strong>Responsibility:</strong> Apply domain-specific mapping rules to convert parsed string values into the canonical schema used by the pipeline. The adapter should accept either a <code>mapper</code> callable or a <code>csv_mapper</code> module — support both function-style and object-style mappers. Mapping steps: (1) column-to-field mapping (aliasing), (2) simple type casts (dates, decimals) if safe, (3) normalization hooks (trim, collapse whitespace), and (4) emit mapping-level diagnostics (missing required columns, parse warnings). The mapping function must not perform heavy I/O, must be deterministic, and should return a <code>MappingResult</code> structure containing <code>mapped_record</code>, <code>mapping_warnings</code>, and a <code>mapping_checksum</code> (content-hash used for row dedupe/audit). Tests: ambiguous column aliases, numeric values with thousands separators, locale date strings, and mapping-fallback rules. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/csv_adapter.py"> <strong><code>validate_row(mapped_record: Dict[str,Any]) -&gt; ValidationResult</code></strong><br><br><strong>Responsibility:</strong> Apply business validation rules (pinned to validators.py interface) to mapped records: required fields present, ranges, cross-field invariants, and domain-specific semantic checks (e.g., NIP format, tax period boundaries). Return a <code>ValidationResult</code> containing <code>ok: bool</code>, <code>errors: List[Error]</code>, <code>warnings: List[Warning]</code>, and <code>error_codes</code>. Do not mutate the record; validators must be pure. Support fast-fail mode (stop on first critical error) when <code>settings.strict</code> is true, and lenient mode where errors are collected and reported but do not abort processing. Tests: boundary values, mutually-exclusive fields, and validators raising unexpected exceptions (adapter must catch and surface as a validation error). </td></tr><tr><td data-label="Technical Breakdown — src/adapters/csv_adapter.py"> <strong><code>normalize_record(mapped_record: Dict[str,Any]) -&gt; CanonicalRecord</code></strong><br><br><strong>Responsibility:</strong> Coerce typed values to canonical internal representations (native Python <code>Decimal</code>, <code>date</code> objects with timezone-agnostic semantics, normalized enumerations). Apply deterministic transforms used later by calculation engine: canonical currency code normalization, rounding rules, trimming of trailing zeros if policy requires, and generation of stable <code>record_id</code> (content-hash of canonical fields). Must be pure and reversible-when-possible (store original raw fields in metadata). Tests: decimal rounding, date canonicalization across input formats, stable id generation. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/csv_adapter.py"> <strong><code>_handle_row_error(row_no: int, raw_row: bytes, exc: Exception) -&gt; RowErrorRecord</code></strong><br><br><strong>Responsibility:</strong> Create a small structured error record suitable for both diagnostic logs and insertion into the exceptions queue. Must include <code>row_no</code>, <code>error_kind</code> (parse/mapping/validation/size/security), <code>message</code>, <code>exception_class</code>, snippet of the offending fields (redacted for PII), and a small <code>sample_hash</code> for dedupe. Do not store full row if it contains secrets — apply redaction rules consistently (use <code>log.redaction</code> configuration). This helper centralizes error envelope format and must be used by all error pathways. Tests: ensure PII redaction, size-limited error payloads, and idempotent structure. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/csv_adapter.py"> <strong><code>write_snapshot(row_iterator: Iterator[CanonicalRecord], dest_uri: str, *, compress=False, checksum_algo=&#x27;sha256&#x27;) -&gt; SnapshotMeta</code></strong><br><br><strong>Responsibility:</strong> Persist an immutable snapshot object containing the canonical records. Snapshot writer must stream-serialize (e.g., JSONL, ndjson, or parquet depending on settings) without buffering full content in memory, compute a streaming checksum, record row counts, write a small manifest (applied_rule_version, generator_version, record_count, checksum, bytes_written), and optionally compress. Must be idempotent and safe to retry: write to a temp path and atomically rename into <code>dest_uri</code>. When <code>dry_run</code> is set, record an in-memory representation only. Ensure appropriate object-store ACLs on commit and include metadata for origin/source (uploader id, source filename, encoding/dialect). Tests: partial failure during write should remove temp artifacts and not leave partially published snapshots; ensure checksum matches recomputed value on read-back. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/csv_adapter.py"> <strong><code>build_metadata(source_info: dict, stats: dict, sample_rows: List[dict]) -&gt; dict</code></strong><br><br><strong>Responsibility:</strong> Produce the metadata/manifest consumed by downstream systems and included in the snapshot manifest: <code>source_type</code>, <code>source_name</code>, <code>uploader</code>, <code>ingest_time</code> (ISO-8601 UTC), <code>encoding</code>, <code>dialect</code>, <code>row_count</code>, <code>valid_count</code>, <code>invalid_count</code>, <code>applied_rule_version</code>, <code>generator_version</code>, <code>sample_rows</code> (redacted), and <code>content_hash</code>. Keep this object small and stable; produce a <code>to_safe_dict()</code> view that removes secrets for logs. Tests: metadata contains required keys, safe-dict redacts correctly, timestamps are monotonic and timezone aware. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/csv_adapter.py"> <strong><code>enqueue_ingest_job(snapshot_meta: dict, *, idempotency_key: Optional[str]=None, priority:int=0) -&gt; str</code></strong><br><br><strong>Responsibility:</strong> Submit a job to the configured queue (database-backed, redis, or external job-queue) that tells the worker pool to run the calculation pipeline against the snapshot. The function must: compute or accept an idempotency key (default: snapshot content hash), persist an enqueue record with state <code>(queued, snapshot_meta, idempotency_key)</code>, prevent duplicate enqueues within a configured TTL, and return a stable <code>job_id</code>. Respect <code>settings.strict</code> and error policy: if queue is unavailable and <code>strict</code>, raise; otherwise mark snapshot as <code>queued_failed</code> and emit a degraded metric. Tests: duplicate submissions with same idempotency key are no-ops, queue unavailability simulated, and priority honored. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/csv_adapter.py"> <strong><code>handle_upload(source, *, strict=False, dry_run=False, snapshot_dest=None, emit_progress=True) -&gt; UploadResult</code></strong><br><br><strong>Responsibility:</strong> The canonical high-level entrypoint used by HTTP/CLI code. End-to-end flow: open the source, sample bytes → detect encoding/dialect, stream rows → parse → map → validate → normalize → write snapshot → build metadata → enqueue job (unless dry_run). Must return a structured <code>UploadResult</code> with <code>snapshot_meta</code>, <code>job_id</code> (or None), <code>stats</code> and <code>diagnostics</code> (first N errors/warnings). Implementation details: run synchronous CPU-bound steps inline, but allow callers to opt into an async wrapper if required by runtime. The function must support abort/cancel semantics, per-call timeouts, and produce consistent exit codes for known conditions (<code>ok</code>, <code>partial</code>, <code>failed</code>). Observability: emit metrics for <code>ingest.rows</code>, <code>ingest.errors</code>, <code>ingest.duration</code>, plus sampling traces (start/end). Security: validate maximum permitted file size early and reject with clear diagnostics. Tests: full happy-path (snapshot persisted and job enqueued), dry-run path (no snapshot or queue), partial failures where some rows invalid, and strict vs lenient behavior. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/csv_adapter.py"> <strong><code>_emit_progress(processed: int, total: Optional[int]=None, last_ts: Optional[float]=None)</code></strong><br><br><strong>Responsibility:</strong> Centralized progress reporting: call <code>progress_hook</code> if present and also write periodic structured logs (e.g., every N rows or seconds). Avoid high-call-frequency — throttle updates. Include <code>processed</code>, <code>total</code>, <code>rate_rows_per_sec</code>, <code>eta</code> (if total known), and <code>last_row_hash</code>. Ensure progress messages are safe to expose (no PII). Tests: throttling behavior, correct ETA calculation on steady input, and no progress when hook is None. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/csv_adapter.py"> <strong><code>_emit_diagnostics(diag_queue, *, sample_errors=10)</code></strong><br><br><strong>Responsibility:</strong> After processing, prepare a compact diagnostics bundle for storage and upstream reporting: sample errors (bounded, redacted), top N validation error codes with counts, exemplar bad rows (redacted), and a small histogram of row sizes. Do not include full snapshots here. This is consumed by UI or auditor. Tests: ensures size caps are obeyed and PII redaction applied. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/csv_adapter.py"> <strong><code>_security_scan_row(raw_bytes: bytes) -&gt; SecurityResult</code></strong><br><br><strong>Responsibility:</strong> Optional inline content scanner invoked before snapshot commit: quick checks for suspicious patterns (long base64 blobs, known malicious mime fingerprints, presence of obvious secrets like private keys or SSNs). This is a fast heuristic pass only — heavy malware scanning must be delegated to a separate service invoked in startup/shutdown handlers if <code>settings.strict_security_checks</code> is true. Record any findings into metadata but do not block ingest unless configured to do so. Tests: simple pattern detection and correct opt-in/out behavior. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/csv_adapter.py"> <strong><code>close()</code> / <code>aclose()</code></strong><br><br><strong>Responsibility:</strong> Cleanly release adapter-owned resources (temporary files, open handles, background progress timers). Idempotent and safe to call multiple times. Always swallow non-fatal exceptions during close but record them in logs and metrics. Tests: ensure temp files removed and repeated close is a no-op. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/csv_adapter.py"> <strong>Implementation patterns & guardrails</strong><br><br>— <strong>Streaming-first</strong>: Always design row processing as an iterator pipeline (<code>stream_rows</code> → <code>parse_row</code> → <code>map_row</code> → <code>validate_row</code> → sink<code>) to maintain low memory usage. &lt;br&gt;— **Idempotence**: Snapshot writing must use atomic commit semantics and enqueue must be idempotent via </code>idempotency_key<code>. &lt;br&gt;— **Security &amp; PII**: Provide a configurable redaction filter used by </code>_handle_row_error<code>, </code>_emit_diagnostics<code>, and logs. Never log raw field values by default. &lt;br&gt;— **Observability**: Attach trace/span ids to snapshot metadata and include </code>runtime_fingerprint<code> and </code>generator_version<code>. Emit metrics for row-processing latency percentiles and per-error-class counters. &lt;br&gt;— **Failure modes**: Distinguish </code>fatal<code> (disk full, storage permission denied) vs </code>row-level<code> (parse error, invalid field) and surface them differently to callers. Respect </code>settings.strict<code> to decide whether fatal errors abort or mark snapshot degraded. &lt;br&gt;— **Testing hygiene**: Provide </code>build_test_adapter<code> that injects fake </code>storage<code>, </code>queue<code>, and </code>mapper<code>. Unit-test all pure helpers; write small integration tests that run </code>handle_upload<code> with small fixture CSVs (including BOMs, quoted newlines, non-ASCII headers). Include golden fixtures for expected snapshot manifests.   **Recommended tests &amp; CI checks**&lt;br&gt;&lt;br&gt;1. **Unit**: </code>detect_encoding_and_dialect<code>, </code>_normalize_headers<code>, </code>parse_row<code>, </code>map_row<code> (with many alias/locale cases).&lt;br&gt;2. **Integration (fast)**: </code>handle_upload<code> with </code>dry_run<code> to assert parsing, mapping, validation stats and diagnostics. &lt;br&gt;3. **Storage resilience**: snapshot writer interrupted mid-write — ensure cleanup and no published artifact. &lt;br&gt;4. **Idempotency**: multiple </code>handle_upload<code> calls with identical content produce a single published snapshot + single enqueue when idempotency is enabled. &lt;br&gt;5. **Security**: PII redaction unit tests and opt-in security-scan behaviors.&lt;br&gt;6. **Performance**: benchmark streaming throughput for large CSVs (10M rows) with chunked reads and ensure memory stays bounded.   **Operational checklist before production roll**&lt;br&gt;&lt;br&gt;— Ensure </code>max_upload_size<code> and </code>max_field_size<code> are set to defend against resource exhaustion. &lt;br&gt;— Ensure snapshot storage has atomic commit semantics and appropriate ACLs. &lt;br&gt;— Configure </code>idempotency_ttl<code> for enqueue dedupe. &lt;br&gt;— Validate redaction rules and that no PII leaks in logs/diagnostics. &lt;br&gt;— Wire adapter metrics (</code>ingest.rows<code>, </code>ingest.errors<code>, </code>ingest.duration<code>, </code>snapshot.size<code>) into monitoring + alerts. &lt;br&gt;— Provide runbooks for partial snapshot recovery and cleanup of orphaned temp artifacts.   **Maintenance &amp; developer notes**&lt;br&gt;&lt;br&gt;— When adding a new mapping rule, add unit tests with both raw and normalized examples and include a </code>mapping_warnings<code> expectation. &lt;br&gt;— Avoid adding network calls into mapping/validation helpers; those belong in startup-initialized services exposed through </code>app.state._extensions<code>. &lt;br&gt;— If exposing in-process workers for heavy validation/scan, document how to toggle them off for single-process deployments. &lt;br&gt;— Keep this module focused on CSV semantics — heavy conversion (Excel parsing, XML import) belongs in separate adapters (</code>excel_reader.py<code>, </code>bukti_potong_csv.py`) and reuse the mapper/validator interfaces. </td></tr></tbody></table></div><div class="row-count">Rows: 20</div></div><div class="table-caption" id="Table3" data-table="Docu_0164_03" style="margin-top:2mm;margin-left:3mm;"><strong>Table 3</strong></div>
<div class="table-wrapper" data-table-id="table-3"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown — src/adapters/sap_adapter.py"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown — src/adapters/sap_adapter.py</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — src/adapters/sap_adapter.py"> <strong>File-level responsibilities</strong><br><br>This module implements the SAP integration adapter that is responsible for reliably extracting payroll/employee data from SAP and converting it into the project's canonical ingest format. It encapsulates transport (HTTP/SOAP/REST/OData), authentication (token/certificate), pagination and rate-limit handling, field mapping and lightweight normalization, error classification (transient vs permanent), and exposes a small, testable surface for the rest of the pipeline: connection lifecycle, incremental polling, batch/stream reads, and reconciliation checks. Keep side-effects limited to explicit connect/disconnect and documented export/enqueue calls. Heavy transformations belong in <code>csv_mapper.py</code> or canonical mappers; this adapter should focus on durable, observable data movement and robust boundary semantics. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/sap_adapter.py"> <strong><code>class SAPAdapter(adapter_base.AdapterBase)</code></strong><br><br>Constructor returns an adapter instance holding configuration, a lazily-initialized transport client, and runtime state (<code>_connected</code>, <code>_last_synced</code>, <code>_metrics_tags</code>). The class is the public surface used by ingestion code and workers. Implementation guidelines: perform no network I/O in <code>__init__</code>; build only simple value objects and validators. Accept a <code>settings</code> object and optional injected <code>http_client</code>, <code>clock</code>, and <code>logger</code> for deterministic tests. Expose <code>connect()</code>/<code>disconnect()</code> async/sync methods and make them idempotent. Attach a short <code>adapter_version</code> and <code>supported_endpoints</code> list in instance attributes for diagnostics. Unit tests should instantiate with fakes and assert no network activity at construction. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/sap_adapter.py"> <strong><code>configure_client(self) -&gt; None</code></strong><br><br>Prepare a transport client factory without performing network I/O. Responsibilities: validate essential configuration (host, client_id/secret or cert paths, timeout ranges), establish TLS parameters (CA bundle, cert validation mode), set up retry/backoff strategy defaults, and attach request/response hooks used for structured logging and metrics. Must be idempotent and side-effect free beyond in-memory wiring. In tests, ensure that misconfigured TLS or missing credentials raise a clear <code>ConfigurationError</code> and that <code>settings.strict</code> influences whether the adapter refuses to be configured. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/sap_adapter.py"> <strong><code>connect(self) -&gt; None</code> / <code>async def connect(self) -&gt; None</code></strong><br><br>Open any required persistent connections and perform a minimal live check (auth handshake / token exchange). Contracts: timeboxed with a configurable per-adapter timeout (use <code>asyncio.wait_for</code> for async). Do not perform expensive data syncs; only verify credentials, fetch a short-lived token if required, and populate <code>self._client</code> and <code>self._connected = True</code>. On repeated calls, be a no-op. If <code>settings.strict</code> is true, raise on auth failure; otherwise set <code>self._degraded = True</code> and log. Tests: simulate auth failures, expired certs, and ensure idempotence. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/sap_adapter.py"> <strong><code>disconnect(self) -&gt; None</code> / <code>async def disconnect(self) -&gt; None</code></strong><br><br>Close client sessions, cancel background token refresh tasks, and release resources. Must be safe to call multiple times and must suppress exceptions (log them) — do not raise during shutdown. Ensure that outstanding paginated iterators are either drained or raise a clear <code>AdapterClosedError</code>. Tests should verify resource cleanup and that background tasks no longer run after disconnect. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/sap_adapter.py"> <strong><code>health_check(self) -&gt; Dict[str,Any]</code></strong><br><br>Return a small dict <code>{ok: bool, detail: {auth: ok, last_request: ts, latency_ms: n, degraded: bool}}</code> without raising. Perform only lightweight checks: token present and not expired, last known endpoint latency within limits, and TLS verification status. Avoid network calls longer than a short probe timeout. Health output should be safe for unauthenticated internal <code>/internal/health</code> endpoints. Tests: stub token state and time to assert expected flags. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/sap_adapter.py"> <strong><code>fetch_payroll_batches(self, since: Optional[datetime]=None) -&gt; Iterator[Dict]</code></strong><br><br>High-level incremental generator that enumerates payroll batch identifiers (or snapshot descriptors) from SAP since the provided timestamp. Responsibilities: call the SAP list endpoint with correct filters, handle pagination, map SAP pagination metadata to iterator semantics, and yield canonical batch descriptors <code>{batch_id, run_date, source_etag, meta}</code>. Side-effects: read-only network calls. Must be idempotent (calling twice with same <code>since</code> yields same sequence unless SAP changed). On transient failures, apply adapter-level retry policy (configurable: max_retries, backoff). Tests: validate pagination handling, empty responses, duplicate suppression, and behavior under rate-limits. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/sap_adapter.py"> <strong><code>fetch_employee_records(self, batch_id: str) -&gt; Iterator[Dict]</code></strong><br><br>Stream all employee/payroll lines for a given payroll batch. Responsibilities: efficient, memory-safe iteration (support chunked pagination), stable ordering (if provided by SAP), and returning raw SAP record blobs plus minimal transport metadata (<code>_sap_row_id</code>, <code>_received_ts</code>, <code>_page_etag</code>). Do not mutate records here — mapping happens in <code>map_record_to_canonical</code>. Ensure the iterator raises <code>NotFoundError</code> for non-existent <code>batch_id</code>, and <code>AdapterTimeout</code> on per-page timeouts. Tests: force single-page, multi-page, and malformed page scenarios. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/sap_adapter.py"> <strong><code>stream_payroll_run(self, run_id: str) -&gt; Iterator[Dict]</code></strong><br><br>Alternative streaming entrypoint optimized for long-running streams (server-side events or chunked OData responses). Must support graceful resume via <code>Range</code>/continuation tokens when available. Provide hooks/callbacks for progress reporting (bytes, rows). Implement backpressure-safe behavior: consumer may <code>.close()</code> the iterator; adapter must cancel underlying request. Tests: ensure resume via continuation token and that consumer-initiated close cancels network activity. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/sap_adapter.py"> <strong><code>map_record_to_canonical(self, sap_record: Dict) -&gt; Tuple[Dict, List[str]]</code></strong><br><br>Convert one SAP record to the canonical schema used by the pipeline. Return <code>(canonical_record, diagnostics)</code> where <code>diagnostics</code> contains warnings about coercions, missing optional fields, or truncated values. Guidelines: be pure (no network I/O), deterministic, and small; prefer composition with <code>csv_mapper</code> if mapping rules are large. Validate types and normalize datetime/timezone, currency (ISO code), numeric formats, and ID canonicalization. Never drop non-sensitive fields silently — record mapping warnings. Unit tests should include numerous edge cases (nulls, locale-formatted numbers, timezone offsets). </td></tr><tr><td data-label="Technical Breakdown — src/adapters/sap_adapter.py"> <strong><code>validate_sap_response(self, http_resp: HttpResponse) -&gt; None</code></strong><br><br>Inspect transport responses and raise domain-specific exceptions: <code>TransientError</code> for 5xx and connection resets, <code>RateLimitError</code> for 429 with retry-after info, and <code>PermanentAdapterError</code> for 4xx other than 429. This function centralizes decoding of SAP-specific error structures (SOAP fault, OData error payload). It must return nothing on success and attach parsed <code>error_code</code>/<code>hint</code> to raised exceptions for downstream metrics. Tests: include SOAP fault, OAuth failure, malformed JSON, and 429 with/without <code>Retry-After</code>. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/sap_adapter.py"> <strong><code>_paginate(self, endpoint: str, params: Dict) -&gt; Iterator[Dict]</code></strong><br><br>Internal helper to implement robust pagination across varying SAP APIs (cursor, skip/top, next-link). Responsibilities: normalize different continuation token styles, unify backoff on transient errors, and yield page-by-page. Provide observability hooks: emit metrics per-page (<code>sap.page_latency_ms</code>, <code>sap.page_size</code>) and structured logging per page (idempotent). Must preserve idempotency semantics by returning stable page tokens where supported. Tests: emulate different pagination schemas and assert correct stop conditions (no infinite loops). </td></tr><tr><td data-label="Technical Breakdown — src/adapters/sap_adapter.py"> <strong><code>_refresh_token(self) -&gt; None</code></strong><br><br>Perform credential refresh (OAuth client credentials or certificate handshake) and atomically replace the cached bearer token. Must be safe to call concurrently: use an in-process lock to avoid thundering herd. Attach token expiry to a background refresh schedule if necessary. Never log raw secrets or tokens; when logging, redact. On refresh failure, follow <code>settings.strict</code> to decide whether to raise or mark degraded. Tests: concurrent callers, expired token, and injected refresh failures. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/sap_adapter.py"> <strong><code>_handle_rate_limit(self, http_resp: HttpResponse) -&gt; Optional[timedelta]</code></strong><br><br>Read rate-limit headers or SAP-specific headers to compute recommended backoff. Return parsed <code>Retry-After</code> or algorithmic backoff, and emit a <code>RateLimitEvent</code> metric with scope tags. This helper should be used by <code>_paginate</code> and <code>retry_on_transient</code>. Tests: various header formats, missing headers, and dynamic window size changes. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/sap_adapter.py"> <strong><code>retry_on_transient(self, func: Callable, *args, **kwargs) -&gt; Any</code></strong><br><br>Generic retry wrapper implementing exponential backoff with jitter, honoring <code>Retry-After</code> when provided, capping retries and total elapsed time. Retry policy must be configurable via settings and must treat specific adapter exceptions as non-retryable (e.g., auth failure with invalid client). This wrapper should be used sparingly — prefer retry at call-sites that know idempotency semantics. Tests: verify backoff, jitter bounds, honoring <code>Retry-After</code>, and non-retryable exception propagation. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/sap_adapter.py"> <strong><code>enqueue_export(self, canonical_rows: Iterable[Dict], metadata: Dict) -&gt; str</code></strong><br><br>Convenience method that prepares an export snapshot for downstream object-store persistence and enqueues an ingest job. Responsibilities: compute content hash, attach <code>source: sap</code>, ensure idempotency via idempotency key (content hash + batch_id), and return an internal <code>snapshot_id</code>. This method performs local I/O and queue enqueue (both must be abstracted behind injected factories in constructor to allow tests). Must validate that <code>canonical_rows</code> are serializable and error clearly if the export is duplicate. Tests: verify idempotency and failure modes when object-store is unavailable. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/sap_adapter.py"> <strong><code>reconcile_with_manifest(self, manifest: Dict) -&gt; Dict</code></strong><br><br>Compare a given manifest or external reference to what SAP reports for the same identifiers: check checksums, counts, and basic invariants. Return a reconciliation report <code>{matched: int, missing: [...], mismatched: [...], source_meta: {...}}</code>. This function should be pure except for calls to SAP read endpoints if manifest requires live checks (use a <code>live=False</code> flag to avoid network I/O in common test paths). Tests: simulate mismatches and partial matches; ensure clear, machine-readable output used by <code>recon_service</code>. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/sap_adapter.py"> <strong><code>_log_request(self, method: str, url: str, status: int, duration_ms: int, meta: Dict)</code></strong><br><br>Structured logging helper that attaches <code>trace_id</code>, <code>adapter</code>, <code>endpoint</code>, and redacts sensitive headers/fields. Must not include token values or full request body unless <code>settings.debug</code> is true and scrubbing rules are applied. Use a single logger instance to ensure consistent formatting. Tests: assert redaction behavior and presence of expected tags. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/sap_adapter.py"> <strong>Error classes (<code>AdapterError</code>, <code>TransientError</code>, <code>RateLimitError</code>, <code>AdapterClosedError</code>, <code>ConfigurationError</code>)</strong><br><br>Define adapter-specific exceptions with machine-readable attributes (<code>code</code>, <code>http_status</code>, <code>retry_after</code>, <code>sap_error_code</code>). Ensure all raised exceptions are serializable for audit logs and queue messages. Avoid leaking PII in exception messages; include <code>hint</code> fields for operators. Tests: ensure exception attributes are present and consistent. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/sap_adapter.py"> <strong>Observability & metrics integration</strong><br><br>Every public method that performs network I/O must emit metrics: request count, success/failure count, latency histogram, bytes transferred, and rows returned. Include labels/tags: <code>adapter=sap</code>, <code>env</code>, <code>region</code>, and endpoint name. Connect logs to trace context (W3C <code>traceparent</code>) and attach <code>operation_id</code> to long-running flows (fetch batch → process → enqueue). Provide a <code>dry_run</code> mode for tests that emits metrics to an in-memory sink. Tests: assert that metrics are emitted for success and failure paths. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/sap_adapter.py"> <strong>Security, secrets & redaction</strong><br><br>Never store plaintext credentials on disk. Use <code>SecretHandle</code> or equivalent; reveal only inside <code>connect()</code> or <code>_refresh_token()</code> and only for the minimal time required. When logging requests/responses, redact <code>Authorization</code>, <code>Set-Cookie</code>, and fields matching configured redaction keys (SSN, tax_id, bank_account). Ensure TLS certificate paths are validated at <code>configure_client</code> time if present. Tests: attempt to log requests and assert tokens are redacted. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/sap_adapter.py"> <strong>Concurrency & resource-safety</strong><br><br>Design iterators and background tasks so they can be used concurrently across worker threads/processes without sharing client instances unsafely. Use per-instance clients or thread-safe client pools. Provide explicit guidance that in multi-process deploys, the adapter should be re-instantiated per process. Document how to use the adapter from multiple concurrent worker coroutines and how to configure connection pools. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/sap_adapter.py"> <strong>Retry/idempotency contract</strong><br><br>Document which adapter operations are idempotent (list/read operations) versus non-idempotent (if any write operations are added later). Enforce that any operation wrapped with automatic retries must be safe to retry. Persist idempotency keys for enqueue/export operations to prevent duplicate processing. Include a short table in the module docstring mapping endpoints → idempotency semantics. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/sap_adapter.py"> <strong>Testing guidance</strong><br><br>— Unit tests: pure helpers (mapping, validation, pagination token parsing). <br>— Integration (fast): inject an in-memory HTTP server that returns fixture SAP responses (SOAP/OData) and assert adapter behavior under pagination, 429, and 5xx. <br>— Lifecycle tests: Use TestClient lifespans to ensure <code>connect()</code>/<code>disconnect()</code> run and background refresh tasks stop. <br>— Chaos: simulate partial failures (mid-page error) and confirm retry limits and clear error reporting. <br>— Security: automated tests to ensure tokens are never emitted to logs or metrics. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/sap_adapter.py"> <strong>Operational & maintenance checklist</strong><br><br>Before deploying to prod: ensure <code>strict=True</code>, TLS verification enabled, allowed IPs and firewall rules in place, credential rotation set and tested, monitoring dashboards wired for request latency and rate-limits, and alerting on <code>degraded</code> and repeated 5xx rates. Document runbook: how to manually refresh credentials, clear idempotency keys, and replay a batch. When SAP schema changes, add mapping tests and a migration note recording <code>field_map_version</code>. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/sap_adapter.py"> <strong>Implementation patterns & guardrails</strong><br><br>— <strong>Lazy imports</strong>: heavy SAP SDKs or SOAP parsers must be imported inside <code>configure_client</code> or <code>connect</code>. <br>— <strong>No implicit writes</strong>: adapter should not write snapshots or metadata without explicit <code>enqueue_export</code> / <code>persist_snapshot</code> API calls via injected factories. <br>— <strong>Small pure helpers</strong>: mapping and validation functions should be pure and small to enable property tests. <br>— <strong>Idempotent registration</strong>: if the adapter registers metrics or hooks globally, ensure registration is idempotent. <br>— <strong>Safe defaults</strong>: short timeouts, conservative retry counts, and limited concurrency by default. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/sap_adapter.py"> <strong>Recommended changelog & versioning</strong><br><br>Record adapter changes with semantic version bumps. Major when mapping or required credentials change, minor for performance/backoff tweaks, patch for bugfixes. Include <code>adapter_version</code> in exported manifests and audit entries to enable forward compatibility. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/sap_adapter.py"> <strong>Examples of failure modes and operator actions</strong><br><br>— <strong>Auth failure</strong>: rotate client secret/cert and call <code>connect()</code>; if repeated, mark <code>strict</code> to true for fail-fast. <br>— <strong>Rate-limited</strong>: inspect <code>sap_adapter</code> metrics for 429 spikes and increase pagination delay or use a queued backoff. <br>— <strong>Schema drift</strong>: mapping warnings grow — add mapping tests and bump <code>field_map_version</code>. <br>— <strong>Partial export duplicates</strong>: clear idempotency record after operator confirmation before manual re-run. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/sap_adapter.py"> <strong>Maintenance notes</strong><br><br>— When adding new SAP endpoints, add a small pure mapper, unit tests for edge-cases, and a fixture recorded from real SAP responses. <br>— Avoid adding business rules into this module — they belong in rules modules. <br>— Keep public methods minimal: <code>connect</code>, <code>disconnect</code>, <code>fetch_*</code>, <code>map_record_to_canonical</code>, <code>enqueue_export</code>, <code>health_check</code>. <br>— Provide <code>build_test_adapter</code> factory in test helpers to create adapters with fakes for HTTP, queue, and object-store. </td></tr></tbody></table></div><div class="row-count">Rows: 29</div></div><div class="table-caption" id="Table4" data-table="Docu_0164_04" style="margin-top:2mm;margin-left:3mm;"><strong>Table 4</strong></div>
<div class="table-wrapper" data-table-id="table-4"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown — src/adapters/workday_adapter.py"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown — src/adapters/workday_adapter.py</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — src/adapters/workday_adapter.py"> <strong>File-level responsibilities</strong><br><br>This module implements the Workday adapter: a small, well-documented surface that translates Workday's API semantics into the application's canonical ingestion model and contracts. Keep this file focused on: (1) API interaction (auth, pagination, rate-limit/backoff, error mapping), (2) deterministic canonical mapping (Workday → internal schema), (3) lightweight validation and enrichment, and (4) a small orchestration API used by the rest of the pipeline (fetch, stream, preview, export snapshot). At import time the module must do <strong>no</strong> network I/O and should only declare small pure helpers and the <code>WorkdayAdapter</code> class. All heavy dependencies (HTTP clients, secret providers) are injected or lazily imported inside factories. Document configuration knobs (timeouts, page_size, concurrency, strict_mapping_mode, token_cache_ttl, retry_policy) at the module top with examples of how they change runtime behavior. Unit-test pure logic; integration tests use recorded HTTP fixtures (VCR/cassettes) or a local HTTP simulator that reproduces Workday edge cases. <br><br><strong>Primary exported type</strong>: <code>WorkdayAdapter</code> (implements <code>AdapterBase</code> contract). The class exposes a small async/sync-friendly surface used by upload handlers and worker code: <code>fetch_employee_records</code>, <code>fetch_payroll_events</code>, <code>preview</code>, <code>export_snapshot</code>, <code>health_check</code>, and <code>close</code>. Prefer returning iterators/async generators for large result sets; callers control persistence and batching. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/workday_adapter.py"> <strong><code>class WorkdayAdapter(AdapterBase)</code></strong><br><br>Adapter class that encapsulates configuration, authentication, HTTP interaction, mapping, and per-call observability. It is the single place for Workday-specific behaviour and must be easily mockable in tests (accept <code>http_client</code> and <code>secrets_client</code> via constructor). Do not perform network operations in <code>__init__</code> beyond trivial local validation. Provide a <code>from_settings(settings)</code> classmethod factory to build a production-ready instance with proper timeouts, retry policy, and secrets wiring. Track instance-level state in private attributes only; be explicit about which fields are safe to persist across instances (token_cache only). Attach a stable <code>adapter_id</code> (hash of base URL + client_id fingerprint) to <code>instance.meta</code> for auditing. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/workday_adapter.py"> <strong><code>__init__(self, base_url: str, client_id: SecretRef, client_secret: SecretRef, tenant: str, http_client: Optional[HttpClient]=None, secrets_client: Optional[SecretsClient]=None, logger: Optional[Logger]=None, *, page_size: int=1000, timeout: float=30.0, retry_policy: Optional[RetryPolicy]=None, strict_mapping: bool=False)</code></strong><br><br>Constructor: store config and injected clients; validate types and ranges (page_size > 0, timeout > 0). Do <strong>not</strong> call the auth endpoint here. If <code>http_client</code> is not supplied, store a factory to lazily create one on first use. If <code>secrets_client</code> is supplied, only call it inside <code>get_token()</code> or startup handlers. Keep <code>strict_mapping</code> default <code>False</code> to avoid hard failures on new or missing Workday fields; tests exercise both modes. Unit tests should construct this object with fakes and assert no network calls are made on init. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/workday_adapter.py"> <strong><code>async authenticate(self) -&gt; None</code></strong><br><br>Public async method to ensure the adapter holds a valid access token. Responsibilities: fetch client credentials securely (via <code>secrets_client.reveal()</code>), call Workday token endpoint (OAuth2 client_credentials or Workday-specific variant), validate token scope and expiry, populate internal token cache with TTL, and emit a <code>auth_success</code> metric/datum including token_expiry_ts (not the token itself). Must raise a specific <code>AuthError</code> on fatal auth failure; in <code>strict</code> mode bubble up to startup so the app fails fast; otherwise set <code>self._degraded_auth=True</code> and emit event + log. Retries: exponential backoff for transient 5xx or network errors; on 401/400 when fetching token, do not retry indefinitely—treat as configuration error. Tests: mock token endpoint to simulate success, 5xx-backoff, and invalid credentials. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/workday_adapter.py"> <strong><code>_get_access_token(self) -&gt; str</code> (private; sync wrapper)</strong><br><br>Low-level helper that returns a cached token or triggers <code>authenticate</code>. Must be thread/async safe: implement a singleflight (inflight-lock) to avoid thundering herd on token refresh. Cache metadata (expiry) and refresh token a configurable <code>refresh_margin</code> before expiry. Never log tokens; only log token lifecycle events (fetched, refreshed, expired) with redaction. Unit tests should assert lock prevents multiple concurrent inflight refreshes. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/workday_adapter.py"> <strong><code>_build_headers(self) -&gt; Dict[str, str]</code></strong><br><br>Build request headers for API calls: <code>Authorization: Bearer &lt;token&gt;</code>, <code>Accept: application/json</code>, <code>User-Agent</code> including app version and adapter id, and optional <code>X-Trace-Id</code> propagation when <code>context</code> is available. Do not include secrets or long-lived credentials in headers beyond the bearer token. This is a pure function aside from reading token cache. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/workday_adapter.py"> <strong><code>_request(self, method: str, path: str, params: Optional[Dict]=None, json: Optional[Any]=None, stream: bool=False, timeout: Optional[float]=None) -&gt; Response</code></strong><br><br>Unified HTTP request wrapper. Responsibilities: (1) construct full URL safely (avoid double-slashes), (2) attach headers from <code>_build_headers</code>, (3) enforce timeouts (per-call override), (4) apply retry policy for idempotent verbs (GET) and backoff for 429/503 with <code>Retry-After</code> support, (5) map common Workday HTTP error shapes to domain exceptions (AuthError, RateLimitError, RemoteServiceError, NotFound), (6) implement optional circuit-breaker semantics on repeated failures (to avoid busy loop), and (7) capture structured telemetry: request_duration_ms, http_status, bytes_in/out, retries. This method must be idempotent for safe replays and must not swallow 4xx domain errors—map and re-raise. Tests: simulate 429 with Retry-After header, 401 leading to token refresh and retry once, and 5xx with configured retry count. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/workday_adapter.py"> <strong><code>_handle_response(self, response: Response) -&gt; Any</code> (private)</strong><br><br>Parse HTTP response bodies robustly: accept <code>application/json</code>, <code>application/xml</code>, and text fallbacks. Normalize known Workday response wrappers (e.g., <code>{ &quot;responses&quot;: [...], &quot;next&quot;: &quot;...&quot; }</code>) into canonical Python structures. On malformed payloads raise a <code>ParseError</code> with context (request id, status). Ensure PII is scrubbed from error messages before logging. Add unit tests for malformed JSON/XML and unexpected content types. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/workday_adapter.py"> <strong><code>fetch_employee_records(self, since: Optional[datetime]=None, page_size: Optional[int]=None) -&gt; Iterator[Dict]</code></strong><br><br>Top-level public method to stream employee records from Workday. Behavior: uses <code>_request</code> to call the relevant Workday endpoint, handles pagination transparently (cursor or offset-based depending on configured <code>pagination_strategy</code>), emits progress metrics (records_fetched, pages_fetched), yields raw Workday records (not yet mapped), and honors <code>since</code> to implement incremental pulls when supported. Must support both synchronous iterator and async generator (provide <code>fetch_employee_records_async</code> variant or make this method an async generator depending on codebase conventions). Guarantee mapping consumers can resume using a stable <code>resume_cursor</code> returned in metadata. Tests: pagination boundary conditions, empty pages, duplicate records when upstream cursor semantics are weak; assert resume_cursor reproducibility. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/workday_adapter.py"> <strong><code>fetch_payroll_events(self, payroll_period: Optional[str]=None, since: Optional[datetime]=None) -&gt; Iterator[Dict]</code></strong><br><br>Similar to <code>fetch_employee_records</code> but for payroll events (payments, deductions). Responsibilities: normalise date ranges per Workday calendar quirks, support bulk vs incremental modes, and return event-level metadata (event_id, effective_date). Signal when events are partial (e.g., page cut-off) and include <code>is_partial_page</code> flag in metadata. Tests: edge-case payroll periods that cross month boundaries, overlapping events, and timezone normalization. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/workday_adapter.py"> <strong><code>map_employee_to_canonical(self, wd_record: Dict) -&gt; Dict</code></strong><br><br>Pure, deterministic mapper: translate a single Workday employee record to the project's canonical schema. Responsibilities: explicit field mapping table, field-level transformations (dates → ISO8601 UTC, money → minor units, enums → canonical enums), handling of nested structures (addresses, tax ids), and policy for missing/unknown fields: if <code>strict_mapping</code> then raise <code>MappingError</code> with actionable hint; otherwise emit <code>mapping_warning</code> with field-level deltas and return a best-effort canonical record. Keep this function fully pure (no IO). Include a mapping manifest (small JSON doc stored alongside module) describing source→target mappings and acceptance criteria. Unit tests: golden fixtures demonstrating mapping for full record, minimal record, and schema-drifted record. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/workday_adapter.py"> <strong><code>batch_transform(self, records: Iterable[Dict], batch_size: int=500) -&gt; Iterator[List[Dict]]</code></strong><br><br>Take raw Workday records and produce canonical batches ready for validation/persistence. Responsibilities: apply <code>map_employee_to_canonical</code>, call <code>validate_record</code> for each item collecting diagnostics, annotate each record with <code>__adapter_meta__</code> containing source_id, adapter_id, and source_timestamp, and yield batches sized to <code>batch_size</code>. Ensure memory boundedness (prefer streaming). Tests: ensure batch boundary correctness, error aggregation, and that <code>__adapter_meta__</code> is deterministic. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/workday_adapter.py"> <strong><code>validate_record(self, canonical_record: Dict) -&gt; Tuple[bool, List[str]]</code></strong><br><br>Lightweight, deterministic validation: schema alignment checks (required fields exist and types match), value sanity checks (SSN format / NIK length, currency codes), and business rules that are Workday-specific (e.g., pay frequency consistency). Return <code>(ok, issues)</code> where <code>issues</code> contains machine-friendly codes and human-friendly messages. Validation must be pure to keep unit testing trivial. Do not perform heavy cross-row or DB-based validations here. Tests: a suite of invalid examples and expected issue codes. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/workday_adapter.py"> <strong><code>export_snapshot(self, canonical_batches: Iterable[List[Dict]], destination: StorageClient, *, snapshot_meta: Optional[Dict]=None) -&gt; Dict</code></strong><br><br>Persist a full snapshot of canonical records produced from Workday into the object store. Responsibilities: write in atomic, idempotent chunks (use temporary path + atomic rename), compute checksums (per-file and aggregate), write manifest that includes adapter version, applied mapping manifest version, source cursor, record counts, and runtime_fingerprint. Return a <code>snapshot_metadata</code> dict used by job enqueuers. Must avoid leaving partial blobs on failure—use a best-effort cleanup and emit audit events. Tests: simulate storage failures mid-write and assert either rollback or documented partial snapshots. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/workday_adapter.py"> <strong><code>preview(self, sample_size: int = 5) -&gt; Dict</code></strong><br><br>Convenience method used by CLI and UI: fetch a small sample of raw Workday records, run them through mapping/validation, and return a compact preview bundle: <code>[ { source: &lt;raw&gt;, canonical: &lt;mapped&gt;, issues: [...] }, ... ]</code> plus <code>fields_covered</code> and <code>suggested_mapping_delta</code> if fields missing. This method should be rate-limited and cached short-term (for interactive use). Do not use it in large sync pipelines. Tests: snapshot the preview output for regression testing. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/workday_adapter.py"> <strong><code>health_check(self, deep: bool=False) -&gt; Dict</code></strong><br><br>Return a small health dict usable by <code>server.health</code> endpoints and orchestration. <code>deep=False</code> returns last token status, cached endpoint reachability test boolean, and last_success_ts. <code>deep=True</code> performs a lightweight live call to a read-only Workday health endpoint or a minimal list employees call with <code>page_size=1</code> and short timeout. Always redact any tokens or sensitive headers in logs. Health result shape: <code>{ ok: bool, details: { auth: {...}, api: {...}, last_error: { code, message (redacted) } } }</code>. Tests: simulate auth expiry and network partitions. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/workday_adapter.py"> <strong><code>close(self) -&gt; None</code></strong><br><br>Cleanly close any underlying clients (http_client, background refresh tasks). Close must be idempotent and swallow/record exceptions rather than re-raising. Ensure any inflight token refresh task is cancelled safely. Called by app shutdown handlers. Tests: ensure close is idempotent and does not leak tasks. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/workday_adapter.py"> <strong>Private helpers & lower-level utilities</strong><br><br>— <strong><code>_paginate_cursor</code> / <code>_paginate_offset</code></strong>: implement two strategies and select by <code>pagination_strategy</code> config. <br>— <strong><code>_parse_workday_date</code></strong>: canonicalize Workday date/time quirks (business calendars), return timezone-aware UTC datetimes. <br>— <strong><code>_extract_ids</code></strong>: deterministic extraction of primary keys, used for idempotency keys. <br>— <strong><code>_retry_predicate</code></strong>: small pure helper for retry decisions (status codes, idempotent verbs). <br>— <strong><code>_backoff_sleep</code></strong>: injectable to allow tests to skip real sleeps. All private helpers must be small, pure or easily faked in tests. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/workday_adapter.py"> <strong>Error handling & exception mapping</strong><br><br>Map external HTTP and Workday-specific error shapes to the project's exception hierarchy with stable error codes. Specific cases: <code>429</code> → <code>RateLimitError</code> (include <code>retry_after</code> if present), <code>401</code> → trigger token refresh once then raise <code>AuthError</code> if still unauthorized, <code>404</code> → <code>NotFound</code>, other 4xx → <code>BadRequest</code> with Workday <code>error_description</code> if available (redacted), 5xx → <code>RemoteServiceError</code>. Attach <code>adapter_id</code>, <code>request_id</code>, and <code>trace_id</code> to exceptions for SRE lookup. Never include raw request payloads or full response bodies containing PII in exception messages. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/workday_adapter.py"> <strong>Observability & telemetry</strong><br><br>Emit structured metrics and traces at key points: auth latency and success/failure, per-endpoint request duration histograms, page size vs throughput counters, mapping error counters (by code), records_processed, records_failed_mapping, snapshot_write_duration, and rate-limit throttles. Integrate trace context (W3C <code>traceparent</code>) and include <code>adapter_id</code> and <code>applied_mapping_version</code> as tags. Logs must be structured and sampled for heavy objects (log only representative failing record keys, not full personal data). Provide a dry-run logging mode for unit tests (<code>logger=TestMemoryLogger</code>). </td></tr><tr><td data-label="Technical Breakdown — src/adapters/workday_adapter.py"> <strong>Security & secrets</strong><br><br>— Always fetch client credentials from <code>secrets_client</code> lazily and only when <code>authenticate()</code> runs. <br>— Use TLS with certificate verification; allow <code>ca_bundle</code> override only via explicit settings and document risk. <br>— Do not persist tokens to disk. <br>— Ensure mapping removes or canonicalizes PII fields not required downstream (SSNs, passport numbers) and provides a clear policy for when they are retained. <br>— Log redaction: implement a field-level redaction list configurable from <code>settings.redact_keys</code>. Tests: assert that logs and exceptions never contain an SSN. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/workday_adapter.py"> <strong>Idempotency & resume semantics</strong><br><br>— Produce deterministic <code>resume_cursor</code> for every page fetched (prefer server-provided cursor; if not available, use a content-hash + last_seen_id). <br>— When exporting snapshots, include the <code>resume_cursor</code> in manifest so downstream jobs can re-run idempotently. <br>— Provide a stable id extraction strategy used both for de-dup and audit keys. Tests: ingest same snapshot twice and assert idempotency behavior. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/workday_adapter.py"> <strong>Performance & concurrency</strong><br><br>— Use paged streaming to keep memory usage constant. <br>— Allow configurable parallelism for parallel page fetch (careful with Workday rate limits). <br>— Ensure HTTP client connection pooling is configurable and closed on <code>close()</code>. <br>— Provide a knob to switch between synchronous and async modes if the codebase mixes both. Load tests: measure records/sec across representative org sizes and tune <code>page_size</code> and <code>parallel_pages</code>. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/workday_adapter.py"> <strong>Testing strategy</strong><br><br>Unit tests (pure): mapping, validation, date parsing, retry predicates. <br>Integration tests (fast): <code>http_client</code> replaced with deterministic fake that simulates pagination, 429/Retry-After, 401, malformed JSON. Use recorded cassettes for slow Workday integration tests; place those tests behind <code>integration</code> marker in CI. <br>Contract tests: assert expected request shapes (headers, query params) for each endpoint. <br>Chaos tests: simulate token rotation mid-stream and ensure one refresh/retry recovers. <br>Security tests: automated log-scraping ensuring no secrets leak. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/workday_adapter.py"> <strong>Recommended tests & CI checks</strong><br><br>1. Unit: mapping golden fixtures, date parsing, validation code paths.<br>2. Integration (fast): fake Workday server tests for pagination, rate-limits, partial pages.<br>3. End-to-end: recorded HTTP cassette run that produces a snapshot manifest validated by <code>manifest</code> validators.<br>4. Regression: snapshot preview comparisons for expected canonical fields.<br>5. Security: assert redaction rules applied to sample logs. Ensure these run in PR gating. </td></tr><tr><td data-label="Technical Breakdown — src/adapters/workday_adapter.py"> <strong>Operational & maintenance notes</strong><br><br>— Document the adapter version and mapping manifest version in the snapshot manifest. <br>— When Workday API schema changes, add a new mapping manifest version and provide an automatic <code>mapping_migration</code> checklist. <br>— Keep the file small: move complex helpers (retry/backoff, pagination strategies) into <code>adapters/_http_helpers.py</code> or shared infra modules. <br>— When adding new endpoints, add explicit tests for header propagation, retries, and cursor semantics. <br>— For debugging, provide a <code>--preview</code> CLI that calls <code>preview()</code> and outputs a redacted JSON (safe for developer use). </td></tr><tr><td data-label="Technical Breakdown — src/adapters/workday_adapter.py"> <strong>Quick developer checklist before merging changes</strong><br><br>— No network I/O at import time. <br>— All secrets accessed only via <code>secrets_client</code> inside <code>authenticate</code>. <br>— Mapping functions are pure and covered by golden fixtures. <br>— Rate-limit and retry behaviors are covered by tests. <br>— Health endpoint returns minimal safe info by default and <code>deep</code> probe is guarded. <br>— Logging redaction covers configured keys. <br>— Snapshot manifest includes <code>adapter_version</code>, <code>mapping_version</code>, <code>source_cursor</code>, <code>record_count</code>, <code>checksums</code>. </td></tr></tbody></table></div><div class="row-count">Rows: 27</div></div><div class="table-caption" id="Table5" data-table="Docu_0164_05" style="margin-top:2mm;margin-left:3mm;"><strong>Table 5</strong></div>
<div class="table-wrapper" data-table-id="table-5"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown — server.py"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown — server.py</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — server.py"> <strong>File-level responsibilities</strong><br><br>This module is the canonical HTTP surface for the service: it <strong>defines and wires the public endpoints</strong>, the request lifecycle (middlewares, auth, tracing), server configuration for the ASGI runner, and the small glue that converts external triggers (HTTP/CLI/adapters) into pipeline jobs. Keep this file focused on application-level concerns (routing, lifecycles, and operational knobs). All heavy logic (parsing, validation, calculation, exports, persistence) must remain in their respective modules — <code>server.py</code> should only call into them. Document the public endpoints at file top (<code>/health</code>, <code>/simulate</code>, <code>/upload</code>, <code>/export</code>, <code>/reconcile</code>) and list their behavior, auth requirements, rate-limit class, and observability expectations. Prefer pure, small functions (factory-style) and explicit <code>app.state</code> keys for shared resources. Avoid top-level side effects: imports of heavy SDKs must be lazy; creating the app should be side-effect free. Unit tests must be able to import <code>server</code> without network I/O or database access. </td></tr><tr><td data-label="Technical Breakdown — server.py"> <strong><code>create_server(settings: Settings) -&gt; FastAPI|ASGIApp</code></strong><br><br>Factory function that returns a ready-to-use ASGI application but <strong>must not</strong> perform external I/O (no network calls, no DB connections, no opening files other than safe in-memory ones). Responsibilities: validate the <code>settings</code> surface (guard against missing required values when <code>settings.strict</code>), construct the app instance with <code>title</code>, <code>openapi_url</code> (toggled by env), <code>docs_url</code> (dev only), and deterministic <code>app.state</code> keys: <code>_registered = set()</code>, <code>_extensions = {}</code>, <code>_workers = []</code>, <code>runtime_fingerprint</code>. Call the smaller registration helpers in a deterministic, documented order: <code>configure_logging</code>, <code>configure_tracing</code> (if enabled), <code>register_middlewares</code>, <code>register_routes</code>, <code>configure_error_handlers</code>, <code>register_lifespan_handlers</code> (startup + shutdown), and <code>attach_metric_endpoints</code> (if metrics enabled). Attach lazy secret accessors and a <code>shutdown_token</code> object to <code>app.state</code>. Ensure idempotence: calling <code>create_server</code> repeatedly with the same <code>settings</code> produces functionally-equivalent apps. Unit tests should import this function, create an app with minimal <code>Settings</code> and assert required <code>app.state</code> keys exist and no external side-effects occurred. </td></tr><tr><td data-label="Technical Breakdown — server.py"> <strong><code>register_middlewares(app, settings) -&gt; None</code></strong><br><br>Registers cross-cutting concerns. Middleware order must be explicit and preserved: 1) request-id/trace propagation (W3C <code>traceparent</code> normalization), 2) request size and content-type guards, 3) CORS (restrictive defaults; only open in dev), 4) auth introspection (fast path), 5) sampling/redaction layer for request bodies and headers, 6) structured request/response logging (non-blocking), 7) rate limiter (per-endpoint classes), 8) gzip/response compression, then 9) a final exception-normalizing middleware. Each registration must be idempotent (check <code>app.state._registered</code>). Middlewares must be lightweight: expensive work (JWKS refresh, token verification keys) must live behind async cached clients that are created by <code>init_extensions</code> and invoked from middleware; the middleware should call those clients with strict per-request timeouts. When logging request bodies for diagnostics, enforce sampling, and apply redaction rules from <code>settings.log_redaction</code>. Provide a <code>middleware_mode</code> switch (<code>noop|stub|full</code>) for test harnesses. Include unit tests that exercise header normalization, token hashing in logs, rate-limit buckets, and re-registration no-ops. </td></tr><tr><td data-label="Technical Breakdown — server.py"> <strong><code>register_routes(app, settings) -&gt; None</code></strong><br><br>Mount and include routers by surface: <code>api.v1.*</code> (public), <code>internal.*</code> (diagnostics/admin, mounted under <code>/internal</code> or separate host), and <code>health</code> endpoints. Routes should import router modules (which must be pure declarations). When registering each router check <code>app.state._registered</code> to avoid duplicate mounts. For conditional inclusion (debug routes, experimental endpoints), gate with explicit flags in <code>settings</code>. Route-level dependencies (FastAPI <code>Depends</code>) should enforce auth scopes and per-route rate-limits; do not implement authorization ad-hoc inside handlers. Provide compact docstrings for each public endpoint: expected request payload, permissions required, sampling/retention behavior, and expected side-effects (snapshot created, job enqueued, etc.). For <code>server.py</code> specifically, prioritize these endpoints and their constraints: <strong><code>/health</code></strong> (<code>GET</code> — lightweight liveness), <strong><code>/deep-health</code></strong> (<code>GET</code> — readiness, may exercise short timeboxed checks), <strong><code>/upload</code></strong> (<code>POST</code> — multipart/file or JSON payload, size-limited, returns snapshot-id), <strong><code>/simulate</code></strong> (<code>POST</code> — synchronous single-record rules run for debugging, dev-only or scoped), <strong><code>/export</code></strong> (<code>POST</code> — request to generate exports for a job id or snapshot), <strong><code>/reconcile</code></strong> (<code>POST</code>/<code>GET</code> — trigger reconcile checks). Unit tests should assert that the OpenAPI schema contains only the routes enabled by the given <code>settings</code>. </td></tr><tr><td data-label="Technical Breakdown — server.py"> <strong><code>configure_error_handlers(app, settings) -&gt; None</code></strong><br><br>Attach global exception handlers mapping domain exceptions to stable, scrubbed envelopes: <code>{code, message, trace_id, hint}</code>. Canonical mappings: <code>ValidationError -&gt; 400</code>, <code>AuthError -&gt; 401/403</code>, <code>NotFound -&gt; 404</code>, <code>RateLimitExceeded -&gt; 429</code>, <code>ExternalServiceError -&gt; 502/503</code> (depend on transient vs fatal), and uncaught exceptions -> <code>500</code> with minimal user-facing message. Error handlers MUST remove or redact PII and must not include stacktraces unless <code>settings.env == &quot;dev&quot;</code> and the request has a dev authorization scope. Each handler should also emit a structured telemetry event (error type, sanitized message, trace_id, sampling flag) and increment relevant metrics. Include tests that raise each exception inside a route and assert the HTTP status, envelope shape, and that no PII is leaked. </td></tr><tr><td data-label="Technical Breakdown — server.py"> <strong><code>make_health_handler(app, deep: bool=False) -&gt; Callable</code></strong><br><br>Factory that returns a thin health endpoint handler. For <code>deep=False</code> (<code>/health</code>) perform only local and cheap checks: process alive, <code>app.state._extensions</code> keys present, minimal cache availability, and optionally worker process supervision. For <code>deep=True</code> (<code>/deep-health</code>) execute timeboxed async checks against required external dependencies (DB connect ping, object store head, queue connectivity) with per-check timeouts and aggregate into <code>ok|degraded|fail</code>. Include <code>runtime_fingerprint</code>, <code>applied_rule_version</code> (if available), and a condensed <code>checks</code> list (<code>name, ok, latency_ms, meta</code>). Avoid exposing secrets or application config. Tests should mock <code>app.state.health_checks</code> entries to assert aggregation logic and timeout behavior. </td></tr><tr><td data-label="Technical Breakdown — server.py"> <strong><code>upload_handler(request) -&gt; Response</code></strong><br><br>Accept files or structured payloads and perform minimal synchronous validation and dedup checks, then: 1) sanitize metadata, 2) compute a content-hash (idempotency key), 3) persist snapshot atomically to object store (or stage path) via <code>app.state._extensions[&#x27;object_store&#x27;].put_snapshot()</code> factory method, 4) persist metadata record to metadata DB via <code>app.state._extensions[&#x27;metadata_db&#x27;]</code>, and 5) enqueue an ingest job onto the work queue. The handler must: enforce max payload size, validate MIME types, apply sampling/redaction to any stored logs, and return <code>202 Accepted</code> with <code>snapshot_id</code> and job correlation id. For large file uploads prefer streaming to the object store; the handler must never buffer entire large files in memory. Ensure the process is idempotent (based on content-hash) and that repeated upload attempts return the same <code>snapshot_id</code> instead of creating duplicate snapshots. Provide explicit guidance for transactional ordering (first store snapshot, then write metadata record pointing to snapshot URI, then enqueue) to support atomic retry semantics. Unit and integration tests: small fixture upload, repeated upload idempotency, size limit enforcement, metadata written correctly. </td></tr><tr><td data-label="Technical Breakdown — server.py"> <strong><code>simulate_handler(request) -&gt; Response</code></strong><br><br>Dev/debug endpoint to execute the calculation pipeline synchronously against a single canonical record and return the decision trace. Strictly gate by <code>settings</code> (dev-only or protected by <code>developer</code> scope) because this exposes internal traces and possibly sensitive data. Implementation must: validate input schema with <code>parsers.py</code>/<code>validators.py</code>, build a canonical record without side effects, call the pipeline entrypoints (<code>ptkp</code>, <code>ter</code>, <code>progressive</code>, <code>dtp_engine</code>) in a sandboxed, deterministic way (no DB writes), collect the decision trace, and return <code>{input, canonical, results, trace}</code>. Enforce CPU/time limits and fail with <code>504</code> if the run exceeds configured per-request timeout. Tests: deterministic runs for golden fixtures and refusal when unauthorized or disabled. </td></tr><tr><td data-label="Technical Breakdown — server.py"> <strong><code>export_handler(request) -&gt; Response</code></strong><br><br>Endpoint to create or re-run an export/bundle for a given job or snapshot. Responsibilities: validate request, check job and applied rule version, reserve a staging area in object store (unique staging path), call <code>manifest.py</code> and exporter modules (<code>bukti_potong_csv</code>, <code>bukti_potong_xml</code>, <code>spt_masa_prefill</code>) to produce artifacts into staging, run bundle validation/signing (HMAC/signature), atomically publish by renaming or moving to the public area, and update export metadata in DB. Use optimistic concurrency controls to avoid simultaneous publishers for the same target. Return <code>202 Accepted</code> with <code>export_id</code> and publish status. Ensure operations are idempotent (export produce same checksums for same inputs and rule versions) and verify checksums before publish. Tests: simulate exporter run with fakes, assert manifest contents and atomic rename semantics. </td></tr><tr><td data-label="Technical Breakdown — server.py"> <strong><code>reconcile_handler(request) -&gt; Response</code></strong><br><br>Trigger reconcile checks or return stored reconcile results. For on-demand triggers: validate permissions, optionally accept <code>from</code>/<code>to</code> ranges, enqueue a reconcile job, and return <code>202</code>. For status queries: read from <code>recon_db</code> and return compact results (<code>summary</code>, <code>failures_count</code>, <code>sample_failures</code>). Reconcile operations may be heavy — ensure request handlers are lightweight and job orchestration is handled by background workers. Provide rate-limits and RBAC enforcement (internal/admin only). Tests: endpoint enqueues job, status read returns expected shapes. </td></tr><tr><td data-label="Technical Breakdown — server.py"> <strong><code>register_lifespan_handlers(app, settings) -&gt; None</code></strong><br><br>Attach robust startup/shutdown handlers that: validate required <code>app.state._extensions</code> keys, sequentially call <code>connect()</code> / <code>acquire()</code> on each extension with per-extension timeouts and circuit-breaker semantics, optionally warm caches (if <code>settings.startup_mode == &quot;full&quot;</code>), and emit a sanitized startup audit event. On shutdown, close in reverse order, flush telemetry, stop workers and background tasks, wait for a configured <code>settings.shutdown_grace_period</code>, and catch/log but never re-raise exceptions. Use <code>asyncio.wait_for</code> for each external call. Respect <code>settings.strict</code>: if <code>True</code>, failure of a critical extension should raise during startup to fail process start; if <code>False</code>, set <code>app.state._startup_degraded = True</code> and continue. Unit/integration tests: use TestClient lifespan context to assert connect/close calls occurred. </td></tr><tr><td data-label="Technical Breakdown — server.py"> <strong><code>start_background_workers(app, settings) -&gt; None</code></strong><br><br>Start in-process supervisors for short-lived dev/test workers when allowed; otherwise ensure worker processes are external (recommended). If in-process: create asyncio tasks with supervision, attach to <code>app.state._workers</code>, register heartbeat/watchdog, and ensure tasks respect cancellation and have restart caps. Workers must acknowledge durable messages only after persist to external queue or DB. In production, prefer a no-op that emits instructions for external worker orchestration. Tests: start a small in-memory queue and a worker, enqueue work, assert completion, ensure shutdown cancels tasks. </td></tr><tr><td data-label="Technical Breakdown — server.py"> <strong><code>configure_tracing(app, settings) -&gt; None</code></strong><br><br>Initialize tracing integration (OTel or vendor). Do lazy imports inside this function. Integrate with request-id/trace middleware, ensure <code>trace_id</code> flows into logs and telemetry, and attach a <code>tracer_provider</code> into <code>app.state</code> with <code>shutdown</code> semantics. Provide <code>dry_run</code> option for tests to replace exporters with in-memory collectors. Trace sampling and redaction must be configurable; do not send full request bodies to traces. Test that a request generates a span with expected attributes and that <code>trace_id</code> is present in logs. </td></tr><tr><td data-label="Technical Breakdown — server.py"> <strong><code>configure_logging(app, settings) -&gt; None</code></strong><br><br>Set up structured logging (JSON lines in prod) using a namespaced logger (<code>pph21.server</code>). Attach PII redaction filters and integrate <code>trace_id</code>, <code>request_id</code>, and <code>service</code> into each record. Support <code>MemoryHandler</code> for test <code>dry_run</code> configuration. Ensure handler initialization is tolerant to transient network sink failures (non-blocking with backoff). Document how to extend redaction lists and include tests verifying token redaction in a sample log record. </td></tr><tr><td data-label="Technical Breakdown — server.py"> <strong><code>get_uvicorn_config(settings) -&gt; Dict[str,Any]</code></strong><br><br>Return ASGI server launch configuration derived from <code>settings</code>: <code>host</code>, <code>port</code>, <code>workers</code>, <code>limit_concurrency</code>, <code>proxy_headers=True</code>, <code>loop=&quot;auto&quot;</code>, <code>log_config</code> pointing to <code>configure_logging</code> output, and sensible timeouts. Validate numeric ranges and provide overrides for process managers. Document production recommendations (use process manager, readiness/liveness probes). Unit tests: assert deterministic config for <code>dev</code> vs <code>prod</code> <code>Settings</code>. </td></tr><tr><td data-label="Technical Breakdown — server.py"> <strong><code>main(argv: Optional[List[str]] = None) -&gt; int</code></strong><br><br>Small CLI entrypoint for local dev and CI checks. Responsibilities: parse CLI args (<code>--env-file</code>, <code>--check</code>, <code>--validate-external</code>, <code>--reload</code>), build <code>Settings</code> (with secrets redacted), run validation and optionally external checks (only when explicitly requested), and either exit with non-zero code on validation failure or call <code>uvicorn.run(app, **get_uvicorn_config(settings))</code>. Avoid starting real network listeners during <code>--check</code>. Tests: run CLI with <code>--check</code> to assert exit codes for valid/invalid settings. </td></tr><tr><td data-label="Technical Breakdown — server.py"> <strong><code>_idempotence_and_registration_guards</code></strong><br><br>Utility pattern used across the module: every register function (routes, middlewares, extensions, health handlers) must check <code>app.state._registered</code> to ensure idempotence. Use constants for registration keys (e.g., <code>&#x27;middleware:server&#x27;</code>, <code>&#x27;routes:public&#x27;</code>) and record a human-readable timestamp when the registration happened. Tests should call registration functions multiple times and assert no duplicate middleware/routes are added. </td></tr><tr><td data-label="Technical Breakdown — server.py"> <strong><code>lazy_import_helpers</code></strong><br><br>Helpers to import heavy dependencies only when needed (e.g., exporters, SDK clients). Implement a small <code>lazy_import(name)</code> pattern or inline <code>if enabled: from heavy.module import Client</code>. This keeps import-time fast and safe for unit tests. Document which modules are heavy and where lazy import is used (object store clients, tracing SDKs, adapter SDKs). </td></tr><tr><td data-label="Technical Breakdown — server.py"> <strong><code>attach_extensions(app, settings) -&gt; None</code></strong><br><br>Populate <code>app.state._extensions</code> with factory wrappers (objects exposing <code>connect()</code>/<code>close()</code> or async <code>aclose()</code>). Do <strong>not</strong> open connections here. When configuration incomplete and <code>settings.strict</code> is false, install <code>NullClient</code> placeholders that raise on use and log clear diagnostic messages. Provide a test seam so unit tests can inject fakes (<code>app.state._extensions[&#x27;db&#x27;] = FakeDb</code>). </td></tr><tr><td data-label="Technical Breakdown — server.py"> <strong><code>metrics_and_observability_bindings</code></strong><br><br>Register metrics endpoints and collectors (Prometheus or other). Expose a <code>/metrics</code> or <code>/internal/metrics</code> endpoint behind RBAC in production. Instrument: request count, latency histograms, route-specific error counts, ingest rate, queue depth (via extension API), worker throughput, export duration, and startup duration. Add tracing correlation: expose <code>trace_id</code> in logs and include <code>runtime_fingerprint</code> on health responses. Tests: simulate requests and assert metric increments. </td></tr><tr><td data-label="Technical Breakdown — server.py"> <strong><code>security_and_rbac</code></strong><br><br>Server-level security patterns: default-deny CORS, strict content-type checks on payloads, scoped auth for each endpoint, and separate host binding for <code>internal</code> routes if supported. All tokens/secrets must be <code>SecretRef</code> and only <code>.reveal()</code> inside startup handlers or when strictly necessary. Use short-lived service accounts for inter-service calls and avoid embedding long-lived credentials in config. Provide test fixtures for auth: <code>admin_token</code>, <code>developer_token</code>, <code>noop</code> for unit tests. </td></tr><tr><td data-label="Technical Breakdown — server.py"> <strong><code>testing_recommendations</code></strong><br><br>Unit tests: create a minimal <code>Settings</code> and call <code>create_server</code> asserting no external calls and presence of registration markers. Test each register function in isolation using a fake <code>app</code> object. Integration tests: use FastAPI <code>TestClient</code> for lifecycle tests (startup/shutdown handlers executed), test <code>upload</code> with small fixture uploaded to a fake object store extension, simulate in-memory worker execution. E2E smoke: containerized stack (postgres/sqlite, object store minio, queue) to validate end-to-end (upload → enqueue → worker → export). Include chaos tests that simulate startup dependency failure toggling <code>settings.strict</code>. </td></tr><tr><td data-label="Technical Breakdown — server.py"> <strong><code>observability_and_operational_checklist</code></strong><br><br>Before production: ensure <code>strict=True</code>, CORS locked to allowed origins, rate-limits configured, telemetry exporters validated, redaction rules fully cover PII keys, and CI runs <code>--check</code> and sample external checks. Confirm liveness/readiness probes are wired to the <code>/health</code> and <code>/deep-health</code> semantics described here. Document runbooks for degraded mode and failure scenarios (e.g., object store read-only, MQ down) and ensure <code>/health</code> returns <code>degraded</code> while <code>deep-health</code> signals failure. </td></tr><tr><td data-label="Technical Breakdown — server.py"> <strong><code>implementation_guardrails</code></strong><br><br>— Keep <code>server.py</code> focused: complex behavior belongs in small modules. <br>— Idempotence: always check <code>app.state._registered</code>. <br>— Lazy imports for heavy SDKs. <br>— Attach factories (not live clients) to <code>app.state._extensions</code>. <br>— Timebox external calls with <code>asyncio.wait_for</code>. <br>— Never leak secrets in logs or error bodies. <br>— Prefer returning <code>202 Accepted</code> for long-running operations and offer a status endpoint for progress. <br>— Ensure endpoints are well-documented and covered by unit/integration tests. </td></tr><tr><td data-label="Technical Breakdown — server.py"> <strong>Recommended tests & CI checks (summary)</strong><br><br>1. Unit: <code>create_server</code>, <code>get_uvicorn_config</code>, register helpers, idempotence checks. <br>2. Integration: TestClient lifecycle to confirm startup/shutdown call <code>connect()/close()</code> on extensions. <br>3. Middleware contract: trace propagation, token redaction, rate-limiter behavior. <br>4. Upload flow: small fixture upload → snapshot created → metadata recorded → job enqueued. <br>5. Simulate: deterministic rule run on golden fixture. <br>6. Export: produce and validate bundle in staging and atomic publish. <br>7. Chaos: simulate partial failures and assert <code>strict</code> vs <code>lenient</code> behavior. </td></tr><tr><td data-label="Technical Breakdown — server.py"> <strong>Maintenance & developer notes</strong><br><br>— When adding a new route, add a route-level rate-limit category and an auth scope. <br>— When adding an extension client, register it in <code>app.state._extensions</code> and add connect/close semantics to lifespan handlers. <br>— When adding background worker types, document safe in-process vs external modes. <br>— Keep server.py < ~400–800 lines: if it grows larger, split route wiring and lifecycle management into small modules (<code>routes.py</code>, <code>lifespan.py</code>, <code>middleware_registry.py</code>) while keeping <code>create_server</code> as the small orchestrator. </td></tr></tbody></table></div><div class="row-count">Rows: 26</div></div><div class="table-caption" id="Table6" data-table="Docu_0164_06" style="margin-top:2mm;margin-left:3mm;"><strong>Table 6</strong></div>
<div class="table-wrapper" data-table-id="table-6"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown — health.py"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown — health.py</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — health.py"> <strong>File-level responsibilities</strong><br><br>This module centralizes the application's liveness/readiness probe logic and exposes the lightweight HTTP handlers used by the server (<code>/health</code>, <code>/deep-health</code>, or equivalent). Keep this file focused: it should only contain the small, deterministic coordination logic for running a set of short health checks, assembling a stable envelope, and returning HTTP responses. Heavy or environment-specific checks (DB connectivity, S3, MQ, complex schema validation) must live in small, testable helper modules or in extension adapters and be wired in via registration functions. The module must: define the probe contract (signature + result shape), expose functions to register checks, run checks with per-check timeouts and concurrency limits, map aggregated outcomes to a stable HTTP envelope, and provide test hooks (in-process stub checks, dry-run mode, synchronous runner for unit tests). Avoid opening network connections at import time. Document the response schema at the head of the file (fields: <code>status</code>, <code>checks[]</code>, <code>runtime_fingerprint</code>, <code>applied_rule_version</code>, <code>timestamp</code>, <code>duration_ms</code>). </td></tr><tr><td data-label="Technical Breakdown — health.py"> <strong>Public contract & response schema</strong><br><br>Define and document a single canonical response envelope used by both "shallow" (liveness) and "deep" (readiness) probes. Minimal recommended shape: <code>{ &quot;status&quot;: &quot;ok|degraded|fail&quot;, &quot;timestamp&quot;: &quot;&lt;ISO8601&gt;&quot;, &quot;duration_ms&quot;: &lt;int&gt;, &quot;runtime_fingerprint&quot;: &quot;&lt;str&gt;&quot;, &quot;applied_rule_version&quot;: &quot;&lt;str|null&gt;&quot;, &quot;checks&quot;: [ { &quot;name&quot;: &quot;&lt;id&gt;&quot;, &quot;ok&quot;: true|false, &quot;required&quot;: true|false, &quot;duration_ms&quot;: &lt;int&gt;, &quot;meta&quot;: { ... }, &quot;error&quot;: &quot;&lt;short sanitized message|null&gt;&quot; } ] }</code>. Insist on small, stable JSON fields (avoid embedding long stack traces). The mapping from <code>checks[]</code> to top-level <code>status</code> must be explicit: any required check failing -> <code>fail</code>; optional failing -> <code>degraded</code> if at least one optional failed but all required OK; otherwise <code>ok</code>. Unit tests must assert envelope stability across versions. </td></tr><tr><td data-label="Technical Breakdown — health.py"> <strong><code>HealthCheck</code> contract (interface)</strong><br><br>Define a canonical type/signature for registered checks: <code>async def check(ctx) -&gt; dict</code> or <code>def check(ctx) -&gt; dict</code> with <code>ctx</code> minimal and well-documented (<code>{timeout_ms, trace_id, request}</code> or None). The check returns: <code>{&quot;name&quot;: &quot;&lt;id&gt;&quot;, &quot;ok&quot;: bool, &quot;meta&quot;: dict | None}</code> or raises an exception. Provide an adapter that normalizes these outputs (catch exceptions and convert to <code>{ok: False, error: &quot;&lt;sanitized message&gt;&quot;}</code>). Require checks to be fast (< configured per-check timeout) and side-effect free; if a check needs warm-up or heavy I/O it must be gated behind deep-probe only. Tests should include synchronous and async check variants. </td></tr><tr><td data-label="Technical Breakdown — health.py"> <strong><code>register_health_check(app, *, name, callable, required=True, tags=None)</code></strong><br><br>Public registration helper that stores checks in <code>app.state.health_checks</code> (or equivalent registry). Responsibilities: validate arguments (unique <code>name</code>), wrap callable to canonical adapter, store structured metadata <code>(required, tags)</code>. Must be idempotent (re-register with same name is a no-op or explicit replace with a flag). Should support optional <code>priority</code> or <code>category</code> for ordering. Provide simple unit tests asserting registry idempotence and replacement behavior. For test harnesses allow injecting <code>app.state.health_checks = []</code> or a fake registry. </td></tr><tr><td data-label="Technical Breakdown — health.py"> <strong><code>validate_health_registry(app)</code></strong><br><br>Small sanity function executed at startup (or lazily on first probe) to check the registry shape: unique names, presence of at least one liveness check, and that required checks are configured. In <code>settings.strict</code> mode this should raise during startup if required checks are missing; otherwise log and mark health as degraded. Write unit tests to simulate missing checks and toggling <code>strict</code>. </td></tr><tr><td data-label="Technical Breakdown — health.py"> <strong><code>_run_check_with_timeout(check_callable, timeout_ms, ctx) -&gt; dict</code></strong><br><br>Internal wrapper that executes one check enforcing a per-check timeout and capturing duration, exceptions, and structured metadata. Implementation notes: use <code>asyncio.wait_for</code> for async checks; for sync checks run them in an executor but keep their runtime small; measure wall-clock duration using a monotonic clock; sanitize exception messages (strip PII); return normalized record: <code>{name, ok, required, duration_ms, meta, error}</code>. Must always return a dict (never propagate exceptions). Unit tests must assert behavior for slow checks (timeout->ok=False + error marker), raising checks, and value-returning checks. </td></tr><tr><td data-label="Technical Breakdown — health.py"> <strong><code>run_health_checks(app, *, deep: bool=False, per_check_timeout_ms: Optional[int]=None, overall_timeout_ms: Optional[int]=None) -&gt; dict</code></strong><br><br>Coordinator that runs the configured checks in parallel with controlled concurrency, aggregates results, and returns the complete <code>checks</code> list plus derived <code>status</code> and timing. Responsibilities and guardrails: 1) Filter which checks to run by <code>deep</code> flag and check tags (<code>deep</code> may include expensive checks). 2) Use a concurrency limiter (Semaphore or an executor pool) to avoid thundering on resource-limited dependencies. 3) Apply per-check timeouts via <code>_run_check_with_timeout</code>. 4) Apply an optional <code>overall_timeout_ms</code> to abort the whole probe early and mark unknown checks as <code>degraded</code> or <code>fail</code> depending on required flags. 5) If a check smells like flaky (configurable failure-rate threshold), include <code>meta.risky=true</code>. Return shape must include <code>duration_ms</code> and per-check durations. Tests: simulate mixed sync/async checks, slow external checks, and overall timeout behavior. </td></tr><tr><td data-label="Technical Breakdown — health.py"> <strong><code>make_health_handler(app, *, deep=False, require_auth_for_deep=True) -&gt; Callable</code></strong><br><br>Factory that returns the ASGI/WSGI route handler callable for the given probe. Responsibilities: 1) extract request-level context (trace/request id), 2) enforce auth if <code>deep</code> and <code>require_auth_for_deep</code> (verify bearer token or internal header; default: require a special internal token or mutual-TLS), 3) call <code>run_health_checks</code> with appropriate timeouts and the app's <code>runtime_fingerprint</code> and <code>applied_rule_version</code>, 4) compute HTTP status code mapping: <code>ok -&gt; 200</code>, <code>degraded -&gt; 200</code> (or 200 with <code>X-Health-Status: degraded</code>) or optionally 200/503 split depending on ops preference; recommend <code>200</code> for degraded for external load balancers and <code>503</code> for <code>fail</code> so orchestrators can act, 5) serialize JSON using a safe encoder, 6) add appropriate caching/no-cache headers, 7) response must be small and redacted. Handler must be idempotent and cheap to call. Provide tests using the framework's TestClient to assert status codes, headers, and JSON shape for both shallow and deep probes. </td></tr><tr><td data-label="Technical Breakdown — health.py"> <strong><code>format_health_response(check_results, runtime_fingerprint, applied_rule_version, start_ts, duration_ms) -&gt; dict</code></strong><br><br>Pure helper that maps raw check results into the canonical response envelope and calculates aggregated top-level <code>status</code>. Responsibilities: 1) compute <code>status</code> per contract, 2) include <code>runtime_fingerprint</code> and <code>applied_rule_version</code>, 3) attach timestamps and durations, 4) ensure sensitive fields are redacted (no secrets in <code>meta</code>), 5) ensure deterministic order of <code>checks</code> (sort by name). This function should be thoroughly unit tested (table-driven tests) for all combinations of required/optional check outcomes. </td></tr><tr><td data-label="Technical Breakdown — health.py"> <strong><code>periodic_health_cache_worker(app, interval_seconds, ttl_seconds)</code></strong><br><br>Optional background helper (start only in single-process dev or when enabled by <code>settings.startup_mode</code>). Responsibilities: periodically run the deep probe, cache last result in <code>app.state._cached_health</code> with TTL to speed up frequently polled readiness checks and reduce pressure on downstream services. Guardrails: only run when explicitly enabled, cancel cleanly on shutdown, and mark cached results with <code>meta.cached=true</code>. If enabled, ensure cache staleness is visible in the response <code>meta</code>. Tests: assert cached result returned when fresh and that worker cancels during shutdown. </td></tr><tr><td data-label="Technical Breakdown — health.py"> <strong>Prebuilt checks to register (adapters, not direct implementation)</strong><br><br>Provide lightweight adapters (but not heavy implementations) for common subsystems so they're easy to test: <code>db_ping_adapter(db_factory, required=True, timeout_ms=200)</code>, <code>object_store_probe(client_factory, required=True, timeout_ms=500, path=&quot;/health-check&quot;)</code>, <code>queue_depth_probe(queue_client, threshold, warn_only=True)</code>, <code>telemetry_exporter_flush_probe(exporter, required=False)</code>. Each adapter should be pure (only create client at runtime when called) and not open long-lived connections. Document expected return <code>meta</code> keys (e.g., <code>meta: {&quot;latency_ms&quot;: 12, &quot;replica&quot;: &quot;read-1&quot;}</code>). Tests should use fakes for each adapter. </td></tr><tr><td data-label="Technical Breakdown — health.py"> <strong>Security & exposure controls</strong><br><br>Deep probes often reveal sensitive topology; by default: 1) require authentication for <code>deep</code> endpoints and document how to disable in dev; 2) ensure responses redact secrets and internal hostnames (apply a configured redaction list), 3) rate-limit health endpoints to prevent abuse (e.g., 20 req/s unlimited for LB; lower for public-facing), 4) set CORS to restrictive defaults for deep probes, 5) avoid embedding full error stacks—only short, actionable hints. Add tests verifying that deep endpoint returns 401 when unauthenticated and that sensitive fields (DB connection strings, secret keys) are not present in responses. </td></tr><tr><td data-label="Technical Breakdown — health.py"> <strong>Observability: metrics & structured logs</strong><br><br>Emit lightweight metrics for each probe run: <code>health.run.count</code>, <code>health.run.duration_ms</code>, <code>health.check.&lt;name&gt;.duration_ms</code>, <code>health.check.&lt;name&gt;.fail_count</code>. Emit structured log events for <code>fail</code> states including <code>trace_id</code>, <code>runtime_fingerprint</code>, <code>applied_rule_version</code>, <code>failure_summary</code> (short list of failing check names). Make logging non-blocking and tolerant to exporter unavailability. Include sampling for noisy repeated failures to avoid log storms. Unit/integration tests should assert metric counters increment. </td></tr><tr><td data-label="Technical Breakdown — health.py"> <strong>Timeouts, concurrency, and resource protection</strong><br><br>Key guardrails: per-check default timeout (e.g., 250–1000ms depending on check class), overall probe timeout (e.g., 2s), concurrency limit for checks (e.g., 10 simultaneous checks), and a safe default for request-level timeouts imposed by web server. Ensure checks that do blocking I/O are executed in threadpool with small worker count to avoid starving event loop. Document how to tune these values via settings and provide unit tests covering slow checks, thrashing scenarios, and semaphore exhaustion. </td></tr><tr><td data-label="Technical Breakdown — health.py"> <strong>Idempotence & safe startup behavior</strong><br><br>Health registry functions and handler factories must be idempotent: calling <code>register_health_check</code> twice with the same name is either a no-op or requires explicit <code>replace=True</code>. Import-time usage must be side-effect free; avoid creating DB clients or touching external services until the checks run at request time or during controlled startup handlers. Provide <code>build_test_app</code> helper that returns an app with fake health checks for tests. </td></tr><tr><td data-label="Technical Breakdown — health.py"> <strong>Testing guidance & recommended test matrix</strong><br><br>Unit tests: cover <code>format_health_response</code>, <code>_run_check_with_timeout</code>, and registry validation. Integration tests (fast): run TestClient to call <code>/health</code> and <code>/deep-health</code> with faked checks (sync/async, long-running, raising) and assert HTTP status, envelope, and metrics. End-to-end: enable a real lightweight dependency (sqlite or local in-memory queue) and run deep probe to assert readiness. Chaos tests: simulate flaky external dependency and assert <code>degraded</code> vs <code>fail</code> under <code>settings.strict</code> toggles. Security tests: assert deep endpoints require auth and that redaction works. </td></tr><tr><td data-label="Technical Breakdown — health.py"> <strong>Operational runbooks & run-time knobs (document at top)</strong><br><br>List the key settings operators will tune: <code>HEALTH_PER_CHECK_TIMEOUT_MS</code>, <code>HEALTH_OVERALL_TIMEOUT_MS</code>, <code>HEALTH_CONCURRENCY_LIMIT</code>, <code>HEALTH_DEEP_REQUIRES_AUTH</code>, <code>HEALTH_CACHE_TTL_SEC</code>, <code>HEALTH_STRICT_ON_STARTUP</code>. Document how <code>strict</code> affects startup vs runtime probe behavior and hand-off instructions for: (a) what to do when <code>/health</code> is <code>degraded</code> vs <code>fail</code>, (b) how to temporarily disable expensive checks for maintenance, and (c) how to add a new check safely (register then smoke-test). </td></tr><tr><td data-label="Technical Breakdown — health.py"> <strong>Failure modes & remediation notes</strong><br><br>Common failures and how to handle them: 1) Flaky DB pings — increase per-check timeout and add connection pool health endpoint in DB adapter; 2) Large response sizes — trim <code>meta</code> and avoid including full state snapshots; 3) Slow aggregated probes under load — enable cached probe worker or increase probe interval for load-balancer health checks; 4) Secrets leaked in <code>meta</code> — add explicit redaction rules and run CI check that fails if certain keys appear in health output. Each remediation should be short, documented, and automated where possible. </td></tr><tr><td data-label="Technical Breakdown — health.py"> <strong>Backward/forward compatibility & API stability</strong><br><br>Health responses are consumed by load balancers, orchestration, and SRE tools — keep the top-level <code>status</code> and <code>checks[].name</code> stable. Add new keys under a namespaced <code>meta</code> object to avoid breaking consumers. Version the response only if you must change the top-level contract (include <code>schema_version</code> in envelope for future-proofing). Add a small migration policy in the module docstring describing how to add optional fields without breaking existing probes. </td></tr><tr><td data-label="Technical Breakdown — health.py"> <strong>Examples of recommended unit tests (concise)</strong><br><br>1. <code>test_format_health_response_all_ok</code> — two required checks ok → top-level <code>ok</code>. 2. <code>test_required_check_failure_is_fail</code> — required check fails → <code>fail</code> and HTTP 503. 3. <code>test_optional_check_failure_is_degraded</code> — optional fails → <code>degraded</code>. 4. <code>test_timeout_becomes_failure_or_degraded</code> — slow check times out and maps correctly. 5. <code>test_deep_endpoint_requires_auth</code> — deep handler rejects unauthenticated. 6. <code>test_registry_idempotence</code> — double-register is stable. 7. <code>test_cached_worker_returns_cached_result</code> — when enabled. Each test should assert no external network is used (use fakes). </td></tr><tr><td data-label="Technical Breakdown — health.py"> <strong>Implementation anti-patterns to avoid</strong><br><br>— Running heavy migrations or schema changes as part of a probe. <br>— Opening persistent connections at import time. <br>— Returning raw exception traces or credentials in the probe response. <br>— Using probe failures as the only signal for SRE (complement with metrics/alerts). <br>— Mounting debug or highly sensitive internal checks on the public <code>/health</code> route without auth. </td></tr><tr><td data-label="Technical Breakdown — health.py"> <strong>Maintenance notes for future contributors</strong><br><br>Keep <code>health.py</code> small: any complex check belongs in a dedicated adapter module with its own tests. When adding new checks, update <code>app.state._extensions</code> registration docs and add the new check to the <code>build_test_app</code> helper. Prefer composition: register adapters via small factory functions. Document which checks are <em>critical</em> vs <em>optional</em> in a single place (<code>HEALTH_CRITICAL</code> constant or settings). Keep probes deterministic and fast. </td></tr></tbody></table></div><div class="row-count">Rows: 22</div></div><div class="table-caption" id="Table7" data-table="Docu_0164_07" style="margin-top:2mm;margin-left:3mm;"><strong>Table 7</strong></div>
<div class="table-wrapper" data-table-id="table-7"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown — upload.py"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown — upload.py</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — upload.py"> <strong>File-level responsibilities</strong><br><br>This module is the canonical ingress service for file-based data entering the pipeline. It exposes the HTTP/adapter entrypoints (upload endpoints and adapter adapters), validates and normalizes incoming payloads into the canonical intermediate format, enforces idempotency and size/security guards, persists immutable snapshots to object storage, records metadata to the metadata DB, and enqueues idempotent ingest jobs for downstream workers. Keep the module focused on orchestration and side-effect coordination — parsing, storage drivers, validators, and queue clients should be imported as thin interfaces. Ensure all public functions are safe to call from multiple threads/processes and document the exact semantics for idempotency, eventual consistency, and failure modes (partial success vs fatal). The module must never perform heavy CPU-bound transformations (those are delegated to rules/engine modules) and must avoid opening persistent DB connections at import time. Unit- and integration-test harnesses depend on deterministic, easily-mockable side-effect points: <code>object_store.write_snapshot</code>, <code>metadata_db.insert_record</code>, <code>queue.enqueue_job</code>, <code>parsers.parse_stream</code>. <br><br><strong>Operational contracts (short)</strong>: idempotency via content-hash, streaming-friendly (no full-file reads unless necessary), size limits and quota enforcement, strict upload authentication/authorization checks, structured observability (trace_id propagation, upload metrics, validation counters), and robust cleanup of temp artefacts on failure. </td></tr><tr><td data-label="Technical Breakdown — upload.py"> <strong><code>register_routes(app, settings) -&gt; None</code></strong><br><br>Responsibility: attach one or more HTTP routes to the web framework (e.g., <code>/upload</code>, <code>/upload/stream</code>, <code>/adapters/:name</code>) and hook middleware for auth and rate-limiting. Must be idempotent (check <code>app.state._registered_upload_routes</code>). Routes should document accepted content-types, max sizes, and whether streaming is supported. When registering, ensure route handlers accept a <code>trace_id</code> and propagate it into downstream calls (<code>write_snapshot</code>, <code>enqueue_ingest_job</code>). Unit tests: assert route registration is a no-op on repeated calls and that <code>app</code> gains route names expected by integration tests. </td></tr><tr><td data-label="Technical Breakdown — upload.py"> <strong><code>handle_upload_request(request) -&gt; Response</code></strong><br><br>Canonical HTTP entrypoint for small/medium uploads. Responsibilities: authenticate and authorize the caller, enforce per-user and per-endpoint quotas, validate content-length and content-type headers, and route to the right processing flow (streaming vs buffered). Must: (1) validate <code>X-Trace-Id</code> or produce one, (2) perform fast pre-flight checks (size guard, file count), (3) compute a short deterministic idempotency key when possible from provided metadata (prefer explicit idempotency header, fall back to content hash), (4) reject disallowed types early, and (5) return well-formed JSON error envelopes for client-visible errors. Error mapping: 400 for malformed requests, 413 for too large, 401/403 for auth, 415 for unsupported media type, 429 for rate-limit, 500 for internal errors. Observability: emit <code>upload.request_received</code>, <code>upload.preflight_passed/failed</code>. Tests should simulate all header permutations and verify idempotency decisions. </td></tr><tr><td data-label="Technical Breakdown — upload.py"> <strong><code>stream_upload_endpoint(request, response_streaming=False) -&gt; Response</code></strong><br><br>Responsibility: efficient handling of very large files or multi-GB streams without buffering the whole payload in memory. Behavioural contract: read the incoming request as a stream/chunks, compute rolling content-hash (e.g., BLAKE2b/SHA256) while simultaneously piping to a temporary staging sink (file-backed or chunked object writer), and optionally run streaming parsers for early validation sampling. The function must: (1) assert per-chunk timeouts and per-request throughput limits, (2) perform early rejection if content-type/initial bytes indicate invalid format, (3) maintain an upload context object containing <code>trace_id</code>, <code>tmp_path</code>, <code>rolling_hash</code>, <code>byte_count</code>, sample bytes for detection, and <code>first_validation_errors</code> list, (4) ensure chunk-level exceptions lead to immediate abort with clean cancellation of underlying IO and deletion of partial temp artefacts, (5) return a location/manifest that references the stored snapshot on success. Critical: avoid double-writing — implement atomic rename/promotion for staged objects in object-store compatible way. Unit/integration tests: chunk reordering, partial aborts, slow clients, and concurrent streaming uploads. </td></tr><tr><td data-label="Technical Breakdown — upload.py"> <strong><code>process_upload_file(stream_or_path, filename, content_type, uploader_identity, metadata) -&gt; dict</code></strong><br><br>High-level orchestrator invoked after preflight. Responsibilities: (1) detect format and dialect, (2) parse/validate a sample (configurable N rows) to detect schema and to produce early validation diagnostics, (3) compute or finalize content hash (idempotency key), (4) persist immutable snapshot to object store, (5) write metadata record to the metadata DB, and (6) enqueue an ingest job referencing the snapshot id and metadata. Must be idempotent for repeated calls with same content-hash: if snapshot exists, reuse it and avoid duplicate DB rows/enqueues. Return an explicit result shape <code>{snapshot_id, content_hash, row_count_estimate, validation_summary, job_enqueued: bool, metadata_record_id}</code>. Implementation notes: separate the "analysis" path (sample parsing/validation) from "persist" path. Tests: verify correct behaviour for duplicate content, content with trailing junk, and when object-store write fails mid-stream. </td></tr><tr><td data-label="Technical Breakdown — upload.py"> <strong><code>detect_file_format(filename, content_type, sample_bytes) -&gt; FormatDescriptor</code></strong><br><br>Responsibility: return a small, deterministic descriptor (format name, subtype, dialect hints) used by parsers. Use a prioritized detection strategy: explicit content-type header → file extension → magic bytes sniff → sample heuristics (CSV delimiter, Excel header bytes, XML root tag). Must avoid false positives: when ambiguous, prefer a conservative "unknown" that triggers clearer error to caller (rather than mis-parsing). Provide dialect hints for CSV (delimiter, quoting) and for Excel (xls/xlsx) so downstream parsers are fast. Unit tests: a matrix of filename+sample combos that exercise fallback ordering and ambiguity. </td></tr><tr><td data-label="Technical Breakdown — upload.py"> <strong><code>parse_stream_to_records(stream, format_descriptor, parse_options) -&gt; Iterable[Record]</code></strong><br><br>Responsibility: convert raw bytes into canonical row records (the project's canonical schema). This is thin delegation to <code>parsers.py</code>/<code>csv_mapper.py</code>/<code>excel_reader.py</code> but must manage streaming resource lifecycles and backpressure. Contract: yield row objects (or tuples) as they become available; never buffer the entire file. Handle parsing errors at row granularity — emit <code>RecordError</code> metadata but continue where policy allows (configurable <code>fail_on_first_error</code>). Provide hooks for line-number / byte-offset attribution for auditability. When format is binary (Excel), ensure row yields are deterministic and stable across library versions. Tests: malformed rows, mixed encodings, non-UTF8 fallbacks, and delimiter detection. </td></tr><tr><td data-label="Technical Breakdown — upload.py"> <strong><code>validate_and_classify_records(records_iter, validators, options) -&gt; (valid_iter, invalid_report)</code></strong><br><br>Responsibility: apply row-level and record-level validation rules from <code>validators.py</code>. Should be pure-functional where possible: validators return <code>(ok: bool, transformed_record_or_errors)</code>. Provide streaming-friendly behaviour: accumulate counts/first-N errors only, write invalid rows into a validation report (structured JSONL) saved to object store, and forward pass-through valid records to the next stage (or simply count them for snapshot metadata). Ensure validators can add warnings (non-fatal) and that all PII redaction rules are applied before any diagnostics records are persisted or logged. Tests: coverage for every rule, sampling, and performance under large row counts. </td></tr><tr><td data-label="Technical Breakdown — upload.py"> <strong><code>compute_content_hash(stream_or_path, algorithm=&#x27;sha256&#x27;) -&gt; HexDigest</code></strong><br><br>Responsibility: produce a stable idempotency key for the snapshot. Accepts either a seekable path or stream; if stream, compute hash while writing to the staging sink to avoid double reads. Important semantics: the hash algorithm and canonicalisation rules must be pinned in settings and recorded in the metadata manifest (algorithm name + digest). Document canonicalisation: whether newline normalization, charset normalization, or header-stripping are applied — prefer minimal canonicalisation and make transformations explicit (transformations must change the content-hash process or be recorded as a different "normalization_version"). Tests: identical content across different upload methods must yield identical digest. </td></tr><tr><td data-label="Technical Breakdown — upload.py"> <strong><code>write_snapshot(object_store_client, staging_path_or_stream, dest_path, metadata) -&gt; snapshot_id</code></strong><br><br>Responsibilities: write the immutable snapshot to the canonical object store location with atomic semantics. Implementation must: (1) choose an object key scheme (e.g., <code>snapshots/{content_hash}/{timestamp}.{ext}</code>), (2) perform write via single atomic PUT or upload-then-rename pattern (store-dependent), (3) store content-type and checksum in object metadata, (4) optionally compute and store chunked checksums for very large blobs, and (5) enforce retention-class and ACLs as per settings. On partial failure, perform cleanup of any temporary staging parts and emit telemetry events. Return a stable snapshot identifier useful to downstream workers. Tests: simulate partial object-store failures and eventual consistency delays. </td></tr><tr><td data-label="Technical Breakdown — upload.py"> <strong><code>write_metadata_record(metadata_db, record) -&gt; metadata_id</code></strong><br><br>Responsibility: persist upload metadata (uploader identity, content_hash, snapshot_id, filename, detected_format, row_counts, validation_summary, applied_rule_version_hint, serialized headers, ingestion_flags) as an append-only record. Must be idempotent: use <code>content_hash</code> as a unique constraint and perform an upsert semantics or return existing metadata_id if the content already exists. Important to redact secrets before writing (APIs keys in metadata). Also attach <code>runtime_fingerprint</code> and <code>uploader_trace_id</code> to the record for later correlation. If DB insert fails after snapshot write, consider a compensating audit event (and choose whether to garbage collect snapshot or keep for manual reconciliation) — document chosen approach. Tests: uniqueness, transactional consistency with snapshot creation, and simulated DB deadlocks. </td></tr><tr><td data-label="Technical Breakdown — upload.py"> <strong><code>enqueue_ingest_job(queue_client, job_payload, idempotency_key) -&gt; job_id | replicate_result</code></strong><br><br>Responsibility: enqueue a job representing further processing (rules engine, audit, exports). Must implement strong idempotency guarantees: before enqueue, check <code>job_registry</code> (DB or dedupe service) for existing job keyed by <code>idempotency_key</code> (content_hash + pipeline_version). If not present, insert marker and call <code>queue_client.push(job_payload)</code>. Use transactional or compare-and-set semantics to avoid double-enqueue in races. The job payload should be compact (snapshot_id, content_hash, metadata_id, uploader, applied_rule_version_hint, requested_actions). Optionally support delayed scheduling or priority flags. On enqueue failure, retry with exponential backoff (configurable) and escalate to exception queue if permanent. Return a stable job id and whether the call enqueued a new job or returned an existing job. Tests: concurrent enqueue attempts, queue outages, and dedupe races. </td></tr><tr><td data-label="Technical Breakdown — upload.py"> <strong><code>build_upload_response(result) -&gt; Response</code></strong><br><br>Responsibility: craft a stable, machine-readable response following the project's error/envelope schema. On success return <code>{ &quot;status&quot;: &quot;accepted&quot;, &quot;snapshot&quot;: snapshot_id, &quot;content_hash&quot;: hash, &quot;metadata_id&quot;: id, &quot;job_enqueued&quot;: true/false, &quot;job_id&quot;: id, &quot;validation_summary&quot;: {counts, sample_errors} }</code>. For synchronous small uploads that trigger immediate validation failures, structure must include <code>status: rejected</code> plus <code>validation_report_url</code>. All responses must include <code>trace_id</code> and <code>runtime_fingerprint</code>. Ensure the response body is signed or HMAC'd when required by caller (e.g., external adapter) and that no secrets are leaked. Tests: envelope shape, headers, and behavior for all error classes. </td></tr><tr><td data-label="Technical Breakdown — upload.py"> <strong><code>_save_temp_file(stream_iter, tmp_dir, max_size) -&gt; tmp_path</code></strong><br><br>Helper: robust temporary persistence with preallocated space and strict quotas. Must stream-write with per-write checks against <code>max_size</code>, fsync and durable rename semantics. Ensure permission bits are restrictive (owner-only) and cleanup on any exception path. Tests: disk-full, permission denied, exceeded-size. </td></tr><tr><td data-label="Technical Breakdown — upload.py"> <strong><code>_cleanup_resources(context) -&gt; None</code></strong><br><br>Helper: best-effort cleanup invoked on both success and failure. Responsibilities: delete temp files, cancel streaming IO, close DB transactions, and emit a <code>upload.cleanup</code> metric with reason tag. Cleanup must never raise — swallow and log errors. Use a deterministic order (reverse of allocation). Tests: ensure no temporary files remain after simulated failures. </td></tr><tr><td data-label="Technical Breakdown — upload.py"> <strong><code>configure_upload_policy(settings) -&gt; UploadPolicy</code></strong><br><br>Small factory that centralizes upload-related knobs: max file size, allowed content-types, streaming chunk size, per-uploader quota, validation sampling size, idempotency window, and whether to accept adapters. Must be deterministic from the <code>settings</code> object and safe to call from tests. Provide a human-readable <code>to_safe_dict()</code> for diagnostics. Tests: different settings fixtures produce expected policy. </td></tr><tr><td data-label="Technical Breakdown — upload.py"> <strong><code>adapter_ingest_entry(adapter_name, adapter_payload, headers) -&gt; Response</code></strong><br><br>Endpoint shim for adapter integrations (SAP, Workday, CSV adapters). Responsibilities: validate adapter identity and signature, normalize adapter payload into the same internal shape the upload endpoints expect, attach adapter-specific metadata, and call <code>process_upload_file(...)</code>. Must enforce adapter-level rate-limits, per-adapter ACLs, and accept multipart or specialized streaming encodings. Ensure adapter paths are auditable and produce an adapter-specific <code>ingest_trace</code>. Tests: adapter auth failure, malformed adapter payloads, and replay protection. </td></tr><tr><td data-label="Technical Breakdown — upload.py"> <strong><code>health_check() -&gt; dict</code></strong><br><br>Minimal handler returning upload-subsystem liveness and quick readiness indicators: ability to write and list to a small object-store test key (optionally mocked), connectivity to metadata DB (light select), and queue client reachable. Do not perform heavy checks (no full DB schema migrations). Return structured shape <code>{ok: bool, component_statuses: {...}}</code>. Tests: mock each dependency to produce degraded vs ok. </td></tr><tr><td data-label="Technical Breakdown — upload.py"> <strong>Error handling, retries & failure semantics</strong><br><br>Design rule: distinguish transient vs permanent failure classes. Transient (object-store/network/queue) → retry with exponential backoff and jitter; permanent (schema mismatch, unsupported format, auth failure) → surface to client and optionally publish to exception queue. For partially completed steps (snapshot written but metadata DB write failed) record a compensating audit entry and choose a documented retention policy for orphan snapshots (recommended: keep 7 days for manual reconciliation unless automatic garbage collection is configured). Ensure every exception path attaches <code>trace_id</code> and minimal sanitized context to logs/telemetry. </td></tr><tr><td data-label="Technical Breakdown — upload.py"> <strong>Idempotency & deduplication</strong><br><br>Idempotency must be deterministic and globally observable. Use <code>content_hash + pipeline_version</code> as the canonical idempotency key. All writes to metadata DB and job registry must be guarded by this key. When multiple uploads of identical content occur, return the same <code>snapshot_id</code> and an idempotent acknowledgement (indicate <code>job_enqueued:false</code> if prior job exists). Avoid maintaining in-memory dedupe caches that break across process restart; prefer DB-backed unique constraints or a small dedupe service. Tests: repeated uploads by different credentials but same file; concurrent uploads. </td></tr><tr><td data-label="Technical Breakdown — upload.py"> <strong>Observability & telemetry</strong><br><br>Emit structured metrics and traces at these key points: <code>upload.request</code>, <code>upload.stream.bytes</code>, <code>upload.snapshot.write_time</code>, <code>upload.validation.counts</code>, <code>upload.enqueue.latency</code>, <code>upload.errors</code>. Include tags: <code>uploader_id</code>, <code>format</code>, <code>content_hash_prefix</code>, <code>outcome</code>. Log context must include <code>trace_id</code>, <code>uploader_id</code>, and sanitized headers. Provide debug sampling for full request/response bodies but gated behind <code>settings.env == &#x27;dev&#x27;</code> and explicit sampling policy. Add health metrics for temp disk usage and currently active streaming uploads. </td></tr><tr><td data-label="Technical Breakdown — upload.py"> <strong>Security & privacy</strong><br><br>Always authenticate callers and authorize per-uploader actions. Apply input sanitization to headers and file metadata. Do not log raw payloads or unredacted PII — redact SSNs, bank account numbers, and tokens before persistence or logs. Ensure temp files are written with restrictive permissions and are scrubbed on deletion. If the system supports signed upload URLs, validate signatures and TTLs. When exposing public manifests or validation reports, sign or HMAC them to prevent tampering. Document allowed CORS and ensure preflight responses are conservative. Provide JWK/Crypto helpers through the <code>crypto.py</code> module for signing responses when required. </td></tr><tr><td data-label="Technical Breakdown — upload.py"> <strong>Testing guidance</strong><br><br>Provide layered tests: unit tests for <code>detect_file_format</code>, <code>compute_content_hash</code>, and <code>build_upload_response</code>; fast integration tests for <code>process_upload_file</code> using in-memory object-store and queue fakes; and slow end-to-end tests that exercise streaming large files in controlled environments (CI optional). Include property-based tests for hashing invariants. Include chaos tests for partial write failures to validate cleanup paths. Provide a <code>build_test_uploader()</code> helper to create deterministic uploader identities and quotas. </td></tr><tr><td data-label="Technical Breakdown — upload.py"> <strong>Edge cases & recommended decisions</strong><br><br>— Character encoding ambiguity: default to UTF-8 with configurable legacy encodings in policy; detect and reject mixed encodings.<br>— Very large files: require adapter or pre-signed URL upload path that writes directly to object-store to avoid API server resource pressure.<br>— Validation-only mode: provide a <code>?validate_only=1</code> flag that runs parsing & validation and returns a validation report without persisting snapshot or enqueuing jobs.<br>— Partial success semantics: when some rows valid and others invalid, persist snapshot, persist validation report, enqueue job but mark job with <code>has_warnings: true</code> so downstream pipeline can decide automated handling. </td></tr><tr><td data-label="Technical Breakdown — upload.py"> <strong>Documentation & developer notes</strong><br><br>— Document exported functions and their expected inputs/outputs (types, allowed keys).<br>— Keep the module small — push heavy logic into <code>parsers.py</code>, <code>validators.py</code>, and storage/queue clients.<br>— When adding new adapter types, implement an adapter translator that converts adapter payloads into a single canonical upload invocation rather than adding bespoke flows.<br>— Add <code>smoke</code> integration tests for any change touching idempotency or object-store write logic.<br>— Record versioning for upload semantics (<code>upload_api_version</code>) in every metadata record to support future migrations. </td></tr><tr><td data-label="Technical Breakdown — upload.py"> <strong>Recommended tests & CI checks</strong><br><br>1. Unit: <code>compute_content_hash</code>, <code>detect_file_format</code>, <code>build_upload_response</code>.<br>2. Integration (fast): in-memory object store + fake queue run of <code>process_upload_file</code> for CSV/Excel sample fixtures.<br>3. E2E streaming: simulated chunked upload with mid-stream abort to check cleanup.<br>4. Concurrency: parallel uploads of identical content to validate dedupe.<br>5. Security: ensure no PII in logs and temp file perms are correct.<br>6. Resilience: simulate object-store and DB transient failures and verify retry/backoff patterns.<br>7. Contract: ensure response envelopes contain trace_id and runtime_fingerprint. </td></tr><tr><td data-label="Technical Breakdown — upload.py"> <strong>Operational checklist before production</strong><br><br>— Confirm <code>settings.strict</code> for upload (reject partial behaviors unless explicitly allowed).<br>— Verify object-store credentials and ACLs; ensure retention/archival is set.<br>— Ensure metadata DB has unique constraint on <code>content_hash</code> and <code>upload_api_version</code> semantics.<br>— Configure rate-limits and quotas per-uploader.<br>— Wire telemetry exporters; verify traces propagate into distributed tracing backend.<br>— Enable monitoring for temp disk usage and active streaming uploads. </td></tr></tbody></table></div><div class="row-count">Rows: 27</div></div><div class="table-caption" id="Table8" data-table="Docu_0164_08" style="margin-top:2mm;margin-left:3mm;"><strong>Table 8</strong></div>
<div class="table-wrapper" data-table-id="table-8"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown — simulate.py"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown — simulate.py</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — simulate.py"> <strong>File-level responsibilities</strong><br><br><em>simulate.py</em> is the single, authoritative module that implements the <strong>single-record deterministic rules simulation</strong> surface used by the <code>/simulate</code> HTTP handler, CLI <code>simulate</code> command, and internal tooling (golden fixtures, CI checks). Its duties are strictly: (1) accept a canonical input (a single normalized record), (2) validate and sanitise that input, (3) select and load the exact rule bundle to apply, (4) build a constrained execution context, (5) execute the deterministic rules pipeline (pure calculation functions and orchestrated rule steps), (6) produce a fully-attributed result object that contains outputs, decision trace, applied-rule-version, runtime metadata and diagnostics; and (7) optionally record a non-mutating audit entry. The module must enforce <em>no persistent side-effects by default</em> (simulate is read-only), be idempotent, deterministic, timeboxed, and safe to call from tests and the public API. Keep functions small, pure where possible, and make the top-level API easy to unit-test with dependency injection for rule loaders, timeouts, and telemetry hooks. Document expected input shape, minimal golden fixtures, and the exact schema of the response object at the top of the file. </td></tr><tr><td data-label="Technical Breakdown — simulate.py"> <strong>Public API: <code>simulate_record(record, *, rules_version=None, run_audit=False, timeout_ms=5000, ctx_overrides=None) -&gt; dict</code></strong><br><br><strong>Purpose & contract</strong>: canonical entrypoint for programmatic simulation. Accepts a single canonical record (dict), optional rule version override, and flags. Returns a stable response dict with keys <code>{result, trace, applied_rule_version, runtime_fingerprint, duration_ms, warnings, errors}</code>. Must never mutate <code>record</code>. Must be synchronous or expose an async variant; if both exist, the public function delegates to <code>async</code> implementation in tests via an adapter. <br><br><strong>Inputs</strong>: validated canonical record (fields previously normalized by parsers/csv_mapper), optional <code>rules_version</code> (string), <code>run_audit</code> (bool), <code>timeout_ms</code> (int), <code>ctx_overrides</code> (dict). <br><br><strong>Outputs</strong>: result envelope described above. <code>result</code> is the canonical calculation outputs (structured, schema-stable). <code>trace</code> is an ordered list of decision steps with machine-readable IDs, inputs, outputs and determinism signatures. <code>applied_rule_version</code> is always present and exact (source-of-truth: rules manifest). <code>runtime_fingerprint</code> is a short hash of non-secret runtime metadata. <code>duration_ms</code> is total simulated time. <br><br><strong>Behavioral details</strong>: validate inputs early; load rule bundle deterministically; build execution context; run compute pipeline under a hard timeout (wrap per-step and aggregate via <code>asyncio.wait_for</code> or similar). By default the operation is <strong>read-only</strong>; <code>run_audit=True</code> permits writing an audit entry but only via the <code>recorder</code> component invoked through an injectable writer—never write directly to DB. On any validation error return an error envelope (400-like structure) instead of raising, unless called internally where exceptions are more appropriate. <br><br><strong>Idempotency & side effects</strong>: idempotent when <code>run_audit=False</code>. When <code>run_audit=True</code> attach an idempotency key to the audit entry (hash of record+rules_version+runtime_fingerprint) so duplicate simulate calls do not create duplicate audit rows. <br><br><strong>Testing</strong>: unit tests should exercise: valid golden fixtures, determinism across repeated calls, behavior with <code>rules_version</code> override, and <code>run_audit</code> toggles (mock recorder). Add property-based tests for random valid inputs to ensure no accidental state mutation. </td></tr><tr><td data-label="Technical Breakdown — simulate.py"> <strong><code>validate_simulation_input(record) -&gt; (sanitized_record, warnings:list, errors:list)</code></strong><br><br><strong>Purpose</strong>: perform strict, minimal validation that the passed <code>record</code> conforms to the canonical schema expected by rules. This is <em>not</em> the CSV/adaptor-level parsing stage — those live elsewhere; this function assumes the record is already normalized but enforces required fields, types, ranges and domain invariants (dates, numeric ranges, ID formats). Should be cheap and pure. <br><br><strong>Inputs/Outputs</strong>: returns a sanitized copy (never in-place), a list of non-fatal warnings (minor data anomalies corrected or noted), and a list of errors. If <code>errors</code> is non-empty the caller decides whether to short-circuit. Errors use a machine-friendly shape <code>{code, field, message}</code> to be included in the eventual response. <br><br><strong>Edge cases</strong>: loose vs strict mode: accept a <code>strict</code> flag (injected or global config) to toggle whether type-coercion is allowed. When coercion is used, record the transformation in <code>warnings</code>. <br><br><strong>Tests</strong>: table-driven tests covering missing fields, type mismatches, borderline numeric ranges, and coercion behavior. </td></tr><tr><td data-label="Technical Breakdown — simulate.py"> <strong><code>select_rule_bundle(rules_version: Optional[str], record_meta: dict) -&gt; RuleBundle</code></strong><br><br><strong>Purpose</strong>: deterministically resolve which rule bundle (a small immutable object with metadata and callable rule implementations) to apply. If <code>rules_version</code> provided, select that exact version; otherwise use <code>record_meta</code> (e.g., <code>record.effective_date</code>) and the rules manifest to choose the applicable bundle. Must fail noisily if the requested version is unknown or if multiple bundles match without disambiguation. <br><br><strong>Contract & Behavior</strong>: never perform network I/O on hot path — rule bundles are pre-cached by a rule-loader component and this function reads from that cache. Validate the bundle's <code>effective_date</code> and <code>compatibility</code> with the runtime. Attach the <code>applied_rule_version</code> string for return. Keep selection pure for deterministic unit tests. <br><br><strong>Failure mode</strong>: raise <code>RuleNotFound</code> for unknown versions (caller maps to 400/404). Tests should include missing rule bundle and manifest-mismatch cases. </td></tr><tr><td data-label="Technical Breakdown — simulate.py"> <strong><code>load_rule_bundle_from_store(version: str) -&gt; RuleBundle</code></strong><br><br><strong>Purpose</strong>: I/O boundary that populates the in-process rule cache on cold-start or when an explicit version is requested that is not yet cached. This function performs file-system or object-store reads and must be exported as an injectable dependency so tests can stub it. <br><br><strong>Operational notes</strong>: perform lazy parsing of rule source (e.g., YAML/JSON/Python modules) and verify an integrity checksum (manifest checksum). Validate <code>meta.version</code> equals param and compute the bundle fingerprint. Keep load operations idempotent and thread-safe: use an in-memory lock per-version. Avoid long blocking operations on the API thread — callers in production should call a small wrapper that awaits the load in a background worker or ensure the rule cache is warmed at service startup. <br><br><strong>Tests</strong>: unit tests with a fake filesystem, verify checksum validation, and lock race conditions. </td></tr><tr><td data-label="Technical Breakdown — simulate.py"> <strong><code>build_execution_context(record, rules_meta, *, user_ctx=None, trace_id=None) -&gt; ExecutionContext</code></strong><br><br><strong>Purpose</strong>: assemble a constrained, minimal execution context object passed to all rule functions. Contains only non-secret values: the input record copy, rule metadata, injected helpers (pure functions only: date helpers, rounding policies), feature flags that influence local behavior, a <code>trace_id</code>, and a <code>logger</code> adapter that records structured events (not direct file/DB writes). Explicitly exclude any network clients or global mutable state from the context. <br><br><strong>Security & determinism</strong>: ensure context provides deterministic helpers (e.g., fixed decimal rounding) and never exposes secrets. If user-provided values (like <code>user_ctx</code>) are present, validate and copy them. Context must be JSON-serializable for inclusion in audit entries. <br><br><strong>Tests</strong>: assert that context serialization is stable and that rules receive only expected helpers. </td></tr><tr><td data-label="Technical Breakdown — simulate.py"> <strong><code>execute_pipeline(ctx: ExecutionContext, rule_bundle: RuleBundle, *, timeout_ms=5000) -&gt; SimulationResult</code></strong><br><br><strong>Purpose</strong>: orchestrates the actual execution of calculation functions in the rule bundle. The pipeline must be composed of isolated, pure steps with clear inputs and outputs (for traceability): e.g., <code>preparation -&gt; core_calculations -&gt; business_adjustments -&gt; finalization</code>. This function is where <code>ptkp</code>, <code>ter</code>, <code>progressive</code>, and <code>dtp_engine</code> (or their equivalents) are invoked in the correct order. <br><br><strong>Timeout & safety</strong>: every step must be timeboxed and the whole pipeline must be bounded by the provided <code>timeout_ms</code>. Implement per-step telemetry (start/stop) and per-step error mapping. If a step fails transiently, map to a controlled error with a helpful hint; do not leak stack traces to clients. If pipeline times out, return a <code>timeout</code> envelope and <em>do not</em> leave any side-effects partial. Use process or worker isolation for untrusted rule code if rules are dynamically loaded. <br><br><strong>Concurrency & resource control</strong>: pipeline must be concurrency-safe and avoid global mutable state. If rule functions are CPU-bound and heavy, consider executing them in a dedicated process pool to avoid blocking async loop. Caller should be able to swap an execution adapter (sync vs process pool) in tests. <br><br><strong>Outputs</strong>: <code>SimulationResult</code> containing <code>result_payload</code>, <code>decision_trace</code> (ordered steps), <code>diagnostics</code> (per-step timings, counters), and <code>internal_metrics</code> (counters useful for SRE). </td></tr><tr><td data-label="Technical Breakdown — simulate.py"> <strong><code>run_pure_function(fn, inputs, ctx) -&gt; output</code></strong><br><br><strong>Purpose</strong>: utility used by <code>execute_pipeline</code> to call small pure calculation functions (e.g., ptkp, ter handlers) with standardized error handling, input validation, and trace collection. This wrapper enforces pre- and post-conditions: ensure function is annotated as pure, measure execution time, validate that outputs conform to declared schema, and attach a determinism fingerprint (hash of function name + inputs + applied_rule_version). <br><br><strong>Failure semantics</strong>: if the function raises, capture exception, append an error step to trace, and either allow pipeline to continue (if policy says non-fatal) or abort with a mapped error. The policy (fail-fast vs degrade) should be configurable via the rule-bundle metadata. <br><br><strong>Tests</strong>: exercise with functions that return valid outputs, return malformed outputs, and raise exceptions. Verify trace entries and deterministic fingerprints. </td></tr><tr><td data-label="Technical Breakdown — simulate.py"> <strong><code>collect_decision_trace(steps: List[StepResult]) -&gt; List[dict]</code></strong><br><br><strong>Purpose</strong>: canonicalizes the internal step results into the public <code>trace</code> structure: an ordered list of <code>{step_id, name, inputs_summary, outputs_summary, duration_ms, fingerprint, outcome_status, hints}</code>. The trace must be compact (avoid dumping full payloads) but include enough to reconstruct decisions for audits and explainability. The trace format is stable across releases — changes must be versioned. <br><br><strong>Privacy</strong>: scrub PII from traces destined for untrusted consumers. The sanitization policy is injected. For internal audit traces, include richer details if caller authorised. <br><br><strong>Tests</strong>: snapshots for golden fixtures and fuzz tests to ensure trace serialization never fails. </td></tr><tr><td data-label="Technical Breakdown — simulate.py"> <strong><code>format_simulation_response(sim_result, rules_meta, runtime_fingerprint, duration_ms, warnings, errors) -&gt; dict</code></strong><br><br><strong>Purpose</strong>: build the final response payload returned by the public API or CLI. The function maps internal structures to the external schema and enforces sensitive-data redaction per rules. Must attach <code>applied_rule_version</code>, <code>runtime_fingerprint</code>, and a stable <code>response_schema_version</code>. Keep formatting pure and deterministic. <br><br><strong>Contract</strong>: always include <code>meta: {applied_rule_version, runtime_fingerprint, duration_ms}</code> and <code>diagnostics: {warnings, errors}</code>. If <code>errors</code> exist and are fatal, ensure the HTTP layer maps them to appropriate status codes. Keep response size bounded — large traces should be truncated with a <code>trace_truncated: true</code> flag and a pointer to the full internal audit if authorised. <br><br><strong>Tests</strong>: verify schema conformance and redaction behavior. </td></tr><tr><td data-label="Technical Breakdown — simulate.py"> <strong><code>sanitize_for_client(payload, allow_full_trace=False) -&gt; payload</code></strong><br><br><strong>Purpose</strong>: scrub PII and secrets prior to returning results to untrusted clients. The sanitization rules come from a small policy object (fields to redact, regexes for patterns such as SSN, IBAN). For <code>allow_full_trace=True</code> (usually internal), skip redactions but still remove secrets. This function must be tolerant (never raise) and fast. <br><br><strong>Testing</strong>: unit tests enumerating fields and sample redaction cases; benchmark to ensure cost is minimal. </td></tr><tr><td data-label="Technical Breakdown — simulate.py"> <strong><code>authorize_simulation_request(request_ctx) -&gt; AuthContext</code></strong><br><br><strong>Purpose</strong>: thin auth guard used by the HTTP handler to check caller permissions for simulation. Typical checks: API key scopes, caller role (dev vs prod), and rate-limit bucket. The simulate operation is low-risk but may reveal business-sensitive logic; default policy: allow authenticated internal callers and require explicit <code>allow_simulate_public</code> feature flag for public tenants. Return a small <code>AuthContext</code> used to decide whether full trace is returned or only a sanitized summary. <br><br><strong>Failure handling</strong>: raise <code>Unauthorized</code> or return an auth error object the HTTP layer maps to 401/403. Tests should include scope-less tokens and tokens lacking <code>simulate</code> permission. </td></tr><tr><td data-label="Technical Breakdown — simulate.py"> <strong><code>handle_simulate_http(request) -&gt; HTTPResponse</code></strong><br><br><strong>Purpose</strong>: adapter between web framework (server.py) and <code>simulate_record</code>. This function is responsible for: parsing JSON payload, extracting headers (auth, trace-id, rules_version, dry_run), calling <code>authorize_simulation_request</code>, calling <code>simulate_record</code> with instrumentation and error mapping, and serialising the response. It must not contain business logic. <br><br><strong>Contract & robustness</strong>: enforce request size limits, content-type checks, and JSON parsing with clear client-facing errors. Attach <code>traceparent</code> to logs for correlation. On internal exceptions, return a sanitized 500 with a <code>trace_id</code>. Avoid returning stack traces to clients. Implement request-level timeouts; ensure that even on server timeout any spawned workers are cancelled. <br><br><strong>Tests</strong>: integration tests using test client that assert response shape and HTTP status mapping for success, validation error, and server error. </td></tr><tr><td data-label="Technical Breakdown — simulate.py"> <strong>Observability & telemetry</strong><br><br>Every public function must emit structured metrics and logs via injected telemetry client rather than global loggers: counters for <code>simulate.requests</code>, histogram <code>simulate.duration_ms</code>, per-step histograms (e.g., <code>simulate.step.ptkp.duration_ms</code>), and gauge for <code>simulate.concurrent</code> active calls. Logs include <code>trace_id</code>, <code>applied_rule_version</code>, and <code>runtime_fingerprint</code>. Tracing integration (W3C <code>traceparent</code>) must be preserved across the call and included in audit entries. Add <code>dry_run</code> telemetry mode for CI that routes telemetry to an in-memory sink. </td></tr><tr><td data-label="Technical Breakdown — simulate.py"> <strong>Error handling & mapping</strong><br><br>Define a small set of typed exceptions and map them to stable HTTP semantics at the HTTP boundary: <code>ValidationError -&gt; 400</code>, <code>RuleNotFound -&gt; 404</code>, <code>Unauthorized -&gt; 401/403</code>, <code>TimeoutError -&gt; 504</code>, <code>ExecutionError -&gt; 500</code> with sanitized message and <code>hint</code>. Always return <code>trace_id</code> and a machine <code>error.code</code>. Never expose stack traces or secrets. Provide rich troubleshooting hints in internal logs only. </td></tr><tr><td data-label="Technical Breakdown — simulate.py"> <strong>Security & privacy</strong><br><br>- Input sanitization first, then validation. <br>- Strict PII redaction rules by default; the sanitization policy is configurable per-tenant. <br>- Do not execute arbitrary user-provided code. If rules are dynamically loaded, run them under process-level sandboxing and/or a policy that forbids network and file writes. <br>- Use a principle-of-least-privilege for any optional audit writes — the simulate surface must default to non-persistent. <br>- Ensure audit entries are HMACed or signed when stored to avoid tampering. <br>- Rate-limit /simulate to avoid information leakage and expensive compute abuse. </td></tr><tr><td data-label="Technical Breakdown — simulate.py"> <strong>Determinism & testing hygiene</strong><br><br>- Deterministic behavior is mandatory: ensure identical input + applied_rule_version yields identical <code>result</code> and <code>trace</code>. <br>- Provide golden fixtures for each major rules version. <br>- Add snapshot tests for trace formatting and for <code>runtime_fingerprint</code> stability across minor refactors. <br>- Provide a <code>simulate_cli</code> harness used by CI to run fixed golden fixtures and compare outputs. <br>- Provide property tests that run thousands of random canonical records and assert invariants (e.g., numeric totals, no NaNs). </td></tr><tr><td data-label="Technical Breakdown — simulate.py"> <strong>Performance & resource guidance</strong><br><br>- Target sub-100ms median for trivial records and rule sets in normal environments; heavy rule bundles may be slower — detect and document slow paths. <br>- Run CPU-heavy pure calculations in a bounded process/thread pool; use async timeouts to avoid blocking event loop. <br>- Provide a per-call <code>timeout_ms</code> override but protect the system with a global maximum. <br>- Keep memory per-simulation small; avoid creating large temporary objects. <br>- Provide instrumentation endpoints to profile hot rules in production and record slow-step fingerprints for rule owners. </td></tr><tr><td data-label="Technical Breakdown — simulate.py"> <strong>Audit & optional side-effects</strong><br><br>- When <code>run_audit=True</code>, append an append-only audit entry via the injected <code>recorder</code> component. The audit entry must be JSONL, include <code>applied_rule_version</code>, <code>runtime_fingerprint</code>, <code>input_hash</code>, <code>trace_id</code>, and a signed manifest of the result. Audit writes must be idempotent (idempotency key derived from input+rules) and non-blocking (return success to caller even if audit write is deferred, but surface the audit write status in logs). <br><br><strong>Policy</strong>: audits are enabled by default for internal calls only. Public simulate calls are denied audit unless explicitly authorized. </td></tr><tr><td data-label="Technical Breakdown — simulate.py"> <strong>Integration points (explicit dependencies)</strong><br><br>- <strong>rule_loader</strong>: read-only provider of <code>RuleBundle</code> objects. <br>- <strong>recorder</strong>: append-only audit writer (optional). <br>- <strong>telemetry</strong>: metrics & tracing API. <br>- <strong>sanitizer</strong>: PII redaction policy. <br>- <strong>auth_adapter</strong>: token / scope check. <br><br>All dependencies must be injected; avoid module-level singletons to permit easy unit testing. Document expected adapter interfaces in the file header. </td></tr><tr><td data-label="Technical Breakdown — simulate.py"> <strong>Recommended tests & CI checks</strong><br><br>1. Unit tests for each small helper (validation, selection, formatting).<br>2. Integration tests that run <code>simulate_record</code> with golden fixtures for each rules version.<br>3. Determinism test that repeatedly calls <code>simulate_record</code> and asserts identical outputs and identical <code>trace</code> fingerprints.<br>4. Timeout/failure simulations where rule functions sleep/raise to verify timeboxing, error mapping and cleanup.<br>5. Security tests confirming sanitization and no secrets leaked in traces.<br>6. Performance baseline test (smoke) to detect regressions in median latency. Automate these in CI gating. </td></tr><tr><td data-label="Technical Breakdown — simulate.py"> <strong>Maintenance notes & extension guidelines</strong><br><br>- Keep the top-level file focused on orchestration; move heavy utilities (e.g., sanitizers, rule deserializers, sandbox runners) into small, focused modules. <br>- When adding new trace fields, bump the <code>trace_schema_version</code> and record a migration note. <br>- If simulate must support batch simulation later, add a separate module <code>simulate_batch.py</code> instead of expanding <code>simulate.py</code> to avoid complexity. <br>- When adding feature flags that change behavior (degrade vs fail-fast), document exact semantics and default values at the head of the file. <br>- If rules become untrusted, replace in-process execution with a signed, versioned remote runner and keep <code>execute_pipeline</code> semantics the same via adapter pattern. </td></tr><tr><td data-label="Technical Breakdown — simulate.py"> <strong>Appendix — canonical response schema (human description only; no snippets)</strong><br><br>The response is a JSON object with named sections: <code>meta</code> (applied_rule_version, runtime_fingerprint, duration_ms), <code>result</code> (canonical outputs), <code>trace</code> (ordered decision steps with fingerprints and timings), <code>diagnostics</code> (warnings, non-fatal issues), and <code>errors</code> (fatal error envelope when present). The module must produce this schema exactly; any downstream consumers rely on field names and types. </td></tr></tbody></table></div><div class="row-count">Rows: 23</div></div><div class="table-caption" id="Table9" data-table="Docu_0164_09" style="margin-top:2mm;margin-left:3mm;"><strong>Table 9</strong></div>
<div class="table-wrapper" data-table-id="table-9"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown — export.py"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown — export.py</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — export.py"> <strong>File-level responsibilities</strong><br><br>This module is the canonical export orchestration layer: it converts calculation/audit outputs into signed, validated, and atomically-published bundles suitable for external consumption (e.g., tax authorities, downstream partners, or a public object-store). It must <em>not</em> implement low-level export formats (CSV/XML/Prefill) — those live in exporter modules — but it must (1) compose exporter outputs, (2) build a manifest with provenance and checksums, (3) perform bundle-level validation and signing, (4) stage and atomically publish bundles, (5) update export metadata records, and (6) emit notifications/events. Keep this file focused on orchestration and orchestration-only concerns: concurrency control, idempotency, transactional semantics, and observability. All I/O should be behind injectable adapters (storage client, db repository, signer, queue, notifier) so that unit tests can run offline. Document configuration knobs at the top (e.g., <code>staging_prefix</code>, <code>publish_prefix</code>, <code>signing_keyref</code>, <code>validate_on_publish</code>, <code>atomic_publish_mv</code>), and ensure the module exposes a small set of high-level entrypoints used by workers/HTTP API: <code>start_export_job</code>, <code>run_export_job</code>, and <code>publish_pending_exports</code>. </td></tr><tr><td data-label="Technical Breakdown — export.py"> <strong><code>start_export_job(job_id: str, ctx: ExportContext) -&gt; ExportJobResult</code></strong><br><br><strong>Responsibility:</strong> API-facing factory that validates the job request, checks preconditions, acquires an export lock, and enqueues or kicks off <code>run_export_job</code>. <strong>Inputs:</strong> <code>job_id</code> (idempotency key), <code>ctx</code> (includes applied_rule_version, snapshot pointers, output selectors, requestor). <strong>Outputs:</strong> <code>ExportJobResult</code> (status, export_id, staging_path). <strong>Side effects:</strong> writes initial export metadata record (<code>status: pending</code>) in metadata DB, increments a metric <code>export.jobs_started</code>. <strong>Idempotency:</strong> must return the same <code>export_id</code> if called again with same <code>job_id</code>; must check <code>exports</code> table by <code>job_id</code> before creating new. <strong>Errors & retries:</strong> validate arguments and raise <code>InvalidExportRequest</code> early; transient DB/storage errors should bubble with categorized retryable exceptions. <strong>Concurrency:</strong> acquire a distributed lock keyed by <code>job_id</code> (or content-hash) to prevent duplicate parallel exports. <strong>Testing:</strong> unit tests should mock DB and lock; integration should assert a second call with same <code>job_id</code> does not create duplicate export. </td></tr><tr><td data-label="Technical Breakdown — export.py"> <strong><code>run_export_job(export_id: str, ctx: ExportContext) -&gt; ExportResult</code></strong><br><br><strong>Responsibility:</strong> core synchronous orchestration that performs the end-to-end export for an <code>export_id</code>: collect calculation results, call exporters, create manifest, validate/sign, stage, atomically publish, and finalize metadata. <strong>Inputs:</strong> <code>export_id</code>, <code>ctx</code> (or read from DB). <strong>Outputs:</strong> <code>ExportResult</code> with final status and public location. <strong>Flow:</strong> (1) load export metadata and calculation snapshot pointers, (2) call <code>collect_calculation_results</code>, (3) call <code>assemble_export_payloads</code> → produce per-exporter artifacts, (4) <code>compute_checksums</code>, (5) <code>generate_manifest</code>, (6) <code>sign_manifest</code>, (7) <code>validate_bundle</code> (if configured), (8) <code>stage_bundle</code>, (9) <code>atomic_publish</code>, (10) <code>update_export_record(status=published)</code>, (11) <code>notify_caller</code>. <strong>Observability:</strong> record duration and per-step timings, increment failure/success counters, attach trace-id to logs. <strong>Idempotency:</strong> function must be re-entrant: if it finds export already <code>published</code> it returns success; if <code>staged</code> it should continue from staging->publish. <strong>Error handling:</strong> on permanent validation error, mark <code>status=failed</code> with structured <code>failure_reason</code> and push to exception queue. On transient errors, follow configured retry/backoff. <strong>Testing:</strong> full integration test that uses a fake object-store to assert atomic rename/publish and manifest contents. </td></tr><tr><td data-label="Technical Breakdown — export.py"> <strong><code>collect_calculation_results(export_id: str) -&gt; List[CanonicalRecordSet]</code></strong><br><br><strong>Responsibility:</strong> fetch canonical results for the export from snapshot storage or results DB. Must validate presence and schema conformance of required artifacts before exporting. <strong>Inputs:</strong> <code>export_id</code> or snapshot pointers from metadata. <strong>Outputs:</strong> list of canonical datasets grouped by exporter responsibility (e.g., payroll groups). <strong>Side effects:</strong> none (pure IO read). <strong>Error handling:</strong> if required result missing → raise <code>MissingCalculationSnapshot</code> (fatal for the export). For partial missing optional segments, annotate manifest warnings. <strong>Performance:</strong> stream large result sets rather than loading all into memory; provide chunked iterators to downstream exporters. <strong>Tests:</strong> unit tests using an in-memory snapshot provider and a large synthetic dataset to assert streaming behavior. </td></tr><tr><td data-label="Technical Breakdown — export.py"> <strong><code>assemble_export_payloads(results: List[CanonicalRecordSet], export_spec: ExportSpec) -&gt; Dict[str, ExportFile]</code></strong><br><br><strong>Responsibility:</strong> call format-specific exporter adapters (CSV/XML/prefill) with the canonical records to build bytes/blobs for each required artifact. It maps logical exporters (e.g., <code>bukti_potong.csv</code>, <code>bukti_potong.xml</code>, <code>spt_prefill.json</code>) to file blobs and metadata (mime, encoding). <strong>Inputs:</strong> canonical results and an <code>ExportSpec</code> enumerating required outputs and per-exporter options (locale, rounding rules, grouping). <strong>Outputs:</strong> dictionary <code>filename -&gt; ExportFile(metadata, stream_or_bytes)</code>. <strong>Design notes:</strong> exporters must be injected (pluggable) and invoked with streaming where possible. This function must remain format-agnostic and only handle orchestration and per-file metadata. <strong>Idempotency & determinism:</strong> ensure exported files are deterministic given the same inputs; stable sorting, stable serialization critical to checksums. <strong>Error handling:</strong> exporter-specific failures should be captured, logged with exporter name and row sample, and converted into a <code>ExporterFailure</code> error that includes whether the failure is fatal or recoverable. <strong>Testing:</strong> tests should assert deterministic ordering and that malformed rows are handled per exporter policy (skip+warn vs fail). </td></tr><tr><td data-label="Technical Breakdown — export.py"> <strong><code>compute_checksums(payloads: Dict[str, ExportFile], algorithm: str=&#x27;sha256&#x27;) -&gt; Dict[str, str]</code></strong><br><br><strong>Responsibility:</strong> compute canonical checksums for every file, and for the manifest file itself. Must define canonical hashing semantics: hash computed over serialized bytes with newline normalization (if text) and an explicit charset (UTF-8). <strong>Inputs:</strong> map of filename→ExportFile and hashing algorithm. <strong>Outputs:</strong> map filename→hex-checksum. <strong>Security:</strong> ensure hashing operation is constant-time safe for the library used; do not leak secret key material here. <strong>Idempotency:</strong> checksums must be stable across repeated runs on identical bytes. <strong>Observability:</strong> emit histogram of payload sizes and checksum time. <strong>Tests:</strong> property tests that small changes alter checksum and identical content yields same checksum across platforms. </td></tr><tr><td data-label="Technical Breakdown — export.py"> <strong><code>generate_manifest(export_id: str, payload_checksums: Dict[str,str], meta: ExportMeta) -&gt; Manifest</code></strong><br><br><strong>Responsibility:</strong> build a signed-ready manifest object that contains: file listing with checksums, applied_rule_version, generator_version, export_id, timestamps (generated_at, effective_date), content-hash or idempotency key, metadata (uploader, environment), and optional per-file hints (encoding, mime). <strong>Inputs:</strong> <code>export_id</code>, checksums, metadata read from DB. <strong>Outputs:</strong> <code>Manifest</code> object (serializable to JSON/NDJSON/XML depending on spec). <strong>Stability:</strong> manifest ordering must be deterministic; include version field for manifest schema. <strong>Validation:</strong> enforce required fields before signing. <strong>Testing:</strong> unit tests assert <code>to_safe_dict()</code> redacts secrets and stable ordering. </td></tr><tr><td data-label="Technical Breakdown — export.py"> <strong><code>sign_manifest(manifest: Manifest, signer: Signer) -&gt; SignedManifest</code></strong><br><br><strong>Responsibility:</strong> attach cryptographic signatures to the manifest. Use an injected <code>Signer</code> implementation that may support HMAC (symmetric) or asymmetric signatures (RSA/ED25519). Do not expose raw private keys; signer must be a reference to an external KMS or an adapter with <code>sign(bytes) -&gt; signature</code>. <strong>Inputs:</strong> manifest serialized bytes and signer handle. <strong>Outputs:</strong> a <code>SignedManifest</code> containing the manifest and signature(s), optionally with signer metadata (key id, algorithm). <strong>Security:</strong> enforce minimal signer privileges and policy-scoped key usage. <strong>Failure modes:</strong> if signer is unavailable and <code>settings.strict_signing == True</code>, abort export; otherwise mark <code>signed=false</code> and record <code>manifest_warning</code>. <strong>Testing:</strong> mock signer to return deterministic signature; assert exported manifest includes <code>signature</code> and <code>key_id</code>. </td></tr><tr><td data-label="Technical Breakdown — export.py"> <strong><code>validate_bundle(staged_files: Dict[str, StoragePath], manifest: Manifest, validator: BundleValidator) -&gt; ValidationResult</code></strong><br><br><strong>Responsibility:</strong> run bundle-level checks: (1) checksums match for all staged files, (2) manifest schema validation, (3) signature verification (if required), (4) optional business validation (schema of each file e.g., CSV column checks), (5) policy checks (no disallowed PII in filenames). <strong>Inputs:</strong> staged file locations and manifest. <strong>Outputs:</strong> <code>ValidationResult</code> with pass/fail and enumerated issues. <strong>Idempotency:</strong> safe to run repeatedly. <strong>Error handling:</strong> if validation fails, return a detailed failure report and halt publish; move bundle into <code>staged_invalid</code> with logs and provide remediation hints. <strong>Testing:</strong> include negative tests for altered bytes and mismatched manifest. </td></tr><tr><td data-label="Technical Breakdown — export.py"> <strong><code>stage_bundle(export_id: str, files: Dict[str, ExportFile], staging_prefix: str, storage: StorageClient) -&gt; Dict[str, StoragePath]</code></strong><br><br><strong>Responsibility:</strong> upload artifacts to a staging area in object storage under a stable path (e.g., <code>{staging_prefix}/{export_id}/</code>). Use multi-part uploads for large files, set server-side encryption flags, ACLs appropriate for staging (private), and write a <code>staging.marker</code> file on success. <strong>Inputs:</strong> files map, staging prefix, storage client. <strong>Outputs:</strong> map of filename→staged storage path. <strong>Atomicity considerations:</strong> the staging operation itself is not atomic; rely on the subsequent <code>atomic_publish</code> step and marker files to ensure consumers don’t read partial bundles. <strong>Resilience:</strong> implement resumable uploads and clean retry semantics. <strong>Security:</strong> apply bucket/object-level encryption and restrict access by prefix until publishing. <strong>Testing:</strong> integration tests should verify resumable behavior and that partial uploads on failure are cleaned up. </td></tr><tr><td data-label="Technical Breakdown — export.py"> <strong><code>atomic_publish(export_id: str, staged_paths: Dict[str, StoragePath], publish_prefix: str, storage: StorageClient, mv_on_publish: bool=True) -&gt; PublishResult</code></strong><br><br><strong>Responsibility:</strong> make the staged bundle publicly consumable (or move to production prefix) atomically. Preferred approach: write to <code>publish_prefix/tmp-{export_id}</code>, validate the copied set, then perform atomic rename/move (object-store-supported rename or bucket-level manifest swap). If the object-store does not support atomic rename, implement object-level copy+manifest swap with consistent <code>publish.marker</code> and <code>versioned</code> keys. <strong>Inputs:</strong> staged paths and publish prefix. <strong>Outputs:</strong> <code>PublishResult</code> with final public URIs and <code>published_at</code>. <strong>Idempotency:</strong> should be safe to call multiple times — if final path already exists and checksums match, treat as success. <strong>Failure handling:</strong> on failure, attempt rollback: remove partial publish target and keep staged copy for diagnostics. <strong>Observability:</strong> record publish latency and bytes moved. <strong>Testing:</strong> simulate stores with/without rename and assert atomic visibility to consumers. </td></tr><tr><td data-label="Technical Breakdown — export.py"> <strong><code>update_export_record(export_id: str, status: str, meta_updates: Dict[str,Any]) -&gt; None</code></strong><br><br><strong>Responsibility:</strong> persist final state changes to the metadata DB: status transitions (<code>pending</code> → <code>staged</code> → <code>published</code> or <code>failed</code>), public URIs, manifest checksum, signer key id, failure reasons, and processing times. Must enforce state machine semantics and write in a way that supports idempotent transitions (check current state before writing). <strong>Inputs:</strong> export_id, new status, optional metadata. <strong>Outputs:</strong> none. <strong>Side effects:</strong> may emit domain events to an outbox table for eventual publish. <strong>Concurrency:</strong> protect state transitions with optimistic concurrency (row version) or DB transaction. <strong>Testing:</strong> unit tests assert forbidden transitions raise and allowed transitions succeed. </td></tr><tr><td data-label="Technical Breakdown — export.py"> <strong><code>notify_caller(export_id: str, subscriber: NotificationTarget, result: ExportResult) -&gt; None</code></strong><br><br><strong>Responsibility:</strong> send the final notification/event to the caller: webhook, callback URL, message queue, or internal event bus. Use exponential backoff for webhooks and persist delivery attempts in <code>notifications</code> table. <strong>Inputs:</strong> notification target config and result payload (sanitized). <strong>Outputs:</strong> none. <strong>Security:</strong> sign outgoing webhooks with HMAC using a per-subscriber secret (do not log secrets); implement replay protection with <code>X-Export-Id</code> and timestamp. <strong>Idempotency:</strong> deliver exactly-once where possible using an outbox pattern or at-least-once with dedupe at receiver. <strong>Testing:</strong> integration tests should simulate receiver returning different HTTP codes and assert retry/backoff and eventual success/failure recorded. </td></tr><tr><td data-label="Technical Breakdown — export.py"> <strong><code>cleanup_staging(export_id: str, staging_prefix: str, storage: StorageClient, retain_policy: RetentionPolicy) -&gt; None</code></strong><br><br><strong>Responsibility:</strong> remove or archive staging artifacts after successful publish or failed-but-expired exports. Honor <code>retain_policy</code> (e.g., keep staged files for N days for troubleshooting). For archives, copy to a secure, write-once archive location before deletion if retention requires. <strong>Inputs:</strong> export_id, policy. <strong>Outputs:</strong> none. <strong>Safety:</strong> never delete published artifacts; ensure delete is scoped strictly to the staging prefix and export_id. <strong>Testing:</strong> assert that cleanup respects retention windows and does not remove published copies. </td></tr><tr><td data-label="Technical Breakdown — export.py"> <strong><code>handle_export_failure(export_id: str, error: Exception, policy: FailurePolicy) -&gt; None</code></strong><br><br><strong>Responsibility:</strong> central failure handler to (1) mark export record as <code>failed</code> with structured <code>failure_reason</code>, (2) optionally enqueue diagnostic artifacts to <code>exception_queue</code>, (3) trigger alerting (SRE/ops), and (4) apply retry policy if allowed. Must classify errors into transient vs permanent. <strong>Inputs:</strong> error and failure policy. <strong>Outputs:</strong> none. <strong>Audit:</strong> record stack, truncated sample of failing payload, and correlation ids; redact PII. <strong>Testing:</strong> unit tests that inject synthetic exceptions to verify classification and side effects (exception_queue entry created). </td></tr><tr><td data-label="Technical Breakdown — export.py"> <strong><code>lock_export_job(export_id: str, locker: Locker, ttl: int=300) -&gt; LockHandle</code></strong><br><br><strong>Responsibility:</strong> acquire a distributed job lock to prevent concurrent workers from publishing the same export. Use robust distributed-lock semantics (renewal, TTL, owner token). <strong>Idempotency:</strong> acquiring a lock twice by same owner returns same handle. <strong>Failure handling:</strong> on inability to obtain lock, the caller should treat as "another worker owns it" and either re-enqueue or exit. <strong>Testing:</strong> use a fake locker to assert TTL renewal and proper unlock on success/failure. </td></tr><tr><td data-label="Technical Breakdown — export.py"> <strong><code>unlock_export_job(lock_handle: LockHandle) -&gt; None</code></strong><br><br><strong>Responsibility:</strong> release distributed lock; ensure release is safe (only owner can release). <strong>Idempotency:</strong> releasing an already-released lock is a no-op. <strong>Testing:</strong> assert race conditions (owner lost TTL) are handled gracefully. </td></tr><tr><td data-label="Technical Breakdown — export.py"> <strong><code>dry_run_export(export_id: str, ctx: ExportContext) -&gt; DryRunReport</code></strong><br><br><strong>Responsibility:</strong> perform a non-persistent export that exercises all steps up to but excluding <code>atomic_publish</code> and final <code>update_export_record</code>. Useful for validation, simulation, and API <code>--check</code> behavior. Must be deterministic and not mutate production DB or storage. <strong>Inputs:</strong> export_id and context. <strong>Outputs:</strong> <code>DryRunReport</code> with generated manifest preview, checksums, and validation result. <strong>Testing:</strong> unit and integration tests that assert nothing is written to storage or metadata DB. </td></tr><tr><td data-label="Technical Breakdown — export.py"> <strong><code>retry_policy_handler(export_id: str, attempts: int, error: Exception) -&gt; RetryDecision</code></strong><br><br><strong>Responsibility:</strong> centralize retry decision logic for transient failures. Must consult configuration (max_attempts, backoff_strategy), classify errors (network, storage, validation, signing), and return a decision: retry_now, retry_later(delay), or give_up. <strong>Inputs:</strong> attempts count and error object. <strong>Outputs:</strong> <code>RetryDecision</code>. <strong>Testing:</strong> exercise exponential backoff and ensure validation errors return give_up. </td></tr><tr><td data-label="Technical Breakdown — export.py"> <strong><code>record_export_audit(export_id: str, manifest: Manifest, signed_manifest: SignedManifest, actor: str) -&gt; AuditEntry</code></strong><br><br><strong>Responsibility:</strong> append an append-only audit entry for the export in the audit store (JSONL): includes export_id, manifest checksum, applied_rule_version, actor (system/uploader), timestamp, and export outcome. This complements <code>recorder.py</code> but is export-specific. <strong>Immutability:</strong> write-only; never overwrite. <strong>Privacy:</strong> ensure no raw PII from payloads is embedded in the audit; instead store hashes/redacted summaries. <strong>Testing:</strong> verify audit entries are written and that replaying audit entries can reconstruct manifest checksums. </td></tr><tr><td data-label="Technical Breakdown — export.py"> <strong><code>validate_publish_permissions(export_id: str, actor: str, policy: PublishPolicy) -&gt; None</code></strong><br><br><strong>Responsibility:</strong> ensure the caller/worker is authorized to publish the export (role-based + export-scoped checks), and ensure the signer key used is permitted for this kind of export. <strong>Failure handling:</strong> unauthorized attempts must be logged and rejected with <code>UnauthorizedPublishAttempt</code>; create a security alert. <strong>Testing:</strong> assert denial for unauthorized users. </td></tr><tr><td data-label="Technical Breakdown — export.py"> <strong><code>publish_pending_exports(limit: int=10) -&gt; List[PublishResult]</code></strong><br><br><strong>Responsibility:</strong> a scheduler/cron entrypoint that scans metadata DB for <code>status == staged</code> or <code>ready_to_publish</code> exports and drives <code>atomic_publish</code> in controlled batches (respecting concurrency limits). Should coordinate with <code>lock_export_job</code> and honor <code>max_concurrent_publishes</code>. <strong>Observability:</strong> expose metrics for scanned/published/failed counts. <strong>Idempotency:</strong> safe to run concurrently from multiple scheduler processes — rely on locks. <strong>Testing:</strong> integration test simulating many staged exports and asserting batch behavior and retries on transient failures. </td></tr><tr><td data-label="Technical Breakdown — export.py"> <strong>Implementation patterns & guardrails (module-level)</strong><br><br>— <strong>Separation of concerns</strong>: keep format generation in dedicated exporter modules. <code>export.py</code> wires them and implements orchestration only.<br>— <strong>Dependency injection</strong>: accept abstracted <code>StorageClient</code>, <code>Signer</code>, <code>DBRepository</code>, <code>Notifier</code>, <code>Locker</code>, <code>Validator</code> to enable unit testing.<br>— <strong>Idempotence</strong>: every externally-observable operation (metadata DB writes, publish) must be idempotent; use idempotency keys and state-machine checks. <br>— <strong>Atomic publish</strong>: always stage then publish; do not publish directly to public prefix. Implement <code>publish.marker</code> semantics or provider-native atomic move. <br>— <strong>Security</strong>: never expose private keys; sign via KMS adapter. Apply server-side encryption and least-privilege ACLs. Sign webhook notifications and redact PII in logs. <br>— <strong>Observability</strong>: emit structured logs with <code>export_id</code>, trace_id, step, and durations. Record histograms for export sizes and publish times. Use an outbox pattern for notifications to avoid losing events on DB failures. <br>— <strong>Failure modes</strong>: classify errors into permanent (validation, malformed data), transient (network, timeouts), and operational (missing signer). Permanent errors should mark <code>failed</code> and push remediation artifacts; transient errors should be retried with backoff. <br>— <strong>Testing hygiene</strong>: provide <code>build_test_export_orchestrator</code> which injects in-memory storage, fake signer, fake locker, and deterministic exporters. Provide golden fixtures for manifest content (schema, version) and perform property-based tests for checksums and deterministic output. </td></tr><tr><td data-label="Technical Breakdown — export.py"> <strong>Recommended tests & CI checks (module-specific)</strong><br><br>1. <strong>Unit</strong>: per-function tests covering happy-path, idempotency, and edge cases for <code>compute_checksums</code>, <code>generate_manifest</code>, and <code>sign_manifest</code> (use deterministic signer stub).<br>2. <strong>Integration (fast)</strong>: stage/publish flow against a local object-store emulator verifying atomic publish semantics and manifest correctness.<br>3. <strong>E2E/Smoke</strong>: simulated pipeline from <code>collect_calculation_results</code> through <code>publish</code> using sample results and asserting consumer can validate bundle via manifest and signature. <br>4. <strong>Chaos</strong>: simulate signer failures, partial upload interruptions, and storage inconsistency to assert recovery paths and retries.<br>5. <strong>Security</strong>: tests that verify no private keys appear in logs or manifest, and webhooks are HMAC-signed correctly. </td></tr><tr><td data-label="Technical Breakdown — export.py"> <strong>Operational & security checklist (before production flip)</strong><br><br>- Ensure <code>signing_keyref</code> points to a KMS key with narrow usage permissions and rotation policy.<br>- Confirm <code>staging_prefix</code> and <code>publish_prefix</code> have correct encryption and ACLs.<br>- Enable <code>validate_on_publish</code> and strict manifest checks in prod.<br>- Configure alerts for export failure rate and publish latency spikes.<br>- Run a canary export and validate manifest + signature using an independent verifier.<br>- Ensure retention/cleanup policies for staging and archived bundles are enforced. </td></tr><tr><td data-label="Technical Breakdown — export.py"> <strong>Maintenance & developer notes</strong><br><br>- When adding a new export format, add an exporter adapter and update <code>assemble_export_payloads</code> wiring; keep generation logic inside the adapter.<br>- If changing manifest schema, bump <code>manifest.version</code> and provide a migration path; maintain backward-compatible parsing in consumer tools.<br>- Avoid embedding business rules into export orchestration; keep rules in rules engine modules (ptkp.py, ter.py, etc.).<br>- Add smoke tests to CI that build a minimal export bundle and run the verifier (signature + checksum). </td></tr></tbody></table></div><div class="row-count">Rows: 26</div></div><div class="table-caption" id="Table10" data-table="Docu_0164_10" style="margin-top:2mm;margin-left:3mm;"><strong>Table 10</strong></div>
<div class="table-wrapper" data-table-id="table-10"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by **Technical Breakdown — auth.py**"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;"><strong>Technical Breakdown — auth.py</strong></div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — auth.py"> <strong>File-level responsibilities</strong><br><br>This module centralizes all authentication and authorization concerns for the application. Responsibilities: token validation (JWT and opaque), OAuth2/OIDC discovery and JWKS handling, local token issuance for dev/test, permission/scope enforcement, FastAPI dependency factories and optional middleware, token revocation/blacklist management, and audit/telemetry hooks for auth events. Keep network I/O out of import-time execution: all network calls must live behind async functions invoked during startup or at call-time with timeouts. Expose small, well-typed building blocks (fetch_jwks, verify_jwt, oauth2_introspect, auth_dependency) so callers can compose behavior without hidden side-effects. Document operational knobs at the file head: <code>settings.auth.strict</code>, <code>settings.auth.jwks_refresh</code>, <code>settings.auth.token_issuer</code>, <code>settings.auth.accepted_algorithms</code>, <code>settings.auth.dev_mode_signing_key</code>, and <code>settings.auth.introspection_endpoint</code>. Attach a clear idempotence guard for any registration helper (<code>_registered_on_app</code>). Unit tests should mock network endpoints and secret stores. </td></tr><tr><td data-label="Technical Breakdown — auth.py"> <strong><code>init_auth(app, settings) -&gt; None</code></strong><br><br>Startup wiring entrypoint used by <code>create_app</code>. Must be idempotent and side-effect free on import. Responsibilities: validate <code>settings.auth</code> shape, register <code>app.state.auth</code> with required factories (<code>jwks_client</code>, <code>introspector</code>, <code>revocation_store</code>, <code>signer</code>), schedule periodic JWKS refresh task only when <code>settings.auth.jwks_refresh.enabled</code> is true, and register graceful shutdown handlers to cancel background refresh tasks. Must not perform blocking I/O; use async background supervisor for network ops. When <code>settings.strict</code> is true, startup must fail if JWKS fetching or required endpoints are unreachable; otherwise mark <code>app.state.auth_degraded = True</code> and continue. Unit tests: call repeatedly and assert idempotence, and simulate network failure to validate degraded vs fatal modes. </td></tr><tr><td data-label="Technical Breakdown — auth.py"> <strong><code>fetch_jwks(url: str, timeout: float) -&gt; Dict[str,Any]</code></strong><br><br>Pure async network helper to GET JWKS. Must implement: per-request timeout, exponential backoff with jitter on transient HTTP 5xx responses, strict JSON schema validation for JWKS structure (keys array, required <code>kid</code>, <code>kty</code>, <code>alg</code> fields), and robust error messages including URL and HTTP status. Never cache secrets in-memory beyond <code>app.state</code> factories. Return a minimal parsed map <code>kid -&gt; jwk</code>. Tests: validate resiliency to malformed JSON, truncated bodies, and large payloads. </td></tr><tr><td data-label="Technical Breakdown — auth.py"> <strong><code>jwks_cache_factory(urls: List[str], refresh_interval: int, max_age: int) -&gt; Callable[[],Mapping]</code></strong><br><br>Factory that returns a callable accessor to the current JWKS map. Implementation notes: internal state must be thread-/task-safe (asyncio.Lock), refresh logic respects <code>max_age</code> and <code>refresh_interval</code> and supports forced refresh. On refresh, validate and merge multi-issuer JWKS deterministically by <code>issuer</code> or <code>kid</code> preference. Provide a <code>force_refresh()</code> method for tests. Avoid performing refresh at import-time; create background task in <code>init_auth</code> when allowed. Unit tests: concurrent access, race conditions, and merge conflicts. </td></tr><tr><td data-label="Technical Breakdown — auth.py"> <strong><code>get_public_key_for_kid(jwks_map, kid) -&gt; PublicKey</code></strong><br><br>Pure transformation: convert a JWK entry into a cryptography-compatible public key object. Validate supported algorithm families (<code>RS256</code>, <code>ES256</code>, etc.) and raise domain-specific <code>UnsupportedAlgorithm</code> or <code>KeyNotFound</code> exceptions. Do not expose raw JWKs to higher layers; return only safe key objects. Tests: cover RSA/EC conversions and bad <code>kid</code>. </td></tr><tr><td data-label="Technical Breakdown — auth.py"> <strong><code>verify_jwt(token: str, audience: Optional[str], issuer: Optional[str], jwks_accessor: Callable, leeway: int = 0) -&gt; Dict[str,Any]</code></strong><br><br>Main validation routine for JWTs. Requirements: strict header parsing (extract <code>alg</code>, <code>kid</code>), fetch public key via <code>jwks_accessor(kid)</code> with limited blocking and timeout, reject tokens signed with unexpected algorithms, validate standard claims (<code>exp</code>, <code>nbf</code>, <code>iat</code>) with configured leeway, validate <code>aud</code> and <code>iss</code> per settings (support array or string <code>aud</code>), and perform signature verification with constant-time comparisons where applicable. Return normalized claims (strings/ints canonicalized). Errors: raise well-typed exceptions (<code>TokenExpired</code>, <code>InvalidSignature</code>, <code>InvalidClaims</code>, <code>KeyRotationInProgress</code>) that map to HTTP status codes in error handlers. Avoid heavy crypto per-request by using a short-lived key cache and metrics for verification latency. Tests: golden JWTs, expired, nbf in future, wrong audience, and algorithm downgrade attacks. </td></tr><tr><td data-label="Technical Breakdown — auth.py"> <strong><code>introspect_token(token: str, introspection_endpoint: str, client_credentials: Tuple[str,str], timeout: float) -&gt; Dict[str,Any]</code></strong><br><br>OAuth2 opaque token introspection helper. Must perform HTTP POST per RFC7662, include client auth (basic or token), validate <code>active</code> boolean, map standard fields (<code>scope</code>, <code>exp</code>, <code>sub</code>, <code>client_id</code>) into consistent claims. Implement request-level timeouts, backoff on 5xx, and strict schema validation. When the introspection endpoint returns <code>active: false</code>, raise <code>InactiveToken</code>. Do not leak raw response bodies in logs — redact tokens and PII. Tests: simulate active/inactive responses and 500/timeout behaviors. </td></tr><tr><td data-label="Technical Breakdown — auth.py"> <strong><code>is_token_revoked(token_id: str, revocation_store) -&gt; bool</code></strong><br><br>Check against a revocation repository (DB, Redis, or in-memory). Behavior: prefer eventual-consistent stores with TTL; support batched lookup for performance; use probabilistic filters (Bloom) as an optional optimization but ensure deterministic correctness for deny decisions. Design revision: when revocation store is unreachable and <code>settings.auth.strict</code> is true, fail-closed (treat tokens as revoked); otherwise fail-open with audit log and metric increment. Tests: revocation, TTL expiry, store unavailability. </td></tr><tr><td data-label="Technical Breakdown — auth.py"> <strong><code>revoke_token(token_id: str, reason: Optional[str], ttl: Optional[int]) -&gt; None</code></strong><br><br>Record a token revocation in the <code>revocation_store</code>. Ensure idempotency: repeated calls must not error. Attach <code>actor</code> metadata when available (admin action vs user logout). Emit audit event and increment revocation metrics. Use explicit expiry (ttl) to avoid unbounded growth. Tests: idempotence, metadata persisted, and audit entry created. </td></tr><tr><td data-label="Technical Breakdown — auth.py"> <strong><code>create_local_jwt(payload: Dict[str,Any], private_key_ref: SecretRef, alg: str, exp: int) -&gt; str</code></strong><br><br>Dev/test helper to issue self-signed JWTs. Must be gated behind <code>settings.env in (&quot;dev&quot;,&quot;test&quot;)</code> or <code>settings.auth.allow_local_issuance</code>. Load private key lazily via secret provider (do not store raw key in app.state). Support <code>kid</code> header for local rotation. Strictly log only <code>kid</code> and claims summary (no sensitive claims). Tests: creation and verification loops using <code>verify_jwt</code>. </td></tr><tr><td data-label="Technical Breakdown — auth.py"> <strong><code>auth_dependency(scopes: Optional[List[str]] = None) -&gt; Callable</code></strong><br><br>FastAPI dependency factory returning a <code>Depends</code>-style callable that: extracts auth header (Bearer), handles <code>settings.auth.accept_opaque</code> (introspect) and <code>settings.auth.accept_jwt</code> (verify_jwt), checks revocation, enforces required scopes/permissions using constant-time scope matching, and attaches normalized <code>AuthContext</code> object to the request (claims, token_type, raw_claims, principal_id). Behavior must be safe for concurrency, idempotent per-request, and fast-failing on malformed tokens. Provide a <code>raise_http</code> flag to have the dependency raise <code>HTTPException(401/403)</code> or to return <code>None</code> (useful for internal health checks). Tests: dependency in TestClient routes with combinations of tokens and scopes. </td></tr><tr><td data-label="Technical Breakdown — auth.py"> <strong><code>require_scopes(claims: Dict[str,Any], required: Iterable[str], mode: str = &quot;all&quot;) -&gt; None</code></strong><br><br>Permission enforcement helper. Modes: <code>all</code> (every scope required), <code>any</code> (at least one), <code>exact</code> (claim scopes equals required set) and <code>hierarchical</code> (support namespace <code>resource:action</code>). Must handle scope claims encoded as space-separated string or list. Raise <code>PermissionDenied</code> with contextual hint listing missing scopes. Include unit tests for each mode and edge-cases (empty scopes, wildcard <code>*</code>). </td></tr><tr><td data-label="Technical Breakdown — auth.py"> <strong><code>AuthMiddleware</code> (optional) — class-based middleware</strong><br><br>If provided, middleware must be minimal: normalize <code>Authorization</code> header, attach trace/context headers, short-circuit anonymous access only for public endpoints, and forward the request to downstream only after populating <code>request.state.auth</code> using the <code>auth_dependency</code> behavior. Middleware must never perform heavy crypto synchronously — convert middleware to spawn an async task for expensive checks when safe, or prefer route-level dependencies for per-request verification. Middleware must respect <code>settings.middleware_mode</code> (noop/stub for tests). Tests: header normalization, conflict with route dependency, and bypass list. </td></tr><tr><td data-label="Technical Breakdown — auth.py"> <strong><code>map_auth_exceptions(app) -&gt; None</code></strong><br><br>Register error handlers that map auth-specific exceptions to stable error envelopes: <code>InvalidToken -&gt; 401</code>, <code>TokenExpired -&gt; 401</code> (with <code>WWW-Authenticate: Bearer error=&quot;invalid_token&quot;, error_description=&quot;...&quot;</code>), <code>PermissionDenied -&gt; 403</code>, <code>KeyNotFound -&gt; 401/500</code> depending on context. Scrub PII and raw tokens from messages. Attach <code>trace_id</code> and <code>auth_hint</code> in response for operator debugging. Unit tests: trigger each exception and assert response shape and headers. </td></tr><tr><td data-label="Technical Breakdown — auth.py"> <strong><code>get_principal_from_claims(claims: Dict[str,Any]) -&gt; Principal</code></strong><br><br>Transform claims into application-level principal (id, type, email, org, roles). This function must be deterministic and pure. Avoid mapping sensitive claims (SSN) into logs. Provide hooks for optional enrichment (backing DB lookup) but keep enrichment out of this function; enrichment should be separate async function used by higher layers. Tests: mapping variations across issuers. </td></tr><tr><td data-label="Technical Breakdown — auth.py"> <strong><code>build_token_audit_event(token, claims, outcome, metadata=None) -&gt; Dict</code></strong><br><br>Create the structured audit event for each auth decision (success, failure, revocation). Event must include <code>timestamp</code>, <code>outcome</code>, <code>client_ip</code> (if available), <code>issuer</code>, <code>kid</code>, <code>principal_id</code> (if available), <code>masked_token_hash</code> (sha256 of token with salt), and safe <code>hint</code>. Never include raw token. This helper is used by <code>verify_jwt</code>, <code>introspect_token</code>, and <code>revoke_token</code> to emit consistent audit logs and telemetry. Tests: ensure no raw tokens persisted and hash reproducible. </td></tr><tr><td data-label="Technical Breakdown — auth.py"> <strong><code>rotate_local_keys(secret_provider, rotation_policy) -&gt; None</code></strong><br><br>Administrative helper to rotate local signing keys (dev/test or self-hosted CA workflows). Must be explicit (not automatic unless flagged), produce <code>kid</code> mapping, publish new <code>public_jwk</code> to configured JWKS endpoint (if hosting), and schedule deprecation window for old keys. Ensure atomic publish semantics (publish new key before accepting tokens signed with it or vice versa depending on policy) and emit audit entries. Tests: rotation sequence and acceptance/failure windows. </td></tr><tr><td data-label="Technical Breakdown — auth.py"> <strong><code>validate_auth_settings(settings) -&gt; None</code></strong><br><br>Small pure helper used by <code>init_auth</code> and CLI <code>--check</code> to validate <code>settings.auth.*</code>. Validate numeric ranges (timeouts > 0), allowed algorithms list not empty, introspection config consistent (endpoint + client creds), and that dev issuance is disabled in <code>prod</code> by default. Return safe redacted <code>to_safe_dict()</code> for CLI output. Tests: invalid configs produce human-friendly messages. </td></tr><tr><td data-label="Technical Breakdown — auth.py"> <strong><code>register_auth_routes(router, settings) -&gt; None</code></strong><br><br>Optional small admin/debug routes for auth: <code>/internal/.well-known/jwks.json</code> (if service hosts JWKS), <code>/internal/auth/rotate</code> (admin rotate), <code>/internal/auth/introspect</code> (proxy to introspection for internal tools). Gate these strictly behind <code>settings.internal_auth_secret</code> or host-based restrictions; default to disabled. Routes must be pure router mounts (no side-effectful imports). Tests: ensure routes absent in production settings. </td></tr><tr><td data-label="Technical Breakdown — auth.py"> <strong>Observability & telemetry</strong><br><br>Every public function should emit structured metrics/events: JWT verification latency, JWKS refresh failures, introspection error rates, revocation counts, and permission-denied counters. Use non-blocking, async metrics exporter and include <code>trace_id</code> and <code>request_id</code>. Provide a <code>dry_run</code> mode that swaps telemetry with an in-memory sink for tests. </td></tr><tr><td data-label="Technical Breakdown — auth.py"> <strong>Security & threat model notes</strong><br><br>— Always validate <code>alg</code> and reject <code>alg=none</code>. <br>— Avoid accepting tokens signed with weak curves or deprecated algorithms; provide config to opt-in temporarily. <br>— Redact tokens and PII in all logs; never log full headers or bodies. <br>— Secrets must be accessed via <code>SecretRef</code> and revealed only during startup or within a narrow execution scope. <br>— Implement rate-limits on introspection and JWKS endpoints to avoid DoS. <br>— Consider brute-force protections for middleware that decodes many tokens per second. </td></tr><tr><td data-label="Technical Breakdown — auth.py"> <strong>Testing & CI</strong><br><br>Unit tests: verification logic with fixture tokens (valid/invalid), jwks fetch mocking, introspection responses, revocation store stub, dependency wiring with FastAPI TestClient. Integration tests: <code>create_app</code> lifecycle exercising <code>init_auth</code> with <code>dry_run</code> telemetry and secret stubs; ensure startup fails under <code>strict=True</code> when endpoints unreachable. Security tests: algorithm downgrade, token replay, revocation edge-cases, and logging redaction. Fuzz tests: malformed JWT headers, oversized claims, and boundary <code>exp/nbf</code> times. </td></tr><tr><td data-label="Technical Breakdown — auth.py"> <strong>Operational checklist before production</strong><br><br>— Ensure <code>strict=True</code>. <br>— Configure robust JWKS endpoints and monitoring for refresh failures. <br>— Populate <code>accepted_algorithms</code> and reject <code>none</code>. <br>— Ensure revocation backend is highly available or set a safe fail-policy. <br>— Disable local token issuance in production. <br>— Confirm telemetry targets and that auth events are sampled appropriately. <br>— Verify <code>/internal</code> admin endpoints are disabled or strongly authenticated. </td></tr><tr><td data-label="Technical Breakdown — auth.py"> <strong>Maintenance & future changes guidance</strong><br><br>— When adding a new token type or identity provider, add a small adapter implementing a consistent <code>verify(token) -&gt; claims</code> interface and register it in <code>app.state.auth.adapters</code>. <br>— When changing accepted algorithms or key formats, add migration tests and document backward compatibility windows. <br>— Keep crypto and HTTP clients lazy-imported. <br>— Add small, focused modules for key management, introspection, and revocation if file grows. </td></tr></tbody></table></div><div class="row-count">Rows: 24</div></div><div class="table-caption" id="Table11" data-table="Docu_0164_11" style="margin-top:2mm;margin-left:3mm;"><strong>Table 11</strong></div>
<div class="table-wrapper" data-table-id="table-11"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown — pipeline.py"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown — pipeline.py</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — pipeline.py"> <strong>File-level responsibilities</strong><br><br>This module is the canonical orchestrator for the ingestion → calculation → export pipeline. It codifies the sequential and async flows that move data from inbound sources through normalization/validation, snapshot persistence, queued job execution, rules evaluation, auditing, reconciliation hooks and final export/manifest publication. Keep this file focused on orchestration and coordination only — pure business logic, parsing, storage drivers, rule engines, and exporters must be imported from other modules (parsers.py, validators.py, file_ops.py, recorder.py, manifest.py, bukti_potong_*.py, etc.) and called through well-defined adapter interfaces. At import time: no network I/O, no long-running tasks, and only lightweight configuration/constant definitions. All external side effects (DB writes, object-store uploads, queue operations, external API calls) must be performed by small helper functions invoked from within explicit <code>run_*</code>/<code>handle_*</code> functions. Document expected invariants (idempotency keys, content-hash scheme, required metadata fields, applied_rule_version semantics) at the head of the file. Provide test helpers that allow replacing object_store/queue/recorder with fakes. </td></tr><tr><td data-label="Technical Breakdown — pipeline.py"> <strong><code>build_pipeline(settings: Settings) -&gt; Pipeline</code></strong><br><br>File-level factory that returns a configured <code>Pipeline</code> orchestration object (or a small namespace of functions) wired with <code>settings</code>, <code>object_store_client</code>, <code>queue_client</code>, <code>recorder</code>, and <code>exporter_factory</code>. Must perform synchronous validation only: verify that required factories are present, that idempotency strategy is set (e.g., <code>settings.idempotency = &quot;content-hash&quot;</code>), and that timeouts and concurrency limits are sane. Must not open network connections. Attach <code>runtime_fingerprint</code> and a <code>dry_run</code> flag to the returned object. Unit tests should construct fake factories and assert the returned pipeline exposes documented public methods and is idempotent to repeated <code>build_pipeline</code> calls. </td></tr><tr><td data-label="Technical Breakdown — pipeline.py"> <strong><code>ingest_from_http(stream: IO, meta: dict) -&gt; IngestResult</code></strong><br><br>High-level entrypoint used by the upload endpoint and CLI. Responsibilities: (1) compute content hash efficiently (streaming, non-blocking where possible), (2) route to the appropriate parser (via <code>parsers.detect_and_parse</code>), (3) produce a canonical row stream and metadata (original filename, uploader, content_hash, row_count estimate), (4) persist a compressed snapshot to object store (use <code>file_ops.write_snapshot</code>) with deterministic path derived from <code>content_hash</code> and <code>ingest_time</code> truncated to seconds, (5) emit a pre-ingest validation pass (row-level validators with light sampling if <code>settings.strict</code> false), (6) return an <code>IngestResult</code> containing <code>snapshot_id</code>, <code>content_hash</code>, <code>validation_summary</code> and <code>idempotency_action</code> (<code>new|duplicate|replaced</code>). Important: ensure streaming reads don't buffer entire file into memory; support chunked hashing and chunked upload APIs. Tests: stream a large synthetic CSV and assert memory remains bounded and snapshot path determinism. </td></tr><tr><td data-label="Technical Breakdown — pipeline.py"> <strong><code>compute_content_hash(stream: IO) -&gt; str</code></strong><br><br>Pure helper to compute canonical hash over uploaded payload. Must define the canonicalization exactly (e.g., normalize newlines, remove trailing whitespace-only final lines, canonicalize BOMs if text). Use a stable algorithm (SHA-256) and include a version prefix in the final digest to allow future canonicalization changes (<code>v1:sha256:...</code>). Document the exact canonicalization steps in the function docstring. Idempotency relies on this function: any change requires migration. Unit tests must include cross-platform newline permutations and verify identical hash outputs. </td></tr><tr><td data-label="Technical Breakdown — pipeline.py"> <strong><code>persist_snapshot(stream: IO, content_hash: str, meta: dict) -&gt; SnapshotRef</code></strong><br><br>Writes snapshot to configured object store. Responsibilities: write to a staging path, verify checksum after write, atomically rename to final path, write snapshot metadata (content_hash, uploader, mime_type, row_count, stats) to metadata DB (or <code>app.state._metadata_store</code>). Should attach content-type and optionally an encryption key handle (do not embed raw keys). On write failure, ensure partial artifacts are removed or left in a documented <code>&quot;stale&quot;</code> location for forensics. Support <code>dry_run</code> where nothing is written. Tests: simulate transient object-store failures (timeouts) and assert backoff and retry logic. </td></tr><tr><td data-label="Technical Breakdown — pipeline.py"> <strong><code>validate_snapshot(snapshot_ref: SnapshotRef, validators: List[Validator]) -&gt; ValidationReport</code></strong><br><br>Performs row-level validation over a persisted snapshot. Must be implemented as streaming validation to avoid loading full snapshot into memory. Responsibilities: apply validators in a configurable order, emit structured diagnostics (error code, row_index, column, message, severity), apply sampling policy for soft checks (controlled by settings), and produce a summary (<code>ok_count</code>, <code>warn_count</code>, <code>err_count</code>, <code>schema_violations</code>). Decide whether validation failures block ingestion based on <code>settings.strict</code> and <code>meta.source_type</code>. Validation must return both machine-readable report and an artist-friendly short summary for caller UI. Tests: inject validators that raise and ensure failures are captured and do not crash the pipeline. </td></tr><tr><td data-label="Technical Breakdown — pipeline.py"> <strong><code>create_or_resolve_idempotency_record(content_hash: str, meta: dict) -&gt; IdempotencyRecord</code></strong><br><br>Coordinate idempotency checks against the metadata DB. Responsibilities: (1) check for existing records by <code>content_hash</code> and by <code>user_provided_id</code> if present, (2) if duplicate found, compute <code>idempotency_action</code> (<code>skip|update|reprocess</code>) based on <code>settings.idempotency_policy</code>, (3) reserve an idempotency token (DB row with <code>status: processing</code>) to avoid duplicate concurrent ingest writes, (4) expose a lease TTL to allow safe retries if a worker crashes. This function must be transactional to avoid race conditions. Tests: parallel threads try to ingest the same payload — assert only one wins the <code>processing</code> lease. </td></tr><tr><td data-label="Technical Breakdown — pipeline.py"> <strong><code>enqueue_ingest_job(snapshot_ref: SnapshotRef, idempotency_record: IdempotencyRecord, queue_meta: dict) -&gt; JobRef</code></strong><br><br>Create a queue message referencing the snapshot and the idempotency token. Responsibilities: choose queue and priority by <code>meta.source</code> and <code>settings.queue_routing</code>, attach retry policy, include a compact manifest with <code>snapshot_path</code>, <code>content_hash</code>, <code>applied_rule_version</code> (if known) and <code>ingest_time</code>. The enqueue operation must be idempotent: include the idempotency key in the message body/header and persist the queue message id into the idempotency record. Provide <code>visibility_timeout</code> and a <code>dedupe_key</code> if queue backend supports it. Tests: simulate message duplication and ensure worker handles idempotent replays. </td></tr><tr><td data-label="Technical Breakdown — pipeline.py"> <strong><code>worker_main_loop(worker_config: WorkerConfig) -&gt; None</code></strong><br><br>Top-level worker lifecycle entrypoint used by process managers. Responsibilities: connect to clients (object store, metadata DB, recorder, queue), subscribe to queues, supervise a bounded worker pool of job handlers (async tasks or threads depending on runtime), manage graceful shutdown (stop accepting new messages, wait for inflight handlers up to <code>shutdown_grace_period</code>), and emit lifecycle metrics (worker_up, jobs_processed, jobs_failed). Use backoff and jitter on queue reconnects. Must honor <code>settings.max_concurrency</code>. Tests: simulate SIGTERM and assert shutdown sequence. </td></tr><tr><td data-label="Technical Breakdown — pipeline.py"> <strong><code>handle_job(job_ref: JobRef) -&gt; JobResult</code></strong><br><br>Single-job handler that implements the calculation pipeline for a snapshot. Responsibilities (ordered and timeboxed): (1) refresh idempotency record and set status <code>running</code>, (2) load snapshot manifest and data stream, (3) materialize canonical rows (use csv_mapper/excel_reader for mapping), (4) perform final row-level validation if not already done or if snapshot is mutated, (5) load <code>applied_rule_version</code> (from metadata or snapshot manifest) and validate rules availability, (6) call <code>execute_calculation_pipeline</code> (pure-rule executor) with a bounded CPU and memory envelope, (7) persist results (append-only to audit recorder), (8) optionally trigger reconcile pre-check and export orchestration, and (9) mark idempotency record as <code>done</code> with result metadata. Errors must transition idempotency to <code>failed</code> or <code>retry</code> depending on error type. Always return a compact <code>JobResult</code> for observability. Unit + integration tests should assert transacted idempotency transitions. </td></tr><tr><td data-label="Technical Breakdown — pipeline.py"> <strong><code>load_snapshot_rows(snapshot_ref: SnapshotRef) -&gt; Iterator[CanonicalRow]</code></strong><br><br>Streaming loader that returns canonical rows (typed dicts) from snapshot. Responsibilities: handle compressed snapshots (gzip, zstd), detect encoding, map columns to canonical schema using <code>csv_mapper</code>, and emit line-level metadata (source_line_no, original_values). Must be robust to malformed lines (emit validation diagnostics instead of crashing) and support resuming from a byte offset (for long-running jobs). Tests: confirm ability to read extremely large snapshots with minimal memory. </td></tr><tr><td data-label="Technical Breakdown — pipeline.py"> <strong><code>execute_calculation_pipeline(rows_iterator: Iterator[CanonicalRow], ctx: ExecutionContext) -&gt; CalculationResult</code></strong><br><br>Core pure/evaluative function that orchestrates rule execution. Responsibilities: (1) load deterministic pure functions / rule modules for the <code>applied_rule_version</code>, (2) create a per-job deterministic execution context (seeded PRNG, fixed timezone), (3) execute pure modules (ptkp, ter, progressive, dtp_engine) in the documented order, mapping inputs → per-rule outputs and decision traces, (4) capture granular per-row traces (inputs, intermediate outputs, rule hits, warnings) in a size-limited structure (configurable trace sampling), (5) return both aggregated outputs (per-taxline totals, flags) and the decision trace stream for auditing. This function must be CPU-bound and synchronous (no heavy I/O). Unit tests: run against golden fixtures and assert both numeric results and trace content. </td></tr><tr><td data-label="Technical Breakdown — pipeline.py"> <strong><code>apply_rule_module(module_name: str, input_batch: List[CanonicalRow], ctx: ExecutionContext) -&gt; Tuple[outputs, traces]</code></strong><br><br>Small adapter that loads a rule module and applies it to a batch. Responsibilities: sandbox execution (timebox per-batch execution with <code>asyncio.wait_for</code> or OS-level worker where available), convert exceptions into domain <code>RuleExecutionError</code> with contextual metadata (row ids, rule name), and enforce pure-function contract (no network calls). Provide a test harness that runs each module with a telemetry-enabled context to assert determinism. </td></tr><tr><td data-label="Technical Breakdown — pipeline.py"> <strong><code>assemble_results_and_traces(calc_result: CalculationResult, job_meta: dict) -&gt; PersistableResult</code></strong><br><br>Normalize calculation outputs into the stable storage envelope. Responsibilities: create result documents with explicit schema (<code>result_version</code>, <code>applied_rule_version</code>, <code>service_version</code>, <code>computed_fields</code>, <code>aggregates</code>, <code>trace_ids</code>), split large traces into separate archive objects (store traces in compressed JSONL in object store and reference them from the main result), compute checksums for all artifacts, and prepare a storage plan (which files go to audit archive vs metadata DB). Always include <code>trace_id</code> and <code>job_id</code> for correlation. Tests: assert manifest structure and that heavy traces are chunked. </td></tr><tr><td data-label="Technical Breakdown — pipeline.py"> <strong><code>recorder_append_audit(record: PersistableResult) -&gt; AuditRef</code></strong><br><br>Append-only audit write. Responsibilities: write an append-only JSONL record to the audit service (recorder.append), optionally also writing an archival copy to object store for long-term retention, attach cryptographic signature metadata if requested by <code>settings.audit_signing</code>. Must ensure the operation is idempotent and that duplicate calls with same <code>job_id</code> don't produce duplicate logical entries (use dedupe key). Tests: simulate concurrent append attempts and ensure single logical audit entry. </td></tr><tr><td data-label="Technical Breakdown — pipeline.py"> <strong><code>export_orchestrator(job_result: PersistableResult, export_policy: ExportPolicy) -&gt; ExportOutcome</code></strong><br><br>Decide whether and how to export results (immediate synchronous export vs schedule a deferred export job). Responsibilities: validate export policy (e.g., generate bukti_potong CSV/XML, spt_masa_prefill), call exporter factories (bukti_potong_csv, bukti_potong_xml, spt_masa_prefill), create a manifest using <code>manifest.create</code>, compute checksums, sign the bundle (HMAC or asymmetric signing depending on settings), stage the bundle atomically to public object-store paths, and update metadata DB with export status. Must ensure bundle atomicity: publish via rename/atomic move after validation. Tests: integration test that produces a bundle and validates manifest checksums and signature. </td></tr><tr><td data-label="Technical Breakdown — pipeline.py"> <strong><code>generate_bundle(artifacts: List[ArtifactRef], manifest_meta: dict) -&gt; BundleRef</code></strong><br><br>Low-level bundle assembler. Responsibilities: collect artifacts, compute per-artifact checksums, build a manifest including <code>applied_rule_version</code>, <code>generator_version</code>, <code>create_time</code>, <code>content_hash</code>, generate a bundle signature, apply content-addressed layout (e.g., <code>public/{applied_rule_version}/{bundle_hash}/{artifact}</code>), write to staging then atomically promote, and return a stable public URL list. Enforce size limits and multipart strategies for very large bundles. Unit tests: verify manifest correctness and path determinism. </td></tr><tr><td data-label="Technical Breakdown — pipeline.py"> <strong><code>publish_bundle_atomic(staging_path: str, final_path: str) -&gt; bool</code></strong><br><br>Utility that performs atomic publish in object store. Responsibilities: validate staging integrity, perform rename/move if provider supports it, otherwise perform multipart copy with verification and then delete staging copies. Emit publication metrics and return <code>True</code> on success. Provide clear failure modes and best-effort cleanup. Tests: simulate rename-not-supported stores and validate correctness. </td></tr><tr><td data-label="Technical Breakdown — pipeline.py"> <strong><code>reconcile_postcheck(job_result: PersistableResult) -&gt; Optional[ReconcileTaskRef]</code></strong><br><br>Optional function called after calculation/export to enqueue a reconcile job if external systems need alignment (bank statements, DJP). Responsibilities: compute a reconciliation fingerprint (hash of key aggregates), query the recon_service for prior matching entries, and enqueue a reconcile job when discrepancies exceed configured thresholds. Must be opt-in by export policy and respect <code>settings.reconcile.auto_enqueue</code>. Tests: mock recon_service responses and assert correct enqueueing decisions. </td></tr><tr><td data-label="Technical Breakdown — pipeline.py"> <strong><code>handle_transient_and_permanent_failure(job_ref: JobRef, exc: Exception) -&gt; FailureAction</code></strong><br><br>Centralized error classification and backoff policy. Responsibilities: classify exceptions (transient vs permanent vs rule-bug), update idempotency record accordingly (<code>retry_after</code>, <code>dead_letter</code>), emit high-fidelity telemetry (error_kind, rule_name, stack_snippet size-limited), and emit an actionable <code>FailureAction</code> for the worker (retry, escalate, dead-letter). Permanent failures must write to <code>exception_queue</code> with contextual artifacts for human review. Tests: assert classification map and dead-letter write. </td></tr><tr><td data-label="Technical Breakdown — pipeline.py"> <strong><code>simulate_single_record(record: dict, applied_rule_version: str, debug_options: DebugOptions) -&gt; SimulationResult</code></strong><br><br>Developer-facing function used by <code>/simulate</code> and CLI. Responsibilities: run a single-record execution through the same <code>execute_calculation_pipeline</code> code path but in-process with dry-run toggles (no persistent writes), return deterministic trace and rule-level timing, and optionally produce a condensed execution plan. Must enforce strict resource limits to avoid DoS when exposed via HTTP. Include a <code>sensitive</code> flag to redact personally identifiable fields in traces returned to the caller. Tests: golden fixtures and API contract tests. </td></tr><tr><td data-label="Technical Breakdown — pipeline.py"> <strong><code>cli_run_job(snapshot_path: str, args: CLIArgs) -&gt; int</code></strong><br><br>Small CLI wrapper used by local dev and CI smoke tests to run a snapshot through the pipeline synchronously. Responsibilities: construct a minimal pipeline in <code>dry_run</code> or <code>test</code> mode, run validation and execute calculation, optionally write results to a local <code>--output</code> path, and return stable exit codes (0 success, 2 validation fail, 3 runtime error). Must avoid opening network listeners. Tests: CI will call the CLI with golden fixtures to assert invariant outputs. </td></tr><tr><td data-label="Technical Breakdown — pipeline.py"> <strong><code>shutdown_cleanup()</code></strong><br><br>Worker/global cleanup called during process shutdown. Responsibilities: cancel supervisor tasks, flush recorder buffers, close object-store connections, drain open telemetry exporters, and release idempotency leases if possible. Must swallow exceptions but emit final lifecycle logs and metrics (<code>shutdown_complete</code>). Tests: run as part of process lifecycle tests. </td></tr><tr><td data-label="Technical Breakdown — pipeline.py"> <strong><code>emit_metrics(metrics_payload: MetricPayload) -&gt; None</code></strong><br><br>Small helper the pipeline uses to push metrics (ingest_rate, calc_latency_ms, result_size_bytes, queue_depth, worker_uptime). Responsibilities: sample and batch to telemetry exporter; include <code>trace_id</code>, <code>job_id</code>, and <code>applied_rule_version</code> as tags. Support a <code>dry_run</code> stub for unit tests. Unit tests should assert expected metric keys are emitted. </td></tr><tr><td data-label="Technical Breakdown — pipeline.py"> <strong><code>validate_export_policy(policy: ExportPolicy) -&gt; None</code></strong><br><br>Validator for export configuration used before attempting export. Responsibilities: check required cryptographic keys, permitted export formats, destination path prefixes, and maximum bundle size. Throw an explicit <code>ExportPolicyError</code> for invalid policies. Tests: cover the matrix of allowed/disallowed formats and size limits. </td></tr><tr><td data-label="Technical Breakdown — pipeline.py"> <strong>Implementation patterns & guardrails</strong><br><br>— <strong>Small pure functions</strong>: heavy computation (rule evaluation) is implemented in pure, testable modules; pipeline functions orchestrate only. <br>— <strong>Idempotency-first</strong>: every external effect (snapshot write, queue enqueue, audit append, publish) uses an idempotency key and transactional metadata updates. <br>— <strong>Timeboxed operations</strong>: all external calls and rule executions are wrapped in configured timeouts; startup & shutdown are deterministic. <br>— <strong>Streaming & bounded memory</strong>: snapshots and validation must be streaming; traces are sampled and chunked to avoid OOM. <br>— <strong>Observability</strong>: every top-level function emits structured logs with <code>trace_id</code>, <code>job_id</code>, <code>service</code>, <code>applied_rule_version</code>, and metrics. Use consistent field names. <br>— <strong>Secrets</strong>: signing keys and HMAC secrets must be referenced via handles; <code>reveal()</code> only during startup handlers. <br>— <strong>Fail-fast vs degraded</strong>: <code>settings.strict</code> controls whether missing services abort or mark the pipeline degraded; document exactly which services are critical. <br>— <strong>Testing hygiene</strong>: provide <code>build_test_pipeline</code> that returns a pipeline with in-memory object store, queue, and recorder for unit/integration tests. <br>— <strong>Security</strong>: redact PII in logs by default; provide sampling toggles for traces; ensure exported bundles follow the signature and checksum contract. </td></tr><tr><td data-label="Technical Breakdown — pipeline.py"> <strong>Recommended tests & CI checks</strong><br><br>1. <strong>Unit</strong>: content-hash canonicalization, snapshot persistence plan, idempotency transitions, rule module adapter. <br>2. <strong>Integration (fast)</strong>: pipeline run with in-memory object-store & queue confirming end-to-end ingest → calculate → audit append → (no external publish) in a TestClient lifespan. <br>3. <strong>E2E smoke</strong>: produce a real export bundle against test object-store and validate manifest checksums/signature. <br>4. <strong>Concurrency</strong>: parallel ingest of identical payloads to assert only one job is processed. <br>5. <strong>Chaos</strong>: simulate transient object-store / queue failures; assert backoff & job resurrection. <br>6. <strong>Security</strong>: test log redaction and that secrets are not persisted in metadata. <br>7. <strong>Performance</strong>: benchmark <code>execute_calculation_pipeline</code> on representative sample sizes and record latency and memory. </td></tr><tr><td data-label="Technical Breakdown — pipeline.py"> <strong>Operational checklist before production</strong><br><br>— ensure <code>settings.strict=True</code> and <code>max_concurrency</code> tuned to CPU/IO budget; <br>— object-store sidecar/policy supports atomic publish or pipeline must fallback to multipart-verify; <br>— idempotency DB has appropriate indexes and TTLs for processing leases; <br>— signing keys provisioned in secret manager and audited; <br>— health endpoints expose <code>ingest_rate</code>, <code>queue_depth</code>, <code>last_successful_export</code>, and worker process counts; <br>— alerting for repeated <code>RuleExecutionError</code>, audit append failures, or publish failures; <br>— run CI golden fixtures for every applied_rule_version promotion. </td></tr><tr><td data-label="Technical Breakdown — pipeline.py"> <strong>Verification checklist (developer reviewer)</strong><br><br>— Imports are lazy for heavy deps (rule modules, exporters). <br>— No network I/O at module import. <br>— All register/startup hooks are idempotent. <br>— Timeouts exist for every external call and for rule execution. <br>— Idempotency keys are created and persisted before external side-effects. <br>— Traces are sampled and chunked; large artifacts are stored separately. <br>— Error classification maps to retry/dead-letter correctly. <br>— There is a <code>build_test_pipeline</code> helper with in-memory fakes. <br>— The file documents operational knobs: <code>settings.strict</code>, <code>settings.startup_mode</code>, <code>settings.idempotency_policy</code>, <code>settings.trace_sampling</code>. <br><br>Use this checklist in code review to confirm the pipeline module preserves invariants and is safe to deploy. </td></tr></tbody></table></div><div class="row-count">Rows: 29</div></div><div class="table-caption" id="Table12" data-table="Docu_0164_12" style="margin-top:2mm;margin-left:3mm;"><strong>Table 12</strong></div>
<div class="table-wrapper" data-table-id="table-12"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown — anomaly_model.py"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown — anomaly_model.py</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — anomaly_model.py"></td></tr></tbody></table></div><div class="row-count">Rows: 1</div></div><div class="table-caption" id="Table13" data-table="Docu_0164_13" style="margin-top:2mm;margin-left:3mm;"><strong>Table 13</strong></div>
<div class="table-wrapper" data-table-id="table-13"><div class="table-heading-bar" aria-label="Table heading"></div><div class="table-container"><div class="table-header-wrapper" style="display:flex; justify-content:space-between; align-items:center; margin-bottom:5px;"><div class="copy-buttons"><button type="button" class="copy-plain-btn" data-action="copy-plain" onclick="copyTablePlain(this)">Copy Plain Table</button><button type="button" class="copy-markdown-btn" data-action="copy-markdown" onclick="copyTableMarkdown(this)">Copy Markdown Table</button><button type="button" class="export-csv-btn" data-action="export-csv" onclick="exportTableCSV(this)">Export CSV</button><button type="button" class="export-json-btn" data-action="export-json" onclick="exportTableJSON(this)">Export JSON</button><button type="button" class="export-xlsx-btn" data-action="export-xlsx" onclick="exportTableXLSX(this)">Export XLSX</button><button type="button" class="export-pdf-btn" data-action="export-pdf" onclick="exportTablePDF(this)">Export PDF</button><button type="button" class="export-markdown-btn export-markdown-initial" data-action="export-markdown" onclick="exportTableMarkdown(this)" style="display:none" data-initial-hidden="1">Export Markdown</button></div><div style="display:flex; align-items:center;"><button type="button" class="toggle-table-btn" data-action="toggle-collapse" onclick="toggleTable(this)">Collapse Table</button></div></div><table class="tv-table" role="table"><thead><tr><th class="tv-col-left" role="button" aria-label="Sort by Technical Breakdown — explainability.py"><div class="th-with-sort"><div style="flex:1; white-space:nowrap; overflow:hidden; text-overflow:ellipsis;">Technical Breakdown — explainability.py</div><button class="sort-btn sort-state-0" title="Toggle sort" aria-label="Toggle sort"><span class="sort-icon" aria-hidden="true"></span></button></div></th></tr></thead><tbody><tr><td data-label="Technical Breakdown — explainability.py"> <strong>File-level responsibilities</strong><br><br>This module centralizes all logic required to generate, validate, format, persist, and serve model explanations for both local (per-record) and global (dataset-level) use cases. Keep the module focused: <em>no direct model training, no heavy persistence at import time, and no side effects at import</em>. Expose a small set of pure, easily-testable functions and one thin integration layer that wires into the application (routes / background workers). Make the implementation algorithm-agnostic: rely on pluggable explainers (SHAP, LIME, permutation importance, counterfactual engines). Document operational knobs (background sampling strategy, default explainer, max runtime, max memory per-explanation, explanation retention TTL) at the head of the file. All inputs/outputs must be JSON-serializable or provide explicit <code>to_safe_dict()</code> adapters. Prefer returning explanation <em>models</em> (dataclasses/POJOs) rather than raw dicts so serialization/validation are single-responsibility. </td></tr><tr><td data-label="Technical Breakdown — explainability.py"> <strong><code>create_explainability_service(settings, model_registry, storage, telemetry) -&gt; ExplainabilityService</code></strong><br><br>Factory that returns a configured service object used by the rest of the app. Must be synchronous and side-effect free at import time (only register closures, validators, and factories). Responsibilities: validate <code>settings</code> (timeouts, allowed explainers), wire model lookup from <code>model_registry</code>, attach <code>storage</code> adapters (read-only factories for <code>persist_explanation</code>), and expose a logging/telemetry closure that tags events with <code>trace_id</code> and <code>service=&quot;explainability&quot;</code>. Attach <code>service.runtime_fingerprint</code> (hash of allowed explainers + module version) for traceability. Unit tests should assert idempotent behavior and that missing optional components (e.g., <code>telemetry</code>) fall back to no-op adapters. </td></tr><tr><td data-label="Technical Breakdown — explainability.py"> <strong><code>compute_local_explanation(model, input_record, method=&#x27;shap&#x27;, background=None, timeout=None, max_features=50) -&gt; Explanation</code></strong><br><br>Primary synchronous wrapper for computing a single-record explanation. Must: (1) validate <code>input_record</code> schema, (2) coerce inputs to the model's expected numeric tensors, (3) enforce <code>timeout</code> via cooperative cancellation (call through to an async-safe executor if necessary), (4) select configured explainer implementation (SHAP sampling, LIME surrogate, counterfactual), (5) return an <code>Explanation</code> object containing <code>feature_attributions</code>, <code>baseline</code>, <code>method</code>, <code>meta</code> (model_id, model_version, timestamp, runtime_ms, sample_count). The function must be <em>pure</em> given the same inputs (deterministic randomness seeded from <code>trace_id</code> when present) and must never mutate <code>input_record</code>. Edge cases: structured / nested features must be flattened consistently; categorical encodings require mapping to human-readable names; handle missing features by using model defaults or failing fast if strict. Tests: golden fixture asserting attribution sums (where meaningful), JSON schema conformance, timeout behavior. </td></tr><tr><td data-label="Technical Breakdown — explainability.py"> <strong><code>compute_global_explanations(model, dataset_iterable, method=&#x27;permutation&#x27;, sample_fraction=0.1, top_k=50, concurrency=1) -&gt; GlobalExplanation</code></strong><br><br>Compute aggregate/model-level importance or cluster-level explanations. Responsibilities: sample dataset deterministically (respecting a <code>seed</code>), compute per-batch attributions, aggregate with robust statistics (mean, median, mean_abs, std, percentile bands), and return a <code>GlobalExplanation</code> that includes <code>feature_ranking</code>, stability metrics, and an optional per-cluster attribution matrix. Avoid streaming the entire dataset into memory — accept an iterable and implement an incremental aggregator. Instrument with telemetry (samples processed, time per batch). Provide graceful behavior when <code>sample_fraction</code> is low (increase variance warnings) and when the model is stateful (document limitations). Unit/integration tests must include small synthetic datasets with known global importances and assert ranking stability. </td></tr><tr><td data-label="Technical Breakdown — explainability.py"> <strong><code>compute_shap_values(model, inputs, background, nsamples=100, algorithm=&#x27;auto&#x27;) -&gt; ndarray</code></strong><br><br>Low-level bridge to SHAP-style computations. This function isolates the heavy numeric work and should be easy to stub in tests. Responsibilities: convert <code>inputs</code> and <code>background</code> to model-native arrays, batch predictions to avoid OOM, allow deterministic seed, and return a 2D array shaped <code>(n_samples, n_features)</code> containing attributions. Implementation notes: prefer model.predict(batch) vectorization; when calling black-box predict, parallelize carefully and respect <code>max_workers</code> configuration; guard <code>nsamples</code> to avoid explosion in time (document complexity). Provide fallback heuristics for tree models vs. kernel SHAP vs. sampling SHAP. Add tests that validate shapes, handle single-row inputs, and verify numeric stability (no NaNs/inf unless model produces them). </td></tr><tr><td data-label="Technical Breakdown — explainability.py"> <strong><code>compute_lime_surrogate(model, input_record, kernel_width=None, n_features=10, n_samples=500) -&gt; SurrogateResult</code></strong><br><br>Builds a local linear surrogate for explanation. Keep the function limited to purely local surrogate behavior: sample neighborhood, fit a sparse linear model, compute explained variance and coefficients, and return <code>SurrogateResult</code> including <code>coefficients</code>, <code>intercept</code>, <code>local_r2</code>, <code>sampled_points_count</code> and <code>neighborhood_description</code>. Document LIME's instability: always return <code>local_r2</code> and a reproducible seed. Tests: assert surrogate recovers known linear relationships on synthetic data and that coefficient signs align with SHAP when models are locally linear. </td></tr><tr><td data-label="Technical Breakdown — explainability.py"> <strong><code>compute_counterfactuals(model, input_record, target_pred=None, constraints=None, max_changes=3, timeout=None, search_strategy=&#x27;greedy&#x27;) -&gt; List[Counterfactual]</code></strong><br><br>Generate plausible counterfactual inputs that flip or shift model output. Responsibilities: validate <code>constraints</code> (immutable features, per-feature ranges), provide a configurable search strategy (greedy, genetic, gradient-based if differentiable), and emit <code>Counterfactual</code> objects containing <code>input</code>, <code>predicted_value</code>, <code>distance_metric</code>, and <code>explanation_meta</code>. Always respect data realism constraints (categorical domain values, maintain referential integrity). For privacy/security, redact or hash sensitive fields in the persisted counterfactuals. Tests: include small search benchmarks, verify constraint enforcement, and ensure no infinite loops on impossible targets. </td></tr><tr><td data-label="Technical Breakdown — explainability.py"> <strong><code>sample_background_distribution(dataset_iterable, strategy=&#x27;kmeans&#x27;, size=100, seed=None) -&gt; List[BackgroundSample]</code></strong><br><br>Utility to produce background/baseline samples used by sampling explainers. Implement strategies: <code>random</code>, <code>quantile</code>, <code>kmeans</code> (cluster-centroid sampling), and <code>stratified</code> (by a provided label). Must be stream-friendly and avoid loading all data in memory for large inputs — use reservoir sampling or streaming k-means. Expose clear stability guarantees: when <code>seed</code> is set, sampling is reproducible. Tests: validate size, distribution preservation, and edge cases where dataset smaller than requested size. </td></tr><tr><td data-label="Technical Breakdown — explainability.py"> <strong><code>aggregate_explanations(explanations, by=&#x27;feature&#x27;, metric=&#x27;mean_abs&#x27;, include_confidence=True) -&gt; AggregatedExplanation</code></strong><br><br>Combine many local explanations into a robust aggregation for reporting and dashboards. Responsibilities: support robust aggregators (mean_abs, median, trimmed mean), estimate confidence intervals (bootstrap or percentile), optionally compute feature importance per-segment (by provided keys), and supply diagnostics (sample_count, missing_values_count). Performance: implement vectorized aggregation and avoid Python-level loops over features for large batches. Tests: correctness against numpy/pandas baseline, and segmentation correctness. </td></tr><tr><td data-label="Technical Breakdown — explainability.py"> <strong><code>format_explanation_for_ui(explanation, redact_rules=None, max_fields=40, numeric_format=&quot;%.4g&quot;) -&gt; Dict[str,Any]</code></strong><br><br>Transforms internal <code>Explanation</code> objects into sanitized, compact, UI-friendly payloads. Tasks: apply <code>redact_rules</code> to PII (SSN, account numbers), convert raw arrays to concise lists of <code>(feature, value, attribution)</code> sorted by absolute attribution, truncate long strings and large lists with clear <code>truncated=True</code> flags, and include metadata (model_version, runtime_ms, method). This function must <em>not</em> perform heavy computation — it is presentation-only. Provide a <code>dry_run</code> mode for unit tests that verifies redaction and length limits. </td></tr><tr><td data-label="Technical Breakdown — explainability.py"> <strong><code>serialize_explanation(explanation, schema_version=&#x27;1&#x27;, compress=False) -&gt; bytes</code></strong><br><br>Canonical serializer for persistence and transport. Responsibilities: validate against stable JSON schema, produce compact JSON or optionally gzipped bytes, include checksum (sha256) and <code>schema_version</code> in metadata, and support backwards-compatible upgrades through a migration helper. Avoid embedding secrets; strip or mask sensitive fields before serialization if <code>explanation.meta.redact = True</code>. Tests: round-trip decode/encode tests and checksum integrity tests. </td></tr><tr><td data-label="Technical Breakdown — explainability.py"> <strong><code>persist_explanation(storage_client, explanation, key_prefix=None, ttl=None, overwrite=False) -&gt; PersistResult</code></strong><br><br>Store explanation artifacts in object storage or DB. Keep this function thin: perform serialization via <code>serialize_explanation</code>, compute idempotency key (content-hash), and call <code>storage_client.put(key, bytes, metadata)</code>. Enforce atomic writes (write to temp key then rename/publish when storage supports it). Record metadata into <code>storage_client</code>'s metadata store (or a provided metadata DB) including <code>explanation_id</code>, <code>model_version</code>, <code>applied_rule_version</code>, and <code>runtime_fingerprint</code>. Must respect <code>overwrite</code> flag and surface idempotent behavior: if the content-hash exists, return existing location rather than re-writing. Unit tests should stub <code>storage_client</code> to assert correct keys and metadata. </td></tr><tr><td data-label="Technical Breakdown — explainability.py"> <strong><code>validate_explanation_schema(explanation_dict, schema_registry) -&gt; ValidationResult</code></strong><br><br>Validate explanation payloads against a canonical schema (JSON Schema or pydantic). Responsibilities: enforce shape, field types, and value domains (no NaNs/inf), verify <code>method</code> is allowed, check <code>feature</code> names against a whitelist (configurable), and return structured <code>ValidationResult</code> with <code>ok</code>, <code>errors</code>, and <code>hints</code>. This function must be pure and deterministic. Tests: assert rich error messages, test for edge cases (missing fields, unexpected extra fields), and ensure validators are stable across schema versions. </td></tr><tr><td data-label="Technical Breakdown — explainability.py"> <strong><code>explain_request_handler(request, service: ExplainabilityService) -&gt; Response</code></strong><br><br>Thin HTTP handler intended to be wired to <code>/explain</code> or <code>/simulate</code>. Responsibilities: parse and validate incoming request JSON (using <code>validate_input_record</code> from <code>parsers.py</code>/<code>validators.py</code>), enforce request-level rate limits and auth scopes (do not implement auth here—call dependency-injected auth), extract <code>trace_id</code>, set a per-request timeout, call <code>compute_local_explanation</code> or <code>compute_counterfactuals</code> depending on payload, format result via <code>format_explanation_for_ui</code>, and return JSON with appropriate HTTP codes for validation/timeout/errors. Must not do heavy synchronous blocking; if the framework supports async, call compute functions via thread/executor safely. Tests: exercise route-level behavior using TestClient, including auth failure, malformed inputs, and timeout. Instrument with structured logs including <code>trace_id</code> and <code>request_id</code>. </td></tr><tr><td data-label="Technical Breakdown — explainability.py"> <strong><code>register_routes(app, service: ExplainabilityService, settings) -&gt; None</code></strong><br><br>Idempotently register API endpoints (e.g., <code>/explain</code>, <code>/explain/global</code>, <code>/explain/counterfactuals</code>, <code>/explain/status</code>) to the web framework. Middleware ordering must follow project conventions (trace injection -> auth -> request parsing -> rate limiting -> explain handler). Ensure repeated calls are no-ops (check <code>app.state._registered_explainability_routes</code>). Gate dev-only endpoints (like <code>/explain/debug</code>) behind <code>settings.env == &#x27;dev&#x27;</code>. Tests: assert route presence and that handlers call into <code>service</code> not module-level globals. </td></tr><tr><td data-label="Technical Breakdown — explainability.py"> <strong><code>start_background_workers(worker_pool, queue, service: ExplainabilityService, settings) -&gt; None</code></strong><br><br>Optional background loop to serve long-running explanation tasks (batch global explains, scheduled re-computation). Responsibilities: register gracefully-cancellable tasks, supervise retry/backoff for transient failures, record worker health in <code>app.state._workers</code>, and ensure resource limits (max concurrency, memory caps) are enforced. Workers must checkpoint progress and persist partial artifacts if interrupted. During shutdown, cancel tasks and await termination within <code>settings.shutdown_grace_period</code>. Tests: simulate queue messages, verify idempotency, and ensure clean shutdown behavior. </td></tr><tr><td data-label="Technical Breakdown — explainability.py"> <strong><code>explainability_auditor(audit_store, explanation, context) -&gt; None</code></strong><br><br>Append-only auditor that records what explanation was produced for which inputs and why. Responsibilities: produce an auditable record with <code>explanation_id</code>, <code>input_hash</code>, <code>model_id</code>, <code>model_version</code>, <code>method</code>, <code>parameters</code>, <code>user_id</code> (if present), <code>trace_id</code>, <code>timestamp</code>, and a compact <code>diff</code> summarizing the top-K features. Do <strong>not</strong> store raw PII in audit logs — store hashes or placeholders. Auditor should be tolerant to store failures (log & continue, unless configured as strict). Tests: ensure record shape and redaction. </td></tr><tr><td data-label="Technical Breakdown — explainability.py"> <strong><code>score_explainability_quality(explanations, metrics=[&#x27;stability&#x27;,&#x27;faithfulness&#x27;,&#x27;sparsity&#x27;]) -&gt; Dict[str,float]</code></strong><br><br>Compute diagnostic metrics used for monitoring explainability health. Define and implement standard metrics: <code>stability</code> (variance of attributions across small input perturbations), <code>faithfulness</code> (correlation between attribution magnitude and prediction change under perturbation / ablation), <code>sparsity</code> (fraction of features with near-zero attribution), and <code>consistency</code> with global importance. Return labeled floats and per-feature breakdown. This function should be vectorized and testable with synthetic fixtures that have known metric values. </td></tr><tr><td data-label="Technical Breakdown — explainability.py"> <strong><code>explainability_cli_main(argv: Optional[List[str]] = None) -&gt; int</code></strong><br><br>Small CLI to run local experiments and CI checks. Support subcommands: <code>compute-local</code>, <code>compute-global</code>, <code>validate-schema</code>, <code>sample-background</code>. The CLI should load <code>Settings</code> safely, run the requested operation in dry-run when <code>--dry-run</code> is set, and print safe, redacted outputs. Avoid starting network listeners. Tests: unit tests for parsing and sample integration tests for deterministic outputs. </td></tr><tr><td data-label="Technical Breakdown — explainability.py"> <strong><code>_ensure_input_array(input_record, feature_spec) -&gt; numpy.ndarray</code></strong><br><br>Private helper to coerce typed input records into numeric arrays the explainers expect. Responsibilities: apply consistent feature ordering, fill missing numeric values with configured imputation policy, one-hot encode categorical features consistently with <code>feature_spec</code>, and validate type ranges. Throw clear <code>ValidationError</code> when coercion is impossible. Keep this function small and fully unit-tested. </td></tr><tr><td data-label="Technical Breakdown — explainability.py"> <strong>Implementation patterns & guardrails</strong><br><br>— <strong>Purity & determinism</strong>: core compute functions should be deterministic except where randomness is explicit and seeded. <br>— <strong>Lazy imports</strong>: heavy third-party explainability libs (shap, lime, alibi) must be imported inside the functions that use them to keep import-time cheap. <br>— <strong>Idempotence</strong>: persistence operations should deduplicate by content-hash. <br>— <strong>Secrets & PII</strong>: never persist raw PII in explanations or audits. Use <code>hashlib.blake2b</code> or <code>sha256</code> with a project-wide salt stored in secret manager (reveal only during startup). <br>— <strong>Observability</strong>: record timings, sample counts, and metric snapshots for each explanation. Include <code>runtime_fingerprint</code> and <code>applied_rule_version</code> in metadata. <br>— <strong>Fail-fast vs degraded</strong>: expose <code>settings.strict</code>—in strict mode, explanation failures bubble up (HTTP 500) so callers retry; in lenient mode, return partial explanations with <code>degraded: true</code>. <br>— <strong>Testing hygiene</strong>: provide small synthetic fixtures: (linear-model, tree-model) and consistent background datasets. Include unit, integration (TestClient), and performance tests (latency under realistic nsamples). </td></tr><tr><td data-label="Technical Breakdown — explainability.py"> <strong>Recommended tests & CI checks</strong><br><br>1. <strong>Unit</strong>: <code>compute_shap_values</code>, <code>serialize_explanation</code>, private coercion helpers. <br>2. <strong>Integration</strong>: <code>explain_request_handler</code> using a TestClient with fakes for <code>model_registry</code> and <code>storage_client</code>. <br>3. <strong>Stability</strong>: run repeated explanations with small input perturbations and assert <code>stability</code> > threshold. <br>4. <strong>Security</strong>: assert no PII in stored audit records; redaction tests. <br>5. <strong>Performance</strong>: benchmark end-to-end local explanation for 1k nsamples and assert time within configured timeout. <br>6. <strong>Backward compatibility</strong>: schema migration tests for serialized explanations across versions. </td></tr><tr><td data-label="Technical Breakdown — explainability.py"> <strong>Operational & security checklist</strong><br><br>Before enabling explainability in production: ensure <code>settings.strict=True</code> if you want deterministic failures; lock down who can call heavy explainers (rate limits & quotas); verify secrets used for hashing are loaded from secret manager; ensure audit logs are write-only and encrypted at rest; include retention policy for persisted explanations and audits (use <code>retention.py</code> patterns); confirm telemetry destinations and sampling rates; and document runbooks for expensive explain jobs (how to cancel, how to re-run with more resources). </td></tr><tr><td data-label="Technical Breakdown — explainability.py"> <strong>Maintenance & developer notes</strong><br><br>— When adding a new explainer backend, register it in a single mapping in the factory and add a capability entry to <code>service.capabilities</code>. <br>— Avoid changing explanation JSON shapes without a schema bump and migration code. <br>— If adding expensive global analyses, prefer scheduled background jobs and surface a small read-only API to query cached results. <br>— Keep this file focused: heavy numeric kernels belong in <code>explainability/_kernels.py</code> (or similar module) and storage logic in <code>file_ops.py</code> / <code>storage_adapters</code>. </td></tr></tbody></table></div><div class="row-count">Rows: 24</div></div><script src="assets/xlsx.full.min.js?v=1758605028" defer></script>
<script src="assets/script.js?v=1759748863" defer></script>
<script src="assets/worker.js?v=1758331710" defer></script>
<script>
(function(){
  const template = "{table}_{date}";
  const userVal = "";
  const hrefPrefix = "assets";
  function formatName(tableName) {
    const date = (new Date()).toISOString().slice(0,10);
    return template.replace('{table}', tableName).replace('{date}', date).replace('{user}', userVal);
  }
  const btn = document.getElementById('exportBtn');
  if(btn) {
    btn.addEventListener('click', async function() {
      try {
        const html = document.documentElement.outerHTML;
        if(html.length > 2000000) { alert('Export refused: html too large'); return; }
        if(window.Worker) {
          const workerUrl = (function(){ try{ return hrefPrefix + '/worker.js'; }catch(e){ return null; } })();
          if(workerUrl) {
            try {
              const worker = new Worker(workerUrl);
              worker.postMessage({html: html, format: 'pdf'});
              worker.onmessage = function(e) { console.log('worker:', e.data); alert('Worker replied: '+(e.data.msg||e.data.status)); };
            } catch(err) { console.warn('Worker create failed', err); alert('Worker not available'); }
          } else { alert('Worker not available'); }
        } else { alert('Export worker not supported in this environment.'); }
      } catch(err) { console.warn('Export failed', err); alert('Export worker not available. See console for details.'); }
    });
  }
})();
window.addEventListener('load', function(){ try{ document.querySelectorAll('.table-wrapper').forEach(function(e){ e.style.opacity='1'; }); }catch(e){} });
</script>
</div>
</body>
</html>